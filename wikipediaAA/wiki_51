<doc id="12157" url="http://en.wikipedia.org/wiki?curid=12157" title="History of Guatemala">
History of Guatemala

The history of Guatemala began with the arrival of human settlers c. 18,000-11,000 BC. The Mayan civilization (2,000 BC – 250 AD) was among those that flourished in the region, with little contact with cultures outside Mesoamerica.
Most of the great Classic-era (250–900 AD) Maya cities of the Petén Basin region, in the northern lowlands of Guatemala, had been abandoned by the year 1000 AD. The states in the Guatemalan central highlands flourished until the arrival in 1525 of Pedro de Alvarado, the Spanish Conquistador. Called "the invader" by the Mayan peoples, he began subjugating the Indian states with his forces.
Guatemala was part of the Captaincy General of Guatemala, for nearly 300 years; this Captaincy, or "Capitanía", included the territories of Chiapas, Campeche, Tabasco in modern Mexico, and the modern countries of Guatemala, El Salvador, Honduras, Nicaragua and Costa Rica. The "Capitania" became independent in 1821, and became a part of the First Mexican Empire until 1823. From 1824 it was a part of the Federal Republic of Central America, until the Republic dissolved in 1841, when Guatemala became fully independent. In the late 19th century, Guatemala experienced a series of authoritarian governments and significant political instability.
In the late 19th and early 20th century, Guatemala's potential for agricultural exploitation attracted several foreign companies to it, the most prominent being the United Fruit Company (UFC). These companies were supported by the countries authoritarian rulers and the United States government through their support for brutal labor regulations and massive concessions to wealthy landowners. In 1944, the policies of Jorge Ubico led to a popular uprising which began the ten-year Guatemalan Revolution. The presidencies of Juan José Arévalo and Jacobo Árbenz saw sweeping social and economic reform, including a significant increase in literacy and a successful agrarian reform program.
The progressive policies of Arévalo and Árbenz led to the United Fruit Company lobbying the United States government for their overthrow, and a US engineered coup in 1954 ended the revolution and installed a military regime in its place. This was soon followed by other military governments, and sparked of a civil war between the government and leftist guerrillas that lasted from 1960 to 1996. The war saw human rights violations, including a genocide of the indigenous Mayan population by the United States backed military. Following the end of the war in 1996, Guatemala re-established a representative democracy. It has since struggled to enforce the rule of law and suffers a high crime rate, as well as continued extrajudicial killings, often executed by security forces.
Pre-Columbian era.
The earliest human settlements in Guatemala date back to the Paleo-Indian period and were made up of hunters and gatherers. Sites dating back to 6500 BC have been found in Quiché in the Highlands and Sipacate, Escuintla on the central Pacific coast.
Though it is unclear when these groups of hunters and gatherers turned to agricultural cultivation, pollen samples from Petén and the Pacific coast indicate maize cultivation as early as 3500 BC. By 2500 BC, small settlements were developing in Guatemala’s Pacific lowlands in such places as Tilapa, La Blanca, Ocós, El Mesak, and Ujuxte, where the oldest pieces of ceramic pottery from Guatemala have been found. Excavations in the Antigua Guatemala Urías and Rucal, have yielded stratified materials from the Early and Middle Preclassic periods (2000 BC to 400 BC). Paste analyses of these early pieces of pottery in the Antigua Valley indicate they were made of clays from different environmental zones, suggesting people from the Pacific coast expanded into the Antigua Valley.
Guatemala's Pre-Columbian era can be divided into the Preclassic period (from 2000 BC to 250 AD), the Classic period (250 to 900 AD) and the Postclassic period (900 to 1500 AD). Until recently, the Preclassic was regarded as a formative period, consisting of small villages of farmers who lived in huts and few permanent buildings, but this notion has been challenged by recent discoveries of monumental architecture from that period, such as an altar in La Blanca, San Marcos, from 1000 BC; ceremonial sites at Miraflores and El Naranjo from 801 BC; the earliest monumental masks; and the Mirador Basin cities of Nakbé, Xulnal, El Tintal, Wakná and El Mirador.
In Monte Alto near La Democracia, Escuintla, giant stone heads and potbellies (or "barrigones") have been found, dating back to around 1800 BC. The stone heads have been ascribed to the Pre-Olmec Monte Alto Culture and some scholars suggest the Olmec Culture originated in the Monte Alto area. It has also been argued the only connection between the statues and the later Olmec heads is their size. The Monte Alto Culture may have been the first complex culture of Mesoamerica, and predecessor of all other cultures of the region. In Guatemala, some sites have unmistakable Olmec style, such as Chocolá in Suchitepéquez, La Corona in Peten, and Tak'alik A´baj, in Retalhuleu, the last of which is the only ancient city in the Americas with Olmec and Mayan features.
El Mirador was by far the most populated city in pre-Columbian America. Both the El Tigre and Monos pyramids encompass a volume greater than 250,000 cubic meters. Richard Hansen, the director of the archaeological project of the Mirador Basin, believes the Maya at Mirador Basin developed the first politically organized state in America around 1500 BC, named the Kan Kingdom in ancient texts. There were 26 cities, all connected by "sacbeob" (highways), which were several kilometers long, up to 40 meters wide, and two to four meters above the ground, paved with stucco. These are clearly distinguishable from the air in the most extensive virgin tropical rain forest in Mesoamerica.
Hansen believes the Olmec were not the mother culture in Mesoamerica. Due to findings at Mirador Basin in Northern Petén, Hansen suggests the Olmec and Maya cultures developed separately, and merged in some places, such as Tak'alik Abaj in the Pacific lowlands.
Northern Guatemala has particularly high densities of Late Pre-classic sites, including Naachtun, "Xulnal", El Mirador, Porvenir, Pacaya, La Muralla, Nakbé, El Tintal, "Wakná" (formerly "Güiro"), Uaxactún, and Tikal. Of these, El Mirador, Tikal, Nakbé, Tintal, Xulnal and Wakná are the largest in the Maya world, Such size was manifested not only in the extent of the site, but also in the volume or monumentality, especially in the construction of immense platforms to support large temples. Many sites of this era display monumental masks for the first time (Uaxactún, El Mirador, Cival, Tikal and Nakbé). Hansen's dating has been called into question by many other Maya archaeologists, and developments leading to probably extra-regional power by the Late Preclassic of Kaminaljuyu, in the Southern Maya Area, suggest that Maya civilization developed in different ways in the Lowlands and the SMA to produce what we know as the Classic Maya.
The Classic period of Mesoamerican civilization corresponds to the height of the Maya civilization, and is represented by countless sites throughout Guatemala. The largest concentration is found in Petén. This period is characterized by expanded city-building, the development of independent city-states, and contact with other Mesoamerican cultures.
This lasted until around 900 AD, when the Classic Maya civilization collapsed. The Maya abandoned many of the cities of the central lowlands or died in a drought-induced famine. Scientists debate the cause of the Classic Maya Collapse, but gaining currency is the Drought Theory discovered by physical scientists studying lakebeds, ancient pollen, and other tangible evidence.
Spanish conquest.
Second-in-command to Hernán Cortés, Pedro de Alvarado was sent to the Guatemala highlands with 300 Spanish foot soldiers, 120 Spanish horsemen and several hundred Cholula and Tlascala auxiliaries.
Alvarado entered Guatemala from Soconusco on the Pacific lowlands, headed for Xetulul Humbatz, Zapotitlan. He initially allied himself with the Cakchiquel nation to fight against their traditional rivals the K'iche'. The conquistador started his conquest in Xepau Olintepeque, defeating the K'iché's 72,000 men, led by Tecún Umán (now Guatemala's national hero). Alvarado went to Q'umarkaj, (Utatlan), the K'iche' capital, and burned it on March 7, 1524. He proceeded to Iximche, and made a base near there in Tecpan on July 25, 1524. From there he made several campaigns to other cities, including Chuitinamit, the capital of the Tzutuhils, (1524); Mixco Viejo, capital of the Poqomam; and Zaculeu, capital of the Mam (1525). He was named captain general in 1527.
Having secured his position, Alvarado turned against his allies the Cakchiquels, confronting them in several battles until they were subdued in 1530. Battles with other tribes continued up to 1548, when the Q'eqchi' in Nueva Sevilla, Izabal were defeated, leaving the Spanish in complete control of the region.
Not all native tribes were subdued by bloodshed. Bartolomé de las Casas pacified the Kekchí in Alta Verapaz without violence.
After more than a century of colonization, during which mutually independent Spanish authorities in Yucatán and Guatemala made various attempts to subjugate Petén and neighbouring parts of what is now Mexico. In 1697 the Spanish finally conquered Nojpetén, capital of the Itzá Maya, and Zacpetén, capital of the Ko'woj Maya.
19th century.
Independence and Central America civil war.
In 1821, Fernando VII power in Spain was weakened by French invasions and other conflicts, and México declared the Plan de Iguala; this led Mariano Aycinena y Piñol and other criollos to demand the weak Captain General Gabino Gaínza to declare Guatemala and the rest of Central America as an independent entity. Aycinena y Piñol was one of the signatories of the Declaration of Independence of Central America from the Spanish Empire, and then lobbied strongly for the Central America anexation to the Mexican Empire of Agustín de Iturbide, due to its conservative and ecclesiastical nature. Aycinena remained in the legislature and was advisor of the Governors of Guatemala in the next few years.
In October 1826, Central American Federation president Manuel José de Arce y Fagoaga dissolved the Legislature and tried to establish a Unitarian System for the region, switching from the Liberal to the Conservative party, that Aycinena led. The rest of Central America did not want this system; they wanted the Aycinena family out of power altogher, and therefore, the Central American Civil War (1826-1829) started. From this war emerged the dominant figure of the Honduran general Francisco Morazán. Mariano Aycinena y Piñol -leader of the Ayicena family and the conservative power- was appointed as Governor of Guatemala on 1 March 1827 by president Manuel José Arce; Aycinena regime was a dictatorship: he censored free press and any book with liberal ideology was forbidden. He also establisher Martial Law and the retroactive death penalty. He reinstated mandatory tithing for the secular clergy of the Catholic Church
Invasion of General Morazan in 1829.
Morazan and his liberal forces were fighting around San Miguel, in El Salvador beating any conservative federal forces sent by Guatemalan general Manuel Arzú from San Salvador. Then, Arzú decided to take matters in his own hands and left colonel Montúfar in charge of San Salvador and went after Morazan. After realizing that Arzu was after him, Morazan left for Honduras to look for more volunteers for his army. On September 20, Manuel Arzá was close to the Lempa River with 500 men, when he was notified that the rest of his army had capitulated in San Salvador. Morazan then went back to El Salvador with a considerable army and general Arzú, feigning a sickness, fled to Guatemala, leaving lieutenant colonel Antonio de Aycinena in command. Aycinena and his 500 troops were going to Honduras when they were intercepted by Morazan troops in San Antonio, forcing Aycinena to concede defeat on October 9. With Aycinena defeat, there were no more conservative federal troops in El Salvador. On October 23, general Morazán marched triumphally in San Salvador. A few days later, he went to Ahuachapán, to organize an army to take down the conservative aristocrats led by Mariano Aycinena y Piñol in Guatemala and establish a regime favorable to the central American Federation that was the dream of the liberal criollos.
Upon learning this, Aycinena y Piñol tried to negotiate with Morazán to no avail: Morazan was willing to take down the aristrocrats at all costs.
After his victory in San Miguelito, Morazán's army increased in size given that a lot of voluntaries from Guatemala joined him. On March 15, when Morazan and his army were on their way to occupy their previous positions, they were intercepted by federal troops in Las Charcas. However, Morazan had a better position and smashed the federal army. The battle field was left full of corpses, while the allies took a lot of prisoners and weaponry. the allies continued to recapture their old positions in San José Pinula and Aceituno, and place Guatemala City under siege once again. General Verveer, Ambassador from the King of Netherlands and Belgium before the Central American government and who was in Guatemala to negotiate the construction of a transoceanic Canal in Nicaragua, tried to mediate between the State of Guatemala and Morazán, but did not succeed. Military operations continued, with great success for the allies.
To prepare for the siege from Morazán troops, on 18 March 1829, Aycinena decreed Martial Law, but we has completely defeated. On 12 April 1829, Aycinena conceded defeat and he and Morazán signed an armistice pact; then, he was sent to prison, along with his Cabinet members and the Aycinena family was secluded in their Mansion. Morazán, however, annulled the pact on April 20, since his real objective was to take power away from the conservatives and the regular clergy of the Catholic Church in Guatemala, whom the Central American leaders despised since they had had the commerce and power monopoly during the Spanish Colony.
Liberal rule.
Member of the liberal party, Mariano Gálvez was appointed chief of state in 1831, during a period of turmoil that made governing difficult; after the expulsion of the conservative leader of the Aycinena family and the regular clergy in 1829, was appointed by Francisco Morazán as Governor of Guatemala in 1831. Liberal historians such as Ramón Rosa and Lorenzo Montúfar y Rivera, refer that he promoted major innovations in all aspects of the administration, to make if less dependent on the Catholic Church influence. It is also reported that he made public instruction independent of the Church, fostered science and the arts, eliminated religious festivals as holidays, founded the National Library and the National Museum, promoted respect for the laws and the rights of citizens, guaranteed freedom of the press and freedom of thought, established civil marriage and divorce, respected freedom of association and promulgating the Livingston Code (penal code of Louisiana), against much opposition from the population who was not used to the fast pace the changes were taking place; he also initiated judicial reform, reorganized municipal government and established a general head tax which severely impacted the native population. However, this were all changes that the liberals wanted to implement to completely eliminate the political and economic power of the aristrocrats and of the Catholic Church -whose regular orders were expelled in 1829 and the secular clergy was weakened by means of abolishing mandatory tithing.
Among his major errors was a contract made with Michael Bennett -commercial partner of Francisco Morazán in the fine wood business- on 6 August 1834; the contract provided that the territories of Izabal, las Verapaces, Petén and Belize would be colonized within twenty years, but this proved impossible, plus made people irritated by having to deal with "heretics". In February 1835 Galvez was reelected far a second term, during which the Asiatic cholera afflicted the country. The secular clergy that was still in the country, persuaded the uneducated people of the interior that the disease was caused by the poisoning of the springs by order of the government and turned the complaints against Galvez into a religious war. Peasant revolts began in 1837, and under chants of "Hurray for the true religion!" and "Down with the heretics!" started growing and spreading. Gálvez asked the National Assembly to transfer the capital of the Federation from Guatemala City to San Salvador.
His major opponents were Colonel and Juan de Dios Mayorga; also, José Francisco Barrundia and Pedro Molina, who had been his friends and party colleagues, came to oppose him in the later years of his government after he violently tried to repress the peasant revolt using a scorched earth approach against rural communities.
In 1838, Antigua Guatemala, Chiquimula and Salamá withdrew recognition of his government, and in February of that year Rafael Carrera's revolutionary forces entered Guatemala City asking for the Cathedral to be opened to restore order in the catholic communities, obliging Gálvez to relinquish power. Galvez remained in the city after he lost power.
Peasant revolt: rise of Rafael Carrera.
In 1838 the liberal forces of the Honduran leader Francisco Morazán and Guatemalan José Francisco Barrundia invaded Guatemala and reached San Sur, where they executed Pascual Alvarez, Carrera's father-in-law. They impaled his head on a pike as a warning to all followers of the Guatemalan caudillo. On learning this, Carrera and his wife Petrona – who had come to confront Morazán as soon as they learned of the invasion and were in Mataquescuintla – swore they would never forgive Morazán even in his grave; they felt it impossible to respect anyone who would not avenge family members. After sending several envoys, whom Carrera would not receive – especially Barrundia whom Carrera did not want to murder in cold blood – Morazán began a scorched earth offensive, destroying villages in his path and stripping them of their few assets. The Carrera forces had to hide in the mountains . Believing that Carrera was totally defeated, Morazán and Barrundia marched on to Guatemala City, where they were welcomed as saviors by the state governor Pedro Valenzuela and members of the conservative Aycinena Clan, who proposed to sponsor one of the liberal battalions, while Valenzuela and Barrundia gave Morazán all the Guatemalan resources needed to solve any financial problem he had. The criollos of both parties celebrated until dawn that they finally had a criollo caudillo like Morazán, who was able to crush the peasant rebellion. Morazán used the proceeds to support Los Altos and then replaced Valenzuela by Mariano Rivera Paz, member of the Aycinena clan, although he did not return to that clan any property confiscated in 1829; in revenge, Juan José de Aycinena y Piñol voted for the dissolution of the Central American Federation in San Salvador a little later, forcing Morazán to return to El Salvador to fight to save his federal mandate. Along the way, Morazán increased repression in eastern Guatemala, as punishment for helping Carrera. Knowing that Morazán had gone to El Salvador, Carrera tried to take Salamá with the small force that remained, but was defeated, losing his brother Laureano in the combat. With just a few men left, he managed to escape, badly wounded, to Sanarate. After recovering to some extent, he attacked a detachment in Jutiapa and managed to get a small amount of booty which he handed to the volunteers who accompanied him and prepared to attack Petapa – near Guatemala City – where he was victorious, though with heavy casualties. In September of that year, he attempted an assault on the capital of Guatemala, but the liberal general Carlos Salazar Castro defeated him in the fields of Villa Nueva and Carrera had to retreat. After an unsuccessful attempt to take the Quetzaltenango, Carrera was surrounded and wounded, and he had to capitulate to the Mexican General Agustin Guzman, who had been in Quetzaltenango since the time of Vicente Filísola's arrival in 1823. Morazán had the opportunity to shoot Carrera, but did not because he needed the support of the Guatemalan peasants to counter the attacks of Francisco Ferrera in El Salvador; instead, Morazán left Carrera in charge of a small fort in Mita, and without any weapons. Knowing that Morazán was going to attack El Salvador, Francisco Ferrera gave arms and ammunition to Carrera and convinced him to attack Guatemala City.
Meanwhile, despite insistent advice to definitely crush Carrera and his forces, Salazar tried to negotiate with him diplomatically; he even went as far as to show that he neither feared nor distrusted Carrera by removing the fortifications of the Guatemalan capital, in place in since the battle of Villa Nueva. Taking advantage of Salazar's good faith and Ferrera's weapons, Carrera took Guatemala City by surprise on April 13, 1839; Castro Salazar, Mariano Gálvez and Barrundia fled before the arrival of Carrera's militia men. Salazar, in his nightshirt, vaulted roofs of neighboring houses and sought refuge; reaching the border disguised as a peasant. With Salazar gone, Carrera reinstated Rivera Paz as Head of State of Guatemala.
Invasion and Absorption of Los Altos.
On April 2, 1838, in the city of Quetzaltenango, a secessionist group founded the independent State of Los Altos which sought independence from Guatemala. The most important members of the Liberal Party of Guatemala and liberal enemies of the conservative regime moved to Los Altos, leaving their exile in El Salvador. The liberals in Los Altos began severely criticizing the Conservative government of Rivera Paz; they had their own newspaper – "El Popular", which contributed to the harsh criticism. Moreover, Los Altos was the region with the main production and economic activity of the former state of Guatemala; without Los Altos, conservatives lost much of the resources that had given Guatemala hegemony in Central America. Then, the government of Guatemala tried to reach to a peaceful solution, but altenses, protected by the recognition of the Central American Federation Congress, did not accept; Guatemala's government then resorted to force, sending Carrera as commanding general of the Army to subdue Los Altos.
Carrera defeated General Agustin Guzman when the former Mexican officer tried to ambush him and then went on to Quetzaltenango, where he imposed a harsh and hostile conservative regime instead of the liberals. Calling all council members, he told them flatly that he was behaving leniently towards them as it was the first time they had challenged him, but sternly warned them that there would be no mercy if there was a second time. Finally, Guzmán, and the head of state of Los Altos, Marcelo Molina, were sent to the capital of Guatemala, where they were displayed as trophies of war during a triumphant parade on 17 February 1840; in the case of Guzman, shackled, still with bleeding wounds, and riding a mule.
On March 18, 1840, liberal caudillo Morazán invaded Guatemala with 1500 soldiers to avenge the insult done in Los Altos. Fearing that such action would end with liberal efforts to hold together the Central American Federation, Guatemala had a cordon of guards from the border with El Salvador; without a telegraph service, men ran carrying last-minute messages. With the information from these messengers, Carrera hatched a plan of defense leaving his brother Sotero in charge of troops who presented only slight resistance in the city. Carrera pretended to flee and led his ragtag army to the heights of Aceituno, with few men, few rifles and two old cannons. The city was at the mercy of the army of Morazán, with bells of the twenty churches ringing for divine assistance. Once Morazán reached the capital, he took it easily and freed Guzman, who immediately left for Quetzaltenango to give the news that Carrera was defeated; Carrera then, taking advantage of what his enemies believed, applied a strategy of concentrating fire on the Central Park of the city and also employed surprise attack tactics which caused heavy casualties to the army of Morazán, finally forcing the survivors to fight for their lives. Morazán's soldiers lost the initiative and their previous numerical superiority. Furthermore, in unfamiliar surroundings in the city, they had to fight, carry their dead and care for their wounded while resentful and tired from the long march from El Salvador to Guatemala. Carrera, by then an experienced military man, was able to defeat Morazán thoroughly. The disaster for the liberal general was complete: aided by Angel Molina who knew the streets of the city, had to flee with his favorite men, disguised, shouting "Long live Carrera!" through the ravine of "El Incienso" to El Salvador. In his absence, Morazán had been supplanted as Head of State of his country, and had to embark for exile in Perú. In Guatemala, survivors from his troops were shot without mercy, while Carrera was out in unsuccessful pursuit of Morazan. This engagement sealed the status of Carrera and marked the decline of Morazán, and forced the conservative Aycinena clan criollos to negotiate with Carrera and his peasant revolutionary supporters.
Guzmán, who was freed by Morazán when the latter had seemingly defeated Carrera in Guatemala City, had gone back to Quetzaltenango to bring the good news. The city liberal criollo leaders rapidly reinstated the Los Altos State and celebrated Morazán's victory. However, as soon as Carrera and the newly reinstated Mariano Rivera Paz heard the news, Carrera went back to Quetzaltenango with his volunteer army to regain control of the rebel liberal state once and for all. On April 2, 1840, after entering the city, Carrera told the citizens that he had already warned them after he defeated them earlier that year. Then, he ordered the majority of the liberal city hall officials from Los Altos to be shot. Carrera then forcibly annexed Quetzaltenango and much of Los Altos back into conservative Guatemala. After the violent and bloody reinstatement of the State of Los Altos by Carrera in April 1840, Luis Batres Juarros – conservative member of the Aycinena Clan, then secretary general of the Guatemalan government of recently reinstated Mariano Rivera Paz – obtained from the vicar Larrazabal authorization to dismantle the regionalist Church. Serving priests of Quetzaltenango – capital of the would-be-state of Los Altos, Urban Ugarte and his coadjutor, José Maria Aguilar, were removed from their parish and likewise the priests of the parishes of San Martin Jilotepeque and San Lucas Tolimán. Larrazabal ordered the priests Fernando Antonio Dávila, Mariano Navarrete and Jose Ignacio Iturrioz to cover the parishes of Quetzaltenango, San Martin Jilotepeque and San Lucas Toliman, respectively.
The liberal criollos' defeat and execution in Quetzaltenango enhanced Carrera's status with the native population of the area, whom he respected and protected.
In 1840, Belgium began to act as an external source of support for Carrera's independence movement, in an effort to exert influence in Central America. The "Compagnie belge de colonisation" (Belgian Colonization Company), commissioned by Belgian King Leopold I, became the administrator of Santo Tomas de Castilla replacing the failed British Eastern Coast of Central America Commercial and Agricultural Company. Even though the colony eventually crumbled, Belgium continued to support Carrera in the mid-19th century, although Britain continued to be the main business and political partner to Carrera's regime.
Rafael Carrera was elected Guatemalan Governor in 1844. On March 21, 1847, Guatemala declared itself an independent republic and Carrera became its first president.
During the first term as president, Carrera had brought the country back from extreme conservatism to a traditional moderation; in 1848, the liberals were able to drive him from office, after the country had been in turmoil for several months. Carrera resigned of his own free will and left for México. The new liberal regime allied itself with the Aycinena family and swiftly passed a law ordering Carrera's execution if he dared to return to Guatemalan soil. The liberal criollos from Quetzaltenango were led by general Agustín Guzmán who occupied the city after Corregidor general Mariano Paredes was called to Guatemala City to take over the Presidential office. They declared on August 26, 1848 that Los Altos was an independent state once again. The new state had the support of Vasconcelos' regime in El Salvador and the rebel guerrilla army of Vicente and Serapio Cruz who were sworn enemies of Carrera. The interim government was led by Guzmán himself and had Florencio Molina and the priest Fernando Davila as his Cabinet members. On 5 September 1848, the criollos altenses chose a formal government led by Fernando Antonio Martínez.
In the meantime, Carrera decided to return to Guatemala and did so entering by Huehuetenango, where he met with the native leaders and told them that they must remain united to prevail; the leaders agreed and slowly the segregated native communities started developing a new Indian identity under Carrera's leadership. In the meantime, in the eastern part of Guatemala, the Jalapa region became increasingly dangerous; former president Mariano Rivera Paz and rebel leader Vicente Cruz were both murdered there after trying to take over the Corregidor office in 1849.
When Carrera arrived to Chiantla in Huehuetenango, he received two altenses emissaries who told him that their soldiers were not going to fight his forces because that would lead to a native revolt, much like that of 1840; their only request from Carrera was to keep the natives under control. The altenses did not comply, and led by Guzmán and his forces, they started chasing Carrera; the caudillo hid helped by his native allies and remained under their protection when the forces of Miguel Garcia Granados – who arrived from Guatemala City were looking for him.
On learning that officer José Víctor Zavala had been appointed as Corregidor in Suchitepéquez, Carrera and his hundred jacalteco bodyguards crossed a dangerous jungle infested with jaguars to meet his former friend. When they met, Zavala not only did not capture him, but agreed to serve under his orders, thus sending a strong message to both liberal and conservatives in Guatemala City that they would have to negotiate with Carrera or battle on two fronts – Quetzaltenango and Jalapa. Carrera went back to the Quetzaltenango area, while Zavala remained in Suchitepéquez as a tactical maneuver. Carrera received a visit from a Cabinet member of Paredes and told him that the he had control of the native population and that he assured Paredes that he would keep them appeased. When the emissary returned to Guatemala City, he told the president everything Carrera said, and added that the native forces were formidable.
Guzmán went to Antigua Guatemala to meet with another group of Paredes emissaries; they agreed that Los Altos would rejoin Guatemala, and that the latter would help Guzmán defeat his hated enemy and also build a port on the Pacific Ocean. Guzmán was sure of victory this time, but his plan evaporated when, in his absence, Carrera and his native allies had occupied Quetzaltenango; Carrera appointed Ignacio Yrigoyen as Corregidor and convinced him that he should work with the k'iche', mam, q'anjobal and mam leaders to keep the region under control. On his way out, Yrigoyen murmured to a friend: Now he is the King of the Indians, indeed!
Guzmán then left for Jalapa, where he struck a deal with the rebels, while Luis Batres Juarros convinced president Paredes to deal with Carrera. Back in Guatemala City within a few months, Carrera was commander-in-chief, backed by military and political support of the Indian communities from the densely populated western highlands. During the first presidency from 1844 to 1848, he brought the country back from excessive conservatism to a moderate regime, and – with the advice of Juan José de Aycinena y Piñol and Pedro de Aycinena – restored relations with the Church in Rome with a Concordat ratified in 1854. He also kept peace between natives and criollos, with the latter fearing a rising like the one that was taking place in Yucatán at the time.
Caste War of Yucatán.
In Yucatán, then an independent republic north of Guatemala, a war started between the natives and criollo people; this war seemed rooted in the defense of communal lands against the expansion of private ownership, which was accentuated by the boom in the production of henequen, which was an important industrial fiber used to make rope. After discovering the value of the plant, the wealthier Yucateco criollos started plantations, beginning in 1833, to cultivate it on a large scale; not long after the henequen boom, a boom in sugar production led to more wealth. The sugar and henequen plantations encroached on native communal land, and native workers recruited to work on the plantations were mistreated and underpaid.
However, rebel leaders in their correspondence with British Honduras -Belize- were more often inclined to cite taxation as the immediate cause of the war; Jacinto Pat, for example, wrote in 1848 that "what we want is liberty and not oppression, because before we were subjugated with the many contributions and taxes that they imposed on us." Pac's companion, Cecilio Chi added in 1849, that promises made by the rebel Santiago Imán, that he was "liberating the Indians from the payment of contributions" as a reason for resisting the central government, but in fact he continued levying them.
In June 1847, Méndez learned that a large force of armed natives and supplies had gathered at the Culumpich, a property owned by Jacinto Pat, the Maya "batab" (leader), near Valladolid. Fearing revolt, Mendez arrested Manuel Antonio Ay, the principal Maya leader of Chichimilá, accused of planning a revolt, and executed him at the town square of Valladolid. Furthermore, Méndez searching for other insurgents burned the town of Tepich and repressed its residents. In the following months, several Maya towns were sacked and many people arbitrarily killed. In his letter of 1849, Cecilio Chi noted that Santiago Mendez had come to "put every Indian, big and little, to death" but that the Maya had responded to some degree, in kind, writing "it has pleased God and good fortune that a much greater portion of them [whites] than of the Indians [have died].
Cecilio Chi, the native leader of Tepich, along with Jacinto Pat attacked Tepich on 30 July 1847, in reaction to the indiscriminate massacre of Mayas, ordered that all the non-Maya population be killed. By spring of 1848, the Maya forces had taken over most of the Yucatán, with the exception of the walled cities of Campeche and Mérida and the south-west coast, with Yucatecan troops holding the road from Mérida to the port of Sisal. The Yucatecan governor Miguel Barbachano had prepared a decree for the evacuation of Mérida, but was apparently delayed in publishing it by the lack of suitable paper in the besieged capital. The decree became unnecessary when the republican troops suddenly broke the siege and took the offensive with major advances.
Governor Barbachano sought allies anywhere he could find them, in Cuba (for Spain), Jamaica (for the United Kingdom) and the United States, but none of these foreign powers would intervene, although the matter was taken seriously enough in the United States to be debated in Congress. Subsequently, therefore, he turned to Mexico, and accepted a return to Mexican authority. Yucatán was officially reunited with Mexico on 17 August 1848. Yucateco forces rallied, aided by fresh guns, money, and troops from Mexico, and pushed back the natives from more than half of the state.
By 1850 the nativesoccupied two distinct regions in the southeast and they were inspired to continue the struggle by the apparition of the "Talking Cross". This apparition, believed to be a way in which God communicated with the Maya, dictated that the War continue. Chan Santa Cruz, or Small Holy Cross became the religious and political center of the Maya resistance and the rebellion came to be infused with religious significance. Chan Santa Cruz also became the name of the largest of the independent Maya states, as well as the name of the capital city which is now the city of Felipe Carrillo Puerto, Quintana Roo. The followers of the Cross were known as the "Cruzob".
The government of Yucatán first declared the war over in 1855, but hopes for peace were premature. There were regular skirmishes, and occasional deadly major assaults into each other's territory, by both sides. The United Kingdom recognized the Chan Santa Cruz Maya as a "de facto" independent nation, in part because of the major trade between Chan Santa Cruz and British Honduras.
Battle of La Arada.
After Carrera returned from exile in 1849, Vasconcelos granted asylum to the Guatemalan liberals, who harassed the Guatemalan government in several different forms: José Francisco Barrundia did it through a liberal newspaper established with that specific goal; Vasconcelos gave support during a whole year to a rebel faction "La Montaña", in eastern Guatemala, providing and distributing money and weapons. By late 1850, Vasconcelos was getting impatient at the slow progress of the war with Guatemala and decided to plan an open attack. Under that circumstance, the Salvadorean head of state started a campaign against the conservative Guatemalan regime, inviting Honduras and Nicaragua to participate in the alliance; only the Honduran government led by Juan Lindo accepted.
Meanwhile in Guatemala, where the invasion plans were perfectly well known, President Mariano Paredes started taking precautions to face the situation, while the Guatemalan Archbishop, Francisco de Paula García Peláez, ordered peace prayers in the archdiocese.
On 4 January 1851, Doroteo Vasconcelos and Juan Lindo met in Ocotepeque, Honduras, where they signed an alliance against Guatemala. The Salvadorean army had 4,000 men, properly trained and armed and supported by artillery; the Honduran army numbered 2,000 men. The coalition army was stationed in Metapán, El Salvador, due to its proximity with both the Guatemalan and Honduran borders.
On 28 January 1851, Vasconcelos sent a letter to the Guatemalan Ministry of Foreign Relations, in which he demanded that the Guatemalan president relinquish power, so that the alliance could designate a new head of state loyal to the liberals and that Carrera be exiled, escorted to any of the Guatemalan southern ports by a Salvadorean regiment. The Guatemalan government did not accept the terms and the Allied army entered Guatemalan territory at three different places. On 29 January, a 500-man contingent entered through Piñuelas, Agua Blanca and Jutiapa, led by General Vicente Baquero, but the majority of the invading force marched from Metapán. The Allied army was composed of 4,500 men led by Vasconcelos, as Commander in Chief. Other commanders were the generals José Santos Guardiola, Ramón Belloso, José Trinidad Cabañas and Gerardo Barrios. Guatemala was able to recruit 2,000 men, led by Lieutenant General Carrera as Commander in Chief, with several colonels.
Carrera's strategy was to feign a retreat, forcing the enemy forces to follow the "retreating" troops to a place he had previously chosen; on February 1, 1851, both armies were facing each other with only the San José river between them. Carrera had fortified the foothills of La Arada, its summit about 50 m above the level of the river. A meadow 300 m deep lay between the hill and the river, and boarding the meadow was a sugar cane plantation. Carrera divided his army in three sections: the left wing was led by Cerna and Solares; the right wing led by Bolaños. He personally led the central battalion, where he placed his artillery. Five hundred men stayed in Chiquimula to defend the city and to aid in a possible retreat, leaving only 1,500 Guatemalans against an enemy of 4,500.
The battle began at 8:30 AM, when Allied troops initiated an attack at three different points, with an intense fire opened by both armies. The first Allied attack was repelled by the defenders of the foothill; during the second attack, the Allied troops were able to take the first line of trenches. They were subsequently expelled. During the third attack, the Allied force advanced to a point where it was impossible to distinguish between Guatemalan and Allied troops. Then, the fight became a melée, while the Guatemalan artillery severely punished the invaders. At the height of the battle when the Guatemalans faced an uncertain fate, Carrera ordered that sugar cane plantation around the meadow to be set on fire. The invading army was now surrounded: to the front, they faced the furious Guatemalan firepower, to the flanks, a huge blaze and to the rear, the river, all of which made retreat very difficult. The central division of the Allied force panicked and started a disorderly retreat. Soon, all of the Allied troops started retreating.
The 500 men of the rearguard pursued what was left of the Allied army, which desperately fled for the borders of their respective countries. The final count of the Allied losses were 528 dead, 200 prisoners, 1,000 rifles, 13,000 rounds of ammunition, many pack animals and baggage, 11 drums and seven artillery pieces. Vasconcelos sought refuge in El Salvador, while two Generals mounted on the same horse were seen crossing the Honduran border. Carrera regrouped his army and crossed the Salvadorean border, occupying Santa Ana, before he received orders from the Guatemalan President, Mariano Paredes, to return to Guatemala, since the Allies were requesting a cease-fire and a peace treaty.
Concordat of 1854.
The Concordat of 1854 was an international treaty between Carrera and the Holy See, signed in 1852 and ratified by both parties in 1854. Through this, Guatemala gave the education of Guatemalan people to regular orders of the Catholic Church, committed to respect ecclesiastical property and monasteries, imposed mandatory tithing and allowed the bishops to censor what was published in the country; in return, Guatemala received dispensations for the members of the army, allowed those who had acquired the properties that the liberals had expropriated from the Church in 1829 to keep those properties, received the taxes generated by the properties of the Church, and had the right to judge certain crimes committed by clergy under Guatemalan law. The concordat was designed by Juan José de Aycinena y Piñol and not only reestablished but reinforced the relationship between Church and State in Guatemala. It was in force until the fall of the conservative government of Field Marshal Vicente Cerna y Cerna.
In 1854, by initiative of Manuel Francisco Pavón Aycinena, Carrera was declared "supreme and perpetual leader of the nation" for life, with the power to choose his successor. He was in that position until he died on April 14, 1865. While he pursued some measures to set up a foundation for economic prosperity to please the conservative landowners, military challenges at home and in a three-year war with Honduras, El Salvador, and Nicaragua dominated his presidency. His rivalry with Gerardo Barrios, President of El Salvador, resulted in open war in 1863. At Coatepeque the Guatemalans suffered a severe defeat, which was followed by a truce. Honduras joined with El Salvador, and Nicaragua and Costa Rica with Guatemala. The contest was finally settled in favor of Carrera, who besieged and occupied San Salvador, and dominated Honduras and Nicaragua. He continued to act in concert with the Clerical Party, and tried to maintain friendly relations with the European governments. Before his death, Carrera nominated his friend and loyal soldier, Army Marshall Vicente Cerna y Cerna, as his successor.
Wyke-Aycinena treaty: Limits convention about Belize.
The Belize region in the Yucatan peninsula was never occupied by either Spain or Guatemala, even though Spain made some exploratory expeditions in the 16th century that serve as her basis to claim the area as hers; Guatemala simply inherited that argument to claim the territory, even they it never sent any expedition to the area after the Independence from Spain in 1821, due to the Central American civil war that ensued and lasted until 1860. On the other hand, the British had set a small settlement there since middle of the 17th century, mainly as buccaneers quarters y then for fine wood production; the settlements were never recognized as British colonies even though they were somewhat under the jurisdiction of the Jamaican British government. In the 18th century, Belize became the main smuggling center for Central America, even though the British accepter Spain sovereignty over the region by means of the 1783 and 1786 treaties, in exchange for a cease fire and the authorization for the Englishmen to work with the precious woods from Belize.
After the Central America independence from Spain in 1821, Belize became the leading edge of the commercial entrance of Britain in the isthmus; British commercial brokers established themselves there and began prosper commercial routes with the Caribbean harbors of Guatemala, Honduras and Nicaragua.
When Carrera came to power in 1840, stopped the complaints over Belize, and established a Guatemalan consulate in the region to oversee the Guatemalan interests in that important commercial location. Belize commerce was booming in the region until 1855, when the Colombians built a transoceanic railway, which allowed commerce to flow more efficiently to the port at the Pacific; from then on, Belize commercial importance began a steep decline. When the Caste War of Yucatán began in the Yucatan peninsula-native people raising that results in thousands of murdered European settlers- the Belize and Guatemala representatives were in high alert; Yucatan refugees fled into both Guatemala and Belize and even Belize superintendent came to fear that Carrera -given his strong alliance with Guatemalan natives- could be support the native risings in Central America. In the 1850s, the British showed their good will to settle the territorial differences with the Central American countries: they withdraw from the Mosquito Coast in Nicaragua and began talks that would end up in the restoration of the territory to Nicaragua in 1894: returned the Bay Islands to Honduras and even negotiated with the American filibuster William Walker in an effort to avoid the invasion of Honduras. They also signed a treaty about with Guatemala about Belize borders, which has been called by Guatemalans as the worst mistake made by the conservative regime of Rafael Carrera-.
Pedro de Aycinena y Piñol, as Foreign Secretary, had made an extra effort to keep good relations with the British crown. In 1859, William Walker's threat loomed again over Central America; in order to get the weapons needed to face the filibuster, Carrera's regime had to come to terms about Belize with the British Empire. On 30 April 1859, the Wyke-Aycinena treaty was signed, between the English and Guatemalan representatives. The controversial Wyke-Aycinena from 1859 had two parts:
Among those who signed the treaty was José Milla y Vidaurre, who worked with Aycinena in the Foreign Ministry at the time. Rafael Carrera ratified the treaty on 1 May 1859, while Charles Lennox Wyke, British consul in Guatemala, travelled to Great Britain and got the royal approval on 26 September 1859. there were some protests coming from the American consul, Beverly Clarke, and some liberal representatives, but the issue was settled. As of 1850, it was estimated that Guatemala had a population of 600,000.
Guatemala's "Liberal Revolution" came in 1871 under the leadership of Justo Rufino Barrios, who worked to modernize the country, improve trade, and introduce new crops and manufacturing. During this era coffee became an important crop for Guatemala. Barrios had ambitions of reuniting Central America and took the country to war in an unsuccessful attempt to attain it, losing his life on the battlefield in 1885 against forces in El Salvador.
Justo Rufino Barrios government.
The Conservative government in Honduras gave military backing to a group of Guatemalan Conservatives wishing to take back the government, so Barrios declared war on the Honduran government. At the same time, Barrios, together with President Luis Bogran of Honduras, declared an intention to reunify the old United Provinces of Central America.
During his time in office, Barrios continued with the liberal reforms initiated by García Granados, but he was more aggressive implementing them. A summary of his reforms is:
Barrios had a National Congress totally pledge to his will, and therefore he was able to create a new constitution in 1879, which allowed him to be reelected as president for another six-year term.
He also was intolerant with his political opponents, forcing a lot of them to flee the country and building the infamous Guatemalan Central penitentiary where he had numerous people incarcerated and tortured.
Appleton's guide for México and Guatemala from 1884, shows the twenty departments in which Guatemala was divided during Barrios time in office:
Decree #177
Day Laborers regulations(NOTE: Only main sections are presented)
From: Martínez Peláez, S. "La Patria del Criollo, intepretation essay of Gautemala Colonial reality " México. 1990 
During Barrios tenue, the "indian land" that the conservative regime of Rafael Carrera had so strongly defended was confiscated and distributed among those officers that helped him during the Liberal Revolution in 1871. Decree # 170 (a.k.a. Census redemption decree) made it easy to confiscate those lands in favor of the army officers and the German settlers in Verapaz as it allowed to publicly sell those common Indian lots. Therefore, the fundamental characteristic of the productive system during Barrios regime was the accumulation of large extension of land among few owners and a sort of «farmland servitude», based on the exploitation of the native day laborers.
In order to make sure that there was a steady supply of day laborers for the coffee plantations, which required a lot of them, Barrios government decreed the "Day Laborer regulations", labor legislation that placed the entire native population at the disposition of the new and traditional Guatemalan landlords, except the regular clergy, who were eventually expelled form the country and saw their properties confiscated. This decree set the following for the native Guatemalans:
In 1879, a constitution was ratified for Guatemala (the Republic's first as an independent nation, as the old Conservador regime had ruled by decree). In 1880, Barrios was reelected President for a six-year term. Barrios unsuccessfully attempted to get the United States of America to mediate the disputed boundary between Guatemala and Mexico.
Government of Manuel Lisandro Barillas.
General Manuel Lisandro Barillas Bercián was able to become interim president of Guatemala after the death of President Justo Rufino Barrios in the Batalla of Chalchuapa in El Salvador in April 1885 and after the resignation of first designate Alejandro Manuel Sinibaldi Castro, by means of a clever scam: he went to the General Cemetery when Barrios was being laid to rest and told the Congress president: "please prepare room and board for the 5,000 troops that I have waiting for my orders in Mixco". The congress president was scared of this, and declared Barillas interim president on the spot. By the time he realized that it was all a lie, it was too late to change anything.
Instead of calling for elections, as he should have, Barillas Bercián was able to be declared President on 16 March 1886 and remained in office until 1892.
During the government of general Barillas Bercián, the Carrera theater was remodeled to celebrate the Discovery of America fourth centennial; the Italian community in Guatemala donated a statue of Christopher Columbus -Cristóbal Colón, in Spanish- which was placed next to the theater. Since then, the place was called «Colón Theater».
In 1892, Barillas called for elections as he wanted to take care of his personal business; it was the first election in Guatemala that allowed the candidates to make propaganda in the local newspapers. The candidates who ran for office were:
Barillas Bercian was unique among all liberal presidents of Guatemala between 1871 and 1944: he handed over power to his successor peacefully. When election time approached, he sent for the three Liberal candidates to ask them what their government plan would be. Happy with what he heard from general Reyna Barrios, Barillas made sure that a huge column of Quetzaltenango and Totonicapán Indigenous people came down from the mountains to vote for general Reyna Barrios. The official agents did their job: Reyna was elected president and, not to offend the losing candidates, Barillas gave them checks to cover the costs of their presidential campaigns. Reyna Barrios, of course, received nothing, but he went on to become President on March 15, 1892.
20th century.
In the 1890s, the United States began to implement the Monroe Doctrine, pushing out European colonial powers and establishing U.S. hegemony over resources and labor in Latin American nations. The dictators that ruled Guatemala during the late 19th and early 20th century were generally very accommodating to U.S. business and political interests; thus, unlike other Latin American nations such as Haiti, Nicaragua and Cuba the U.S. did not have to use overt military force to maintain dominance in Guatemala. The Guatemalan military/police worked closely with the U.S. military and State Department to secure U.S. interests. The Guatemalan government exempted several U.S. corporations from paying taxes, especially the United Fruit Company, privatized and sold off publicly owned utilities, and gave away huge swaths of public land.
Early 20th century: Manuel Estrada Cabrera regime.
After the assassination of general José María Reina Barrios on 8 February 1898, the Guatemalan cabinet called an emergency meeting to appoint a new successor, but declined to invite Estrada Cabrera to the meeting, even though he was the First Designated to the Presidency. There are two versions on how he was able to get the Presidency: (a) Estrada Cabrera entered "with pistol drawn" to assert his entitlement to the presidency and (b) Estrada Cabrera showed up unarmed to the meeting and demanded to be given the presidency as he was the First Designated".
The first Guatemalan head of state taken from civilian life in over 50 years, Estrada Cabrera overcame resistance to his regime by August 1898 and called for September elections, which he won handily. At that time, Estrada Cabrera was 44 years old; he was stocky, of medium height, dark, and broad-shouldered. The mustache gave him plebeian appearance. Black and dark eyes, metallic sounding voice and was rather sullen and brooding. At the same time, he already showed his courage and character. This was demonstrated on the night of the death of Reina Barrios when he stood in front of the ministers, meeting in the Government Palace to choose a successor, "Gentlemen, let me please sign this decree. As First Designated, you must hand me the Presidency". "His first decree was a general amnesty and the second was to reopen all the elementary schools closed by Reyna Barrios, both administrative and political measures aimed to gain the public opinion. Estrada Cabrera was almost unknown in the political circles of the capital and one could not foresee the features of his government or his intentions.
In 1898 the Legislature convened for the election of President Estrada Cabrera, who triumphed thanks to the large number of soldiers and policemen who went to vote in civilian clothes and to the large number of illiterate family that they brought with them to the polls. Also, the effective propaganda that was written in the official newspaper "the Liberal Idea '. The latter was run by the poet Joaquin Mendez, and among the drafters were Enrique Gómez Carrillo, Rafael Spinola, Máximo Soto Hall and Juan Manuel Mendoza, and others. Gómez Carrillo received as a reward for his work as political propagandist the appointment as General Consul in Paris, with 250 gold pesos monthly salary and immediately went back to Europe 
One of Estrada Cabrera's most famous and most bitter legacies was allowing the entry of the United Fruit Company into the Guatemalan economical and political arena. As a member of the Liberal Party, he sought to encourage development of the nation's infrastructure of highways, railroads, and sea ports for the sake of expanding the export economy. By the time Estrada Cabrera assumed the presidency, there had been repeated efforts to construct a railroad from the major port of Puerto Barrios to the capital, Guatemala City. Yet due to lack of funding exacerbated by the collapse of the internal coffee trade, the railway fell sixty miles short of its goal. Estrada Cabrera decided, without consulting the legislature or judiciary, that striking a deal with the United Fruit Company was the only way to get finish the railway. Cabrera signed a contract with UFCO's Minor Cooper Keith in 1904 that gave the company tax-exemptions, land grants, and control of all railroads on the Atlantic side.
Estrada Cabrera often employed brutal methods to assert his authority, as that was the school of government in Guatemala at the time. Like him, presidents Rafael Carrera y Turcios and Justo Rufino Barrios had led tyrannical governments in the country. Right at the beginning of his first presidential period, he started prosecuting his political rivals and soon established a well-organized web of spies. One American Ambassador returned to the United States after he learned the dictator had given orders to poison him. Former President Manuel Barillas was stabbed to death in Mexico City, on a street outside of the Mexican Presidential Residence on Cabrera's orders; the street now bears the name of Calle Guatemala. Also, Estrada Cabrera responded violently to workers' strikes against UFCO. In one incident, when UFCO went directly to Estrada Cabrera to resolve a strike (after the armed forces refused to respond), the president ordered an armed unit to enter the workers' compound. The forces "arrived in the night, firing indiscriminately into the workers' sleeping quarters, wounding and killing an unspecified number."
In 1906 Estrada faced serious revolts against his rule; the rebels were supported by the governments of some of the other Central American nations, but Estrada succeeded in putting them down. Elections were held by the people against the will of Estrada Cabrera and thus he had the president-elect murdered in retaliation. In 1907 the brothers Avila Echeverría and group of friends decided to kill the president using a bomb along his way. They came from prominent families in Guatemala and studied in foreign universities, but when they returned to their homeland, they found a situation where everybody live in constant fear and the president ruled without any opposition. Everything was carefully planned. When Estrada Cabrera went for a ride in his carriage, the bomb exploded, killing the horse and the driver, but only slightly injuring the President. Since their attack failed and they were forced to take their own lives; their families also suffered, as they were jailed in the infamous "Penitenciaría Central". Conditions in the Penitentiary were cruel and foul. Political offenses were tortured daily and their screams could be heard all over the Penitentiary. Prisoners regularly died under these conditions since political crimes had no pardon. It has been suggested that the extreme despotic characteristics of the man did not emerge until after an attempt on his life in 1907.
Estrada Cabrera continued in power until forced to resign by new revolts in 1920. By that time, his power had declined drastically and he was reliant on the loyalty of a few generals. While the United States threatened intervention if he was removed through revolution, a bipartisan coalition came together to remove him from the presidency. He was removed from office after the national assembly charged that he was mentally incompetent, and appointed Carlos Herrera in his place on April 8, 1920.
In 1920, prince Wilhelm of Sweden visited Guatemala and made a very objective description of both Guatemalan society and Estrada Cabrera government in his book "Between two continents, notes from a journey in Central America, 1920". The prince explained the dynamics of the Guatemalan society at the time pointing out that even though it called itself a "Republic", Guatemala had three sharply defined classes:
Jorge Ubico regime (1931-1944).
In 1931, the dictator general Jorge Ubico came to power, backed by the United States, and initiated one of the most brutally repressive governments in Central American history. Just as Estrada Cabrera had done during his government, Ubico created a widespread network of spies and informants and had large numbers of political opponents tortured and put to death. A wealthy aristocrat (with an estimated income of $215,000 per year in 1930s dollars) and a staunch anti-communist, he consistently sided with the United Fruit Company, Guatemalan landowners and urban elites in disputes with peasants. After the crash of the New York Stock Exchange in 1929, the peasant system established by Barrios in 1875 to jumpstart coffee production in the country was not good enough anymore, and Ubico was forced to implement a system of debt slavery and forced labor to make sure that there was enough labor available for the coffee plantations and that the UFCO workers were readily available. Allegedly, he passed laws allowing landowners to execute workers as a "disciplinary" measure. He also openly identified as a fascist; he admired Mussolini, Franco, and Hitler, saying at one point: "I am like Hitler. I execute first and ask questions later." Ubico was disdainful of the indigenous population, calling them "animal-like", and stated that to become "civilized" they needed mandatory military training, comparing it to "domesticating donkeys." He gave away hundreds of thousands of hectares to the United Fruit Company (UFCO), exempted them from taxes in Tiquisate, and allowed the U.S. military to establish bases in Guatemala. Ubico considered himself to be "another Napoleon". He dressed ostentatiously and surrounded himself with statues and paintings of the emperor, regularly commenting on the similarities between their appearances. He militarized numerous political and social institutions—including the post office, schools, and symphony orchestras—and placed military officers in charge of many government posts. He frequently travelled around the country performing "inspections" in dress uniform, followed by a military escort, a mobile radio station, an official biographer, and cabinet members.
On the other hand, Ubico was an efficient administrator:
October Revolution (1944).
After 14 years, Ubico's repressive policies and arrogant demeanor finally led to pacific disobedience by urban middle-class intellectuals, professionals, and junior army officers in 1944. On 1 July 1944 Ubico resigned from office amidst a general strike and nationwide protests. Initially, he had planned to hand over power to the former director of police, General Roderico Anzueto, whom he felt he could control. But his advisors noted that Anzueto's pro-Nazi sympathies had made him very unpopular, and that he would not be able to control the military. So Ubico instead chose to select a triumvirate of Major General Bueneventura Piñeda, Major General Eduardo Villagrán Ariza, and General Federico Ponce Vaides. The three generals promised to convene the national assembly to hold an election for a provisional president, but when the congress met on 3 July, soldiers held everyone at gunpoint and forced them to vote for General Ponce rather than the popular civilian candidate, Dr. Ramón Calderón. Ponce, who had previously retired from military service due to alcoholism, took orders from Ubico and kept many of the officials who had worked in the Ubico administration. The repressive policies of the Ubico administration were continued.
Opposition groups began organizing again, this time joined by many prominent political and military leaders, who deemed the Ponce regime unconstitutional. Among the military officers in the opposition were Jacobo Árbenz and Major Francisco Javier Arana. Ubico had fired Árbenz from his teaching post at the "Escuela Politécnica", and since then Árbenz had been living in El Salvador, organizing a band of revolutionary exiles. On 19 October 1944 a small group of soldiers and students led by Árbenz and Arana attacked the National Palace in what later became known as the "October Revolution". Ponce was defeated and driven into exile; and Árbenz, Arana, and a lawyer name Jorge Toriello established a junta. They declared that democratic elections would be held before the end of the year.
The winner of the 1944 elections was a teaching major named Juan José Arévalo, PhD, who had earned a scholarship in Argentina during the government of general Lázaro Chacón due to his superb professor skills. Arévalo remained in South America during a few years, working as a University professor in several countries. Back in Guatemala during the early years of the Jorge Ubico regime, his colleagues asked him to present a project to the president to create the Faculty of Humanism at the National University, to which Ubico was strongly opposed. Realizing the dictatorial nature of Ubico, Arévalo left Guatemala and went back to Argentina. He went back to Guatemala after the 1944 Revolution and ran under a coalition of leftist parties known as the "Partido Acción Revolucionaria" ("Revolutionary Action Party", PAR), and won 85% of the vote in elections that are widely considered to have been fair and open. Arévalo implemented social reforms, including minimum wage laws, increased educational funding, near-universal suffrage (excluding illiterate women), and labor reforms. But many of these changes only benefited the upper-middle classes and did little for the peasant agricultural laborers who made up the majority of the population. Although his reforms were relatively moderate, he was widely disliked by the United States government, the Catholic Church, large landowners, employers such as the United Fruit Company, and Guatemalan military officers, who viewed his government as inefficient, corrupt, and heavily influenced by Communists. At least 25 coup attempts took place during his presidency, mostly led by wealthy liberal military officers.
Presidency of Juan José Arévalo.
Árbenz served as defense minister under President Arévalo. He was the first minister of this portfolio, since it was previously called the "Ministry of War". In 1947 Dr. Arévalo, in company with a friend and two Russian dancers who were visiting Guatemala, had a terrible car accident on the road to Panajachel: fell into a ravine and was seriously injured, while all his companions were killed. The official party leaders signed a pact with Lieutenant Colonel Arana, in which he pledged not to attempt any coup against the ailing president, in exchange for the revolutionary parties as the official candidate in the next election. However, the recovery of the sturdy president was almost miraculous and soon he was able to take over the government. Lieutenant Colonel Francisco Javier Arana had accepted this pact because he wanted to be known as a Democratic hero of the uprising against Ponce and believed that the "Barranco Pact" ensured his position when the time of the presidential elections came.
Arana was a very influential person in Arévalo government, and had managed to be nominated as the next presidential candidate, ahead of Captain Arbenz, who was told that because of his young age he would have no problem in waiting turn to the next election. Arana died in a gun battle against military civilian who wanted to capture him on July 18, 1949, at the Bridge of Glory, in Amatitlán, where he and his assistant commander had gone to check on weapons and that had been seized at the Aurora Air Base a few days before There are different versions about who ambushed him, and those who ordered the attack; Arbenz and Arévalo have been accused of instigating an attempt to get Arana out of the presidential picture.
The death of Lieutenant Colonel Arana is of critical importance in the history of Guatemala, because it was a pivotal event in the history of the Guatemalan revolution: his death not only paved the way for the election of Colonel Arbenz as president of the republic in 1950 but also caused an acute crisis in the government of Dr. Arévalo Bermejo, who all of a sudden had against him an army that was more faithful to Arana than to him, and elite civilian groups that used the occasion to protest strongly against his government.
Before his death, Arana had planned to run in the upcoming 1950 presidential elections. His death left Árbenz without any serious contenders in the elections (leading some, including the CIA and U.S. military intelligence, to speculate that Árbenz personally had him eliminated for this reason). Árbenz got more than 3 times as many votes as the runner-up, Miguel Ydígoras Fuentes. Fuentes claimed that electoral fraud benefited Árbenz; however scholars have pointed out that while fraud may possibly have given Árbenz some of his votes, it was not the reason that he won the election. In 1950s Guatemala, only literate men were able to vote by secret ballot; illiterate men and literate women voted by open ballot. Illiterate women were not enfranchised at all.
For the campaign of 1950, Arbenz asked José Manuel Fortuny - a high-ranking member of the Guatemalan Communist party - to write some speeches. The central theme of these was the land reform, the "pet project" of Arbenz. They shared a comfortable victory in elections in late 1950 and, thereafter, the tasks of government. While many of the leaders of the ruling coalition fought hard closeness to the president seeking personal benefits, the leaders of the Guatemalan Labor Party, and especially Fortuny, were the closest advisors and Arbenz were his private practice.
The election of Árbenz alarmed U.S. State Department officials, who stated that Arana "has always represented [the] only positive conservative element in [the] Arévalo administration", that his death would "strengthen Leftist[sic] materially", and that "developments forecast sharp leftist trend within [the] government."
Presidency of Jacobo Árbenz Guzman.
In his inaugural address, Árbenz promised to convert Guatemala from "a backward country with a predominantly feudal economy into a modern capitalist state." He declared that he intended to reduce dependency on foreign markets and dampen the influence of foreign corporations over Guatemalan politics. He also stated that he would modernize Guatemala's infrastructure and do so without the aid of foreign capital.
Based on his plan of government, he did the following: 
Árbenz was a Christian socialist and governed as a European-style democratic socialist, and took great inspiration from Franklin Delano Roosevelt's New Deal. According to historian Stephen Schlesinger, while Árbenz did have a few communists in lower-level positions in his administration, he “was not a dictator, he was not a crypto-communist.” Nevertheless, some of his policies, particularly those involving agrarian reform, would be branded as "communist" by the upper classes of Guatemala and the United Fruit Company.
Land Reform.
Prior to Árbenz's election in 1950, a handful of U.S. corporations controlled Guatemala's primary electrical utilities, the nation's only railroad, and the banana industry, which was Guatemala's chief agricultural export industry. By the mid-1940s, Guatemalan banana plantations accounted for more than one quarter of all of United Fruit Company's production in Latin America. Land reform was the centerpiece of Árbenz's election campaign. The revolutionary organizations that had helped put Árbenz in power put constant pressure on him to live up to his campaign promises regarding land reform. Árbenz continued Arévalo's reform agenda and in June 1952, his government enacted an agrarian reform program. Árbenz set land reform as his central goal, as only 2% of the population owned 70% of the land.
On 17 June 1952 Árbenz's administration enacted an agrarian reform law known as Decree 900. The law empowered the government to create a network of agrarian councils which would be in charge of expropriating uncultivated land on estates that were larger than 672 acre. The land was then allocated to individual families. Owners of expropriated land were compensated according to the worth of the land claimed in May 1952 tax assessments (which they had often dramatically understated to avoid paying taxes). Land was paid for in twenty-five year bonds with a 3 percent interest rate. The program was in effect for 18 months, during which it distributed 1500000 acre to about 100,000 families. Árbenz himself, a landowner through his wife, gave up 1700 acre of his own land in the land reform program.
In 1953, the reform was ruled unconstitutional by the Supreme Court, however the democratically elected Congress later impeached four judges associated with the ruling.
Decree 900, for the Agrarian Reform in Guatemala created the possibility of gaining crops for those field workers who had no land of their own. The effect of this law was similar to what occurred in Europe after the bubonic plague in the Middle Ages: after the plague, which killed one third of Europe's population at the time, the number of landowners decreased, which released many of the terrestrial land, increased supply and lowered land price. At the same time, many farmers also died from the plague, so that the labor force declined; this shift in supply of workers increased wages. The economic effects of the plague are very similar to those caused by the land reform in Guatemala: During the first harvest after the implementation of the law, the average income of farmers increased from Q225.00/year TO Q700.00/year. Some analysts say that conditions in Guatemala improved after the reform and that there was a "fundamental transformation of agricultural technology as a result of the decrease labor supply." Rising living standards also happened in Europe in the fifteenth century, while large-scale technological advances occurred. Missing workforce after the plague was "the mother of invention." The benefits from the reform were not limited solely to the working class of fields: There were increases in consumption, production and domestic private investment.
Construction of transport infrastructure.
In order to establish the necessary physical infrastructure to make possible the "independent" and national capitalist development that could get rid of extreme dependence on the United States and break the American monopolies operating in the country, basically the economy of the banana enclave, Arbenz and his government began the planning and construction of the Atlantic Highway, which was intended to compete in the market with the monopoly on land transport exerted by the United Fruit Company, through one of its subsidiaries: the International Railways of Central America (IRCA), which had the concesion since 1904, when is was granted by then president Manuel Estrada Cabrera. Construction of the highway began by the Roads Department of the Ministry of Communications, with the help of the military engineering battalion. It was planned to be built parallel along the railway line, as much as possible. The construction of the new port was also aimed to break another UFCO monopoly: Puerto Barrios was owned and operated solely by The Great White Fleet, another UFCO's subsidiary.
National power plant Jurun Marinalá.
The Jurun Marinalá electric power generation plant was planned as the first national hydroelectric power plant in Guatemala. The goal was to disrupt the monopoly of the Electric Company, a subsidiary of American Electric Bond and Share (Ebasco), which did not make use of indigenous water resources, but ran fossil fuel-powered plants, thus creating a drain on foreign currency reserves. Owing to its massive economic importance, construction continued beyond the Árbenz presidency. The plant was finally completed under President Julio Cesar Mendez Montenegro in 1968. It is located in the village of Agua Blanca, inside El Salto, Escuintla.
Catholic Campaign national pilgrimage against communism.
The Catholic Church, who possessed a large share of power in Central America during the Colonial Era, was gradually losing it after the emancipation from Spain. First, it was the struggle of the liberals who overtook power from Guatemalan conservatives (among whom was included the Major Clergy of the Church); conservatives and the Church lost all of their power quota in the provinces of Central America, Guatemala remaining as their last bastion. In 1838, with the fall of the liberal president Mariano Galvez, the figure of Lieutenant General Rafael Carrera arose and became the country's conservative leader. He rallied his party and the Church back to power, at least in the province of Guatemala. With this state of affairs, the Central American Federation could not be carried out because it was liberal in nature and Guatemala's military power and that of its leader Carrera were invincible in his time; so much so, that Carrera eventually founded the Republic of Guatemala on March 21, 1847. After Carrera's death in 1865, Guatemalan Liberals saw their chance to seize power again, and conducted the Liberal Revolution in 1871. Since that time, the attacks on the senior clergy of the Catholic Church raged in Guatemala and secular education, freedom of religion, the expulsion of several religious orders and the expropriation of many church property were decreed. This situation continued throughout all the liberal governments that followed, until October Revolution in 1944, in which the religious situation worsened: now the attacks towards the Church were not only economic, but also religious, as many revolutionaries began to declare themselves opposed to any kind of religion.
By 1951, Archbishop Mariano Rossell Arellano found that it was urgent to recover the elite position of the Catholic Church in Guatemala, and for that reason he allied himself to the interests of the United Fruit Company through the National Liberation Movement and aimed to overthrow the revolutionary governments, which he branded as atheists and communists. After the consecration of the Shrine of Esquipulas (1950), and as part of the smear campaign launched against the Arbenz government, he requested sculptor Julio Urruela Vásquez to carve a replica of the Christ of Esquipulas, which was transferred to bronze in 1952 and converted the following year in symbol and banner of the national pilgrimage against Communism. This Christ was then appointed as "Commander in Chief" of the forces of the National Liberation Movement during the invasion of June 1954.
On April 4, 1954, Rossell Arellano issued a pastoral letter in which he criticized the progress of communism in the country, and made a call to Guatemalans to rise up and fight the common enemy of God and the homeland. This pastoral was distributed throughout the country.
National Liberation (1954).
Agrarian Reform and UFCo conflict.
In 1953, when the government implemented Agrarian Reform, it intended to redistribute large holdings of unused land to peasants, both Ladino and Amerindian, for them to develop for subsistence farming. It expropriated 250,000 of 350,000 "manzanas" held by the United Fruit Company (UFC). According to the government's Decree 900, it would redistribute the land for agricultural purposes. UFCO continued to hold thousands of acres in pasture as well as substantial forest reserves. The Guatemalan government had offered the company a Q 609,572 in compensation for the appropriated land. The company fought the land expropriation, making several legal arguments. It said the government had misinterpreted its own law. The Agrarian Reform Law was directed at redistributing unused land able to be developed for agricultural purposes. Thus land in pasture, specified forest cover and under cultivation was to be left with the owners and untouched by the expropriators. The company argued that most of the land taken from them was cultivated and in use, so it was illegal for the government to take it.
Secondly, they argued that the offered compensation was insufficient for the amount and value of the land taken. However, the valuations of United Fruit Company's rural property were based on the values declared by the company in its own tax filings. In 1945 Arevalo's administration ordered new assessments, to be complete by 1948. UFCo had submitted the assessment by the due date; but, when the Agrarian Reform was implemented, the company declared that they wanted the value of its property changed from the values the company had previously used to dodge taxes. The government had investigated in 1951, but a new assessment was never completed. UFCo said that the 1948 assessment was outdated, and claimed its land value was much greater. They had estimated just compensation would be as high as Q 15,854,849, nearly twenty times more than what the Guatemalan government had offered.
The U.S. State Department and the embassy actively began to support the position of UFCo, which was a major US company. The Guatemalan government had to fight the pressure. The US officially acknowledged that Guatemala had the right to conduct their own politics and business, but U.S. representatives said they were trying to protect UFCo, a US company that generated much revenue and contributed to the US economy. Arbenz's administration said that Guatemala needed Agrarian Reform to improve its own economy. Arbenz said he would adopt policies for a nationalist economic development if necessary. He argued that all foreign investment would be subject to Guatemalan laws. Arbenz was firm in promoting the Agrarian Reform and within a couple of years had acted quickly; he claimed that Guatemalan government was not prepared to make an exception for the U.S. concerning Decree 900 and that is was not Guatemalan's fault that the American corporation had lied in its tax forms and declared an artificially low value on their land.
Because Arbenz could not be pressured to take into consideration the arguments made to prevent expropriation from UFCo, his government was undermined with propaganda. For U.S. the national security was also highly important. They had combined both political and economic interests. The fear of allowing communist practices in Guatemala was shared by the urban elite and middle classes, who would not relinquish their privileges that easily. The local media-such as newspapers "El Imparcial" and "La Hora"- took advantage of the freedom of press of the regime, and with the sponsorship of UFCo were critical of communism and of the government's legal recognition of the party. The opposing political parties organized anticommunism campaigns; thousands of people appeared at the periodic rallies, and the membership in anticommunist organizations had grown steadily.
Arrival of John Peurifoy to Guatemala.
Between 1950 and 1955, during the government of General Eisenhower in the United States, a witch hunt for communists was conducted: McCarthyism. This was characterized by persecuting innocent people by mere suspicion, with unfounded accusations, interrogation, loss of labor, passport denial, and even imprisonment. These mechanisms of social control and repression in the United States skirted dangerously with the totalitarian and fascist methods.
One of the main characters of McCarthyism was John Peurifoy, who was sent as the ambassador of the United States to Guatemala, as this was the first country in the American sphere of influence after World War II that included elements openly communists in his government. He came from Greece, where he had already done considerable anticommunist activity, and was installed as Ambassador in November 1953, when Carlos Castillo Armas was already organizing his tiny revoucionary army. After a long meeting, Peurifoy made it clear to President Arbenz that the US was worried about the communist elements in his government, and then reported to the Department of State that the Guatemalan leader was not a communist, but that surely a Communist leader would come after him; furthermore, in January 1954 he told "Time" magazine: "American public opinion could force us to take some measures to prevent Guatemala from falling into the orbit of international communism".
Operation PBSUCCESS.
The Communist Party was never the center of the Communist movement in Guatemala until Jacobo Arbenz came to power in 1951. Prior to 1951, Communism lived within the urban labor forces in small study groups during 1944 to 1953 which it had a tremendous influence on these urban labor forces. Despite its small size within Guatemala, many leaders were extremely vocal about their beliefs (for instance, in their protests and, more importantly, their literature). In 1949 in Congress, the Communist party only had less than forty members, however, by 1953 it went up to nearly four thousand. Before Arbenz come to power in 1951, the Communist movement preferred to carry out many of their activities through the so-called mass organization. In addition to Arbenz success, Guatemalan Communist Party moved forward its activities into public.
After Jacobo Arbenz came to power in 1951, he extended political freedom, allowing Communists in Guatemala to participate in politics. This move by Arbenz let many opponents in Ubico’s regime to recognize themselves as Communists. By 1952, Arbenz supported a land reform, and took unused agricultural land, about 225,000 acre, from owners who had large properties, and made it available to rural workers and farmers. These lands were to be taken from the United Fruit Company with compensation; however, the UFC believed the compensation was not enough. Meantime, Arbenz allowed the Communist Party to organize and include leaders notably his adviser who were leftist. The propaganda effort that was led by United Fruit Company against the revolution in Guatemala persuaded the U.S. government to fight against communism in Guatemala. The United States clutched on small details to prove the existence of widespread Communism in Guatemala. The Eisenhower administration at the time in the U.S. were not happy about the Arbenz government, they considered Arbenz to be too close to Communism; there have been reports that Arbenz’s wife was a Communist and part of the Communist Party in Guatemala. Even though it was impossible for the U.S. to gather evidence and information about Guatemala’s relations to the Soviet Union, Americans wanted to believe that Communism existed in Guatemala. Many groups of Guatemalan exiles were armed and trained by the CIA, and commanded by Colonel Carlos Castillo Armas they invaded Guatemala on June 18, 1954. The Americans called it an Anti-Communist Coup against Arbenz. The coup was supported by CIA radio broadcasts and so the Guatemalan army refused to resist the coup, Arbenz was forced to resign. In 1954 a military government replaced Arbenz' government and disbanded the legislature and they arrested communist leaders, Castillo Armas became president.
Arbenz proceeded to nationalize and redistribute un-utilized land owned by the United Fruit Company, which had a practical monopoly on Guatemalan fruit production and some industry. In response, United Fruit lobbied the Eisenhower administration to remove Arbenz. Of still greater importance, though, was the widespread American concern about the possibility of a so-called "Soviet beachhead" opening up in the Western Hemisphere. Arbenz's sudden legalization of the Communist party and importing of arms from then Soviet-satellite state of Czechoslovakia, among other events, convinced major policy makers in the White House and CIA to try for Arbenz's forced removal, although his term was to end naturally in two years. This led to a CIA-orchestrated coup in 1954, known as Operation PBSUCCESS, which saw Arbenz toppled and forced into exile by Colonel Carlos Castillo Armas. Despite most Guatemalans' attachment to the original ideals of the 1944 uprising, some private sector leaders and the military began to believe that Arbenz represented a Communist threat and supported his overthrow, hoping that a successor government would continue the more moderate reforms started by Arevalo. After the CIA coup, hundreds of Guatemalans were rounded up and killed.
Civil war (1960-1996).
The government, right-wing paramilitary organizations, and left-wing insurgents were all engaged in the Guatemalan Civil War (1960–96). A variety of factors contributed: social and economic injustice and racial discrimination suffered by the indigenous population, the 1954 coup which reversed reforms, weak civilian control of the military, the United States support of the government, and Cuban support of the insurgents. The Historical Clarification Commission (commonly known as the "Truth Commission") after the war estimated that more than 200,000 people were killed — the vast majority of whom were indigenous civilians. 93% of the human rights abuses reported to the Commission were attributed to the military or other government-supported forces. It also determined that in several instances, the government was responsible for acts of genocide.
In response to the increasingly autocratic rule of Gen. Ydígoras Fuentes, who took power in 1958 following the murder of Col. Castillo Armas, a group of junior military officers revolted in 1960. When they failed, several went into hiding and established close ties with Cuba. This group became the nucleus of the forces who mounted armed insurrection against the government for the next 36 years.
Shortly after President Julio César Méndez Montenegro took office in 1966, the army launched a major counterinsurgency campaign that largely broke up the guerrilla movement in the countryside.
The guerrillas concentrated their attacks in Guatemala City, where they assassinated many leading figures, including U.S. Ambassador John Gordon Mein in 1968. During the next nearly two decades, Méndez Montenegro was the only civilian to head Guatemala until the inauguration of Vinicio Cerezo in 1986.
Franja Transversal del Norte.
The first settler project in the FTN was in Sebol-Chinajá in Alta Verapaz. Sebol, then regarded as a strategic point and route through Cancuén river, which communicated with Petén through the Usumacinta River on the border with Mexico and the only road that existed was a dirt one built by President Lázaro Chacón in 1928. In 1958, during the government of General Miguel Ydígoras Fuentes the Inter-American Development Bank (IDB) financed infrastructure projects in Sebol. In 1960, then Army captain Fernando Romeo Lucas Garcia inherited Saquixquib and Punta de Boloncó farms in northeastern Sebol. In 1963 he bought the farm "San Fernando" El Palmar de Sejux and finally bought the "Sepur" farm near San Fernando. During those years, Lucas was in the Guatemalan legislature and lobbied in Congress to boost investment in that area of the country.
In those years, the importance of the region was in livestock, exploitation of precious export wood and archaeological wealth. Timber contracts we granted to multinational companies such as Murphy Pacific Corporation from California, which invested US$30 million for the colonization of southern Petén and Alta Verapaz, and formed the North Impulsadora Company. Colonization of the area was made through a process by which inhospitable areas of the FTN were granted to native peasants.
In 1962, the DGAA became the National Institute of Agrarian Reform (INTA), by Decree 1551 which created the law of Agrarian Transformation. In 1964, INTA defined the geography of the FTN as the northern part of the departments of Huehuetenango, Quiché , Alta Verapaz and Izabal and that same year priests of the Maryknoll order and the Order of the Sacred Heart began the first process of colonization, along with INTA, carrying settlers from Huehuetenango to the Ixcán sector in Quiché.
"It is of public interest and national emergency, the establishment of Agrarian Development Zones in the area included within the municipalities: San Ana Huista, San Antonio Huista, Nentón, Jacaltenango, San Mateo Ixcatán, and Santa Cruz Barillas in Huehuetenango; Chajul and San Miguel Uspantán in Quiché; Cobán, Chisec, San Pedro Carchá, Lanquín, Senahú, Cahabón and Chahal, in Alta Verapaz and the entire department of Izabal." 
Decreto 60-70, artítulo 1o. 
The Northern Transversal Strip was officially created during the government of General Carlos Arana Osorio in 1970, by Decree 60-70 in the Congress, for agricultural development.
The Guerrilla Army of the Poor.
On January 19, 1972 members of a new Guatemalan guerrilla movement entered Ixcán, from Mexico, and were accepted by many farmers; in 1973, after an exploratory foray into the municipal seat of Cotzal, the insurgent group decided to set up camp underground in the mountains of Xolchiché, municipality of Chajul.
In 1974 the insurgent guerrilla group held its first conference, where it defined its strategy of action for the coming months and called itself Guerrilla Army of the Poor (-Ejército Guerrillero de los Pobres -EGP-). In 1975 the organization had spread around the area of the mountains of northern municipalities of Nebaj and Chajul. As part of its strategy EGP agreed to perform acts that notoriety was obtained and through which also symbolize the establishment of a "social justice" against the inefficiency and ineffectiveness of the judicial and administrative organs of the State. They saw also that with these actions the indigenous rural population of the region is identified with the insurgency, thus motivating joining their ranks. As part of this plan was agreed to so-called "executions". To determine who would be these people subject to "execution", the EGP attended complaints received from the public. For example, they selected two victims: Guillermo Monzón, who was a military Commissioner in Ixcán and José Luis Arenas, the largest landowner in the area of Ixcán, and who had been reported to the EGP for allegedly having land conflicts with neighboring settlements and abusing their workers.
On Saturday, 7 June 1975, José Luis Arenas was killed by unknowns when he was in the premises of his farm "La Perla" to pay wage workers. In front of his office there were approximately two to three hundred people to receive their payment and four members of EGP mixed among farmers. Subsequently, the guerrilla members destroyed the communication radio of the farm and executed Arenas. After having murdered José Luis Arenas, guerrilla members spoke in Ixil language to the farmers, informing them that they were members of the Guerrilla Army of the Poor and had killed the "Tiger Ixcán." They requested to prepare beasts to help the injured and were transported to Chajul to receive medical care. Then the attackers fled towards Chajul.
José Luis Arenas' son, who was in San Luis Ixcán at the time, seek refuge in a nearby mountain, waiting for a plane to arrive to take him to the capital, in order to immediately report the matter to the Minister of Defense. The defense minister replied, "You are mistaken, there are no guerrillas in the area".
Panzós massacre.
In Alta Verapaz in the late nineteenth century German farmers came to concentrate in their hands three quarters of the total area of 8686 square kilometers that had the departmental territory. In this department came insomuch land grabbing and women [slaves] by German agricultural entrepreneurs, a political leader noted that farmers disappeared from their villages overnight, fleeing the farmers. 
Julio Castellanos Cambranes 
Also located in the Northern Transversal Strip, the valley of the Polochic River was inhabited since ancient times by k'ekchí and P'okomchi people. In the second half of the nineteenth century, President Justo Rufino Barrios (1835-1885) began the allocation of land in the area to German farmers. Decree 170 (or decree of Census Redemption Decree) facilitated the expropriation of Indian land in favor of the Germans, because it promoted the auction of communal lands. Since that time, the main economic activity was export-oriented, especially coffee, bananas and cardamom. The communal property, dedicated to subsistence farming, became private property led to the cultivation and mass marketing of agricultural products. Therefore, the fundamental characteristic of the Guatemalan production system has since that time been the accumulation of property in few hands, and a sort of "farm servitude" based on the exploitation of "farmer settlers".
In 1951, the agrarian reform law that expropriated idle land from private hands was enacted, but in 1954, with the National Liberation Movement coup supported by the United States, most of the land that had been expropriated, was awarded back to its former landowners. Flavio Monzón was appointed mayor and in the next twenty years he became one of the largest landowners in the area. In 1964, several communities settled for decades on the shore of Polochic River claimed property titles to INTA which was created in October 1962, but the land was awarded to Flavio Monzón. A Mayan peasant from Panzós later said that Flavio Monzón "got the signatures of the elders before he went before INTA to talk about the land. When he returned, gathered the people and said that, by an INTA mistake, the land had gone to his name." Throughout the 1970s, Panzós farmers continued to claim INTA regularization of land ownership receiving legal advice from the FASGUA (Autonomous Trade Union Federation of Guatemala), an organization that supported the peasants' demands through legal procedures. However, no peasant received a property title, ever. Some obtained promises while other had provisional property titles, and there were also some that only had received permission to plant. The peasants began to suffer evictions from their land by farmers, the military and local authorities in favor of the economic interests of Izabal Mining Operations Company (EXMIBAL) and Transmetales.ref group=Note>Another threat at that time for peasant proprietors were mining projects and exploration of oil: Exxon, Shenandoah, Hispanoil and Getty Oil all had exploration contracts; besides there was the need for territorial expansion of two megaprojects of that era: Northern Transversal Strip and Chixoy Hydroelectric Plant.
In 1978 a military patrol was stationed a few kilometers from the county seat of Panzós, in a place known as "Quinich". At this time organizational capacity of peasant had increased through committees who claimed titles to their land, a phenomenon that worried the landlord sector. Some of these owners -among them Flavio Monzón- stated: "Several peasants living in the villages and settlements want to burn urban populations to gain access to private property", and requested protection from Alta Verapaz governor.
On 29 May 1978, peasant from Cahaboncito, Semococh, Rubetzul, Canguachá, Sepacay villages, finca Moyagua and neighborhood La Soledad, decided to hold a public demonstration in the Plaza de Panzós to insist on the claim of land and to express their discontent caused by the arbitrary actions of the landowners and the civil and military authorities. Hundreds of men, women, indigenous children went to the square of the municipal seat of Panzós, carrying their tools, machetes and sticks. One of the people who participated in the demonstration states: "The idea was not to fight with anyone, what was required was the clarification of the status of the land. People came from various places and they had guns."
There are different versions on how the shooting began: some say it began when "Mama Maquín" -an important peasant leader- pushed a soldier who was in her way; others argue that it started because people kept pushing trying to get into the municipality, which was interpreted by the soldiers as an aggression. The mayor at the time, Walter Overdick, said that "people of the middle of the group pushed those who in front." A witness says one protester grabbed the gun from a soldier but did not use it and several people argue that a military voice yelled: One, two, three! Fire!" In fact, the lieutenant who led the troops gave orders to open fire on the crowd.
The shots that rang for about five minutes, were made by regulation firearms carried by the military as well as the three machine guns located on the banks of the square. 36 Several peasants with machetes wounded several soldiers. No soldier was wounded by gunfire. The square was covered with blood.
Immediately, the army closed the main access roads, despite that "indigenous felt terrified." An army helicopter flew over the town before picking up wounded soldiers.
Transition between Laugerud and Lucas Garcia regimes.
Due to his seniority in both the military and economic elites in Guatemala, as well as the fact that he spoke perfectly the q'ekchi, one of the Guatemalan indigenous languages, Lucas García the ideal official candidate for the 1978 elections; and to further enhance his image, he was paired with the leftist doctor Francisco Villagrán Kramer as running mate. Villagrán Kramer was a man of recognized democratic trajectory, having participated in the Revolution of 1944, and was linked to the interests of transnational corporations and elites, as he was one of the main advisers of agricultural, industrial and financial chambers of Guatemala. Despite the democratic facade, the electoral victory was not easy and the establishment had to impose Lucas García, causing further discredit the electoral system -which had already suffered a fraud when General Laugerud was imposed in the 1974 elections.
In 1976 student group called "FRENTE" emerged in the University of San Carlos, which completely swept all student body positions that were up for election that year. FRENTE leaders were mostly members of the Patriotic Workers' Youth, the youth wing of the Guatemalan Labor Party (-Partido Guatemalteco del Trabajo- (PGT), the Guatemalan communist party who had worked in the shadows since it was illegalized in 1954. Unlike other Marxist organizations in Guatemala at the time, PGT leaders trusted the mass movement to gain power through elections.
FRENTE used its power within the student associations to launch a political campaign for the 1978 university general elections, allied with leftist Faculty members grouped in "University Vanguard". The alliance was effective and Oliverio Castañeda de León was elected as President of the Student Body and Saúl Osorio Paz as President of the University; plus they had ties with the University workers union (STUSC) thru their PGT connections. Osorio Paz gave space and support to the student movement and instead of having a conflictive relationship with students, different representations combined to build a higher education institution of higher social projection. In 1978 the University of San Carlos became one of the sectors with more political weight in Guatemala; that year the student movement, faculty and University Governing Board -Consejo Superior Universitario- united against the government and were in favor of opening spaces for the neediest sectors. In order to expand its university extension, the Student Body (AEU) rehabilitated the "Student House" in downtown Guatemala City; there, they welcomed and supported families of villagers and peasant already sensitized politically. They also organized groups of workers in the informal trade.
At the beginning of his tenure as President, Saúl Osorio founded the weekly "Siete Días en la USAC", which besides reporting on the activities of the University, constantly denounced the violation of human rights, especially the repression against the popular movement. It also told what was happening with revolutionary movements in both Nicaragua and El Salvador . For a few months, the state university was a united and progressive institution, preparing to confront the State head on.
Now, FRENTE had to face the radical left, represented then by the Student Revolutionary Front "Robin García" (FERG), which emerged during the Labor Day march of 1 May 1978. FERG coordinated several student associations on different colleges within University of San Carlos and public secondary education institutions. This coordination between legal groups came from the Guerrilla Army of the Poor (EGP), a guerrilla group that had appeared in 1972 and had its headquarters in the oil rich region of northern Quiché department -i.e., the Ixil Triangle of Ixcán, Nebaj and Chajul in Franja Transversal del Norte. Although not strictly an armed group, FERG sought confrontation with government forces all the time, giving prominence to measures that could actually degenerate into mass violence and paramilitary activity. Its members were not interested in working within an institutional framework and never asked permission for their public demonstrations or actions.
On 7 March 1978 Lucas Garcia was elected President; shortly after, on 29 May 1978 -in the late days of General Laugerud García government- in the central square of Panzós, Alta Verapaz, members of the Zacapa Military Zone attacked a peaceful peasant demonstration, killing a lot of people. The deceased, indigenous peasants who had been summoned in place, were fighting for the legalization of public lands they had occupied for years. Their struggle faced them directly with investors who wanted to exploit the mineral wealth of the area, particularly oil reserves -by Basic Resources International and Shenandoah Oil- and nickel -EXMIBAL. The Panzós Massacre caused a stir at the University by the high number of victims and conflicts arose from the exploitation of natural resources by foreign companies. In 1978 for example, Osorio Paz and other university received death threats for their outspoken opposition to the construction of an inter-oceanic pipeline that would cross the country to facilitate oil exploration. On June 8 the AEU organized a massive protest in downtown Guatemala City where speakers denounced the slaughter of Panzós and expressed their repudiation of Laugerud García regime in stronger terms than ever before.
Escalation of violence.
After the "execution" of José Luis Arenas population of Hom, Ixtupil, Sajsivan and Sotzil villages, neighbors of La Perla and annexes, increased support for the new guerrilla movement, mainly due to the land dispute that peasants kept with the owners of the farm for several years and that the execution was seen as an act of "social justice".
The murder owner of the farm "La Perla", located in the municipality of Chajul, resulted in the escalation of violence in the area: part of the population moved closer to the guerrillas, while another part of the inhabitants of Hom kept out of the insurgency. In 1979 the owners of the farm "La Perla" established links with the army and for the first time a military detachment was installed within the property; in this same building the first civil patrol of the area was established. The Army high command, meanwhile, was very pleased with the initial results of the operation and was convinced it had succeeded in destroying most of the social basis of EGP, which had to be expelled from the "Ixil Triangle". At this time the presence of EGP in the area decreased significantly due to the repressive actions of the Army, who developed its concept of "enemy" without necessarily including the notion of armed combatants; the officers who executed the plan were instructed to destroy all towns suspect of cooperate with EGP and eliminate all sources of resistance. Army units operating in the "Ixil Triangle" belonged to the Mariscal Zavala Brigade, stationed in Guatemala City. Moreover, although the guerrillas did not intervene directly when the army attacked the civilian population allegedly because they lacked supplies and ammunition, it did support some survival strategies. It streamlined, for example, "survival plans" designed to give evacuation instructions in assumption that military incursions took place. Most of the population began to participate in the schemes finding that them represented their only alternative to military repression.
Lucas Garcia presidency.
The election of Lucas García on 7 March 1978 marked the beginning of a full return to the counterinsurgency practices of the Arana period. This was compounded by the strong reaction of the Guatemalan military to the situation unfolding in Nicaragua at the time, where the popularly-supported Sandinista insurgency was on the verge of toppling the Somoza regime. With the aim of preventing an analogous situation from unfolding in Guatemala, the government intensified its repressive campaign against the predominantly indigenous mass movement. The repression not only intensified, but became more overt.
On 4 August 1978, high school and university students, along with other popular movement sectors, organized the mass movement's first urban protest of the Lucas García period. The protests, intended as a march against violence, were attended by an estimated 10,000 people. The new minister of the interior under President Lucas García, Donaldo Alvarez Ruiz, promised to break up any protests done without government permission. Having refused to ask for permission, the protesters were met by the Pelotón Modelo (Model Platoon) of the National Police. Employing new anti-riot gear donated by the United States Government, Platoon agents surrounded marchers and tear-gassed them. Students were forced to retreat and dozens of people, mostly school-aged adolescents, were hospitalized. This was followed by more protests and death squad killings throughout the later part of the year. In September 1978 a general strike broke out to protest sharp increases in public transportation fares; the government responded harshly, arresting dozens of protesters and injuring many more. However, as a result of the campaign, the government agreed to the protesters' demands, including the establishment of a public transportation subsidy. Fearful that this concession would encourage more protests, the military government, along with state-sponsored paramilitary death squads, generated an unsafe situation for public leaders.
The administrator of a large cemetery in Guatemala City informed the press that in the first half of 1978, more than 760 unidentified bodies had arrived at the cemetery, all apparent victims of death squads. Amnesty International stated that disappearances were an "epidemic" in Guatemala and reported more than 2,000 killings between mid-1978 and 1980. Between January and November 1979 alone the Guatemalan press reported 3,252 disappearances.
Spanish Embassy fire.
On 31 January 1980, a group of displaced K'iche' and Ixil peasant farmers occupied the Spanish Embassy in Guatemala City to protest the kidnapping and murder of peasants in Uspantán by elements of the Guatemalan Army. In the subsequent police raid, over the protests of the Spanish ambassador, the police attacked the building with incendiary explosives. A fire ensued as police prevented those inside of the embassy from exiting the building. In all, 36 people were killed in the fire. The funeral of the victims (including the hitherto obscure father of Rigoberta Menchú, Vicente Menchú) attracted hundreds of thousands of mourners, and a new guerrilla group was formed commemorating the date, the "Frente patriotico 31 de enero" (Patriotic Front of 31 January). The incident has been called "the defining event" of the Guatemalan Civil War. The Guatemalan government issued a statement claiming that its forces had entered the embassy at the request of the Spanish Ambassador, and that the occupiers of the embassy, whom they referred to as "terrorists," had "sacrificed the hostages and immolated themselves afterward." Ambassador Cajal denied the claims of the Guatemalan government and Spain immediately terminated diplomatic relations with Guatemala, calling the action a violation of "the most elementary norms of international law." Relations between Spain and Guatemala were not normalized until September 22, 1984.
Increased insurgency and state repression: 1980–1982.
In the months following the Spanish Embassy Fire, the human rights situation continued to deteriorate. The daily number of killings by official and unofficial security forces increased from an average of 20 to 30 in 1979 to a conservative estimate of 30 to 40 daily in 1980. Human rights sources estimated 5,000 Guatemalans were killed by the government for "political reasons" in 1980 alone, making it the worst human rights violator in the hemisphere after El Salvador. In a report titled "Guatemala: A Government Program of Political Murder," Amnesty International stated, "Between January and November of 1980, some 3,000 people described by government representatives as "subversives" and "criminals" were either shot on the spot in political assassinations or seized and murdered later; at least 364 others seized in this period have not yet been accounted for." 
The repression and excessive force used by the government against the opposition was such that it became source of contention within Lucas Garcia's administration itself. This contention within the government caused Lucas Garcia's Vice President Francisco Villagrán Kramer to resign from his position on September 1, 1980. In his resignation, Kramer cited his disapproval of the government's human rights record as one of the primary reasons for his resignation. He then went into voluntary exile in the United States, taking a position in the Legal Department of the Inter-American Development Bank.
Insurgent mobilization.
The effects of state repression on the population further radicalized individuals within the mass movement and led to increased popular support for the insurgency. In late 1979, the EGP expanded its influence, controlling a large amount of territory in the Ixil Triangle in El Quiche and holding many demonstrations in Nebaj, Chajul and Cotzal. At the same time the EGP was expanding its presence in the Altiplano, a new insurgent movement called the ORPA (Revolutionary Organization of Armed People) made itself known. Composed of local youths and university intellectuals, the ORPA developed out of a movement called the Regional de Occidente, which split from the FAR-PGT in 1971. The ORPA's leader, Rodrigo Asturias (a former activist with the PGT and first-born son of Nobel Prize-winning author Miguel Ángel Asturias), formed the organization after returning from exile in Mexico. The ORPA established an operational base in the mountains and rain-forests above the coffee plantations of southwestern Guatemala and in the Atitlan where it enjoyed considerable popular support. On September 18, 1979, the ORPA made its existence publicly known when it occupied the Mujulia coffee farm in the coffee-growing region of the Quezaltenango province to hold a political education meeting with the workers.
Insurgent movements active in the initial phase of the conflict such as the FAR also began to reemerge and prepare for combat. In 1980, guerrilla operations on both the urban and rural fronts greatly intensified, with the insurgency carrying out a number of overt acts of armed propaganda and assassinations of prominent right-wing Guatemalans and landowners. In 1980, armed insurgents assassinated prominent Ixil landowner Enrique Brol, and president of the CACIF (Coordinating Committee of Agricultural, Commercial, Industrial, and Financial Associations) Alberto Habie. Encouraged by guerrilla advances elsewhere in Central America, the Guatemalan insurgents, especially the EGP, began to quickly expand their influence through a wide geographic area and across different ethnic groups, thus broadening the appeal of the insurgent movement and providing it with a larger popular base. In October 1980, a tripartite alliance was formalized between the EGP, the FAR and the ORPA as a precondition for Cuban-backing.
In early 1981, the insurgency mounted the largest offensive in the country's history. This was followed by an additional offensive towards the end of the year, in which many civilians were forced to participate by the insurgents. Villagers worked with the insurgency to sabotage roads and army establishments, and destroy anything of strategic value to the armed forces. By 1981, an estimated 250,000 to 500,000 members of Guatemala's indigenous community actively supported the insurgency. Guatemalan Army Intelligence (G-2) estimated a minimum 360,000 indigenous supporters of the EGP alone. Since late 1981 the Army applied a strategy of "scorched earth" in Quiché, to eliminate the guerilla social support EGP. In some communities of the region's military forced all residents to leave their homes and concentrate in the county seat under military control. Some families obeyed; others took refuge in the mountains. K'iche's who took refuge in the mountains, were identified by the Army with the guerrillas and underwent a military siege, and continuous attacks that prevented them from getting food, shelter and medical care.
La Llorona massacre, El Estor.
La Llorona, located about 18 kilometers from El Estor, department of Izabal (part of the Northern Transversal Strip), was a small village with no more than twenty houses. Most of the first settlers arrived from the areas of Senahú and Panzós, both in Alta Verapaz. In 1981 the total population was about 130 people, all belonging to q'eqchi' ethnic group. Few people spoke Spanish and most work in their own cornfields, sporadicly working for the to local landowners. In the vicinity are the villages El Bongo, Socela, Benque, Rio Pita, Santa Maria, Big Plan and New Hope. Conflicts in the area were related to land tenure, highlighting the uncertainty about the boundaries between farms and communities, and the lack of titles. As in the National Institute of Agrarian Transformation (INTA) was not registered a legitimate owner of land occupied La Llorona, the community remained in the belief that the land belonged to the state, which had taken steps to obtain title property. However, a farmer with great influence in the area occupied part of the land, generating a conflict between him and the community; men of the village, on its own initiative, devised a new boundary between community land and the farmer, but the problem remained dormant.
In the second half of the seventies were the first news about the presence of guerrillas in the villages, the commander aparacimiento Ramon, talking to people and saying they were the Guerrilla Army of the Poor. They passed many villages asking what problems people had and offering to solve them. The told peasants that the land belonged to the poor and that they should trust them. In 1977, Ramon a -guerilla commander- regularly visited the village of La Llorona and after finding that the issue of land was causing many problems in the community, taught people to practice new measurements, which spread fear among landowners. That same year, the group under Ramon arbitrarily executed the Spanish landowner José Hernández, near El Recreo, which he owner. Following this, a clandestine group of mercenaries, dubbed "fighters of the rich" was formed to protect the interests of landlords; public authority of El Estor organized the group and paid its members, stemming from the funding of major landowners. The group, irregular, was related to the military commissioners of the region and with commanders of the Army, although mutual rivalries also took place. The secret organization murdered several people, including victims who had no connection whatsoever with insurgent groups.
In December 1978, the EGP group leader, Ramon, was captured by soldiers of the military detachment in El Estor and transferred to the military zone of Puerto Barrios; after two years returned to El Estor; but this time as an officer in the Army G2 and joined a group of soldiers that came to the village. On the evening of 28 September 1981, an army officer accompanied by four soldiers and a military commissioner met with about thirty civilians. At seven o'clock, over thirty civilians, mostly from "Nueva Esperanza', including several 'informants' known to military intelligence, gathered around La Llorona along with some military commissioners and a small group of soldiers and army officers. Then they entered the village. Civilians and commissioners entered twelve houses, and each of them were pulling men and shot them dead outside their own homes; those who tried to escape were also killed. Women who tried to protect their husbands were beatn. While the military commissioners and civilians executed men, soldiers subtracted belongings of the victims; within half an hour, the authors of the assault left the village. The victim bodies, fourteen in all, were in front of houses. Women, despite having been threatened with death if tell what happened, ran to the nearest village, El Bongo, for help. After a few hours, women came back with people who helped to bury the bodies. Days later, widows, with almost 60 fatherless children were welcomed by the parish of El Estor for several days, until the soldiers forced them to return to their village. Two widows of those executed on September 29 established close relations with the military commissioners from Bongo. This situation led to divisions that still exist in the community.
The economic and social activity was disrupted in the village: widows had to take the jobs of their husbands; because of their lack of knowledge in the cultivation of land, harvested very little corn and beans. There were diseases, especially among children and the elderly, there was no food or clothing. The teacher of the village came only part-time, mostly out of fear, but left after he realized it was not worth to stay because young people had to work. Nor could they spend money on travel. The village had no teacher for the next four years. The events generated finally the breakup of the community. Some village women though that their husbands were killed because of three others who were linked with the guerrillas and were involved in a land dispute.
According to the Historical Clarification Commission, the landlord with whom the villagers had the land dispute took advantage of the situation to appropriate another twelve acres of land.
List of other massacres perpetrated by the Army in Franja Transversal del Norte.
The report of the Recovery of Historical Memory lists 422 massacres committed by both sides in the conflict; however, it also states that they did the best they could in terms of obtaining information and therefore the list is incomplete; therefore here are the cases that have also been documented in other reports as well.
List of massacres perpetrated by the EGP in FTN.
According to a report by the rightist magazine "Crónica", there were 1258 guerrilla actions against civilians and infrastructure in Guatemala, including more than two hundred murders, sixty eight kidnappings, eleven bombs against embassies and three hundred twenty-nine attacks against civilians. Almost all guerrilla massacres occurred in 1982 when further militarization reigned and there was widespread presence of PAC in communities; many of them were victims of non-cooperation with the guerrillas and in some cases they came after a previous attack by the PAC. In the massacres perpetrated by the guerrillas there is no use of informants, or concentration of population, or separation of groups; also, there are no recounts of rape or repetitive slaughter. There are cases of razed villages and less tendency to mass flight, even thought it occurred in some cases. the use of lists was also more frequent.
In a publication of the Army of Guatemala, sixty massacres perpetrated by the EGP were reported, arguing that they were mostly ignored by REHMI and the Historical Clarification Commission reports. It is also reported that in mid-1982, 32 members of "Star Guerilla Front " were shot for not raising the EGP flag.
Civil war in the city.
"Beheaded corpses hanging from their legs in between what is left from blown up cars, shapeless bodies among glass shards and tree branches all over the place is what a terrorist attack caused yesterday at 9:35 am. El Gráfico reporters were able to get to exact place where the bomb went off, only seconds after the horrific explosion, and found a truly infernal scene in the corner of the 6th avenue and 6th street -where the Presidential Office is located- which had turned into a huge oven -but the solid building where the president worked was safe-. The reporters witnessed the dramatic rescue of the wounded, some of them critical, like the man that completely lost a leg and had only stripes of skin instead." 
"El Gráfico", 6 September 1980 
On 31 January 1980, Guatemala got worldwide attention when the Spanish Embassy in Guatemala City was burnt down, resulting in 37 deaths, including embassy personnel and high ranked Guatemalan former government officials. A group of native people from El Quiché occupied the embassy in a desperate attempt to bring attention to the issues they were having with the Army in that region of the country, which was rich in oil and had been recently populated as part of the "Franja Transversal del Norte" agricultural program. In the end, thirty seven people died after a fire started within the embassy after the police force tried to occupy the building; after that, Spain broke its diplomatic relationships with Guatemala.
On 5 September 1980 took place a terrorist attack by Ejército Guerrillero de los Pobres (EGP) right in front of the Guatemalan National Palace, then the heardquarters of the Guatemalan government. The intention was to prevent the Guatemalan people to support a huge demonstration that the government of general Lucas Garcia had prepared for Sunday 7 September 1980. In the attack, six adults and a little boy died after two bombs inside a vehicle went off.
There was an undetermined number of wounded and heavy material losses, not only from art pieces from the National Palace, but from all the surrounding buildings, particularly in the Lucky Building, which is right across the Presidential Office.
The attacks against private financial, commercial and agricultural targets increased in the Lucas Garcia years, as the leftist marxist groups saw those institutions as "reactionaries" and "millionaire exploiters" that were collaborating with the genocidal government. The following is a non-exhaustive list of the terrorist attacks that occurred in Guatemala city and are presented in the UN Commission report:
Despite advances by the insurgency, the insurgency made a series of fatal strategic errors. The successes made by the revolutionary forces in Nicaragua against the Somoza regime combined with the insurgency's own successes against the Lucas government led rebel leaders to falsely conclude that a military equilibrium was being reached in Guatemala, thus the insurgency underestimated the military strength of the government. The insurgency subsequently found itself overwhelmed, and was unable to secure its advances and protect the indigenous civilian population from reprisals by the security forces.
'Operation Ceniza'.
In response to the guerilla offensive in early 1981, the Guatemalan Army began mobilizing for a large-scale rural counter-offensive. The Lucas government instituted a policy of forced recruitment and began organizing a "task-force" model for fighting the insurgency, by which strategic mobile forces were drawn from larger military brigades. To curtail civilian participation in the insurgency and provide greater distinction between "hostile" and compliant communities in the countryside, the army resorted to a series of "civic action" measures. The army under Chief of Staff Benedicto Lucas García (the President’s brother) began to search out communities in which to organize and recruit civilians into pro-government paramilitary patrols, who would combat the insurgents and kill their collaborators.
In 1980 and 1981, the United States under Reagan administration delivered $10.5 million worth of Bell 212 and Bell 412 helicopters and $3.2 million worth of military trucks and jeeps to the Guatemalan Army. In 1981, the Reagan administration also approved a $2 million covert CIA program for Guatemala.
On April 15, 1981, EGP rebels attacked a Guatemalan Army patrol from the village of Cocob near Nebaj, killing five personnel. On April 17, 1981, a reinforced company of Airborne troops was deployed to the village. They discovered fox holes, guerrillas and a hostile population. The local people appeared to fully support the guerrillas. "The soldiers were forced to fire at anything that moved." The army killed 65 civilians, including 34 children, five adolescents, 23 adults and two elderly people.
In July 1981, the armed forces initiated a new phase of counterinsurgency operations under the code-name "Operación Ceniza," or "Operation Ashes," which lasted through March 1982. The purpose of the operation was to "separate and isolate the insurgents from the civilian population." During "Operación Ceniza" some 15,000 troops were deployed on a gradual sweep through the predominantly-indigenous Altiplano region, comprising the departments of El Quiché and Huehuetenango.
Large numbers of civilians were killed or displaced in the Guatemalan military's counterinsurgency operations. To alienate the insurgents from their civilian base, the army carried out large-scale mass killing of unarmed civilians, burned villages and crops, and butchered animals, destroying survivors' means of livelihood. Sources with the human rights office of the Catholic Church estimated the death toll from the counterinsurgency in 1981 at 11,000, with most of the victims indigenous peasants of the Guatemalan highlands. Other sources and observers put the death toll due to government repression in 1981 at between 9,000 and 13,500.
As army repression intensified in the countryside, relations between the Guatemalan military establishment and the Lucas Garcia regime worsened. Professionals within the Guatemalan military considered the Lucas approach counterproductive, on grounds that the Lucas government's strategy of military action and systematic terror overlooked the social and ideological causes of the insurgency while radicalizing the civilian population. Additionally, Lucas went against the military's interests by endorsing his defense minister, Angel Anibal Guevara, as a candidate in the March 1982 presidential elections.
The guerrilla organizations in 1982 combined to form the Guatemalan National Revolutionary Unity (URNG). At the same time, extreme right-wing groups of self-appointed vigilantes, including the Secret Anti-Communist Army (ESA) and the White Hand ("La Mano Blanca"), tortured and murdered students, professionals, and peasants suspected of involvement in leftist activities.
On March 23, 1982, army troops commanded by junior officers staged a coup d'état to prevent the assumption of power by General Ángel Aníbal Guevara, the hand-picked candidate of outgoing President and General Romeo Lucas García. They denounced Guevara's electoral victory as fraudulent. The coup leaders asked retired Gen. Efraín Ríos Montt to negotiate the departure of Lucas Guevara. Ríos Montt had been the candidate of the Christian Democracy Party in the 1974 presidential election and was widely regarded as having been denied his own victory through fraud.
Ríos Montt was by this time a lay pastor in the evangelical Protestant Church of the Word. In his inaugural address, he stated that his presidency resulted from the will of God. He was widely perceived as having strong backing from the Reagan administration in the United States. He formed a three-member military junta that annulled the 1965 constitution, dissolved Congress, suspended political parties and canceled the electoral law. After a few months, Ríos Montt dismissed his junta colleagues and assumed the "de facto" title of "President of the Republic".
Guerrilla forces and their leftist allies denounced Ríos Montt, who sought to defeat them by a combination of military actions and economic reforms; in his words, "rifles and beans". In May 1982, the Conference of Catholic Bishops accused Ríos Montt of responsibility for growing militarization of the country and for continuing military massacres of civilians. An army officer was quoted in the "New York Times" of 18 July 1982 as telling an audience of indigenous Guatemalans in Cunén that: "If you are with us, we'll feed you; if not, we'll kill you."
The Plan de Sánchez massacre occurred on the same day.
The government began to form local civilian defense patrols (PACs). Participation was in theory voluntary, but in practice, many rural Guatemalan men (including young boys and the elderly), especially in the northwest, had no choice but to join either the PACs or be considered guerrillas. At their peak, the PACs are estimated to have included 1 million conscripts. Ríos Montt's conscript army and PACs recaptured essentially all guerrilla territory. The insurgents' activity lessened and was largely limited to hit-and-run operations. Ríos Montt won this partial victory at an enormous cost in civilian deaths.
Ríos Montt's brief presidency was probably the most violent period of the 36-year internal conflict, which resulted in thousands of deaths of mostly unarmed indigenous civilians. Although leftist guerrillas and right-wing death squads also engaged in summary executions, forced disappearances, and torture of noncombatants, the vast majority of human rights violations were carried out by the Guatemalan military and the PACs they controlled. The internal conflict is described in great detail in the reports of the Historical Clarification Commission (CEH) and the Archbishop's Office for Human Rights (ODHAG). The CEH estimates that government forces were responsible for 93% of the violations; ODHAG earlier estimated that government forces were responsible for 80%.
On August 8, 1983, Ríos Montt was deposed by his Minister of Defense, General Óscar Humberto Mejía Victores, who succeeded him as "de facto" president of Guatemala. Mejía justified his coup, based on problems with "religious fanatics" in government and "official corruption". Seven people were killed in the coup. Ríos Montt survived to found a political party (the Guatemalan Republic Front) and to be elected President of Congress in 1995 and again in 2000.
Awareness in the United States of the conflict in Guatemala, and its ethnic dimension, increased with the 1983 publication of the "testimonial" account "I, Rigoberta Menchú", a memoir by a leading activist. Rigoberta Menchú was awarded the 1992 Nobel Peace Prize for her work in favor of broader social justice. In 1998 a book by U.S. anthropologist David Stoll challenged some of the details in Menchú's book, creating an international controversy. After the publication of Stoll's book, the Nobel Committee reiterated that it had awarded the Peace Prize based on Menchú's uncontested work promoting human rights and the peace process.
General Mejía allowed a managed return to democracy in Guatemala, starting with a July 1, 1984 election for a Constituent Assembly to draft a democratic constitution. On May 30, 1985, after nine months of debate, the Constituent Assembly finished drafting a new constitution, which took effect immediately. Vinicio Cerezo, a civilian politician and the presidential candidate of the Christian Democracy Party, won the first election held under the new constitution with almost 70% of the vote, and took office on January 14, 1986.
1986 to 1996: from constitution to peace accords.
Upon its inauguration in January 1986, President Cerezo's civilian government announced that its top priorities would be to end the political violence and establish the rule of law. Reforms included new laws of habeas corpus and "amparo" (court-ordered protection), the creation of a legislative human rights committee, and the establishment in 1987 of the Office of Human Rights Ombudsman. The Supreme Court embarked on a series of reforms to fight corruption and improve legal system efficiency.
With Cerezo's election, the military returned to the more traditional role of providing internal security, specifically by fighting armed insurgents. The first two years of Cerezo's administration were characterized by a stable economy and a marked decrease in political violence. Dissatisfied military personnel made two coup attempts in May 1988 and May 1989, but the military leadership supported the constitutional order. The government was strongly criticized for its reluctance to investigate or prosecute cases of human rights violations.
The final two years of Cerezo's government were marked by a failing economy, strikes, protest marches, and allegations of widespread corruption. The government's inability to deal with many of the nation's social and health problems — such as infant mortality, illiteracy, deficient health and social services, and rising levels of violence — contributed to popular discontent.
Presidential and congressional elections were held on November 11, 1990. After a runoff ballot, Jorge Antonio Serrano Elías was inaugurated on January 14, 1991, completing the first successful transition from one democratically elected civilian government to another. Because his Movement of Solidarity Action (MAS) Party gained only 18 of 116 seats in Congress, Serrano entered into a tenuous coalition with the Christian Democrats and the National Union of the Center (UCN) to form a government.
The Serrano administration's record was mixed. It had some success in consolidating civilian control over the army, replacing a number of senior officers and persuading the military to participate in peace talks with the URNG. He took the politically unpopular step of recognizing the sovereignty of Belize, which had long been officially, though fruitlessly, claimed as a province by Guatemala. The Serrano government reversed the economic slide it inherited, reducing inflation and boosting real growth.
In 1992 Efraín Bámaca, a notable guerrilla leader also known as Comandante Everardo, "disappeared." It was later found that Bámaca was tortured and killed that year by Guatemalan Army officers. His widow, the American Jennifer Harbury, and members of the Guatemala Human Rights Commission, based in Washington, DC, raised protests that ultimately led the United States to declassify documents going back to 1954 related to its actions in Guatemala. It was learned that the CIA had been funding the military, although Congress had prohibited such funding since 1990 because of the Army's human rights abuses. Congress forced the CIA to end its aid to the Guatemalan Army.
On May 25, 1993, Serrano illegally dissolved Congress and the Supreme Court and tried to restrict civil freedoms, allegedly to fight corruption. The "autogolpe" (palace coup) failed due to unified, strong protests by most elements of Guatemalan society, international pressure, and the army's enforcement of the decisions of the Court of Constitutionality, which ruled against the attempted takeover. In the face of this pressure, Serrano fled the country.
On June 5, 1993, Congress, pursuant to the 1985 constitution, elected the Human Rights Ombudsman, Ramiro de León Carpio, to complete Serrano's presidential term. De León was not a member of any political party. Lacking a political base but with strong popular support, he launched an ambitious anti-corruption campaign to "purify" Congress and the Supreme Court, demanding the resignations of all members of the two bodies.
Despite considerable congressional resistance, presidential and popular pressure led to a November 1993 agreement brokered by the Catholic Church between the administration and Congress. This package of constitutional reforms was approved by popular referendum on January 30, 1994. In August 1994, a new Congress was elected to complete the unexpired term. Controlled by the anti-corruption parties: the populist Guatemalan Republican Front (FRG) headed by Ríos Montt, and the center-right National Advancement Party (PAN), the new Congress began to abandon the corruption that characterized its predecessors.
Under de León, the peace process, now brokered by the United Nations, took on new life. The government and the URNG signed agreements on human rights (March 1994), resettlement of displaced persons (June 1994), historical clarification (June 1994), and indigenous rights (March 1995). They also made significant progress on a socioeconomic and agrarian agreement.
National elections for president, Congress, and municipal offices were held in November 1995. With almost 20 parties competing in the first round, the presidential election came down to a January 7, 1996 runoff in which PAN candidate Álvaro Arzú Irigoyen defeated Alfonso Portillo Cabrera of the FRG by just over 2% of the vote. Arzú won because of his strength in Guatemala City, where he had previously served as mayor, and in the surrounding urban area. Portillo won all of the rural departments except Petén. Under the Arzú administration, peace negotiations were concluded, and the government signed peace accords ending the 36-year internal conflict in December 1996. (See section on peace process)
1996 Peace Accords to present.
The human rights situation remained difficult during Arzú's tenure, although some initial steps were taken to reduce the influence of the military in national affairs. The most notable human rights case of this period was the brutal slaying of Bishop Juan José Gerardi on 24 April 1998, two days after he had publicly presented a major Catholic Church-sponsored human rights report known as "", summarizing testimony about human rights abuses during the Civil War. It was prepared by the Recovery of Historical Memory project, known by the acronym of REMHI. In 2001 three Army officers were convicted in civil court and sentenced to lengthy prison terms for his murder.
Guatemala held presidential, legislative, and municipal elections on November 7, 1999, and a runoff presidential election on December 26. Alfonso Portillo was criticized during the campaign for his relationship with the FRG's chairman, former president Ríos Montt. Many charge that some of the worst human rights violations of the internal conflict were committed under Ríos Montt's rule.
In the first round the Guatemalan Republican Front (FRG) won 63 of 113 legislative seats, while the National Advancement Party (PAN) won 37. The New Nation Alliance (ANN) won nine legislative seats, and three minority parties won the remaining four. In the runoff on December 26, Alfonso Portillo (FRG) won 68% of the vote to 32% for Óscar Berger (PAN). Portillo carried all 22 departments and Guatemala City, which was considered the PAN's stronghold.
Portillo's impressive electoral triumph, with two-thirds of the vote in the second round, gave him a mandate from the people to carry out his reform program. He pledged to maintain strong ties to the United States, enhance Guatemala's growing cooperation with Mexico, and join in the integration process in Central America and the Western Hemisphere. Domestically, he vowed to support continued liberalization of the economy, increase investment in human capital and infrastructure, establish an independent central bank, and increase revenue by stricter enforcement of tax collections rather than increasing taxation.
Portillo also promised to continue the peace process, appoint a civilian defense minister, reform the armed forces, replace the military presidential security service with a civilian one, and strengthen protection of human rights. He appointed a pluralist cabinet, including indigenous members and individuals who were independent of the FRG ruling party.
Progress in carrying out Portillo's reform agenda during his first year in office was slow. As a result, public support for the government sank to nearly record lows by early 2001. The administration made progress on such issues as taking state responsibility for past human rights cases and supporting human rights in international fora. It struggled to prosecute past human rights cases, and to achieve military reforms or a fiscal pact to help finance programs to implement peace. It is seeking legislation to increase political participation by residents.
The prosecution by Portillo's government of suspects in Bishop Gerardi's murder set a precedent in 2001; it was the first time military officers in Guatemala had been tried in civil courts.
Faced with a high crime rate, a public corruption problem, often violent harassment and intimidation by unknown assailants of human rights activists, judicial workers, journalists, and witnesses in human rights trials, the government began serious attempts in 2001 to open a national dialogue to discuss the considerable challenges facing the country.
In July 2003, the Jueves Negro demonstrations rocked the capital, forcing the closing of the US embassy and the UN mission. Supporters of Ríos Montt called for his return to power, demanding that the courts lift a ban against former coup leaders participating in government. They wanted Ríos Montt to run as a presidential candidate in the 2003 elections. The FRG fed the demonstrators.
On November 9, 2003, Óscar Berger, a former mayor of Guatemala city, won the presidential election with 38.8% of the vote. As he failed to achieve a fifty percent majority, he had to go through a runoff election on December 28, which he also won. He defeated the center-left candidate Álvaro Colom. Allowed to run, Ríos Montt trailed a distant third with 11% of the vote.
In early October 2005, Guatemala was devastated by Hurricane Stan. Although a relatively weak storm, it triggered a flooding disaster, resulting in at least 1,500 people dead and thousands homeless.
Determined to make progress against crime and internal police corruption, Óscar Berger in December 2006 came to agreement with the United Nations to gain support for judicial enforcement of its laws. They created the International Commission against Impunity in Guatemala (CICIG), an independent institution, which is to assist the Office of the Prosecutor of Guatemala, the National Police Force, and other investigative institutions. Their goal was to prosecute cells linked to organised crime and to drug trafficking. CICIG has the authority to conduct its own inquiries, and to refer the most significant cases to the national judiciary. The stated objective of CICIG is to "reinforce the national criminal justice system and to help it with its reforms."
As of 2010, CICIG has led inquiries into some 20 cases. It is acting as Deputy Prosecutor in eight other cases. CICIG conducted the investigations leading to an arrest warrant against Erwin Sperisen, former Head of the National Civilian Police (Policia Nacional Civil – PNC) from 2004 to 2007. With dual Swiss-Guatemalan citizenship, he fled to Switzerland to escape prosecution in Guatemala for numerous extrajudicial killings and police corruption. In addition, 17 other persons are covered by arrest warrants related to these crimes, including several former highly placed political figures of Guatemala.
The 2007 presidential election was won by the centre-left Álvaro Colóm.
Bibliography.
</dl>

</doc>
<doc id="12191" url="http://en.wikipedia.org/wiki?curid=12191" title="Economy of Guinea-Bissau">
Economy of Guinea-Bissau

Guinea-Bissau is among the world's least developed nations and one of the 10 poorest countries in the world, and depends mainly on agriculture and fishing. Cashew crops have increased remarkably in recent years, and the country now ranks sixth in cashew production.
Guinea-Bissau exports fish and seafood along with small amounts of peanuts, palm kernels, and timber. License fees for fishing provide the government with some revenue. Rice is the major crop and staple food.
Economic history.
Early colonialism.
From a European viewpoint, the economic history of the Guinea Coast is largely associated with slavery. Indeed one of the alternative names for the region was the Slave Coast. When the Portuguese first sailed down the Atlantic coast of Africa in the 1430s, they were interested in gold. Ever since Mansa Musa, king of the Mali Empire, made his pilgrimage to Mecca in 1325, with 500 slaves and 100 camels (each carrying gold) the region had become synonymous with such wealth. The trade from sub-Saharan Africa was controlled by the Islamic Empire which stretched along Africa's northern coast. Muslim trade routes across the Sahara, which had existed for centuries, involved salt, kola, textiles, fish, grain and slaves.
As the Portuguese extended their influence around the coast, Mauritania, Senegambia (by 1445) and Guinea, they created trading posts. Rather than becoming direct competitors to the Muslim merchants, the expanding market opportunities in Europe and the Mediterranean resulted in increased trade across the Sahara. In addition, the Portuguese merchants gained access to the interior via the Sénégal and Gambia rivers which bisected long-standing trans-Saharan routes.
The Portuguese brought in copper ware, cloth, tools, wine and horses. Trade goods soon also included arms and ammunition. In exchange, the Portuguese received gold (transported from mines of the Akan deposits), pepper (a trade which lasted until Vasco da Gama reached India in 1498) and ivory.
There was a very small market for African slaves as domestic workers in Europe, and as workers on the sugar plantations of the Mediterranean. The Portuguese found they could make considerable amounts of gold transporting slaves from one trading post to another, along the Atlantic coast of Africa. Muslim merchants had a high demand for slaves, which were used as porters on the trans-Saharan routes, and for sale in the Islamic Empire. The Portuguese found Muslim merchants entrenched along the African coast as far as the Bight of Benin.
Before the arrival of the Europeans, the African slave trade, centuries old in Africa, was not yet the major feature of the coastal economy of Guinea. The expansion of trade occurs after the Portuguese reach this region in 1446, bringing great wealth to several local slave trading tribes. The Portuguese used slave labour to colonize and develop the previously uninhabited Cape Verde islands where they founded settlements and grew cotton and indigo. They then traded these goods, in the estuary of the Geba River, for black slaves captured by other black peoples in local African wars and raids.
The slaves were sold in Europe and, from the 16th century, in the Americas. The Company of Guinea was a Portuguese governative institution whose task was to deal with the spices and to fix the prices of the goods. It was called "Casa da Guiné", "Casa da Guiné e Mina" from 1482 to 1483 and "Casa da Índia e da Guiné" in 1499. The local African rulers in Guinea, who prosper greatly from the slave trade, have no interest in allowing the Europeans any further inland than the fortified coastal settlements where the trading takes place. The Portuguese presence in Guinea was therefore largely limited to the port of Bissau.
Colonial era.
As with the other Portuguese territories in mainland Africa (Portuguese Angola and Portuguese Mozambique), Portugal exercised control over the coastal areas of Portuguese Guinea when first laying claim to the whole region as a colony. For three decades there are costly and continuous campaigns to suppress the local African rulers. By 1915 this process was complete, enabling Portuguese colonial rule to progress in a relatively unruffled state - until the emergence of nationalist movements all over Africa in the 1950s.
For a brief period in the 1790s the British attempted to establish a rival foothold on an offshore island, at Bolama, but by the 19th century the Portuguese were sufficiently secure in Bissau to regard the neighbouring coastline as their own special territory. It was therefore natural for Portugal to lay claim to this region, soon to be known as Portuguese Guinea, when the European scramble for Africa began in the 1880s. Britain's interest in the region declined since the end of the British slave trade in 1807. After the abolition of slavery in the Portuguese overseas territories in the 1830s, the slave trade went into serious decline.
Portugal's main rival were the French, their colonial neighbours along the coast on both sides - in Senegal and in the region which became French Guinea. The Portuguese presence in Guinea was not disputed by the French. The only point at issue was the precise line of the borders. This was established by agreement between the two colonial powers in two series of negotiations, in 1886 and 1902-5. Until the end of the 19th century, rubber was the main export.
As an overseas province.
In 1951, when the Portuguese government overhauled the entire colonial system, all Portugal's colonies, including Portuguese Guinea, were renamed Overseas Provinces ("Províncias Ultramarinas"). New infrastructures were built for education, health, agriculture, transportation, commerce, services, and administration.
Cashew, peanut, rice, timber, livestock and fish were the main economic productions. The port of Bissau was one of the main employers and a very important source of taxes for the province's authorities.
Independence war.
The fight for independence began in 1956, when Amílcar Cabral founded the "Partido Africano da Independência da Guiné e Cabo Verde" (Portuguese: "African Party for the Independence of Guinea and Cape Verde"), the PAIGC. In 1961, when a purely political campaign for independence had made predictably little progress, the PAIGC adopted guerrilla tactics. Although heavily outnumbered by Portuguese troops (approximately 30,000 Portuguese to some 10,000 guerrillas), the PAIGe had the great advantage of safe havens over the border in Senegal and Guinea, both recently independent of French rule.
Several communist countries supported the guerrillas with weapons and military training. The conflict in Portuguese Guinea involving the PAIGC guerrillas and the Portuguese Army was the most intense and damaging of all Portuguese Colonial War. Thus, during the 1960s and early 1970s, Portuguese development plans promoting strong economic growth and effective socioeconomic policies, like those applied by the Portuguese in the other two theaters of war (Portuguese Angola and Portuguese Mozambique), were not possible. In 1972 Cabral sets up a government in exile in Conakry, the capital of neighbouring Guinea. It was there, in 1973, that he was assassinated outside his house - just a year before a left-wing military coup in Portugal dramatically altered the political situation.
By 1973 the PAIGC controlled most of the interior of the country, while the coastal and estuary towns, including the main populational and economic centres remained under Portuguese control. The village of Madina do Boé in the southeasternmost area of the territory, close to the border with neighbouring Guinea, was the location where PAIGC guerrillas declared the independence of Guinea-Bissau on September 24, 1973. The war in the colonies was increasingly unpopular in Portugal itself as the people got weary of war and balked at its ever-rising expense. Following the coup d'état in Portugal in 1974, the new left-wing revolutionary government of Portugal began to negotiate with the PAIGC and decided to offer independence to all the overseas territories.
After independence.
As his brother Amílcar Cabral had been assassinated in 1973, Luís Cabral became the first president of independent Guinea-Bissau in the time after independence was granted on September 10, 1974. Already as the President of Guinea-Bissau, Luís Cabral tried to impose a planned economy in the country, and supported a socialist model that left the economy of Guinea-Bissau itself ruined. Similarly, the repression the authoritarian single-party regime he led imposed on the population and severe food shortages also left marks and, despite having always denied, Luís Cabral was accused of being responsible for the death of a large number of black Guinea-Bissauan soldiers who had fought along with the Portuguese Army against the PAIGC guerrillas during the Portuguese Colonial War.
Luís Cabral served from 1974 to 1980, when a military "coup d'état" led by João Bernardo "Nino" Vieira deposed him. After the military coup, in 1980 PAIGC admitted in its official newspaper "Nó Pintcha" (dated November 29, 1980) that many were executed and buried in unmarked collective graves in the woods of Cumerá, Portogole and Mansabá. All these events did not help the new-country to reach the level of prosperity, economic growth and development the new rulers had promised to its population.
Present day.
Macro-economic trend.
This is a chart of trend of gross domestic product of Guinea-Bissau at market prices estimated by the International Monetary Fund and EconStats with figures in millions of CFA Francs.
Current GDP per capita of Guinea-Bissau grew just 3.40% in the turbulent 1970s and reached a peak growth of 71% in the 1980s. But this proved unsustainable and it consequently shrank by 34% in the 1990s. Average wages in 2007 hover around $1–2 per day.
Intermittent fighting between Senegalese-backed government troops and a military junta destroyed much of the country's infrastructure and caused widespread damage to the economy in 1998; the civil war led to a 28% drop in GDP that year, with partial recovery in 1999. Agricultural production is estimated to have fallen by 17% during the conflict, and the civil war led to a 28% overall drop in GDP in 1998. Cashew nut output, the main export crop, declined in 1998 by an estimated 30%. World cashew prices dropped by more than 50% in 2000, compounding the economic devastation caused by the conflict. Real GDP has steadily grown at an average of 2.3% from 2003 onwards.
Before the war, trade reform and price liberalization were the most successful part of the country's structural adjustment program under IMF sponsorship. The tightening of monetary policy and the development of the private sector had also begun to reinvigorate the economy. Under the government’s post-conflict economic and financial program, implemented with IMF and World Bank input, real GDP recovered in 1999 by almost 8%. In December 2000 Guinea-Bissau qualified for almost $800 million in debt-service relief under the first phase of the enhanced HIPC initiative and is scheduled to submit its Poverty Reduction Strategy Paper in March 2002. Guinea-Bissau will receive the bulk of its assistance under the enhanced HIPC initiative when it satisfies a number of conditions, including implementation of its Poverty Reduction Strategy Paper.
Because of high costs, the development of petroleum, phosphate, and other mineral resources is not a near-term prospect. It produces 400,000 barrels/day of petrol.
Mean wages were $0.52 per manhour in 2009.
Income from waste dumping.
In the 1980s Guinea-Bissau was part of a trend in the African continent toward the dumping of waste as a source of income. Plans to import toxic waste from Europe were cancelled after an international campaign to halt the trade. The government was offered a contract to dispose of 15 million tons of toxic waste over a 15-year period. The income from it was equivalent to twice the value of its external debt. After strong pressure from other African countries and environmental groups the Guinea-Bissau government renounced the deal.
Drug trafficking.
Over the last decade European consumption of cocaine is believed to have tripled, and West Africa has become a primary transit point for trafficking the drug from Colombia to Europe. Guinea-Bissau is the leading West African country in this regard, with smugglers taking advantage of government corruption and disorder to operate unimpeded.
The army and police are alleged to be complicit and turn a blind eye to drug shipments from Latin America. Planes fly in, and sometimes use Guinea-Bissau's 88 remote islands, the majority of which are uninhabited.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="12223" url="http://en.wikipedia.org/wiki?curid=12223" title="Great Man theory">
Great Man theory

The Great Man theory is a 19th-century idea according to which history can be largely explained by the impact of "great men", or heroes: highly influential individuals who, due to either their personal charisma, intelligence, wisdom, or political skill utilized their power in a way that had a decisive historical impact. The theory was popularized in the 1840s by Scottish writer Thomas Carlyle. But in 1860 Herbert Spencer formulated a counter-argument that has remained influential throughout the 20th century to the present; Spencer said that such great men are the products of their societies, and that their actions would be impossible without the social conditions built before their lifetimes.
Overview.
Carlyle stated that "The history of the world is but the biography of great men", reflecting his belief that heroes shape history through both their personal attributes and divine inspiration. In his book "On Heroes, Hero-Worship and the Heroic in History", Carlyle set out how he saw history as having turned on the decisions of "heroes", giving detailed analysis of the influence of several such men (including Muhammad, Shakespeare, Luther, Rousseau, and Napoleon). Carlyle also felt that the study of great men was "profitable" to one's own heroic side; that by examining the lives led by such heroes, one could not help but uncover something about one's true nature.
Alongside with Carlyle the Great Man theory was supported by American scholar Frederick Adams Woods. In his work "The Influence of Monarchs: Steps in a New Science of History" Woods investigated 386 rulers in Western Europe from the 12th century till the French revolution in the late 18th century and their influence on the course of historical events.
This theory is usually contrasted with a theory that talks about events occurring in the fullness of time, or when an overwhelming wave of smaller events causes certain developments to occur. The Great Man approach to history was most fashionable with professional historians in the 19th century; a popular work of this school is the "Encyclopædia Britannica Eleventh Edition" (1911) which contains lengthy and detailed biographies about the great men of history, but very few general or social histories. For example, all information on the post-Roman "Migrations Period" of European History is compiled under the biography of Attila the Hun. This heroic view of history was also strongly endorsed by some philosophical figures such as Hegel, Kierkegaard, Nietzsche, and Spengler, but it fell out of favor after World War II.
In "Untimely Meditations", Nietzsche writes that: "...the goal of humanity lies in its highest specimens".
In "Fear and Trembling", Kierkegaard writes that: "...to be able to fall down in such a way that the same second it looks as if one were standing and walking, to transform the leap of life into a walk, absolutely to express the sublime and the pedestrian — that only these knights of faith can do — this is the one and only prodigy."
Hegel, proceeding from providentialist theory, argued that "what is real is reasonable" and World-Historical individuals are World-Spirit's agents. Hegel opined: "Such are great historical men—whose own particular aims involve those large issues which are the will of the World-Spirit." Thus, according to Hegel, a great man does not create historical reality himself but only uncovers the inevitable future.
Criticism.
One of the most forceful critics of Carlyle's formulation of the Great Man theory was Herbert Spencer, who believed that attributing historical events to the decisions of individuals was a hopelessly primitive, childish, and unscientific position. He believed that the men Carlyle called "great men" were merely products of their social environment.
"[Y]ou must admit that the genesis of a great man depends on the long series of complex influences which has produced the race in which he appears, and the social state into which that race has slowly grown... Before he can remake his society, his society must make him."—Herbert Spencer, "The Study of Sociology"
Tolstoy's "War and Peace" features criticism of Great Man Theories as a recurring theme
in the philosophical digressions. According to Tolstoy, the significance of great individuals is imaginary; as a matter of fact they are only "history's slaves" realizing the decree of Providence.
William James in his lecture 'Great Men and Their Environment' underlined the importance of the Great Man's congruence with the surroundings (in the broad sense), though his ultimate point was that environments and individuals shape each other reciprocally, just as environments and individual members of animal species do according to Darwinian theory.
Among modern critics of the theory of Great Man, one, Sidney Hook, is supportive of the idea; he gives credit to those who shape events through their actions, and his book "The Hero in History" is devoted to the role of the hero and in history and influence of the outstanding persons.
Leonid Grinin defines a historical figure (a Great Man) thus:
"Owing to his personal features, or to a chance, or to his social standing, or to the peculiarity of the epoch, an individual by the very fact of his existence, by his ideas or actions (or inaction) directly or indirectly, during his lifetime or after his death may have such an influence upon his own or another society which can be recognized significant as he left a noticeable mark (positive, negative or unambiguous) in history and in the further development of society."
So, he concludes that the role of Great Man depends on a number of factors, or none at all.

</doc>
<doc id="12229" url="http://en.wikipedia.org/wiki?curid=12229" title="Government">
Government

A government is the system by which a state or community is governed. In the Commonwealth of Nations, the word "government" is also used more narrowly to refer to the collective group of people that exercises executive authority in a state. This usage is analogous to what is called an "administration" in American English. Furthermore, especially in American English, the concepts of "the state" and "the government" may be used synonymously to refer to the person or group of people exercising authority over a politically organized territory. Finally, "government" is also sometimes used in English as a synonym for "governance".
In the case of its broad associative definition, government normally consists of legislators, administrators, and arbitrators. Government is the means by which state policy is enforced, as well as the mechanism for determining the policy of the state. A form of government, or form of state governance, refers to the set of political systems and institutions that make up the organisation of a specific government.
Government of any kind currently affects every human activity in many important ways. For this reason, political scientists generally argue that government should not be studied by itself; but should be studied along with anthropology, economics, environmentalism, history, philosophy, science, and sociology.
Political science.
Classifying government.
In political science, it has long been a goal to create a typology or taxonomy of polities, as typologies of political systems are not obvious. It is especially important in the political science fields of comparative politics and international relations.
On the surface, identifying a form of government appears to be easy, as all governments have an official form. The United States is a federal republic, while the former Soviet Union was a socialist republic. However self-identification is not objective, and as Kopstein and Lichbach argue, defining regimes can be tricky. For example, elections are a defining characteristic of a democracy, but in practice elections in the former Soviet Union were not "free and fair" and took place in a single party state. Thus in many practical classifications it would not be considered democratic.
Identifying a form of government is also difficult because a large number of political systems originate as socio-economic movements and are then carried into governments by specific parties naming themselves after those movements; all with competing political-ideologies. Experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.
Other complications include general non-consensus or deliberate "distortion or bias" of reasonable technical definitions to political ideologies and associated forms of governing, due to the nature of politics in the modern era. For example: The meaning of "conservatism" in the United States has little in common with the way the word's definition is used elsewhere. As Ribuffo (2011) notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism". Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.
Every country in the world is ruled by a system of governance that combines at least 2 (or more) of the following attributes (for example, the United States is not a true capitalist society, since the government actually provides social services for its citizens). Additionally, one person's opinion of the type of government may differ from another's (for example, some may argue that the United States is a plutocracy rather than a democracy since they may believe it is ruled by the wealthy). There are always shades of gray in any government. Even the most liberal democracies limit rival political activity to one extent or another, and even the most tyrannical dictatorships must organise a broad base of support, so it is very difficult "pigeonholing" every government into narrow categories.
The dialectical forms of government.
The Classical Greek philosopher Plato discusses five types of regimes. They are aristocracy, timocracy, oligarchy, democracy and tyranny. Plato also assigns a man to each of these regimes to illustrate what they stand for. The tyrannical man would represent tyranny for example. These five regimes progressively degenerate starting with aristocracy at the top and tyranny at the bottom.
In "Republic", while Plato spends much time having Socrates narrate a conversation about the city he founds with Glaucon and Adeimantus "in speech", the discussion eventually turns to considering four regimes that exist in reality and tend to degrade successively into each other: timocracy, oligarchy (also called plutocracy), democracy and tyranny (also called despotism).
Etymology.
"", prefix derived from the Greek archon, 'rulership', which means "higher in hierarchy". The Greek word κράτος "krátos", 'power', which means "right to lead" is the suffix root in words like "aristocrat" and "democracy".
Forms of government by associated attributes.
Descriptions of governments can be based on the following attributes:
By elements of where decision-making power is held.
Aristarchic attributes.
Governments with "aristarchy" attributes are traditionally controlled and organised by a small group of the most-qualified people, with no intervention from the most part of society; this small group usually shares some common trait. The opposite of an aristarchic government is kakistocracy.
Autocratic attributes.
Governments with "autocratic " attributes are dominated by one person who has all the power over the people in a country. The Roman Republic made "dictators" to lead during times of war; the Roman dictators only held power for a small time. In modern times, an autocrat's rule is not stopped by any rules of law, constitutions, or other social and political institutions. After World War II, many governments in Latin America, Asia, and Africa were ruled by autocratic governments. Examples of autocrats include Idi Amin, Muammar Gaddafi, Adolf Hitler and Gamal Abdul Nasser.
Monarchic attributes.
Governments with "monarchic" attributes are ruled by a king/emperor or a queen/empress who usually holds their position for life. There are two types of monarchies: absolute monarchies and constitutional monarchies. In an absolute monarchy, the ruler has no limits on their wishes or powers. In a constitutional monarchy a ruler's powers are limited by a document called a constitution. The constitution was put in place to put a check to these powers.
Pejorative attributes.
Regardless of the form of government, the actual governance may be influenced by sectors with political power which are not part of the formal government. Certain actions of the governors, such as corruption, demagoguery, or fear mongering, may disrupt the intended way of working of the government if they are widespread enough. 
By elements of who elects the empowered.
Democratic attributes.
Governments with "democratic" attributes are most common in the Western world and in some countries of the east that have been influenced by western society, often by being colonised by western powers over the course of history. In democracies, large proportions of the population may vote, either to make decisions or to choose representatives to make decisions. Commonly significant in democracies are political parties, which are groups of people with similar ideas about how a country or region should be governed. Different political parties have different ideas about how the government should handle different problems.
Oligarchic attributes.
Governments with "oligarchic" attributes are ruled by a small group of segregated, powerful and/or influential people, who usually share similar interests and/or family relations. These people may spread power and elect candidates equally or not equally. An oligarchy is different from a true democracy because very few people are given the chance to change things. An oligarchy does not have to be hereditary or monarchic. An oligarchy does not have one clear ruler, but several rulers.
Some historical examples of oligarchy are the former Union of Soviet Socialist Republics. Some critics of representative democracy think of the United States as an oligarchy. The Athenian democracy used sortition to elect candidates, almost always male, white, Greek, educated citizens holding a minimum of land, wealth and status.
By elements of how power distribution is structured.
Republican attributes.
A republic is a form of government in which the country is considered a "public matter" (Latin: res publica), not the private concern or property of the rulers, and where offices of states are subsequently directly or indirectly elected or appointed rather than inherited. 
Federalism attributes.
Federalism is a political concept in which a "group" of members are bound together by covenant (Latin: "foedus", covenant) with a governing representative head. The term "federalism" is also used to describe a system of government in which sovereignty is constitutionally divided between a central governing authority and constituent political units (such as states or provinces). Federalism is a system based upon democratic rules and institutions in which the power to govern is shared between national and provincial/state governments, creating what is often called a federation. Proponents are often called federalists.
Forms of government by other characteristic attributes.
By socio-economic system attributes.
Historically, most political systems originated as socioeconomic ideologies; experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.
By significant constitutional attributes.
Certain major characteristics are defining of certain types; others are historically associated with certain types of government.
By approach to regional autonomy.
This list focuses on differing approaches that political systems take to the distribution of sovereignty, and the autonomy of regions within the state.
Theoretical and speculative attributes.
These currently have no citable real-world examples outside of fiction.

</doc>
<doc id="12236" url="http://en.wikipedia.org/wiki?curid=12236" title="GDP (disambiguation)">
GDP (disambiguation)

GDP or gross domestic product is the basic measure of a country's overall economic output.
GDP may also refer to:

</doc>
<doc id="12240" url="http://en.wikipedia.org/wiki?curid=12240" title="Gold">
Gold

Gold is a chemical element with symbol Au (from Latin: "aurum") and atomic number 79. In its purest form, it is a bright, slightly reddish yellow, dense, soft, malleable and ductile metal. Chemically, gold is a transition metal and a group 11 element. It is one of the least reactive chemical elements, and is solid under standard conditions. The metal therefore occurs often in free elemental (native) form, as nuggets or grains, in rocks, in veins and in alluvial deposits. It occurs in a solid solution series with the native element silver (as electrum) and also naturally alloyed with copper and palladium. Less commonly, it occurs in minerals as gold compounds, often with tellurium (gold tellurides).
Gold's atomic number of 79 makes it one of the higher atomic number elements that occur naturally in the universe. It is thought to have been produced in supernova nucleosynthesis and to have been present in the dust from which the Solar System formed. Because the Earth was molten when it was just formed, almost all of the gold present in the early Earth probably sank into the planetary core. Therefore most of the gold that is present today in the Earth's crust and mantle is thought to have been delivered to Earth later, by asteroid impacts during the late heavy bombardment, about 4 billion years ago.
Gold resists attacks by individual acids, but it can be dissolved by aqua regia ("royal water" [nitro-hydrochloric acid], so named because it dissolves "the king of metals"). The acid mixture causes the formation of a soluble gold tetrachloride anion. Gold metal also dissolves in alkaline solutions of cyanide, which are used in mining and electroplating. It dissolves in mercury, forming amalgam alloys; it is insoluble in nitric acid, which dissolves silver and base metals, a property that has long been used to confirm the presence of gold in items, giving rise to the term "acid test."
This metal has been a valuable and highly sought-after precious metal for coinage, jewelry, and other arts since long before the beginning of recorded history. In the past, a gold standard was often implemented as a monetary policy within and between nations, but gold coins ceased to be minted as a circulating currency in the 1930s, and the world gold standard (see article for details) was finally abandoned for a fiat currency system after 1976. The historical value of gold was rooted in its medium rarity, easy handling and minting, easy smelting, non-corrodability, distinct color, and non-reactivity to other elements.
A total of 174,100 tonnes of gold have been mined in human history, according to GFMS as of 2012. This is roughly equivalent to 5.6 billion troy ounces or, in terms of volume, about 9020 m3, or a cube 21 m on a side. The world consumption of new gold produced is about 50% in jewelry, 40% in investments, and 10% in industry.
Gold’s high malleability, ductility, resistance to corrosion and most other chemical reactions, and conductivity of electricity have led to its continued use in corrosion resistant electrical connectors in all types of computerized devices (its chief industrial use). Gold is also used in infrared shielding, colored-glass production, and gold leafing. Certain gold salts are still used as anti-inflammatories in medicine.
Etymology.
"Gold" is cognate with similar words in many Germanic languages, deriving via Proto-Germanic *"gulþą" from Proto-Indo-European *"ǵʰelh₃-" ("to shine, to gleam; to be yellow or green").
The symbol Au is from the Latin: "aurum", the Latin word for "gold". The Proto-Indo-European ancestor of "aurum" was "*h₂é-h₂us-o-", meaning "glow". This word is derived from the same root (Proto-Indo-European "*h₂u̯es-" "to dawn") as "*h₂éu̯sōs", the ancestor of the Latin word Aurora, "dawn". This etymological relationship is presumably behind the frequent claim in scientific publications that "aurum" meant "shining dawn".
Characteristics.
Gold is the most malleable of all metals; a single gram can be beaten into a sheet of 1 square meter, or an ounce into 300 square feet. Gold leaf can be beaten thin enough to become transparent. The transmitted light appears greenish blue, because gold strongly reflects yellow and red. Such semi-transparent sheets also strongly reflect infrared light, making them useful as infrared (radiant heat) shields in visors of heat-resistant suits, and in sun-visors for spacesuits. Gold is a good conductor of heat and electricity and reflects infrared radiation strongly.
In addition, gold is very dense: a cubic meter has a mass of 19,300 kg. By comparison, the density of lead is 11,340 kg/m3, and that of the densest element, osmium, is 22,588 ± 15 kg/m3.
Chemistry.
Gold is unaffected by oxygen at any temperature; similarly, it does not react with ozone. Gold is strongly attacked by fluorine at dull-red heat to form gold(III) fluoride.
Powdered gold reacts with chlorine at 180℃ to form AuCl3. Gold reacts with bromine at 140℃ to form gold(III) bromide, but reacts only very slowly with iodine to form the monoiodide.
Gold does not react with sulfur directly, but gold(III) sulfide can be made by passing hydrogen sulfide through a dilute solution of gold(III) chloride or chlorauric acid.
Gold readily dissolves in mercury at room temperature to form an amalgam, and forms alloys with many other metals at higher temperatures. These alloys can be produced to modify the hardness and other metallurgical properties, to control melting point or to create exotic colors.
Gold reacts with potassium, rubidium, or caesium, to form the respective auride salts, containing the Au- ion. Caesium auride is perhaps the most famous.
Gold is unaffected by most acids. It does not react with hydrofluoric, hydrochloric, hydrobromic, hydriodic, sulfuric, or nitric acid. It does react with selenic acid when hot and concentrated, as well as mixture of nitric and hydrochloric acids known as aqua regia.
Gold is similarly unaffected by most bases. It does not react with aqueous, solid, or molten sodium or potassium hydroxide. It does however, react with sodium or potassium cyanide under alkaline conditions when oxygen is present to form soluble complexes.
Common oxidation states of gold include +1 (gold(I) or aurous compounds) and +3 (gold(III) or auric compounds). Gold ions in solution are readily reduced and precipitated as metal by adding any other metal as the reducing agent. The added metal is oxidized and dissolves, allowing the gold to be displaced from solution and be recovered as a solid precipitate.
Color.
Whereas most other pure metals are gray or silvery white, gold is slightly reddish yellow. This color is determined by the density of loosely bound (valence) electrons; those electrons oscillate as a collective "plasma" medium described in terms of a quasiparticle called a plasmon. The frequency of these oscillations lies in the ultraviolet range for most metals, but it falls into the visible range for gold due to subtle relativistic effects that affect the orbitals around gold atoms. Similar effects impart a golden hue to metallic caesium.
Common colored gold alloys such as rose gold can be created by the addition of various amounts of copper and silver, as indicated in the triangular diagram to the left. Alloys containing palladium or nickel are also important in commercial jewelry as these produce white gold alloys. Less commonly, addition of manganese, aluminium, iron, indium and other elements can produce more unusual colors of gold for various applications.
Isotopes.
Gold has only one stable isotope, 197Au, which is also its only naturally occurring isotope. Thirty-six radioisotopes have been synthesized ranging in atomic mass from 169 to 205. The most stable of these is 195Au with a half-life of 186.1 days. The least stable is 171Au, which decays by proton emission with a half-life of 30 µs. Most of gold's radioisotopes with atomic masses below 197 decay by some combination of proton emission, α decay, and β+ decay. The exceptions are 195Au, which decays by electron capture, and 196Au, which decays most often by electron capture (93%) with a minor β− decay path (7%). All of gold's radioisotopes with atomic masses above 197 decay by β− decay.
At least 32 nuclear isomers have also been characterized, ranging in atomic mass from 170 to 200. Within that range, only 178Au, 180Au, 181Au, 182Au, and 188Au do not have isomers. Gold's most stable isomer is 198m2Au with a half-life of 2.27 days. Gold's least stable isomer is 177m2Au with a half-life of only 7 ns. 184m1Au has three decay paths: β+ decay, isomeric transition, and alpha decay. No other isomer or isotope of gold has three decay paths.
Modern applications.
The world consumption of new gold produced is about 50% in jewelry, 40% in investments, and 10% in industry.
Jewelry.
Because of the softness of pure (24k) gold, it is usually alloyed with base metals for use in jewelry, altering its hardness and ductility, melting point, color and other properties. Alloys with lower carat rating, typically 22k, 18k, 14k or 10k, contain higher percentages of copper or other base metals or silver or palladium in the alloy. Copper is the most commonly used base metal, yielding a redder color.
Eighteen-carat gold containing 25% copper is found in antique and Russian jewelry and has a distinct, though not dominant, copper cast, creating rose gold. Fourteen-carat gold-copper alloy is nearly identical in color to certain bronze alloys, and both may be used to produce police and other badges. Blue gold can be made by alloying with iron and purple gold can be made by alloying with aluminium, although rarely done except in specialized jewelry. Blue gold is more brittle and therefore more difficult to work with when making jewelry.
Fourteen- and eighteen-carat gold alloys with silver alone appear greenish-yellow and are referred to as green gold. White gold alloys can be made with palladium or nickel. White 18-carat gold containing 17.3% nickel, 5.5% zinc and 2.2% copper is silvery in appearance. Nickel is toxic, however, and its release from nickel white gold is controlled by legislation in Europe.
Alternative white gold alloys are available based on palladium, silver and other white metals, but the palladium alloys are more expensive than those using nickel. High-carat white gold alloys are far more resistant to corrosion than are either pure silver or sterling silver. The Japanese craft of Mokume-gane exploits the color contrasts between laminated colored gold alloys to produce decorative wood-grain effects.
By 2014 the gold jewelry industry was escalating despite a dip in gold prices. Demand in the first quarter of 2014 pushed turnover to $23.7 billion according to a World Gold Council report.
Investment.
Many holders of gold store it in form of bullion coins or bars as a hedge against inflation or other economic disruptions. However, economist Martin Feldstein does not believe gold serves as a hedge against inflation or currency depreciation.
The ISO 4217 currency code of gold is XAU.
Modern bullion coins for investment or collector purposes do not require good mechanical wear properties; they are typically fine gold at 24k, although the American Gold Eagle and the British gold sovereign continue to be minted in 22k (0.92) metal in historical tradition, and the South African Krugerrand, first released in 1967, is also 22k (0.92). The "special issue" Canadian Gold Maple Leaf coin contains the highest purity gold of any bullion coin, at 99.999% or 0.99999, while the "popular issue" Canadian Gold Maple Leaf coin has a purity of 99.99%.
Several other 99.99% pure gold coins are available. In 2006, the United States Mint began producing the American Buffalo gold bullion coin with a purity of 99.99%. The Australian Gold Kangaroos were first coined in 1986 as the Australian Gold Nugget but changed the reverse design in 1989. Other modern coins include the Austrian Vienna Philharmonic bullion coin and the Chinese Gold Panda.
Electronics connectors.
Only 10% of the world consumption of new gold produced goes to industry, but by far the most important industrial use for new gold is in fabrication of corrosion-free electrical connectors in computers and other electrical devices. For example, according to the World Gold council, a typical cell phone may contain 50 mg of gold, worth about 50 cents. But since nearly one billion cell phones are produced each year, a gold value of 50 cents in each phone adds to $500 million in gold from just this application.
Though gold is attacked by free chlorine, its good conductivity and general resistance to oxidation and corrosion in other environments (including resistance to non-chlorinated acids) has led to its widespread industrial use in the electronic era as a thin-layer coating on electrical connectors, thereby ensuring good connection. For example, gold is used in the connectors of the more expensive electronics cables, such as audio, video and USB cables. The benefit of using gold over other connector metals such as tin in these applications has been debated; gold connectors are often criticized by audio-visual experts as unnecessary for most consumers and seen as simply a marketing ploy. However, the use of gold in other applications in electronic sliding contacts in highly humid or corrosive atmospheres, and in use for contacts with a very high failure cost (certain computers, communications equipment, spacecraft, jet aircraft engines) remains very common.
Besides sliding electrical contacts, gold is also used in electrical contacts because of its resistance to corrosion, electrical conductivity, ductility and lack of toxicity. Switch contacts are generally subjected to more intense corrosion stress than are sliding contacts. Fine gold wires are used to connect semiconductor devices to their packages through a process known as wire bonding.
The concentration of free electrons in gold metal is 5.90×1022 cm−3. Gold is highly conductive to electricity, and has been used for electrical wiring in some high-energy applications (only silver and copper are more conductive per volume, but gold has the advantage of corrosion resistance). For example, gold electrical wires were used during some of the Manhattan Project's atomic experiments, but large high-current silver wires were used in the calutron isotope separator magnets in the project.
Commercial chemistry.
Gold is attacked by and dissolves in alkaline solutions of potassium or sodium cyanide, to form the salt gold cyanide—a technique that has been used in extracting metallic gold from ores in the cyanide process. Gold cyanide is the electrolyte used in commercial electroplating of gold onto base metals and electroforming.
Gold chloride (chloroauric acid) solutions are used to make colloidal gold by reduction with citrate or ascorbate ions. Gold chloride and gold oxide are used to make cranberry or red-colored glass, which, like colloidal gold suspensions, contains evenly sized spherical gold nanoparticles.
Medicine.
Metallic and gold compounds have been used for medicinal purposes historically and are still in use. The apparent paradox of the actual toxicology of the substance suggests the possibility of serious gaps in the understanding of the action of gold in physiology.
Gold (usually as the metal) is perhaps the most anciently administered medicine (apparently by shamanic practitioners) and known to Dioscorides. In medieval times, gold was often seen as beneficial for the health, in the belief that something so rare and beautiful could not be anything but healthy. Even some modern esotericists and forms of alternative medicine assign metallic gold a healing power.
In the 19th century gold had a reputation as a "nervine," a therapy for nervous disorders. Depression, epilepsy, migraine, and glandular problems such as amenorrhea and impotence were treated, and most notably alcoholism (Keeley, 1897).
Only salts and radioisotopes of gold are of pharmacological value, since elemental (metallic) gold is inert to all chemicals it encounters inside the body (i.e., ingested gold cannot be attacked by stomach acid). Some gold salts do have anti-inflammatory properties and at present two are still used as pharmaceuticals in the treatment of arthritis and other similar conditions in the US (sodium aurothiomalate and auranofin). These drugs have been explored as a means to help to reduce the pain and swelling of rheumatoid arthritis, and also (historically) against tuberculosis and some parasites.
Gold alloys are used in restorative dentistry, especially in tooth restorations, such as crowns and permanent bridges. The gold alloys' slight malleability facilitates the creation of a superior molar mating surface with other teeth and produces results that are generally more satisfactory than those produced by the creation of porcelain crowns. The use of gold crowns in more prominent teeth such as incisors is favored in some cultures and discouraged in others.
Colloidal gold preparations (suspensions of gold nanoparticles) in water are intensely red-colored, and can be made with tightly controlled particle sizes up to a few tens of nanometers across by reduction of gold chloride with citrate or ascorbate ions. Colloidal gold is used in research applications in medicine, biology and materials science. The technique of immunogold labeling exploits the ability of the gold particles to adsorb protein molecules onto their surfaces. Colloidal gold particles coated with specific antibodies can be used as probes for the presence and position of antigens on the surfaces of cells. In ultrathin sections of tissues viewed by electron microscopy, the immunogold labels appear as extremely dense round spots at the position of the antigen.
Gold, or alloys of gold and palladium, are applied as conductive coating to biological specimens and other non-conducting materials such as plastics and glass to be viewed in a scanning electron microscope. The coating, which is usually applied by sputtering with an argon plasma, has a triple role in this application. Gold's very high electrical conductivity drains electrical charge to earth, and its very high density provides stopping power for electrons in the electron beam, helping to limit the depth to which the electron beam penetrates the specimen. This improves definition of the position and topography of the specimen surface and increases the spatial resolution of the image. Gold also produces a high output of secondary electrons when irradiated by an electron beam, and these low-energy electrons are the most commonly used signal source used in the scanning electron microscope.
The isotope gold-198 (half-life 2.7 days) is used, in nuclear medicine, in some cancer treatments and for treating other diseases.
Monetary exchange (historical).
Gold has been widely used throughout the world as money, for efficient indirect exchange (versus barter), and to store wealth in hoards. For exchange purposes, mints produce standardized gold bullion coins, bars and other units of fixed weight and purity.
The first coins containing gold were struck in Lydia, Asia Minor, around 600 BC. The "talent" coin of gold in use during the periods of Grecian history both before and during the time of the life of Homer weighed between 8.42 and 8.75 grams. From an earlier preference in using silver, European economies re-established the minting of gold as coinage during the thirteenth and fourteenth centuries.
Bills (that mature into gold coin) and gold certificates (convertible into gold coin at the issuing bank) added to the circulating stock of gold standard money in most 19th century industrial economies.
In preparation for World War I the warring nations moved to fractional gold standards, inflating their currencies to finance the war effort. 
Post-war, the victorious countries, most notably Britain, gradually restored gold-convertibility, but international flows of gold via bills of exchange remained embargoed; international shipments were made exclusively for bilateral trades or to pay war reparations.
After World War II gold was replaced by a system of nominally convertible currencies related by fixed exchange rates following the Bretton Woods system. Gold standards and the direct convertibility of currencies to gold have been abandoned by world governments, led in 1971 by the United States' refusal to redeem its dollars in gold. Fiat currency now fills most monetary roles. Switzerland was the last country to tie its currency to gold; it backed 40% of its value until the Swiss joined the International Monetary Fund in 1999.
Central banks continue to keep a portion of their liquid reserves as gold in some form, and metals exchanges such as the London Bullion Market Association still clear transactions denominated in gold, including future delivery contracts.
Today, gold mining output is declining. 
With the sharp growth of economies in the 20th century, and increasing foreign exchange, the world's gold reserves and their trading market have become a small fraction of all markets and fixed exchange rates of currencies to gold have been replaced by floating prices for gold and gold future contract.
Though the gold stock grows by only 1 or 2% per year, very little metal is irretrievably consumed. Inventory above ground would satisfy many decades of industrial and even artisan uses at current prices.
The gold content of alloys is measured in carats (k). Pure gold is designated as 24k. English gold coins intended for circulation from 1526 into the 1930s were typically a standard 22k alloy called crown gold, for hardness (American gold coins for circulation after 1837 contained the slightly lower amount of 0.900 fine gold, or 21.6 kt).
Although the prices of some platinum group metals can be much higher, gold has long been considered the most desirable of precious metals, and its value has been used as the standard for many currencies. Gold has been used as a symbol for purity, value, royalty, and particularly roles that combine these properties. Gold as a sign of wealth and prestige was ridiculed by Thomas More in his treatise "Utopia". On that imaginary island, gold is so abundant that it is used to make chains for slaves, tableware, and lavatory seats. When ambassadors from other countries arrive, dressed in ostentatious gold jewels and badges, the Utopians mistake them for menial servants, paying homage instead to the most modestly dressed of their party.
Cultural history.
Gold artifacts found at the Nahal Kana cave cemetery dated during the 1980s, showed these to be from within the Chalcolithic, and considered the earliest find from the Levant (Gopher "et al." 1990). Gold artifacts in the Balkans also appear from the 4th millennium BC, such as those found in the Varna Necropolis near Lake Varna in Bulgaria, thought by one source (La Niece 2009) to be the earliest "well-dated" find of gold artifacts. Gold artifacts such as the golden hats and the Nebra disk appeared in Central Europe from the 2nd millennium BC Bronze Age.
Egyptian hieroglyphs from as early as 2600 BC describe gold, which King Tushratta of the Mitanni claimed was "more plentiful than dirt" in Egypt. Egypt and especially Nubia had the resources to make them major gold-producing areas for much of history. One of the earliest known maps, known as the Turin Papyrus Map, shows the plan of a gold mine in Nubia together with indications of the local geology. The primitive working methods are described by both Strabo and Diodorus Siculus, and included fire-setting. Large mines were also present across the Red Sea in what is now Saudi Arabia.
The legend of the golden fleece may refer to the use of fleeces to trap gold dust from placer deposits in the ancient world. Gold is mentioned frequently in the Old Testament, starting with Genesis 2:11 (at Havilah), the story of The Golden Calf and many parts of the temple including the Menorah and the golden altar. In the New Testament, it is included with the gifts of the magi in the first chapters of Matthew. The Book of Revelation 21:21 describes the city of New Jerusalem as having streets "made of pure gold, clear as crystal". Exploitation of gold in the south-east corner of the Black Sea is said to date from the time of Midas, and this gold was important in the establishment of what is probably the world's earliest coinage in Lydia around 610 BC. From the 6th or 5th century BC, the Chu (state) circulated the Ying Yuan, one kind of square gold coin.
In Roman metallurgy, new methods for extracting gold on a large scale were developed by introducing hydraulic mining methods, especially in Hispania from 25 BC onwards and in Dacia from 106 AD onwards. One of their largest mines was at Las Medulas in León (Spain), where seven long aqueducts enabled them to sluice most of a large alluvial deposit. The mines at Roşia Montană in Transylvania were also very large, and until very recently, still mined by opencast methods. They also exploited smaller deposits in Britain, such as placer and hard-rock deposits at Dolaucothi. The various methods they used are well described by Pliny the Elder in his encyclopedia Naturalis Historia written towards the end of the first century AD.
During Mansa Musa's (ruler of the Mali Empire from 1312 to 1337) hajj to Mecca in 1324, he passed through Cairo in July 1324, and was reportedly accompanied by a camel train that included thousands of people and nearly a hundred camels where he gave away so much gold that it depressed the price in Egypt for over a decade. A contemporary Arab historian remarked:
Gold was at a high price in Egypt until they came in that year. The mithqal did not go below 25 dirhams and was generally above, but from that time its value fell and it cheapened in price and has remained cheap till now. The mithqal does not exceed 22 dirhams or less. This has been the state of affairs for about twelve years until this day by reason of the large amount of gold which they brought into Egypt and spent there [...].—Chihab Al-Umari, Kingdom of Mali
The European exploration of the Americas was fueled in no small part by reports of the gold ornaments displayed in great profusion by Native American peoples, especially in Mesoamerica, Peru, Ecuador and Colombia. The Aztecs regarded gold as literally the product of the gods, calling it "god excrement" ("teocuitlatl" in Nahuatl), and after Moctezuma II was killed, most of this gold was shipped to Spain. However, for the indigenous peoples of North America gold was considered useless and they saw much greater value in other minerals which were directly related to their utility, such as obsidian, flint, and slate. Rumors of cities filled with gold fueled legends of El Dorado.
Gold played a role in western culture, as a cause for desire and of corruption, as told in children's fables such as Rumpelstiltskin, where the peasant's daughter turns hay into gold, in return for giving up her child when she becomes a princess; and the stealing of the hen that lays golden eggs in Jack and the Beanstalk.
The top prize at the Olympic games is the gold medal.
75% of all gold ever produced has been extracted since 1910. It has been estimated that all the gold ever refined would form a single cube 20 m (66 ft) on a side (equivalent to 8,000 m3).
One main goal of the alchemists was to produce gold from other substances, such as lead — presumably by the interaction with a mythical substance called the philosopher's stone. Although they never succeeded in this attempt, the alchemists promoted an interest in what can be done with substances, and this laid the foundation for today's chemistry. Their symbol for gold was the circle with a point at its center (☉), which was also the astrological symbol and the ancient Chinese character for the Sun.
Golden treasures have been rumored to be found at various locations, following tragedies such as the Jewish temple treasures in the Vatican, following the temple's destruction in 70 AD, a gold stash on the Titanic, the Nazi gold train – following World War II.
The Dome of the Rock on the Jerusalem temple site is covered with an ultra-thin golden glasure. The Sikh Golden temple, the Harmandir Sahib, is a building covered with gold. Similarly the Wat Phra Kaew emerald Budha temple in Thailand has ornamental gold statues walls and roofs. Some European king and queen's crowns were made of gold, and gold was used for the bridal crown since antiquity. An ancient Talmudic text circa 100 AD describes Rachel, Rabbi Akiba's wife asking for a "Jerusalem of Gold" (crown). A Greek burial crown made of gold was found in a grave circa 370 BC.
Occurrence.
Gold's atomic number of 79 makes it one of the higher atomic number elements that occur naturally. Traditionally, gold is thought to have formed by the R-process in supernova nucleosynthesis, but a relatively recent paper suggests that gold and other elements heavier than iron may also be produced in quantity by the collision of neutron stars. In both cases, satellite spectrometers only indirectly detect the resulting gold: "we have no spectroscopic evidence that [such] elements have truly been produced."
These gold nucleogenesis theories hold that the resulting explosions scattered metal-containing dusts (including heavy elements such as gold) into the region of space in which they later condensed into our solar system and the Earth. Because the Earth was molten when it was just formed, almost all of the gold present on Earth sank into the core. Most of the gold that is present today in the Earth's crust and mantle is thought to have been delivered to Earth later, by asteroid impacts during the Late Heavy Bombardment.
The asteroid that formed Vredefort crater 2.020 billion years ago is often credited with seeding the Witwatersrand basin in South Africa with the richest gold deposits on earth. However, the gold-bearing Witwatersrand rocks were laid down between 700 and 950 million years before the Vredefort impact. These gold-bearing rocks had furthermore been covered by a thick layer of Ventersdorp lavas and the Transvaal Supergroup of rocks before the meteor struck. What the Vredefort impact achieved, however, was to distort the Witwatersrand basin in such a way that the gold-bearing rocks were brought to the present erosion surface in Johannesburg, on the Witwatersrand, just inside the rim of the original 300 km diameter crater caused by the meteor strike. The discovery of the deposit in 1886 launched the Witwatersrand Gold Rush. Nearly 50% of all the gold ever mined on earth has been extracted from these Witwatersrand rocks.
On Earth, gold is found in ores in rock formed from the Precambrian time onward. It most often occurs as a native metal, typically in a metal solid solution with silver (i.e. as a gold silver alloy). Such alloys usually have a silver content of 8–10%. Electrum is elemental gold with more than 20% silver. Electrum's color runs from golden-silvery to silvery, dependent upon the silver content. The more silver, the lower the specific gravity.
Native gold occurs as very small to microscopic particles embedded in rock, often together with quartz or sulfide minerals such as "Fool's Gold", which is a pyrite. These are called lode deposits. The metal in a native state is also found in the form of free flakes, grains or larger nuggets that have been eroded from rocks and end up in alluvial deposits called placer deposits. Such free gold is always richer at the surface of gold-bearing veins owing to the oxidation of accompanying minerals followed by weathering, and washing of the dust into streams and rivers, where it collects and can be welded by water action to form nuggets.
Gold sometimes occurs combined with tellurium as the minerals calaverite, krennerite, nagyagite, petzite and sylvanite (see telluride minerals), and as the rare bismuthide maldonite (Au2Bi) and antimonide aurostibite (AuSb2). Gold also occurs in rare alloys with copper, lead, and mercury: the minerals auricupride (Cu3Au), novodneprite (AuPb3) and weishanite ((Au, Ag)3Hg2).
Recent research suggests that microbes can sometimes play an important role in forming gold deposits, transporting and precipitating gold to form grains and nuggets that collect in alluvial deposits.
Another recent study has claimed water in faults vaporizes during an earthquake, depositing gold. When an earthquake strikes, it moves along a fault. Water often lubricates faults, filling in fractures and jogs. About 6 miles (10 kilometers) below the surface, under incredible temperatures and pressures, the water carries high concentrations of carbon dioxide, silica, and gold. During an earthquake, the fault jog suddenly opens wider. The water inside the void instantly vaporizes, flashing to steam and forcing silica, which forms the mineral quartz, and gold out of the fluids and onto nearby surfaces.
Seawater.
The world's oceans contain gold. Measured concentrations of gold in the Atlantic and Northeast Pacific are 50–150 fmol/L or 10–30 parts per 1,000,000,000,000,000 quadrillion (about 10–30 g/km3). In general, gold concentrations for south Atlantic and central Pacific samples are the same (~50 fmol/L) but less certain. Mediterranean deep waters contain slightly higher concentrations of gold (100–150 fmol/L) attributed to wind-blown dust and/or rivers. At 10 parts per quadrillion the Earth's oceans would hold 15,000 tonnes of gold. These figures are three orders of magnitude less than reported in the literature prior to 1988, indicating contamination problems with the earlier data.
A number of people have claimed to be able to economically recover gold from sea water, but so far they have all been either mistaken or acted in an intentional deception. Prescott Jernegan ran a gold-from-seawater swindle in the United States in the 1890s. A British fraudster ran the same scam in England in the early 1900s. Fritz Haber (the German inventor of the Haber process) did research on the extraction of gold from sea water in an effort to help pay Germany's reparations following World War I. Based on the published values of 2 to 64 ppb of gold in seawater a commercially successful extraction seemed possible. After analysis of 4,000 water samples yielding an average of 0.004 ppb it became clear that the extraction would not be possible and he stopped the project. No commercially viable mechanism for performing gold extraction from sea water has yet been identified. Gold synthesis is not economically viable and is unlikely to become so in the foreseeable future.
Production.
At the end of 2009, it was estimated that all the gold ever mined totaled 165,000 tonnes. This can be represented by a cube with an edge length of about 20.28 meters. At $1,600 per troy ounce, 165,000 metric tonnes of gold would have a value of $8.5 trillion.
World production for 2011 was at 2,700 tonnes, compared to 2,260 tonnes for 2008.
Mining.
Since the 1880s, South Africa has been the source for a large proportion of the world's gold supply, with about 50% of all gold ever produced having come from South Africa. Production in 1970 accounted for 79% of the world supply, producing about 1,480 tonnes. In 2007 China (with 276 tonnes) overtook South Africa as the world's largest gold producer, the first time since 1905 that South Africa has not been the largest.
As of 2013, China was the world's leading gold-mining country, followed in order by Australia, the United States, Russia, and Peru. South Africa, which had dominated world gold production for most of the 20th Century, had declined to sixth place. Other major producers are the Ghana, Burkina Faso, Mali, Indonesia and Uzbekistan.
In South America, the controversial project Pascua Lama aims at exploitation of rich fields in the high mountains of Atacama Desert, at the border between Chile and Argentina.
Today about one-quarter of the world gold output is estimated to originate from artisanal or small scale mining.
The city of Johannesburg located in South Africa was founded as a result of the Witwatersrand Gold Rush which resulted in the discovery of some of the largest gold deposits the world has ever seen. The gold fields are confined to the northern and north-western edges of the Witwatersrand basin, which is a 5–7 km thick layer of archean rocks located, in most places, deep under the Free State, Gauteng and surrounding provinces. These Witwatersrand rocks are exposed at the surface on the Witwatersrand, in and around Johannesburg, but also in isolated patches to the south-east and south-west of Johannesburg, as well as in an arc around the Vredefort Dome which lies close to the center of the Witwatersrand basin. From these surface exposures the basin dips extensively, requiring some of the mining to occur at depths of nearly 4000 m, making them, especially the Savuka and TauTona mines to the south-west of Johannesburg, the deepest mines on earth. The gold is found only in six areas where archean rivers from the north and north-west formed extensive pebbly braided river deltas before draining into the "Witwatersrand sea" where the rest of the Witwatersrand sediments were deposited.
The Second Boer War of 1899–1901 between the British Empire and the Afrikaner Boers was at least partly over the rights of miners and possession of the gold wealth in South Africa.
Prospecting.
During the 19th century, gold rushes occurred whenever large gold deposits were discovered. The first documented discovery of gold in the United States was at the Reed Gold Mine near Georgeville, North Carolina in 1803. The first major gold strike in the United States occurred in a small north Georgia town called Dahlonega. Further gold rushes occurred in California, Colorado, the Black Hills, Otago in New Zealand, Australia, Witwatersrand in South Africa, and the Klondike in Canada.
Bioremediation.
A sample of the fungus "Aspergillus niger" was found growing from gold mining solution; and was found to contain cyano metal complexes; such as gold, silver, copper iron and zinc. The fungus also plays a role in the solubilization of heavy metal sulfides.
Extraction.
Gold extraction is most economical in large, easily mined deposits. Ore grades as little as 0.5 mg/kg (0.5 parts per million, ppm) can be economical. Typical ore grades in open-pit mines are 1–5 mg/kg (1–5 ppm); ore grades in underground or hard rock mines are usually at least 3 mg/kg (3 ppm). Because ore grades of 30 mg/kg (30 ppm) are usually needed before gold is visible to the naked eye, in most gold mines the gold is invisible.
The average gold mining and extraction costs were about US$317/oz in 2007, but these can vary widely depending on mining type and ore quality; global mine production amounted to 2,471.1 tonnes.
Refining.
After initial production, gold is often subsequently refined industrially by the Wohlwill process which is based on electrolysis or by the Miller process, that is chlorination in the melt. The Wohlwill process results in higher purity, but is more complex and is only applied in small-scale installations. Other methods of assaying and purifying smaller amounts of gold include parting and inquartation as well as cupellation, or refining methods based on the dissolution of gold in aqua regia.
Synthesis from other elements.
Gold was synthesized from mercury by neutron bombardment in 1941, but the isotopes of gold produced were all radioactive. In 1924, a Japanese physicist, Hantaro Nagaoka, accomplished the same feat.
Gold can currently be manufactured in a nuclear reactor by irradiation either of platinum or mercury.
Only the mercury isotope 196Hg, which occurs with a frequency of 0.15% in natural mercury, can be converted to gold by neutron capture, and following electron capture-decay into 197Au with slow neutrons. Other mercury isotopes are converted when irradiated with slow neutrons into one another, or formed mercury isotopes which beta decay into thallium.
Using fast neutrons, the mercury isotope 198Hg, which composes 9.97% of natural mercury, can be converted by splitting off a neutron and becoming 197Hg, which then disintegrates to stable gold. This reaction, however, possesses a smaller activation cross-section and is feasible only with un-moderated reactors.
It is also possible to eject several neutrons with very high energy into the other mercury isotopes in order to form 197Hg. However such high-energy neutrons can be produced only by particle accelerators.
Consumption.
The consumption of gold produced in the world is about 50% in jewelry, 40% in investments, and 10% in industry.
According to World Gold Council, China is the world's largest single consumer of gold in 2013 and toppled India for the first time with Chinese consumption increasing by 32 percent in a year, while that of India only rose by 13 percent and world consumption rose by 21 percent. Unlike India where gold is used for mainly for jewellery, China uses gold for manufacturing and retail.
Pollution.
Gold production is associated with contribution to hazardous pollution. The ore, generally containing less than one ppm gold metal, is ground and mixed with sodium cyanide or mercury to react with gold in the ore for gold separation. Cyanide is a highly poisonous chemical, which can kill living creatures when exposed in minute quantities. Many cyanide spills from gold mines have occurred in both developed and developing countries which killed marine life in long stretches of affected rivers. Environmentalists consider these events major environmental disasters. When mercury is used in gold production, minute quantity of mercury compounds reach water bodies, causing heavy metal contamination. Mercury can then enter into the human food chain in the form of methylmercury. Mercury poisoning in humans causes incurable brain function damage and severe retardation.
Thirty tons of used ore is dumped as waste for producing one troy ounce of gold. Gold ore dumps are the source of many heavy elements such as cadmium, lead, zinc, copper, arsenic, selenium and mercury. When sulfide bearing minerals in these ore dumps are exposed to air and water, the sulfide transforms into sulfuric acid which in turn dissolves these heavy metals facilitating their passage into surface water and ground water. This process is called acid mine drainage. These gold ore dumps are long term, highly hazardous wastes second only to nuclear waste dumps.
Gold extraction is also a highly energy intensive industry, extracting ore from deep mines and grinding the large quantity of ore for further chemical extraction requires 25 kW·h of electricity per gram of gold produced.
Chemistry.
Although gold is the most noble of the noble metals, it still forms many diverse compounds. The oxidation state of gold in its compounds ranges from −1 to +5, but Au(I) and Au(III) dominate its chemistry. Au(I), referred to as the aurous ion, is the most common oxidation state with soft ligands such as thioethers, thiolates, and tertiary phosphines. Au(I) compounds are typically linear. A good example is Au(CN)2−, which is the soluble form of gold encountered in mining. Curiously, aurous complexes of water are rare. The binary gold halides, such as AuCl, form zigzag polymeric chains, again featuring linear coordination at Au. Most drugs based on gold are Au(I) derivatives.
Au(III) (auric) is a common oxidation state, and is illustrated by gold(III) chloride, Au2Cl6. The gold atom centers in Au(III) complexes, like other d8 compounds, are typically square planar, with chemical bonds that have both covalent and ionic character.
Aqua regia, a 1:3 mixture of nitric acid and hydrochloric acid, dissolves gold. Nitric acid oxidizes the metal to +3 ions, but only in minute amounts, typically undetectable in the pure acid because of the chemical equilibrium of the reaction. However, the ions are removed from the equilibrium by hydrochloric acid, forming AuCl4− ions, or chloroauric acid, thereby enabling further oxidation.
Some free halogens react with gold. Gold also reacts in alkaline solutions of potassium cyanide. With mercury, it forms an amalgam.
Less common oxidation states.
Less common oxidation states of gold include −1, +2, and +5.
The −1 oxidation state occurs in compounds containing the Au− anion, called aurides. Caesium auride (CsAu), for example, crystallizes in the caesium chloride motif. Other aurides include those of Rb+, K+, and tetramethylammonium (CH3)4N+. Gold has the highest Pauling electronegativity of any metal, with a value of 2.54, making the auride anion relatively stable.
Gold(II) compounds are usually diamagnetic with Au–Au bonds such as [Au(CH2)2P(C6H5)2]2Cl2. The evaporation of a solution of Au(OH)3 in concentrated H2SO4 produces red crystals of gold(II) sulfate, Au2(SO4)2. Originally thought to be a mixed-valence compound, it has been shown to contain Au24+ cations. A noteworthy, legitimate gold(II) complex is the tetraxenonogold(II) cation, which contains xenon as a ligand, found in [AuXe4](Sb2F11)2.
Gold pentafluoride, along with its derivative anion, AuF6-, and its difluorine complex, gold heptafluoride, is the sole example of gold(V), the highest verified oxidation state.
Some gold compounds exhibit "aurophilic bonding", which describes the tendency of gold ions to interact at distances that are too long to be a conventional Au–Au bond but shorter than van der Waals bonding. The interaction is estimated to be comparable in strength to that of a hydrogen bond.
Mixed valence compounds.
Well-defined cluster compounds are numerous. In such cases, gold has a fractional oxidation state. A representative example is the octahedral species {Au(P(C6H5)3)}62+. Gold chalcogenides, such as gold sulfide, feature equal amounts of Au(I) and Au(III).
Toxicity.
Pure metallic (elemental) gold is non-toxic and non-irritating when ingested and is sometimes used as a food decoration in the form of gold leaf. Metallic gold is also a component of the alcoholic drinks Goldschläger, Gold Strike, and Goldwasser. Metallic gold is approved as a food additive in the EU (E175 in the Codex Alimentarius). Although the gold ion is toxic, the acceptance of metallic gold as a food additive is due to its relative chemical inertness, and resistance to being corroded or transformed into soluble salts (gold compounds) by any known chemical process which would be encountered in the human body.
Soluble compounds (gold salts) such as gold chloride are toxic to the liver and kidneys. Common cyanide salts of gold such as potassium gold cyanide, used in gold electroplating, are toxic by virtue of both their cyanide and gold content. There are rare cases of lethal gold poisoning from potassium gold cyanide. Gold toxicity can be ameliorated with chelation therapy with an agent such as dimercaprol.
Gold metal was voted Allergen of the Year in 2001 by the American Contact Dermatitis Society. Gold contact allergies affect mostly women. Despite this, gold is a relatively non-potent contact allergen, in comparison with metals like nickel.
Price.
Gold is currently valued at around US$1,200 per troy ounce (US$39,000 per kilogram).
Like other precious metals, gold is measured by troy weight and by grams. When it is alloyed with other metals the term "carat" or "karat" is used to indicate the purity of gold present, with 24 carats being pure gold and lower ratings proportionally less. The purity of a gold bar or coin can also be expressed as a decimal figure ranging from 0 to 1, known as the millesimal fineness, such as 0.995 being very pure.
History.
The price of gold is determined through trading in the gold and derivatives markets, but a procedure known as the Gold Fixing in London, originating in September 1919, provides a daily benchmark price to the industry. The afternoon fixing was introduced in 1968 to provide a price when US markets are open.
Historically gold coinage was widely used as currency; when paper money was introduced, it typically was a receipt redeemable for gold coin or bullion. In a monetary system known as the gold standard, a certain weight of gold was given the name of a unit of currency. For a long period, the United States government set the value of the US dollar so that one troy ounce was equal to $20.67 ($664.56/kg), but in 1934 the dollar was devalued to $35.00 per troy ounce ($1125.27/kg). By 1961, it was becoming hard to maintain this price, and a pool of US and European banks agreed to manipulate the market to prevent further currency devaluation against increased gold demand.
On 17 March 1968, economic circumstances caused the collapse of the gold pool, and a two-tiered pricing scheme was established whereby gold was still used to settle international accounts at the old $35.00 per troy ounce ($1.13/g) but the price of gold on the private market was allowed to fluctuate; this two-tiered pricing system was abandoned in 1975 when the price of gold was left to find its free-market level. Central banks still hold historical gold reserves as a store of value although the level has generally been declining. The largest gold depository in the world is that of the U.S. Federal Reserve Bank in New York, which holds about 3% of the gold ever mined, as does the similarly laden U.S. Bullion Depository at Fort Knox.
In 2005 the World Gold Council estimated total global gold supply to be 3,859 tonnes and demand to be 3,754 tonnes, giving a surplus of 105 tonnes.
Sometime around 1970 the price began in trend to greatly increase, and since 1968 the price of gold has ranged widely, from a high of $850/oz ($27,300/kg) on 21 January 1980, to a low of $252.90/oz ($8,131/kg) on 21 June 1999 (London Gold Fixing). Prices increased rapidly from 2001, but the 1980 high was not exceeded until 3 January 2008 when a new maximum of $865.35 per troy ounce was set. Another record price was set on 17 March 2008 at $1023.50/oz ($32,900/kg).
In late 2009, gold markets experienced renewed momentum upwards due to increased demand and a weakening US dollar. On 2 December 2009, Gold reached a new high closing at $1,217.23. Gold further rallied hitting new highs in May 2010 after the European Union debt crisis prompted further purchase of gold as a safe asset. On 1 March 2011, gold hit a new all-time high of $1432.57, based on investor concerns regarding ongoing unrest in North Africa as well as in the Middle East.
Since April 2001 the gold price has more than quintupled in value against the US dollar, hitting a new all-time high of $1,913.50 on 23 August 2011, prompting speculation that this long secular bear market has ended and a bull market has returned.
Symbolism.
Great human achievements are frequently rewarded with gold, in the form of gold medals, golden trophies and other decorations. Winners of athletic events and other graded competitions are usually awarded a gold medal. Many awards such as the Nobel Prize are made from gold as well. Other award statues and prizes are depicted in gold or are gold plated (such as the Academy Awards, the Golden Globe Awards, the Emmy Awards, the Palme d'Or, and the British Academy Film Awards).
Aristotle in his ethics used gold symbolism when referring to what is now commonly known as the golden mean. Similarly, gold is associated with perfect or divine principles, such as in the case of the golden ratio and the golden rule.
Gold is further associated with the wisdom of aging and fruition. The fiftieth wedding anniversary is golden. Our most valued or most successful latter years are sometimes considered "golden years". The height of a civilization is referred to as a "golden age".
In some forms of Christianity and Judaism, gold has been associated both with holiness and evil. In the Book of Exodus, the Golden Calf is a symbol of idolatry, while in the Book of Genesis, Abraham was said to be rich in gold and silver, and Moses was instructed to cover the Mercy Seat of the Ark of the Covenant with pure gold. In Byzantine iconography the halos of Christ, Mary and the Christian saints are often golden.
According to Christopher Columbus, those who had something of gold were in possession of something of great value on Earth and a substance to even help souls to paradise.
Wedding rings have long been made of gold. It is long lasting and unaffected by the passage of time and may aid in the ring symbolism of eternal vows before God and/or the sun and moon and the perfection the marriage signifies. In Orthodox Christian wedding ceremonies, the wedded couple is adorned with a golden crown (though some opt for wreaths, instead) during the ceremony, an amalgamation of symbolic rites.
In popular culture gold has many connotations but is most generally connected to terms such as good or great, such as in the phrases: "has a heart of gold", "that's golden!", "golden moment", "then you're golden!" and "golden boy". It remains a cultural symbol of wealth and through that, in many societies, success.
State emblem.
In 1965, the California Legislature designated gold "the State Mineral and mineralogical emblem".
In 1968, the Alaska Legislature named gold "the official state mineral".

</doc>
<doc id="12507" url="http://en.wikipedia.org/wiki?curid=12507" title="Great Schism">
Great Schism

Great schism may refer to:

</doc>
<doc id="12511" url="http://en.wikipedia.org/wiki?curid=12511" title="Gerald Schroeder">
Gerald Schroeder

Gerald Lawrence Schroeder is an Orthodox Jewish physicist, author, lecturer and teacher at College of Jewish Studies Aish HaTorah's Discovery Seminar, Essentials and Fellowships programs and Executive Learning Center, who focuses on what he perceives to be an inherent relationship between science and spirituality.
Education.
Schroeder received his BSc in 1959, his MSc in 1961, and his PhD in nuclear physics and earth and planetary sciences in 1965, from the Massachusetts Institute of Technology (MIT). He worked five years on the staff of the MIT physics department. He was a member of the United States Atomic Energy Commission.
Aliyah to Israel.
After emigrating to Israel in 1971, Schroeder was employed as a researcher at the Weizmann Institute of Science, the Volcani Research Institute, and the Hebrew University of Jerusalem. He currently teaches at Aish HaTorah College of Jewish Studies.
Religious views and scientific theories.
His works frequently cite Talmudic, Midrashic and medieval commentaries on Biblical creation accounts, such as commentaries written by the Jewish philosopher Nachmanides. Among other things, Schroeder attempts to reconcile a six-day creation as described in Genesis with the scientific evidence that the world is billions of years old using the idea that the perceived flow of time for a given event in an expanding universe varies with the observer’s perspective of that event. He attempts to reconcile the two perspectives numerically, calculating the effect of the stretching of space-time, based on Einstein's general relativity.
Namely, that from the perspective of the point of origin of the Big Bang, according to Einstein's equations of the 'stretching factor', time dilates by a factor of roughly 1,000,000,000,000, meaning one trillion days on earth would appear to pass as one day from that point, due to the stretching of space. When applied to the estimated age of the universe at 13.8 billion years, from the perspective of the point of origin, the universe today would appear to have just begun its sixth day of existence, or if the universe is 15 billion years old from the perspective of earth, it would appear to have just completed its sixth day.
Antony Flew, an academic philosopher who promoted atheism for most of his adult life indicated that the arguments of Gerald Schroeder had influenced his decision to become a deist.
His theories to reconcile faith and science have drawn some criticism from both religious and non-religious scientists, and his works remain controversial in scientific circles. 
Personal.
Schroeder's wife Barbara Sofer is a popular columnist for the English language Israeli newspaper, Jerusalem Post. The couple have five children.
Prizes.
In 2012, Schroeder was awarded the Trotter Prize by Texas A&M University's College of Science.

</doc>
<doc id="12529" url="http://en.wikipedia.org/wiki?curid=12529" title="Grímnismál">
Grímnismál

Grímnismál ("Sayings of Grímnir") is one of the mythological poems of the Poetic Edda. It is preserved in the Codex Regius manuscript and the AM 748 I 4to fragment. It is spoken through the voice of "Grímnir", one of the many guises of the god Odin, who is (through an error) tortured by King Geirröth. This was to prove a fatal mistake since Odin caused him to fall upon his own sword.
The work starts out with a lengthy prose section describing the circumstances leading up to Grímnir's monologue, which comprises 54 stanzas of poetic verse. The last part of the poem is also prose, a brief description of Geirröth's demise, his son's ascension, and Odin's disappearance. The prose sections were most likely not part of the original oral versions of Grímnismál. Henry Adams Bellows suggests that they were added in the 12th or 13th century and based on some sort of narrative tradition regarding the poem. This is not entirely certain. The poem itself was likely composed in the first half of the 10th century.
Synopsis.
The narrative commences at a point when Odin and his wife, Frigg, were sitting in Hlidskjalf, looking out on the worlds. They turned their eyes towards King Geirröth, who was reigning in the stead of his late father, King Hrauthung. Geirröth and his older brother Agnarr had been raised by Odin and Frigg, respectively. The god and goddess disguised themselves as a peasant and his wife, and taught the children wisdom. Geirröth returned to his father's kingdom where he became king upon his father's death, while Agnarr dwelt in company with a giantess in a cave.
In Hliðskjálf, Odin remarked to Frigg that his foster-child Geirröth seemed to be prospering more so than her Agnarr. Frigg retorted that Geirröth was so parsimonious and inhospitable that he would torture his guests if he thought there were too many of them. Odin disputed this, and the couple entered into a wager in this respect. Frigg then sent her maid Fulla to Geirröth, advising him that a magician would soon enter his court to bewitch him, and saying that he could be recognised by the fact that no dog was fierce enough to leap up at him.
Geirröth heeded Fulla's false warning. He ordered his men to capture the man the dogs wouldn't attack, which they did. Odin-as-Grímnir, dressed in a dark blue cloak, allowed himself to be captured. He stated that his name was Grímnir, but he would say nothing further of himself.
Geirröth then had him tortured to force him to speak, putting him between two fires for eight nights. After this time, Geirröth's son, Agnarr, named after his brother, came to Grímnir and gave him a full horn from which to drink, saying that his father, the king, was not right to torture him.
Grímnir then spoke, saying that he had suffered eight days and nights, without succour from any save Agnarr, Geirröth's son, whom Grímnir prophesied would be Lord of the Goths. He then revealed himself for who he was, as the Highest One, promising Agnarr reward for the drink which he brought him.
In the body of the poem, Odin describes at great length the cosmogony of the worlds, the dwelling places of its inhabitants, and talks about himself and his many guises.
Eventually, Grímnir turns to Geirröth and promises him misfortune, revealing his true identity. Geirröth then realized the magnitude of his mistake. Having learned that he is undone, he rose quickly to pull Odin from the fire, but the sword which he had lain upon his knee slipped, fell hilt down, the king stumbled and impaled himself upon it. Odin then vanished, and Agnarr, his son, ruled in his stead.
In popular culture.
The 12th album of Valhalla is based on the poem.

</doc>
<doc id="12535" url="http://en.wikipedia.org/wiki?curid=12535" title="Golden Heroes">
Golden Heroes

Golden Heroes is a superhero role-playing game that was originally written and published on an amateur basis in 1982. Games Workshop then published a more complete version in 1984. It was written by Simon Burley and Peter Haines and was illustrated by a group of artists who were working for "2000 AD" at the time.
The game was published in a box, the rules books features fake bar codes and Comics Code approval badges.
The character generation system involves players rolling completely random characters. A character can only keep his full set of powers if he can justify them all in a plausible origins story. The system really strives to recreate comics, with the actions occurring in "frames" and a lot of classic comics assumptions being written into the rules.
The game has since be revised and re-issued under a new name, Squadron UK. http://squadronuk.co.uk/

</doc>
<doc id="12539" url="http://en.wikipedia.org/wiki?curid=12539" title="Genitive case">
Genitive case

In grammar, genitive (abbreviated gen; also called the possessive case or second case) is the grammatical case that marks a noun as modifying another noun. It often marks a noun as being the possessor of another noun; however, it can also indicate various other relationships than possession: certain verbs may take arguments in the genitive case, and it may have adverbial uses ("see" Adverbial genitive).
Placing the modifying noun in the genitive case is one way to indicate that two nouns are related in a genitive construction. Modern English typically does not morphologically mark nouns for a genitive case in order to indicate a genitive construction; instead, it uses either the "‍‍ '​‍s" clitic or a preposition (usually "of"). However, the personal pronouns do have distinct possessive forms. There are various other ways to indicate a genitive construction, as well. For example, many Afroasiatic languages place the head noun (rather than the modifying noun) in the construct state.
Depending on the language, specific varieties of genitive-noun–main-noun relationships may include:
Depending on the language, some of the relationships mentioned above have their own distinct cases different from the genitive.
Possessive pronouns are distinct pronouns, found in Indo-European languages such as English, that function like pronouns inflected in the genitive. They are considered separate pronouns if contrasting to languages where pronouns are regularly inflected in the genitive. For example, English "my" is either a separate possessive adjective or an irregular genitive of "I", while in Finnish, for example, "minun" is regularly agglutinated from "minu-" "I" and "-n" (genitive).
In some languages, nouns in the genitive case also agree in case with the nouns they modify (that is, it is marked for two cases). This phenomenon is called suffixaufnahme.
In some languages, nouns in the genitive case may be found in inclusio – that is, between the main noun’s article and the noun itself.
Many languages have a genitive case, including Albanian, Arabic, Armenian, Basque, Dutch, Estonian, Finnish, Georgian, German, Greek, Icelandic, Irish, Latin, Latvian, Lithuanian, Romanian, Sanskrit, Scottish Gaelic, Turkish and all Slavic languages except Bulgarian and Macedonian. English does not have a proper genitive case, but a possessive ending, "-’s", although some pronouns have irregular possessive forms which may more commonly be described as genitives; see English possessive.
Chinese (Cantonese).
The particle 嘅 ("ge") or the possessed noun's classifier is used to denote possession for singular nouns, while the particle 啲 ("dī") is used for plural nouns.
Examples:
Chinese (Mandarin).
In Mandarin Chinese, the genitive case is made by use of the particle 的 (de).
For instance: 我的猫 (My cat).
我 = I
猫 = Cat
However, when talking about persons in relation to one's self, it is common to drop 的 when the context allows for it to be easily understood.
For instance: 我妈妈 and 我的妈妈 both mean "My mother".
English.
Old English had a genitive case, which has left its mark in modern English in the form of the possessive ending "-'s" (now sometimes referred to as the "Saxon genitive"), as well as possessive pronoun forms such as "his", "theirs", etc., and in certain words derived from adverbial genitives such as "once" and "afterwards". (Other Old English case markers have generally disappeared completely.) The modern English possessive forms are not normally considered to represent a grammatical case, although they are sometimes referred to as genitives or as belonging to a possessive case. One of the reasons that the status of "-'s" as a case ending is often rejected is that it attaches to the end of a noun phrase and not necessarily to the head noun itself, as in "the king of Spain's daughter", not "the king's daughter of Spain" as would be expected if "-'s" were a case inflection on the noun "king" (and as was done in older forms of English).
Finnic genitives and accusatives.
Finnic languages (Finnish, Estonian) have genitive cases.
In Finnish, prototypically the genitive is marked with "-n", e.g. "maa – maan" "country – of the country". The stem may change, however, with consonant gradation and other reasons. For example, in certain words ending in consonants, "-e-" is added, e.g. "mies – miehen" "man – of the man", and in some, but not all words ending in "-i", the "-i" is changed to an "-e-", to give "-en", e.g. "lumi – lumen" "snow – of the snow". The genitive is used extensively, with animate and inanimate possessors. In addition to the genitive, there is also a partitive case (marked "-ta/-tä" or "-a/-ä") used for expressing that something is a part of a larger mass, e.g. "joukko miehiä" "a group of men".
In Estonian, the genitive marker "-n" has elided with respect to Finnish. Thus, the genitive always ends with a vowel, and the singular genitive is sometimes (in a subset of words ending with a vocal in nominative) identical in form to nominative.
In Finnish, in addition to the uses mentioned above, there is a construct where the genitive is used to mark a surname. For example, "Juhani Virtanen" can be also expressed "Virtasen Juhani" ("Juhani of the Virtanens").
A complication in Finnic languages is that the accusative case "-(e)n" is homophonic to the genitive case. This case does not indicate possession, but is a syntactic marker for the object, additionally indicating that the action is telic (completed). In Estonian, it is often said that only a "genitive" exists. However, the cases have completely different functions, and the form of the accusative has developed from *"-(e)m". (The same sound change has developed into a synchronic mutation of a final "m" into "n" in Finnish, e.g. genitive "sydämen" vs. nominative "sydän".) This homophony has exceptions in Finnish, where a separate accusative "-(e)t" is found in pronouns, e.g. "kenet" "who (telic object)", vs. "kenen" "whose".
A difference is also observed in some of the related Sámi languages, where the pronouns and the plural of nouns in the genitive and accusative are easily distinguishable from each other, e.g., "kuä'cǩǩmi" "eagles' (genitive plural)" and "kuä'cǩǩmid" "eagles (accusative plural)" in Skolt Sami.
German.
The genitive case is used in the German language to show possession. For example:
An "s" is simply added to the end of the name if the identity of the possessor is specified. For example:
There is also a genitive case of German pronouns such as "dein" (your) and "mein" (my).
The genitive case is also used for objects of some prepositions, such as "trotz" (despite), "wegen" (because of), "[an]statt" (instead of), "während" (during), and is required as the case of the direct object for some verbs, e.g. "gedenken", "sich erfreuen", "bedürfen": usage: "wir gedachten der Verstorbenen " - We remembered the dead; "wir erfreuen uns des schönen Wetters " - We're happy about the nice weather.
All of the articles change in the genitive case.
Adjective endings in genitive case:
The following prepositions can take the genitive: "außerhalb", "innerhalb", "statt", "trotz", "während", "wegen", and "dank".
The genitive case is widely avoided in most colloquial and dialectal varieties of German. It is replaced by the dative case after verbs and prepositions, and by means of the preposition "von" ("of") in other contexts. However, this usage is not accepted in the written standard language.
Japanese.
The Japanese possessive is constructed by using the suffix "-no" 〜の to make the genitive case. For example:
It also uses the suffix "-na" 〜な for adjectival noun; in some analyses adjectival nouns are simply nouns that take "-na" in the genitive, forming a complementary distribution ("-no" and "-na" being allomorphs).
Typically, languages have nominative case nouns converting into genitive case. It has been found, however, that Japanese will in rare cases allow accusative case to convert to genitive, if specific conditions are met in the clause in which the conversion appears. This is referred to as "Accusative-Genitive conversion."
Korean.
The possessive in Korean can be formed using the ending "-ui" '의'.
Latin.
The genitive is one of the cases of nouns and pronouns in Latin. Latin genitives still have certain modern scientific uses:
Irish.
The Irish language also uses a genitive case ("tuiseal ginideach"). For example in the phrase "bean an tí" (woman of the house), "tí" is the genitive case of "teach", meaning "house". Another example is "barr an chnoic", "top of the hill", where "cnoc" means "hill", but is changed to "chnoic", which also incorporates lenition.
Persian.
Old Persian had a true genitive case inherited from Proto-Indo-European. By the time of Middle Persian, the genitive case had been lost and replaced by an analytical construction which is now called Ezāfe. This construction was inherited by New Persian, and was also later borrowed into numerous other Iranic, Turkic and Indo-Aryan languages languages of Western and South Asia.
Semitic languages.
Genitive case marking existed in Proto-Semitic, Akkadian, and Ugaritic. It indicated possession, and it is preserved today only in literary Arabic
Arabic.
Called المجرور "al-majrūr" (meaning "dragged") in Arabic, the Genitive case functions both as an indication of ownership (ex. the door of the house) and for nouns following a preposition.
The Arabic genitive marking also appears after prepositions.
The Semitic genitive should not be confused with the pronominal possessive suffixes that exist in all the Semitic languages
Slavic languages.
With the exception of Bulgarian and Macedonian, all Slavic languages decline the nouns and adjectives in accordance with the genitive case using a variety of endings depending on the word's lexical category, its gender, and number (singular or plural).
Possessives.
To indicate possession the ending of the noun indicating the possessor changes to "а, я, ы or и", depending on the word's ending in the nominative case and similar cases in other Slavic languages. For example:
Possessives can also be formed by the construction "У [subject] есть [object]".
In sentences where the possessor includes an associated pronoun, the pronoun also changes:
And in sentences denoting negative possession, the ending of the object noun also changes:
To express negation.
The genitive case is also used in sentences expressing negation, even when no possessive relationship is involved. The ending of the subject noun changes just as it does in possessive sentences. The genitive, in this sense, can only be used to negate nominative, accusative and genitive sentences, and not other cases.
Use of genitive for negation is obligatory in Slovene, Polish and Old Church Slavonic. The East Slavic languages (Russian, Ukrainian, and Belorussian) employ either the accusative or genitive for negation, albeit the genitive is more commonly used. In Czech, Slovak and Serbo-Croatian, negating with the genitive case is perceived as rather archaic and the accusative is preferred, but genitive negation in these languages is still not uncommon, especially in music and literature.
Partial direct object.
The genitive case is used with some verbs and mass nouns to indicate that the action covers only a part of the direct object (having a function of non-existing partitive case), whereas similar constructions using the Accusative case denote full coverage. Compare the sentences:
In Russian, special partitive case or sub-case is observed for some uncountable nouns which in some contexts have preferred alternative form on -у/ю instead of standard genitive on -а/я: выпил чаю ('drank some tea'), but сорта чая ('sorts of tea').
Prepositional constructions.
The genitive case is also used in many prepositional constructions.
Turkish.
The Turkish possessive is constructed using two suffixes: a genitive case for the possessor and a possessive suffix for the possessed object. For example:
Albanian.
The genitive in Albanian is formed with the help of clitics. For example:
If the possessed object is masculine, the clitic is "i". If the possessed object is feminine, the clitic is "e". If the possessed object is plural, the clitic is "e" regardless of the gender.
The genitive is used with some prepositions: "me anë" ('by means of'), "nga ana" ('on behalf of', 'from the side of'), "për arsye" ('due to'), "për shkak" ('because of'), "me përjashtim" ('with the exception of'), "në vend" ('instead of').
Kannada.
In Kannada, the genitive case-endings are:
for masculine or feminine nouns ending in "ಅ" (a): ನ (na)
for neuter nouns ending in "ಅ" (a): ದ (da)
for all nouns ending in "ಇ" (i), "ಈ" (ī), "ಎ" (e), or "ಏ" (ē): ಅ (a)
for all nouns ending in "ಉ" (u), "ಊ" (ū), "ಋ" (r̥), or "ೠ" (r̥̄): ಇನ (ina)
Most postpositions in Kannada take the genitive case.

</doc>
<doc id="12545" url="http://en.wikipedia.org/wiki?curid=12545" title="General surgery">
General surgery

General surgery is a surgical specialty that focuses on abdominal contents including esophagus, stomach, small bowel, colon, liver, pancreas, gallbladder and bile ducts, and often the thyroid gland (depending on local reference patterns). They also deal with diseases involving the skin, breast, soft tissue, and hernias.
Scope.
General surgeons may sub-specialize into one or more of the following disciplines:
Trauma surgery.
In the United States and Canada, the overall responsibility for trauma care falls under the auspices of general surgery. Some general surgeons obtain advanced training and specialty certification in this field alone. General surgeons must be able to deal initially with almost any surgical emergency. Often they are the first port of call to critically ill or gravely injured patients, and must perform a variety of procedures to stabilize such patients, such as burr hole, cricothyroidotomy, and emergency laparotomy or thoracotomy to stanch bleeding.
All general surgeons are trained in emergency surgery. Bleeding, infections, bowel obstructions and organ perforations are the main problems they deal with. Cholecystectomy, the surgical removal of the gallbladder, is one of the most common surgical procedures done worldwide. This is most often done electively, but the gallbladder can become acutely inflamed and require an emergency operation. Ruptures of the appendix and small bowel obstructions are other common emergencies.
Laparoscopic surgery.
This is a relatively new specialty dealing with minimal access techniques using cameras and small instruments inserted through 0.3 to 1 cm incisions. Robotic surgery is now evolving from this concept (see below). Gallbladders, appendices, and colons can all be removed with this technique. Hernias are now repaired mostly laparoscopically. Most bariatric surgery is performed laparoscopically. General surgeons that are trained today are expected to be proficient in laparoscopic procedures.
Colorectal surgery.
General surgeons treat a wide variety of major and minor colon and rectal diseases including inflammatory bowel diseases (such as ulcerative colitis or Crohn's disease), diverticulitis, colon and rectal cancer, gastrointestinal bleeding and hemorrhoids.
Breast surgery.
General surgeons perform a majority of all non-cosmetic breast surgery from lumpectomy to mastectomy, especially pertaining to the evaluation and diagnosis, of breast cancer.
Vascular surgery.
General surgeons can perform vascular surgery if they receive special training and certification in vascular surgery. Otherwise, these procedures are performed by vascular surgery specialists. However, general surgeons are capable of treating minor vascular disorders.
Endocrine surgery.
General surgeons are trained to remove all or part of the thyroid and parathyroid glands in the neck and the adrenal glands just above each kidney in the abdomen. In many communities, they are the only surgeon trained to do this. In communities that have a number of subspecialists, other subspecialty surgeons may assume responsibility for these procedures.
Transplant surgery.
Responsible for all aspects of pre-operative, operative, and post-operative care of abdominal organ transplant patients. Transplanted organs include liver, kidney, pancreas, and more rarely small bowel.
Surgical oncology.
Surgical oncologist refers to a general surgical oncologist (a subspecialty of general surgery), but thoracic surgical oncologists, gynecologic oncologists and so forth can all be considered surgeons who specialize in treating cancer patients. The importance of training surgeons who sub-specialize in cancer surgery lies in evidence, supported by a number of clinical trials, that outcomes in surgical cancer care are positively associated to surgeon volume—i.e., the more cancer cases a surgeon treats, the more proficient he or she becomes, and his or her patients experience improved survival rates as a result. This is another controversial point, but it is generally accepted—even as common sense—that a surgeon who performs a given operation more often, will achieve superior results when compared with a surgeon who rarely performs the same procedure. This is particularly true of complex cancer resections such as pancreaticoduodenectomy for pancreatic cancer, and gastrectomy with extended (D2) lymphadenectomy for gastric cancer.
Cardiothoracic surgery.
Most cardiothoracic surgeons in the U.S. (D.O. or M.D.) first complete a general surgery residency (typically 5–7 years), followed by a cardiothoracic surgery fellowship (typically 2–3 years).
Trends.
In the 2000s minimally invasive surgery became more prevalent. Considerable enthusiasm has been built around robotic surgery (also known as "robotic-assisted surgery"), despite a lack of data suggesting it has significant benefits that justify its cost.
Training.
In Canada, Australia, New Zealand, and the United States general surgery is a five to seven year residency and follows completion of medical school, either MD, MBBS, MBChB, or DO degrees. In Australia and New Zealand, a residency leads to eligibility for Fellowship of the Royal Australasian College of Surgeons. In Canada, residency leads to eligibility for certification by and Fellowship of the Royal College of Physicians and Surgeons of Canada, while in the United States, completion of a residency in general surgery leads to eligibility for board certification by the American Board of Surgery or the American Osteopathic Board of Surgery which is also required upon completion of training for a general surgeon to have operating privileges at most hospitals in the United States.
In the United Kingdom, surgical trainees enter training after five years of medical school and two years of the Foundation Programme. During the two to three-year core training programme, doctors will sit the Membership of the Royal College of Surgeons (MRCS) examination. On award of the MRCS examination, surgeons may hold the title 'Mister' or 'Miss/Ms' rather than doctor. This is a tradition dating back hundreds of years in the United Kingdom. Trainees will then go onto Higher Surgical Training (HST), lasting a further five to six years. During this time they may choose to subspecialise. Before the end of HST, the examination of Fellow of the Royal College of Surgeons (FRCS) must be taken in general surgery plus the subspeciality. Upon completion of training, the surgeon will become a consultant surgeon and will be eligible for entry on the GMC Specialist Register and may work both in the NHS and independent sector as a consultant general surgeon. However, with the implementation of the European Working Time Directive limiting UK surgical residents to a 48-hour working week. The introduction of a sub-consultant grade to enable those who have recently received a UK Certificate of Completion of Training may be necessary.

</doc>
<doc id="12550" url="http://en.wikipedia.org/wiki?curid=12550" title="Gallifrey">
Gallifrey

Gallifrey is a planet in the long-running British science fiction television series "Doctor Who" and is the home world of the Doctor and the Time Lords. It was located in a binary star system within the constellation of Kasterborous, at "galactic coordinates ten-zero-eleven-zero-zero by zero-two from galactic zero centre".
During the first decade of the television series, the name of the Doctor's home planet was not revealed, although it was actually shown for the first time in "The War Games" (1969) during the Second Doctor's trial. It was finally identified by name for the first time in "The Time Warrior" (1973–74). It is never definitively stated when the appearances of Gallifrey in the television series take place. As the planet is often reached by means of time travel, its relative present could conceivably exist almost anywhere in the Earth's past or future.
Gallifrey's position in the revived series (2005 onwards) was filled in slowly over the first three years of the series' run. In the series 1 episode "The End of the World", the Ninth Doctor describes the planet as "rocks and dust", "dead [...] before its time" and having "burnt" like the Earth had done in the year 5 billion in a "war" the Doctor's people had lost. The planet was not referred to by name after the show's return in 2005 until the 2006 Christmas special, "The Runaway Bride". It was depicted in a flashback in "The Sound of Drums" in series 3 and played an important role in the plot of "The End of Time". It appeared briefly in the seventh series finale, "The Name of the Doctor", which shows the moment the First Doctor and Susan stole the TARDIS. Gallifrey is revealed at the conclusion of "The Day of the Doctor" to have survived and didn't burn as the Doctor originally believed, though it was frozen in time and shunted into another dimension.
Geography and appearances.
From space, Gallifrey is seen as a yellow-orange planet and was close enough to central space lanes for spacecraft to require clearance from Gallifreyan Space Traffic Control as they pass through its system. The planet was protected from physical attack by an impenetrable barrier called the quantum force field, and from teleportation incursions by the transduction barrier—which could be reinforced to repel most levels of this type of technological attack.
"The Name of the Doctor" presents images of the Time Lord capital. Outside the Capitol is a wilderness with its iconic red grass.
The Doctor's granddaughter Susan first describes her home world (not named as "Gallifrey" at the time) as having bright, silver-leafed trees and a burnt orange sky at night in the serial "The Sensorites" (1964). This casts an amber tint on anything outside the city, as seen in "The Invasion of Time". However, Gallifrey's sky appears blue and Earth-like in "The Five Doctors" (1983) within the isolated Death Zone.
In "The Time Monster", the Third Doctor says that "When I was a little boy, we used to live in a house that was perched halfway up the top of a mountain", explaining, "I ran down that mountain and I found that the rocks weren't grey at all—but they were red, brown and purple and gold. And those pathetic little patches of sludgy snow were shining white. Shining white in the sunlight." In "Gridlock", the Tenth Doctor echoes Susan's description of the world now named as Gallifrey and goes further by mentioning the vast mountain ranges "with fields of deep red grass, capped with snow". He then elaborates how Gallifrey's second sun would "rise in the south and the mountains would shine", with the silver-leafed trees looking like "a forest on fire" in the mornings.
Outer Gallifrey's wastelands are where the "Outsiders" reside, The Doctor Who Role Playing Game released by FASA equates the Outsiders with the "Shobogans", who are briefly mentioned in the serial "The Deadly Assassin". The wastes of Gallifrey include the Death Zone, an area that was used as a gladiatorial arena by the first Time Lords, pitting various species kidnapped from their respective time zones against each other (although Daleks and Cybermen were considered too dangerous to use). Inside the Death Zone stands the Tomb of Rassilon, the founder of Time Lord society.
Somewhere on Gallifrey there is also an institute called the Academy, which the Doctor and various other Time Lords have attended.
The 2013 minisode "The Last Day" mentions birds as something expected in Gallifrey's skies. Gallifrey appeared in the "Doctor Who" 50th anniversary special, "The Day of the Doctor" which aired on November 23, 2013.
Spin-off material.
Several of the spin-off novels have further information about Gallifrey. It is said to have at least two moons, one being the copper-coloured Pazithi Gallifreya (first named in ""); the novel "Lungbarrow" also places Karn (setting of "The Brain of Morbius", 1976) in Gallifrey's solar system, along with a frozen gas giant named Polarfrey and an "astrological figure" of "Kasterborous the Fibster". "Cat's Cradle: Time's Crucible" also mentions edible rodent-like mammals called tafelshrews.
History.
On screen.
Few details on the history of the planet itself emerge from the original series run from 1963–1989. In "The End of the World" (2005), the Ninth Doctor states that his home planet has been destroyed in a war and that he is the last of the Time Lords. The episode also indicates that the Time Lords are remembered in the far future.
Subsequently, in "Dalek" (2005), it is revealed that the last great Time War was fought between the Time Lords and the Daleks, ending in the obliteration of both sides and with only two apparent survivors; the Doctor and a lone Dalek that had somehow fallen through time and crashed on Earth. At the conclusion of that episode, that surviving Dalek self-destructs, leaving the Ninth Doctor believing that he was the sole survivor of the Time War. However, the Daleks return in "Bad Wolf"/"The Parting of the Ways" (2005), and subsequently in several other stories.
The Tenth Doctor's reference to Gallifrey in "The Runaway Bride" (2006) is the first time the name of his homeworld has been given onscreen since the new series began. The Doctor's revelation that he is from Gallifrey elicits terror from the Empress of the Racnoss. The Tenth Doctor in human form (as "John Smith") mentions Gallifrey in "Human Nature" (2007) and is asked if it was in Ireland; this is the same question asked in the 1970s stories "The Hand of Fear" and "The Invisible Enemy".
The planet makes its first appearance in the revived series in "The Sound of Drums" (2007), where the Citadel, enclosed in a glass dome (as described by the Doctor in "Gridlock", 2007), is seen in flashback as the Doctor describes it. Also seen is a ceremony initiating 8-year-old Gallifreyans — in particular the Master — into the Time Lord Academy.
"The End of Time" (2009–10) once again featured Gallifrey. By the end of the war, Gallifrey is depicted in ruins. The dome of the main city, the Time Lord capital, the Citadel, is shattered and dozens of Dalek saucers have crashed on the plains below. The Master releases Gallifrey from its time lock. However, Gallifrey's reemergence is eventually stopped and reversed after it is made clear that the release of Gallifrey would lead to the Time Lords destroying time — in effect destroying the universe — in order to defeat the Daleks and ultimately to preserve the Time Lords at the expense of all creation. Lord President Rassilon also believes that this action would elevate them to a higher form of existence, becoming "creatures of consciousness". Upon realising the scope of Rassilon's plan for self-preservation, the Tenth Doctor recalls that the Doctor that fought in the Time War had attempted to stop them during the war. Eventually, the Master comes to the aid of the Tenth Doctor and prevents Rassilon and the rest of Gallifrey from coming through, breaking the link that held Gallifrey in relative time to 21st century Earth.
It is stated by the Tenth Doctor in "The End of Time" that Gallifrey is not how he and the Master knew it in their youth. Implying that the Time Lords had resorted to desperate and deplorable measures to fight the Daleks, the Doctor is willing to break his code of non-violence to stop the return of the Time Lords. This is reinforced within a short feature that discloses the hitherto unknown circumstances of the Eighth Doctor's regeneration into the War Doctor, entitled "The Night of the Doctor" (2013). A young pilot rejects assistance from the Eighth Doctor due to her fear of the Time Lords.
In the series seven finale, "The Name of the Doctor", two Time Lords were seen on Gallifrey watching the First Doctor and Susan steal a TARDIS.
In the 50th anniversary special of the television series, "The Day of the Doctor" (2013), scenes are shown during the fall of Arcadia, Gallifrey's second city. Subsequently it is shown that Gallifrey wasn't actually destroyed. The final scenes depict three of the Doctor's incarnations—the Tenth Doctor, the Eleventh Doctor and the War Doctor; the interface of the weapon, the Moment, that was supposed to destroy the planet; and the Eleventh Doctor's companion Clara Oswald decide against destroying it. Instead, they freeze the planet in time within a parallel pocket universe. This occurs with the help of the other Doctors, including the Twelfth Doctor, whose eyes alone are seen at this point. Instead of destroying Gallifrey, the Dalek fleet echelons open fire on and destroy each other.
In "The Time of the Doctor" (2013), the Time Lords are depicted as trying to re-enter the universe through a crack in the Universe on the planet Trenzalore. They broadcast a message throughout space and time, the question "Doctor Who?", a question which only the Doctor could answer. When the Doctor answers, they will know that it is safe to leave. However, this message inadvertently attracts various races, including the Daleks and Cybermen, to lay siege to Trenzalore; the Eleventh Doctor remaining to protect the inhabitants, but not wanting to release the Time Lords as this would mean the destruction of Trenzalore and the initiation of another Time War. Hundreds of years later, Clara convinces the Time Lords to help the Doctor, dying from old age in his final regeneration, and the crack closes, before reopening in the sky above Trenzalore. The Time Lords give the Doctor a new regeneration cycle, before the crack seals for good with the Time Lords still lost, but a newly regenerated Twelfth Doctor ready to find them. In "Death in Heaven" (2014), Missy tells the Twelfth Doctor that Gallifrey is located at its original coordinates. These claims prove to be false, leaving the Doctor distraught.
Novels.
Various spin-off novels have expanded on the history and nature of Gallifrey.
Marc Platt's novels "" and "Lungbarrow", provide a detailed backstory for the civilisation seen in the main series. In the Dark Times (occasionally mentioned in the televised serials such as "The Five Doctors"), Gallifrey was at the centre of an empire covering dozens of worlds and continually being extended by heroes such as Prydonius (whom the Time Lord chapter is named after). Ancient Gallifreyans are all telepathic and were ruled by a female cult centred on a figure called the Pythia, who controlled the population through mysticism and prophecies. When the prophetic powers of the last of the Pythias failed her, Rassilion, Omega and a shadowy figure known as The Other seized power in the name of science and rationality. Seeing this, the Pythia committed suicide and cursed Gallifrey, killing all children in their wombs and making the world sterile. To combat this, Rassilion restructured society and used genetic looms to create new generations of Gallifreyans, who emerge from the looms as fully grown adults. Each of the Great Houses is allotted a total of forty-five cousins and given a regeneration cycle of thirteen lives. The Houses themselves are to some degree alive, in the same way TARDISes are, and the furniture can move about, occasionally growing into 'Drudges' who function as servants for the family. The Doctor was loomed in the House of Lungbarrow in the mountains of South Gallifrey, but unique among the house's cousins, he has a belly button. This sterility backstory is contradicted by on screen depictions and descriptions of the Master and the Doctor as children, the Eleventh Doctor stating he slept in the cot he brings out of his TARDIS and references made to the Doctor and the Master having parents and the Doctor himself being a father.
The Virgin New Adventures establish a religion on Gallifrey centred around the three main gods, Time, Death and Pain. The Time Lords use these figures to understand the concepts they represent and in some cases make deals with them and become their chosen champions. The Seventh Doctor is Time's Champion (as well as someone who makes frequent deals with or wages against Death to save his friends) and the audio play "Master" states that the Master is Death's champion. It's also briefly implied in "Vampire Science", that the Eighth Doctor is Life's champion, implying the existence of another unseen figure. "Happy Endings" and other books imply that these gods are Eternals as seen in the serial "Enlightenment".
In the Eighth Doctor Adventures novel "The Ancestor Cell" by Peter Anghelides and Stephen Cole, Gallifrey is destroyed as a result of the Eighth Doctor's desire to prevent the voodoo cult Faction Paradox from starting a war between the Time Lords and an unnamed Enemy. Hints about this future war are dropped in several books earlier in the series beginning with Lawrence Miles' "Alien Bodies". In order to have boltholes or decoys in case of attack, the Time Lords have created nine separate planet Gallifreys (it even hinted that the original Gallifrey may at some point be reduced to ruins) and special looms to constantly produce new soldiers. By this time TARDISes have evolved to point where they appear human and reproduce sexually (the Doctor's companion Compassion is the first such TARDIS). It is also hinted that the Celestial Intervention Agency will evolve into the beings of pure thought known as the Celestis, who observe the war from outside this dimension (the Last Parliament in which they sit resembles the Panopticon on Gallifrey and the closest anyone gets to describing them is similar to the Time Lords' robes). Faction Paradox itself is a counter to Time Lord society, dedicated to creating time-travel paradoxes, in contrast to the Time Lords' web of time. It was founded by a mysterious figure Grandfather Paradox, who it is believed was once a Time Lord from the House of Lungbarrow. "The Ancestor Cell" suggests that he is a future version of the Doctor, but this is retconned in "The Gallifrey Chronicles", to him being everyone's potential future self. When the Doctor destroys Gallifrey the war no longer happens and his actions also apparently (and retroactively) wipe the Time Lords from history.
In the last regular Eighth Doctor novel, "The Gallifrey Chronicles" by Lance Parkin, it is revealed that while Gallifrey was destroyed, the Time Lords were not erased from history. However, the cataclysm sets up an event horizon in time that prevents anyone from entering Gallifrey's relative past or travelling from it to the present or future. The Time Lords also survive within the Matrix, which has been downloaded into the Eighth Doctor's mind, but their reconstruction requires a sufficiently advanced computer. At the novel's end, the question of whether or not the Time Lords will be restored remains unanswered.
Television series executive producer Russell T Davies wrote in "Doctor Who Magazine" #356 that there is no connection between the War of the books and the Time War of the television series. In the same "Doctor Who Magazine" column, Davies compared Gallifrey being destroyed twice with Earth's two World Wars. He also said that he was "usually happy for old and new fans to invent the Complete History of the Doctor in their heads, completely free of the production team's hot and heavy hands".

</doc>
<doc id="12572" url="http://en.wikipedia.org/wiki?curid=12572" title="Grus (constellation)">
Grus (constellation)

Grus (, or colloquially ) is a constellation in the southern sky. Its name is Latin for the crane, a type of bird. It is one of twelve constellations conceived by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. Grus first appeared on a 35-cm (14 in) diameter celestial globe published in 1598 in Amsterdam by Plancius and Jodocus Hondius and was depicted in Johann Bayer's star atlas "Uranometria" of 1603. French explorer and astronomer Nicolas Louis de Lacaille gave Bayer designations to its stars in 1756, some of which had been previously considered part of the neighbouring constellation Piscis Austrinus. The constellations Grus, Pavo, Phoenix and Tucana are collectively known as the "Southern Birds".
The constellation's brightest star, Alpha Gruis, is also known as Alnair and appears as a 1.7-magnitude blue-white star. Beta Gruis is a red giant variable star with a minimum magnitude of 2.3 and a maximum magnitude of 2.0. Six star systems have been found to have planets: the red dwarf Gliese 832 is one of the closest stars to Earth to have a planetary system. Another—WASP-95—has a planet that orbits every two days. Deep-sky objects found in Grus include the planetary nebula IC 5148, also known as the Spare Tyre Nebula, and a group of four interacting galaxies known as the Grus Quartet.
History.
The stars that form Grus were originally considered part of the neighbouring constellation Piscis Austrinus (the southern fish), with Gamma Gruis seen as part of the fish's tail. The stars were first defined as a separate constellation by the Dutch astronomer Petrus Plancius, who created twelve new constellations based on the observations of the southern sky by the Dutch explorers Pieter Dirkszoon Keyser and Frederick de Houtman, who had sailed on the first Dutch trading expedition, known as the "Eerste Schipvaart", to the East Indies. Grus first appeared on a 35-cm diameter celestial globe published in 1598 in Amsterdam by Plancius with Jodocus Hondius. Its first depiction in a celestial atlas was in the German cartographer Johann Bayer's "Uranometria" of 1603. De Houtman included it in his southern star catalogue the same year under the Dutch name "Den Reygher", "The Heron", but Bayer followed Plancius and Hondius in using Grus.
An alternative name for the constellation, "Phoenicopterus" (Latin "flamingo"), was used briefly during the early 17th century, seen in the 1605 work "Cosmographiae Generalis" by Paul Merula of Leiden University and a c. 1625 globe by Dutch globe maker Pieter van den Keere. Astronomer Ian Ridpath has reported the symbolism likely came from Plancius originally, who had worked with both of these people. Grus and the nearby constellations Phoenix, Tucana and Pavo are collectively called the "Southern Birds".
The stars that correspond to Grus were generally too far south to be seen from China. In Chinese astronomy, Gamma and Lambda Gruis may have been included in the tub-shaped asterism "Bàijiù", along with stars from Piscis Austrinus. In Central Australia, the Arrernte and Luritja people living on a mission in Hermannsburg viewed the sky as divided between them, east of the Milky Way representing Arrernte camps and west denoting Luritja camps. Alpha and Beta Gruis, along with Fomalhaut, Alpha Pavonis and the stars of Musca, were all claimed by the Arrernte.
Characteristics.
Grus is bordered by Piscis Austrinus to the north, Sculptor to the northeast, Phoenix to the east, Tucana to the south, Indus to the southwest, and Microscopium to the west. Bayer straightened the tail of Piscis Austrinus to make way for Grus in his "Uranometria". Covering 366 square degrees, it ranks 45th of the 88 modern constellations in size and covers 0.887% of the night sky. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is "Gru". The official constellation boundaries, as set by Eugène Delporte in 1930, are defined as a polygon of 6 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between 21h 27.4m and 23h 27.1m, while the declination coordinates are between −36.31° and −56.39°. Grus is located too far south to be seen by observers in the British Isles and the northern United States, though it can easily be seen from Florida or California; the whole constellation is visible to observers south of latitude 33°N.
Notable features.
Stars.
Keyser and de Houtman assigned twelve stars to the constellation. Bayer depicted Grus on his chart, but did not assign its stars Bayer designations. French explorer and astronomer Nicolas Louis de Lacaille labelled them Alpha to Phi in 1756 with some omissions. In 1879, American astronomer Benjamin Gould added Kappa, Nu, Omicron and Xi, which had all been catalogued by Lacaille but not given Bayer designations. Lacaille considered them too faint, while Gould thought otherwise. Xi Gruis had originally been placed in Microscopium. Conversely, Gould dropped Lacaille's Sigma as he thought it was too dim.
Grus has several bright stars. Marking the left wing is Alpha Gruis, a blue-white star of spectral type B6V and apparent magnitude 1.7, around 101 light-years from Earth. Its traditional name, Alnair, means "the bright one" and refers to its status as the brightest star in Grus. Alnair is around 380 times as luminous and has over 3 times the diameter of the Sun. Lying 5 degrees west of Alnair, denoting the Crane's heart is Beta Gruis, a red giant of spectral type M5III. It has a diameter of 0.8 astronomical units (AU) (if placed in the Solar System it would extend to the orbit of Venus) located around 170 light-years from Earth. It is a variable star with a minimum magnitude of 2.3 and a maximum magnitude of 2.0. An imaginary line drawn from the Great Square of Pegasus through Fomalhaut will lead to Alnair and Beta Gruis.
Lying in the northwest corner of the constellation and marking the crane's eye is Gamma Gruis, a blue-white subgiant of spectral type B8III and magnitude 3.0 lying around 211 light-years from Earth. Also known as Al Dhanab, it has finished fusing its core hydrogen and has begun cooling and expanding, which will see it transform into a red giant.
There are several naked-eye double stars in Grus. Forming a triangle with Alnair and Beta, Delta Gruis is an optical double whose components—Delta1 and Delta2—are separated by 45 arcseconds. Delta1 is a yellow giant of spectral type G7III and magnitude 4.0, 309 light-years from Earth, and may have its own magnitude 12 orange dwarf companion. Delta2 is a red giant of spectral type M4.5III and semiregular variable that ranges between magnitudes 3.99 and 4.2, located 325 light-years from Earth. It has around 3 times the mass and 135 times the diameter of our sun. Mu Gruis, composed of Mu1 and Mu2, is also an optical double—both stars are yellow giants of spectral type G8III around 2.5 times as massive as the Sun with surface temperatures of around 4900 K. Mu1 is the brighter of the two at magnitude 4.8 located around 275 light-years from Earth, while Mu2 the dimmer at magnitude 5.11 lies 265 light-years distant from Earth. Pi Gruis, an optical double with a variable component, is composed of Pi1 Gruis and Pi2. Pi1 is a semi-regular red giant of spectral type S5, ranging from magnitude 5.31 to 7.01 over a period of 191 days, and is around 532 light-years from Earth. One of the brightest S-class stars to Earth viewers, it has a companion star of apparent magnitude 10.9 with sunlike properties, being a yellow main sequence star of spectral type G0V. The pair make up a likely binary system. Pi2 is a giant star of spectral type F3III-IV located around 130 light-years from Earth, and is often brighter than its companion at magnitude 5.6. Marking the right wing is Theta Gruis, yet another double star, lying 5 degrees east of Delta1 and Delta2.
RZ Gruis is a binary system of apparent magnitude 12.3 with occasional dimming to 13.4, whose components—a white dwarf and main sequence star—are thought to orbit each other roughly every 8.5 to 10 hours. It belongs to the UX Ursae Majoris subgroup of cataclysmic variable star systems, where material from the donor star is drawn to the white dwarf where it forms an accretion disc that remains bright and outshines the two component stars. The system is poorly understood, though the donor star has been calculated to be of spectral type F5V. These stars have spectra very similar to novae that have returned to quiescence after outbursts, yet they have not been observed to have erupted themselves. The American Association of Variable Star Observers recommends watching them for future events. CE Gruis (also known as Grus V-1) is a faint (magnitude 18–21) star system also composed of a white dwarf and donor star; in this case the two are so close they are tidally locked. Known as polars, material from the donor star does not form an accretion disc around the white dwarf, but rather streams directly onto it.
Six star systems are thought to have planetary systems. Tau1 Gruis is a yellow star of magnitude 6.0 located around 106 light-years away. It may be a main sequence star or be just beginning to depart from the sequence as it expands and cools. In 2002 the star was found to have a planetary companion. HD 215456, HD 213240 and WASP-95 are yellow sunlike stars discovered to have two planets, a planet and a remote red dwarf, and a hot Jupiter respectively; this last—WASP-95b—completes an orbit round its sun in a mere two days. Gliese 832 is a red dwarf of spectral type M1.5V and apparent magnitude 8.66 located only 16.1 light-years distant; hence it is one of the nearest stars to the Solar System. A Jupiter-like planet—Gliese 832 b—orbiting the red dwarf over a period of 9.4±0.4 years was discovered in 2008. WISE 2220−3628 is a brown dwarf of spectral type Y, and hence one of the coolest star-like objects known. It has been calculated as being around 26 light-years distant from Earth.
Deep-sky objects.
Nicknamed the spare-tyre nebula, IC 5148 is a planetary nebula located around 1 degree west of Lambda Gruis. Around 3000 light-years distant, it is expanding at 50 kilometres a second, one of the fastest rates of expansion of all planetary nebulae.
Northeast of Theta Gruis are four interacting galaxies known as the Grus Quartet. These galaxies are NGC 7552, NGC 7590, NGC 7599, and NGC 7582. The latter three galaxies occupy an area of sky only 10 arcminutes across and are sometimes referred to as the "Grus Triplet," although all four are part of a larger loose group of galaxies called the IC 1459 Grus Group. NGC 7552 and 7582 are exhibiting high starburst activity; this is thought to have arisen because of the tidal forces from interacting. Located on the border of Grus with Piscis Austrinus, IC 1459 is a peculiar E3 giant elliptical galaxy. It has a fast counterrotating stellar core, and shells and ripples in its outer region. The galaxy has an apparent magnitude of 11.9 and is around 80 million light years distant.
NGC 7424 is a barred spiral galaxy with an apparent magnitude of 10.4. located around 4 degrees west of the Grus Triplet. Approximately 37.5 million light years distant, it is about 100,000 light years in diameter, has well defined spiral arms and is thought to resemble the Milky Way. Two ultraluminous X-ray sources and one supernova have been observed in NGC 7424. SN 2001ig was discovered in 2001 and classified as a Type IIb supernova, one that initially showed a weak hydrogen line in its spectrum, but this emission later became undetectable and was replaced by lines of oxygen, magnesium and calcium, as well as other features that resembled the spectrum of a Type Ib supernova. A massive star of spectral type F, A or B is thought to be the surviving binary companion to SN 2001ig, which was believed to have been a Wolf–Rayet star.
Located near Alnair is NGC 7213, a face-on type 1 Seyfert galaxy located approximately 71.7 million light years from Earth. It has an apparent magnitude of 12.1. Appearing undisturbed in visible light, it shows signs of having undergone a collision or merger when viewed at longer wavelengths, with disturbed patterns of ionized hydrogen including a filament of gas around 64,000 light-years long. It is part of a group of ten galaxies.
NGC 7410 is a spiral galaxy discovered by British astronomer John Herschel during observations at the Cape of Good Hope in October 1834. The galaxy has a visual magnitude of 11.7 and is approximately 122 million light years distant from Earth.
External links.
Coordinates: 

</doc>
<doc id="12611" url="http://en.wikipedia.org/wiki?curid=12611" title="GTE">
GTE

GTE Corporation, formerly General Telephone & Electric Corporation (1955-1982) was the largest independent telephone company in the United States during the days of the Bell System.
Originally founded in 1926 as Associated Telephone Utilities, it went bankrupt in 1933 during the Great Depression, and reorganized as General Telephone in 1934. In 1991, it acquired the third largest independent, Continental Telephone (ConTel). They also owned Automatic Electric, a telephone equipment supplier similar in many ways to Western Electric, and Sylvania Lighting, the only non-communications-oriented company under GTE ownership. GTE provided local telephone service to a large number of areas of the U.S. through operating companies, much as American Telephone & Telegraph provided local telephone service through its 22 Bell Operating Companies.
The company also acquired BBN Planet, one of the earliest Internet service providers, in 1997. That division became known as GTE Internetworking, and was later spun off into the independent company Genuity (a name recycled from another Internet company GTE acquired in 1997) to satisfy Federal Communications Commission (FCC) requirements regarding the GTE-Bell Atlantic merger that created Verizon.
GTE operated in Canada via large interests in subsidiary companies such as BC TEL and Quebec-Téléphone. When foreign ownership restrictions on telecommunications companies were introduced, GTE's ownership was grandfathered. When BC Tel merged with Telus (the name given the privatized Alberta Government Telephones (AGT)) to create BCT.Telus, GTE's Canadian subsidiaries were merged into the new parent, making it the second-largest telecommunications carrier in Canada. As such, GTE's successor, Verizon Communications, was the only foreign telecommunications company with a greater than 20% interest in a Canadian carrier, until Verizon completely divested itself of its shares in 2004.
In the Caribbean, CONTEL purchased several major stakes in the newly independent countries of the British West Indies (namely in Barbados, Jamaica, and Trinidad and Tobago).
Prior to GTE's merger with Bell Atlantic, GTE also maintained an interactive television service joint-venture called GTE mainStreet (sometimes also called mainStreet USA) as well as an interactive entertainment and video game publishing operation, GTE Interactive Media.
History.
GTE's heritage can be traced to 1918, when three Wisconsin public utility accountants (John F. O'Connell, Sigurd L. Odegard, and John A. Pratt) pooled $33,500 to purchase the Richland Center Telephone Company, serving 1,466 telephones in the dairy belt of southern Wisconsin. In 1920, the three accountants formed a corporation, Commonwealth Telephone Company, with Odegard as president, Pratt as vice-president, and O'Connell as secretary. Richland Center Telephone became part of Commonwealth Telephone, which quickly purchased telephone companies in three nearby communities. In 1922, Pratt resigned as vice-president and was replaced by Clarence R. Brown, a former Bell System employee.
By the mid-1920s, Commonwealth Telephone had extended beyond Wisconsin borders and purchased the Belvidere Telephone Company in Illinois. It also diversified into other utilities by acquiring two small Wisconsin electrical companies. Expansion was stepped up in 1926, when Odegard secured an option to purchase Associated Telephone Company of Long Beach, California and proceeded to devise a plan for a holding company, to be named Associated Telephone Utilities Company. An aggressive acquisition program was quickly launched in eastern, midwestern, and western states, with the company using its own common stock to complete transactions.
During its first six years, Associated Telephone Utilities acquired 340 telephone companies, which were consolidated into 45 companies operating more than 437,000 telephones in 25 states. By the time the stock market bottomed out in October 1929, Associated Telephone Utilities was operating about 500,000 telephones with revenues approaching $17 million.
In January 1930, a new subsidiary, Associated Telephone Investment Company, was established. Designed to support its parent's acquisition program, the new company's primary business was buying company stock in order to bolster its market value. Within two years, the investment company had incurred major losses, and a $1 million loan had to be negotiated. Associated Telephone Investment was dissolved but not before its parent's financial plight had become irreversible, and in 1933, Associated Telephone Utilities went into receivership.
General Telephone.
The company was reorganized that same year and resurfaced in 1935 as General Telephone Corporation, operating 12 newly consolidated companies. John Winn, a 26-year veteran of the Bell System, was named president. In 1936, General Telephone created a new subsidiary, General Telephone Directory Company, to publish directories for the parent's entire service area.
Like other businesses, the telephone industry was under government restrictions during World War II, and General Telephone was called upon to increase services at military bases and war-production factories. Following the war, General Telephone reactivated an acquisitions program that had been dormant for more than a decade and purchased 118,000 telephone lines between 1946 and 1950. In 1950, General Telephone purchased its first telephone-equipment manufacturing subsidiary, Leich Electric Company, along with the related Leich Sales Corporation.
General Telephone’s holdings included 15 telephone companies across 20 states by 1951, when Donald Clinton Power (attorney, utilities commissioner and former executive secretary for Ohio Governor John Bricker) was named president of the company under chairman and long-time GT executive Morris F. LaCroix, replacing the retiring Harold Bozell (president 1940 - 1951). Power proceeded to expand the company through the 1950s principally through two acquisitions.
In 1955, Theodore Gary & Company, the second-largest independent telephone company, which had 600,000 telephone lines, was merged into General Telephone, which had grown into the largest independent outside the Bell System. The merger gave the company 2.5 million lines. Theodore Gary's assets included telephone operations in the Dominican Republic, British Columbia, and the Philippines, as well as Automatic Electric, the second-largest telephone equipment manufacturer in the U.S. It also had a subsidiary, named the General Telephone and Electric Corporation, formed in 1930 with the Transamerica Corporation and British investors to compete against ITT.
In 1959, General Telephone and Sylvania Electric Products merged, and the parent's name was changed to General Telephone & Electric Corporation (GT&E). The merger gave Sylvania - a leader in such industries as lighting, television and radio, and chemistry and metallurgy - the needed capital to expand. For General Telephone, the merger meant the added benefit of Sylvania's extensive research and development capabilities in the field of electronics. Power also orchestrated other acquisitions in the late 1950s, including Peninsular Telephone Company in Florida, with 300,000 lines, and Lenkurt Electric Company, Inc., a leading producer of microwave and data transmissions systems.
In 1960, the subsidiary GT&E International Incorporated was formed to consolidate manufacturing and marketing activities of Sylvania, Automatic Electric, and Lenkurt, outside the United States. Power was named C.E.O. and chairman In 1961, making way for Leslie H. Warner, formerly of Theodore Gary, to become president. During the next several years, the scope of GT&E's research, development, and marketing activities was broadened. In 1963, Sylvania began full-scale production of color television picture tubes, and within two years, it was supplying color tubes for 18 of the 23 domestic U.S. television manufacturers. About the same time, Automatic Electric began supplying electronic switching equipment for the U.S. defense department's global communications systems, and GT&E International began producing earth-based stations for both foreign and domestic markets. GT&E's telephone subsidiaries, meanwhile, began acquiring community-antenna television systems (CATV) franchises in their operating areas.
In 1964, Warner orchestrated a deal that merged Western Utilities Corporation, the nation's second-largest independent telephone company, with 635,000 telephones, into GT&E. The following year Sylvania introduced the revolutionary four-sided flashcube, enhancing its position as the world's largest flashbulb producer. Acquisitions in telephone service continued under Warner during the mid-1960s. Purchases included Quebec Telephone in Canada, Hawaiian Telephone Company, and Northern Ohio Telephone Company and added a total of 622,000 telephone lines to GT&E operations. By 1969, GT&E was serving ten million telephones.
In the late 1960s, GT&E joined in the search for a railroad car Automatic Car Identification system. It designed the KarTrak optical system, which won over other manufacturer's systems in field trials, but ultimately proved to need too much maintenance. In the late 1970s the system was abandoned.
In March 1970, GT&E's New York City headquarters was bombed by a radical antiwar group in protest of the company's participation in defense work. In December of that year the GT&E board agreed to move the company's headquarters to Stamford, Connecticut. In 1971 GT&E undertook an identity change and became simply GTE, while Sylvania Electric Products became GTE Sylvania. The same year, Donald C. Power retired and Leslie H. Warner became chairman of the Board. Theodore F. Brophy was brought in as president.
After initially proposing to build separate satellite systems, GTE and its telecommunications rival, American Telephone & Telegraph, announced in 1974 joint venture plans for the construction and operation of seven earth-based stations interconnected by two satellites. Also in 1974 Sylvania acquired name and distribution rights for Philco television and stereo products. GTE International expanded its activities during the same period, acquiring television manufacturers in Canada and Israel and a telephone manufacturer in Germany.
In 1976, newly elected chairman Theodore F. Brophy reorganized the company along five global product lines: communications, lighting, consumer electronics, precision materials, and electrical equipment. GTE International was phased out during the reorganization, and GTE Products Corporation was formed to encompass both domestic and foreign manufacturing and marketing operations. At the same time, GTE Communications Products was formed to oversee operations of Automatic Electric, Lenkurt, Sylvania, and GTE Information Systems. In 1979, another reorganization soon followed under new president Thomas A. Vanderslice. GTE Products Group was eliminated as an organizational unit and GTE Electrical Products, consisting of lighting, precision materials, and electrical equipment, was formed. Vanderslice also revitalized the GT&E Telephone Operating Group in order to develop competitive strategies for anticipated regulatory changes in the telecommunications industry.
In 1979, GTE purchased Telenet to establish a presence in the growing packet switching data communications business. GTE Telenet was later included in the US Telecom joint venture.
1980s.
GT&E sold its consumer electronics businesses, including the accompanying brand names of Philco and Sylvania to Philips in 1981, after watching revenues from television and radio operations decrease precipitously with the success of foreign manufacturers. Following AT&T's 1982 announcement that it would divest 22 telephone operating companies, GT&E made a number of reorganization moves.
In 1982, the company adopted the name GTE Corporation and formed GTE Mobilnet Incorporated to handle the company's entrance into the new cellular telephone business. In 1983 GTE sold its electrical equipment, brokerage information services, and cable television equipment businesses. That same year, Automatic Electric and Lenkurt were combined as GTE Network Systems.
GTE became the third-largest long-distance telephone company in 1983 through the acquisition of Southern Pacific Communications Company. At the same time, Southern Pacific Satellite Company was acquired, and the two firms were renamed GTE Sprint Communications Corporation and GTE Spacenet Corporation, respectively. Through an agreement with the Department of Justice, GTE conceded to keep Sprint Communications separate from its other telephone companies and limit other GTE telephone subsidiaries in certain markets. In December 1983 Vanderslice resigned as president and chief operating officer.
In 1984, GTE formalized its decision to concentrate on three core businesses: telecommunications, lighting, and precision metals. That same year, the company's first satellite was launched, and GTE's cellular telephone service went into operation; GTE's earnings exceeded $1 billion for the first time. In 1986, GTE acquired Airfone Inc., a telephone service provider for commercial aircraft and railroads, and Rotaflex plc, a United Kingdom-based manufacturer of lighting fixtures.
Beginning in 1986, GTE spun off several operations to form joint ventures. In 1986 GTE Sprint and United Telecommunication's long-distance subsidiary, US Telecom, agreed to merge and form US Sprint Communications Company, with each parent retaining a 50 percent interest in the new firm. That same year, GTE transferred its international transmission, overseas central office switching, and business systems operations to a joint venture with Siemens AG of Germany, which took 80 percent ownership of the new firm. The following year, GTE transferred its business systems operations in the United States to a new joint venture, Fujitsu GTE Business Systems, Inc., formed with Fujitsu Limited, which retained 80 percent ownership.
In April 1988, after the retirement of Theodore F. Brophy, James L. "Rocky" Johnson was promoted from his position as president and chief operating officer to CEO and chairman of GTE. Under his leadership, GTE divested its consumer communications products unit as part of a telecommunications strategy to place increasing emphasis on the services sector. The following year GTE sold the majority of its interest in US Sprint to United Telecommunications and its interest in Fujitsu GTE Business Systems to Fujitsu.
In 1989, GTE and AT&T formed the joint venture company AG Communication Systems Corporation, designed to bring advanced digital technology to GTE's switching systems. GTE retained 51 percent control over the joint venture, with AT&T pledging to take complete control of the new firm in 15 years.
With an increasing emphasis on telecommunications, in 1989 GTE launched a program to become the first cellular provider offering nationwide service and introduced the nation's first rural service area, providing cellular service on the Hawaiian island of Kauai. The following year GTE acquired the Providence Journal Company's cellular properties in five southern states for $710 million and became the second largest cellular-service provider in the United States.
1990s.
In 1990, GTE reorganized its activities around three business groups: telecommunications products and services, telephone operations, and electrical products. That same year, GTE and Contel Corporation announced merger plans that would strengthen GTE's telecommunications and telephone sectors.
Following action or review by more than 20 governmental bodies, in March 1991 the merger of GTE and Contel was approved. Over half of Contel's $6.6 billion purchase price, $3.9 billion, was assumed debt. In April 1992, James L. "Rocky" Johnson retired after 43 years at GTE, remaining on the GTE board of directors as Chairman Emeritus. Charles "Chuck" Lee was named to succeed Johnson. Lee's first order of business was reduction of that obligation. He sold GTE's North American Lighting business to a Siemens affiliate for over $1 billion, shaved off local exchange properties in Idaho, Tennessee, Utah, and West Virginia to generate another $1 billion, and divested its interest in Sprint in 1992. In 1994, he sold its GTE Spacenet satellite operations to General Electric in 1994 and sold Contel of Maine to Oxford Networks, which placed the company into a newly created subsidiary, Oxford West Telephone.
The Telecommunications Act of 1996 promised to encourage competition among local phone providers, long distance services, and cable television companies. Many leading telecoms prepared for the new competitive realities by aligning themselves with entertainment and information providers. GTE, on the other hand, continued to focus on its core operations, seeking to make them as efficient as possible.
Among other goals, GTE's plan sought to double revenues and slash costs by $1 billion per year by focusing on five key areas of operation: technological enhancement of wireline and wireless systems, expansion of data services, global expansion, and diversification into video services. GTE hoped to cross-sell its large base of wireline customers on wireless, data and video services, launching Tele-Go, a user-friendly service that combined cordless and cellular phone features. The company bought broadband spectrum cellular licenses in Atlanta, Seattle, Cincinnati and Denver, and formed a joint venture with SBC Communications to enhance its cellular capabilities in Texas. In 1995, the company undertook a 15-state test of video conferencing services, as well as a video dialtone (VDT) experiment that proposed to offer cable television programming to 900,000 homes by 1997. GTE also formed a video programming and interservices joint venture with Ameritech Corporation, BellSouth Corporation, SBC, and The Walt Disney Company in the fall of 1995.
Foreign efforts included affiliations with phone companies in Argentina, Mexico, Germany, Japan, Canada, the Dominican Republic, Venezuela and China. The early 1990s reorganization included a 37.5 percent workforce reduction, from 177,500 in 1991 to 111,000 by 1994. Lee's fivefold strategy had begun to bear fruit by the mid-1990s. While the communication conglomerate's sales remained rather flat, at about $19.8 billion, from 1992 through 1994, its net income increased by 43.7 percent, from $1.74 billion to a record $2.5 billion, during the same period.
Merger with Bell Atlantic.
Bell Atlantic merged with GTE on June 30, 2000, and named the new entity Verizon Communications. The GTE operating companies retained by Verizon are now collectively known as Verizon West division of Verizon (including east coast service territories). The remaining smaller operating companies were sold off or transferred into the remaining ones. Additional properties were sold off within a few years after the merger to CenturyTel, Alltel, and Hawaiian Telcom. On July 1, 2010, Verizon sold many former GTE properties to Frontier Communications. 
Operating companies.
Prior to the merger with Bell Atlantic, GTE owned the following operating companies in the US:
Following the merger with Bell Atlantic, some of these companies and/or access lines have been sold off to other companies, such as Alltel, ATEAC, The Carlyle Group, CenturyTel, Citizens/Frontier Communications, and Valor Telecom.

</doc>
<doc id="12628" url="http://en.wikipedia.org/wiki?curid=12628" title="GIMP">
GIMP

GIMP (; an acronym for GNU Image Manipulation Program) is a free and open-source raster graphics editor used for image retouching and editing, free-form drawing, resizing, cropping, photo-montages, converting between different image formats, and more specialized tasks.
GIMP began in 1995 as the school project of two university students; now GIMP is a full-fledged application, available on all distributions of Linux, OS X and Microsoft Windows — XP and later versions. It is released under GPLv3+ licenses and is freely distributed to (and by) anybody, who can look at its contents and its source code and can add features or fix problems.
GIMP is expandable and extensible; it is designed to be augmented with plug-ins and extensions in order to improve its functionality. This is implemented through the use of a scripting interface.
History.
GIMP was originally released as the "General Image Manipulation Program", by creators Spencer Kimball and Peter Mattis. Development of GIMP began in 1995 as a semester-long project at the University of California, Berkeley; the first public release of GIMP (0.54) was made in January 1996. When Richard Stallman visited UC Berkeley the following year, Kimball and Mattis asked him if they could change "General" to "GNU" (the name given to the operating system created by Stallman). With Stallman's approval, the definition of the acronym GIMP was changed to mean the "GNU" Image Manipulation Program, which also reflects its existence under the GNU Project. GIMP is developed by a self-organized group of volunteers under the banner of the GNU Project.
The number of computer architectures and operating systems supported has expanded significantly since its first release. The first release supported UNIX systems, such as Linux, SGI IRIX and HP-UX. Since the initial release, GIMP has been ported to many operating systems, including Microsoft Windows and OS X; the original port to the Windows 32-bit platform was started by Finnish programmer Tor M. Lillqvist (tml) in 1997 and was supported in the GIMP 1.1 release.
GIMP saw formation of a community and rapid adoption following the first release. The community that formed began developing tutorials, artwork and shared better work-flows and techniques.
A GUI toolkit called GTK (GIMP tool kit) was developed to facilitate the development of GIMP. GTK was replaced by its successor GTK+ after being redesigned using object-oriented programming techniques. The development of GTK+ has been attributed to Peter Mattis becoming disenchanted with the Motif toolkit GIMP originally used; Motif was used up until GIMP 0.60.
Development.
GIMP is primarily developed by volunteers as a free software project under the banner of the GNU Project. Development takes place in a public git source code repository, on public mailing lists and in public chat channels on the GIMPNET IRC network.
New features are held in public separate source code branches and merged into the main (or development) branch when the GIMP team is sure they won't damage existing functions. Sometimes this means that features that appear complete do not get merged or take months or years before they become available in GIMP.
GIMP itself is released as source code. After a source code release installers and packages are made for different operating systems by parties who might not be in contact with the maintainers of GIMP.
The version number used in GIMP is expressed in a "major-minor-micro" format, with each number carrying a specific meaning: The first (major) number is incremented only for major developments (and is currently 2). The second (minor) number is incremented with each release of new features, with odd numbers reserved for in-progress development versions and even numbers assigned to stable releases; the third (micro) number is incremented before and after each release (resulting in even numbers for releases, and odd numbers for development snapshots) with any bugfixes subsequently applied and released for a stable version.
Each year GIMP applies for several positions in the Google Summer of Code (GSoC); to date GIMP has participated in all years except 2007. From 2006 to 2009 there have been nine GSoC projects that have been listed as successful, although not all successful projects have been merged into GIMP yet. The healing brush and perspective clone tools and Ruby bindings were created as part of the 2006 GSoC and can be used in version 2.8.0 of GIMP, although there were three other projects that were completed and are not yet available in a stable version of GIMP; those projects being Vector Layers, and a JPEG 2000 plug-in. Several of the GSoC projects were completed in 2008, but have not been merged into a stable GIMP release.
User interface.
The user interface of GIMP is designed by a dedicated design and usability team. This team was formed after the developers of GIMP signed up to join the OpenUsability project. A user interface brainstorming group has since been created for GIMP, where users of GIMP can send in their suggestions as to how they think the GIMP user interface could be improved.
GIMP is presented in two forms, "single" and "multiple" window mode; GIMP 2.8 defaults to the multiple window mode. In multiple window mode a set of windows contain all GIMPs functionality. By default, tools and tool settings are on the left and other dialogues are on the right.
GTK+ (GIMP tool kit) is used to create the graphical user interface. GTK+'s creation and history regarding GIMP is described in the history section above.
Libre Graphics Meetings.
The Libre Graphics Meeting (LGM) is a yearly event where developers of GIMP and other projects meet up to discuss issues related to free and open source graphics software. The GIMP developers hold birds of a feather (BOF) sessions at this event.
Distribution.
The current version of GIMP works with numerous operating systems, including Linux, OS X and Microsoft Windows. Many Linux distributions include GIMP as a part of their desktop operating systems, including Fedora and Debian.
The GIMP website links to binary installers compiled by Jernej Simončič for the platform. MacPorts was listed as the recommended provider of Mac builds of GIMP. This is no longer needed as version 2.8.2 and later run natively on OS X. GTK+ was originally designed to run on an X11 server. Because OS X can optionally use an X11 server, porting GIMP to OS X is simpler compared to creating a Windows port. GIMP is also available as part of the "Ubuntu noroot" package from the Google Play Store on Android.
Professional reviews.
GIMP's fitness for use in professional environments is regularly reviewed and often cited as a possible replacement for Adobe Photoshop. The maintainers seek to fulfill GIMP's product vision rather than replicate the interface of Adobe Photoshop.
GIMP 2.6 has been reviewed twice by Ars Technica. In the first review, Ryan Paul noted that GIMP provides "Photoshop-like capabilities and offers a broad feature set that has made it popular with amateur artists and open source fans. Although GIMP is generally not regarded as a sufficient replacement for high-end commercial tools, it is beginning to gain some acceptance in the pro market." Dave Girard also reviewed GIMP 2.6, specifically with the aim of testing GIMP's fitness for professional tasks. He stated that GIMP had several professional-grade features, but he felt that GIMP's features were deficient overall (such as the lack of non-destructive editing), and that it was missing tools (such as a desaturation brush).
The single-window mode of GIMP 2.8 was also reviewed by Ryan Paul of Ars Technica, who noted that it made the user experience feel "more streamlined and less cluttered."
Mascot.
Wilber is the official GIMP mascot. Wilber has relevance outside of GIMP as a racer in SuperTuxKart and was displayed on the Bibliothèque nationale de France (National Library of France) as part of Project Blinkenlights.
Wilber was created at some time before 25 September 1997 by Tuomas Kuosmanen ("tigert") and has since received additional accessories and a construction kit to ease the process.
Features.
Tools used to perform image editing can be accessed via the toolbox, through menus and dialogue windows. They include filters and brushes, as well as transformation, selection, layer and masking tools.
There are several ways of selecting colours including palettes, colour choosers and using an eyedropper tool to select a colour on the canvas. The built-in colour choosers include RGB/HSV selector or scales, water-colour selector, CMYK selector and a colour-wheel selector. Colours can also be selected using hexadecimal colour codes as used in HTML colour selection. GIMP has native support for indexed colour and RGB colour spaces; other colour spaces are supported using decomposition where each channel of the new colour space becomes a black and white image. CMYK, LAB and HSV (hue, saturation, value) are supported this way. Colour blending can be achieved using the blend tool, by applying a gradient to the surface of an image and using GIMP's colour modes. Gradients are also integrated into tools such as the brush tool, when the user paints this way the output colour slowly changes. There are a number of default gradients included with GIMP; a user can also create custom gradients with tools provided.
GIMP selection tools include a rectangular and circular selection tool, free select tool, and fuzzy select tool (also known as magic wand). More advanced selection tools include the select by colour tool for selecting contiguous regions of colour—and the scissors select tool, which creates selections semi-automatically between areas of highly contrasting colours. GIMP also supports a quick mask mode where a user can use a brush to paint the area of a selection. Visibly this looks like a red colored overlay being added or removed. The foreground select tool is an implementation of Simple Interactive Object Extraction (SIOX) a method used to perform the extraction of foreground elements, such as a person or a tree in focus. The Paths Tool allows a user to create vectors (also known as Bézier curves). Users can use paths to create complex selections around natural curves. They can paint (or "stroke") the paths with brushes, patterns, or various line styles. Users can name and save paths for reuse.
There are many tools that can be used for editing images in GIMP. The more common tools include a paint brush, pencil, airbrush, eraser and ink tools used to create new or blended pixels. Tools such as the bucket fill and blend tools are used to change large regions of space in an image and can be used to help blend images.
GIMP also provides "smart" tools that use a more complex algorithm to do things that otherwise would be time consuming or impossible. These include:
An image being edited in GIMP can consist of many layers in a stack. The user manual suggests that "A good way to visualize a GIMP image is as a stack of transparencies," where in GIMP terminology, each transparency is a layer. Each layer in an image is made up of several channels. In an RGB image there are normally 3 or 4 channels, each consisting of a red, green and blue channel. Colour sublayers look like slightly different gray images, but when put together they make a complete image. The fourth channel that may be part of a layer is the alpha channel (or layer mask). This channel measures opacity where a whole or part of an image can be completely visible, partially visible or invisible. Each layer has a layer mode that can be set to change the colours in the image.
Text layers can be created using the text tool, allowing a user to write on an image. Text layers can be transformed in several ways, such as converting them to a path or selection.
GIMP has approximately 150 standard effects and filters, including Drop Shadow, Blur, Motion Blur and Noise.
GIMP operations can be automated with scripting languages. The Script-Fu is a Scheme-based language implemented using a TinyScheme interpreter built into GIMP. GIMP can also be scripted in Perl, Python (Python-Fu), or Tcl, using interpreters external to GIMP. New features can be added to GIMP not only by changing program code (GIMP core), but also by creating plug-ins. These are external programs that are executed and controlled by the main GIMP program. MathMap is an example of a plug-in written in C.
There is support for several methods of sharpening and blurring images, including the blur and sharpen tool. The unsharp mask tool is used to sharpen an image selectively — it only sharpens areas of an image that are sufficiently detailed. The Unsharp Mask tool is considered to give more targeted results for photographs than a normal sharpening filter. The Selective Gaussian Blur tool works in a similar way, except it blurs areas of an image with little detail.
The "Generic Graphics Library" ("GEGL") was first introduced as part of GIMP on the 2.6 release of GIMP. This initial introduction does not yet exploit all of the capabilities of GEGL; as of the 2.6 release, GIMP can use GEGL to perform high bit-depth colour operations; because of this less information is lost when performing colour operations. When GEGL is fully integrated, GIMP will have a higher colour bit depth and better non-destructive work-flow. Current distribution versions of GIMP only support 8-bit of color, which is much less than what e.g. digital cameras produce (12-bit or more).
GIMP supports importing and exporting with a large number of different file formats, GIMP's native format XCF is designed to store all information GIMP can contain about an image; XCF is named after the e"X"perimental "C"omputing "F"acility where GIMP was authored. Import and export capability can be extended to additional file formats by means of plug-ins.
Forks and derivatives.
Because of the free and open-source nature of GIMP, several forks, variants and derivatives of the computer program have been created to fit the needs of their creators. While GIMP is available for popular operating systems, variants of GIMP may be OS-specific. These variants are neither hosted nor linked on the GIMP site. The GIMP site does not host GIMP builds for Windows or Unix-like operating systems either, although it does include a link to a Windows build.
Well-known variants include:
Extensions.
A GIMP plug-in for creating animations. GAP can save animations in several formats, including GIF and AVI. The animation function relies on GIMP's layering and image file name numbering capability. Animations are created either by placing each frame on its own layer (in other words, treating each layer as an animation cel), or by manipulating each numbered file as if it were a frame in the video: moving, rotating, flipping, changing colours, applying filters, etc. to the layers by taking advantage of interpolation within function calls(plug-in usage), within a specified frame range. The resulting project can be saved as an animated GIF or encoded video file. GAP also provides programmed layer transitions, frame rate change, and move paths, allowing the creation of sophisticated animations.
A collection of brushes and accompanying tool presets, aimed at artists and graphic designers. It speeds up repetitive tasks and can save tool settings between sessions.
See also.
About GIMP
About editing

</doc>
<doc id="12663" url="http://en.wikipedia.org/wiki?curid=12663" title="Green Bay Packers">
Green Bay Packers

The Green Bay Packers are a professional American football team based in Green Bay, Wisconsin. They are members of the North division of the National Football Conference (NFC) in the National Football League (NFL). Green Bay is the third-oldest franchise in the NFL, organized and starting play in 1919. It is the only non-profit, community-owned major league professional sports team based in the United States. Home games are played at Lambeau Field.
The Packers are the last vestige of "small town teams" common in the NFL during the 1920s and 1930s. Founded in 1919 by Earl "Curly" Lambeau and George Whitney Calhoun, the franchise traces its lineage to other semi-professional teams in Green Bay dating back to 1896. In 1919 and 1920 the Packers competed against other semi-pro clubs from around Wisconsin and the Midwest. They joined the American Professional Football Association (APFA), the forerunner of today's NFL, in 1921. Although Green Bay is by far the smallest professional sports market in North America, its local fan and media base extends 120 miles south into Milwaukee, where it played selected home games between 1933 and 1994.
The Packers have won 13 league championships, the most in NFL history, including nine NFL titles prior to the Super Bowl era and four Super Bowl victories—in 1967 (Super Bowl I), 1968 (Super Bowl II), 1997 (Super Bowl XXXI) and 2011 (Super Bowl XLV). The team is long-standing adversaries of the Chicago Bears, Minnesota Vikings and Detroit Lions, who together comprise the NFL's NFC North division. The Bears–Packers rivalry is one of the oldest in NFL history, dating back to 1921.
Founding.
The Green Bay Packers were founded on August 11, 1919 by former high-school football rivals Earl "Curly" Lambeau and George Whitney Calhoun. Lambeau solicited funds for uniforms from his employer, the Indian Packing Company. He was given $500 for uniforms and equipment, on the condition that the team be named for its sponsor. The Green Bay Packers have played in their original city longer than any other team in the NFL.
On August 27, 1921, the Packers were granted a franchise in the new national pro football league that had been formed the previous year. Financial troubles plagued the team and the franchise was forfeited within the year, before Lambeau found new financial backers and regained the franchise the next year. These backers, known as the "Hungry Five", formed the Green Bay Football Corporation.
Notable seasons.
1929–1931: Lambeau's team arrives.
After a near-miss in 1927, Lambeau's squad claimed the Packers' first NFL title in 1929 with an undefeated 12–0–1 campaign, behind a stifling defense which registered eight shutouts. Green Bay would repeat as league champions in 1930 and 1931, bettering teams from New York, Chicago and throughout the league, with all-time greats and future Hall of Famers Mike Michalske, Johnny (Blood) McNally, Cal Hubbard and Green Bay native Arnie Herber. Among the many impressive accomplishments of these years was the Packers' streak of 30 consecutive home games without defeat, an NFL record which still stands.
1935–1945: The Don Hutson era.
The arrival of end Don Hutson from Alabama in 1935 gave Lambeau and the Packers the most-feared and dynamic offensive weapon in the game. Credited with inventing pass patterns, Hutson would lead the league in receptions eight seasons and spur the Packers to NFL championships in 1936, 1939 and 1944. An iron man, Hutson played both ways, leading the league in interceptions as a safety in 1940. Hutson claimed 18 NFL records when he retired in 1945, many of which still stand. In 1951, his number 14 was the first to be retired by the Packers, and he was inducted as a charter member of the Pro Football Hall of Fame in 1963.
1946–1958: Wilderness.
After Hutson's retirement, Lambeau could not stop the Packers' slide. He purchased a large lodge near Green Bay for team members and families to live. Rockwood Lodge was the home of the 1946-1949 Packers, though the 1947 and 1948 seasons produced a record of 12-10-1, and 1949 was even worse at 3-9. The lodge burned down on January 24, 1950, and the insurance money paid for many of the Packers' debts.
Curly Lambeau departed after the 1949 season. Gene Ronzani and Lisle Blackbourn could not coach the Packers back to their former magic, even as a new stadium was unveiled in 1957. The losing would descend to the disastrous 1958 campaign under coach Ray "Scooter" McLean, whose lone 1–10–1 year at the helm is the worst in Packer history.
1959–1967: The Lombardi era and the glory years.
Former New York Giants assistant Vince Lombardi was hired as Packers head coach and general manager on February 2, 1959. Few suspected the hiring represented the beginning of a remarkable, immediate turnaround. Under Lombardi, the Packers would become "the" team of the 1960s, winning five World Championships over a seven-year span, including victories in the first two Super Bowls. During the Lombardi era, the stars of the Packers' offense included Bart Starr, Jim Taylor, Carroll Dale, Paul Hornung (as halfback and placekicker), Forrest Gregg, and Jerry Kramer. The defense included Willie Davis, Henry Jordan, Willie Wood, Ray Nitschke, Dave Robinson, and Herb Adderley.
1959.
The Packers' first regular season game under Lombardi was on September 27, 1959, a 9–6 victory over the Chicago Bears in Green Bay. After winning their first three, the Packers lost the next five before finishing strong by sweeping their final four. The 7–5 record represented the Packers' first winning season since 1947, enough to earn rookie head coach Lombardi the NFL Coach of the Year.
1960.
The next year, the Packers, led by Paul Hornung's 176 points, won the NFL West title and played in the NFL Championship against the Philadelphia Eagles at Philadelphia. In a see-saw game, the Packers trailed by only four points when All-Pro Eagle linebacker Chuck Bednarik tackled Jim Taylor just nine yards short of the goal line as time expired. The Packers claimed that they did not "lose" the game, they were simply behind in the score when time ran out.
1961.
The Packers returned to the NFL Championship game the following season and faced the New York Giants in the first league title game to be played in Green Bay. The Packers scored 24 second-quarter points, including a championship-record 19 by Paul Hornung, on special "loan" from the Army (one touchdown, four extra-points and three field goals), powering the Packers to a 37–0 rout of the Giants, their first NFL Championship since 1944. It was in 1961 that Green Bay became known as "Titletown."
1962.
The Packers stormed back in the 1962 season, jumping out to a 10–0 start, on their way to a 13–1 season. This consistent level of success would lead to Lombardi's Packers becoming one of the most prominent teams of their era, and even to their being featured as the face of the NFL on the cover of "Time" on December 21, 1962, as part of the magazine's cover story on "The Sport of the '60s". Shortly after Time's article, the Packers faced the Giants in a much more brutal championship game than the previous year, but the Packers prevailed on the surprising foot of Jerry Kramer and the determined running of Jim Taylor. The Packers defeated the Giants in New York, 16–7.
1965.
The Packers returned to the championship game in 1965 following a two-year absence, when they defeated the Colts in a playoff for the Western Conference title. That game would be remembered for Don Chandler's controversial tying field goal in which the ball allegedly went wide right, but the officials signaled "good." The 13–10 overtime win earned the Packers a trip to the NFL Championship game, where Hornung and Taylor ran through the defending champion Cleveland Browns, helping the Packers win, 23–12, to earn their third NFL Championship under Lombardi and ninth overall. Goalpost uprights would be made taller the next year.
1966.
The 1966 season saw the Packers led to the first ever Super Bowl by MVP quarterback Bart Starr. The team went 12–2, and as time wound down in the NFL Championship against the Dallas Cowboys, the Packers clung to a 34–27 lead. Dallas had the ball on the Packers' two-yard line, threatening to tie the ballgame. But on fourth down the Packers' Tom Brown intercepted Don Meredith's pass in the end zone to seal the win. The team crowned its season by rolling over the AFL champion Kansas City Chiefs 35–10 in Super Bowl I.
1967.
The 1967 season was the last for Lombardi as the Packers' head coach. The NFL Championship game, a rematch of the 1966 contest against Dallas, became indelibly kown as the "Ice Bowl" as a result of the brutal conditions at Lambeau Field. Still the coldest NFL game ever played, it remains one of the most famous football games at any level in the history of the sport. With 16 seconds left, Bart Starr's touchdown on a quarterback sneak brought the Packers a 21–17 victory and their still unequaled third straight NFL Championship. They then won Super Bowl II with a 33–14 victory over the Oakland Raiders. Lombardi stepped down as head coach after the game, and Phil Bengtson was named his successor. Lombardi remained as general manager for one season, but left in 1969 to become head coach and minority owner of the Washington Redskins.
After Lombardi died unexpectedly on September 3, 1970, a shocked NFL renamed the Super Bowl trophy the Vince Lombardi Trophy in recognition of his accomplishments with the Packers. The city of Green Bay renamed Highland Avenue in his honor in 1968, placing Lambeau Field at 1265 Lombardi Avenue ever since.
1968–91.
For about a quarter century after Lombardi's departure the Packers had relatively little on-field success. In the 24 seasons from 1968 to 1991, they had only five seasons with a winning record, one being the shortened 1982 strike season. They appeared in the playoffs twice, with a 1–2 record. The period saw five different head coaches – Phil Bengtson, Dan Devine, Bart Starr, Forrest Gregg, and Lindy Infante – two of whom, Starr and Gregg, were Lombardi's era stars, while Bengtson was a former Packer coach. Each led the Packers to a poorer record than his predecessor. Poor personnel decisions were rife, notoriously the 1974 trade by acting GM Dan Devine which sent five 1975 or 1976 draft picks (two first-rounders, two second-rounders and a third) to the Los Angeles Rams for aging quarterback John Hadl, who would spend only 1½ seasons in Green Bay. Another came in the 1989 NFL Draft, when offensive lineman Tony Mandarich was taken with the second overall pick ahead of Barry Sanders, Deion Sanders, and Derrick Thomas. Though rated highly by nearly every professional scout at the time, Mandarich's performance failed to meet expectations, earning him ESPN's ranking as the third "biggest sports flop" in the last 25 years.
1992–2007: The Brett Favre era.
The Packers' performance throughout the 1970s, 1980s, and early 1990s led to a shakeup, with Ron Wolf hired as GM and given full control of the team's football operations to start the 1991 season. In 1992, Wolf hired San Francisco 49ers offensive coordinator Mike Holmgren as the Packers' new head coach.
Soon afterward, Wolf acquired quarterback Brett Favre from the Atlanta Falcons for a first-round pick. Favre got the Packers their first win of the 1992 season, stepping in for injured quarterback Don Majkowski and leading a comeback over the Cincinnati Bengals. He started the following week, a win against the Pittsburgh Steelers, and never missed another start for Green Bay through the end of the 2007 season. He would go on to break the record for consecutive starts by an NFL quarterback, starting 297 consecutive games including stints with the New York Jets and Minnesota Vikings with the streak finally coming to an end late in the 2010 season.
The Packers had a 9–7 record in 1992, and began to turn heads around the league when they signed perhaps the most prized free agent in NFL history in Reggie White on the defense in 1993. White believed that Wolf, Holmgren, and Favre had the team heading in the right direction with a "total commitment to winning." With White on board the Packers made it to the second round of the playoffs during both the 1993 and 1994 seasons but lost their 2nd-round matches to their playoff rival, the Dallas Cowboys, playing in Dallas on both occasions. In 1995, the Packers won the NFC Central Division championship for the first time since 1972. After a home playoff 37–20 win against Favre's former team, the Atlanta Falcons, the Packers defeated the defending Super Bowl champion San Francisco 49ers 27–17 in San Francisco on the road to advance to the NFC Championship Game, where they lost again to the Dallas Cowboys 38–27.
1996, Super Bowl XXXI champions.
In 1996, the Packers' turnaround was complete. The team posted a league-best 13–3 record in the regular season, dominating the competition and securing home-field advantage throughout the playoffs. They were ranked no. 1 in offense with Brett Favre leading the way, no. 1 in defense with Reggie White as the leader of the defense and no. 1 in special teams with former Heisman Trophy winner Desmond Howard returning punts and kickoffs for touchdowns. After relatively easy wins against the 49ers in a muddy 35–14 beatdown and Carolina Panthers 30–13 in what was referred to as "Ice Bowl 2", the Packers advanced to the Super Bowl for the first time in 29 years. In Super Bowl XXXI, Green Bay defeated the New England Patriots 35–21 to win their 12th world championship. Desmond Howard was named MVP of the game for his kickoff return for a touchdown that ended the Patriots' bid for a comeback. Then-Packers President Bob Harlan credited Wolf, Holmgren, Favre, and White for ultimately changing the fortunes of the organization and turning the Green Bay Packers into a model NFL franchise. A 2007 panel of football experts at ESPN ranked the 1996 Packers the 6th-greatest team ever to play in the Super Bowl.
1997.
The following season the Packers recorded another 13–3 record and won their second consecutive NFC championship. After defeating the Tampa Bay Buccaneers 21–7 and San Francisco 49ers 23–10 in the playoffs, the Packers returned to the Super Bowl as an 11½ point favorite. The team ended up losing in an upset to John Elway and the Denver Broncos in Super Bowl XXXII, by the score of 31–24.
1998–2006.
In 1998, the Packers went 11–5 and met the San Francisco 49ers in the first-round of the NFC playoffs. It was the fourth consecutive year these teams had met in the playoffs, and the sixth overall contest since the 1995 season. The Packers had won all previous games, and the media speculated that another 49ers loss would result in the dismissal of San Francisco head coach Steve Mariucci. Unlike the previous playoff matches, this game was hotly contested, with the teams frequently exchanging leads. With 4:19 left in the 4th quarter, Brett Favre and the Packers embarked on an 89-yard drive, which concluded with a Favre touchdown pass to receiver Antonio Freeman. This play appeared to give Green Bay the victory. But San Francisco quarterback Steve Young led the 49ers on an improbable touchdrive drive, which culminated when Terrell Owens caught Young's pass between several defenders to give the 49ers a lead with three seconds remaining. Afterwards, the game was mired in controversy. Many argued that during the 49ers game winning drive, Niners receiver Jerry Rice fumbled the ball but officials stated he was down by contact. Television replays appeared to confirm these sentiments, and the next season the NFL re instituted an instant replay system. In the end, this game turned out to be the end of an era in Green Bay. Days later Mike Holmgren left the Packers to become Vice President, General Manager and Head Coach of the Seattle Seahawks. Much of Holmgren's coaching staff went with him, and Reggie White also retired after the season (but later played one season for the Carolina Panthers in 2000). In 1999 and 2000, the team struggled to find an identity after the departure of so many of the individuals responsible for their Super Bowl run.
Ray Rhodes was hired in 1999 as the team's new head coach. Rhodes had served around the league as a highly regarded defensive coordinator, and more recently experienced moderate success as head coach of the Philadelphia Eagles from 1995 to 1998. Ron Wolf believed that Rhodes' experience and player-friendly demeanor would fit nicely in Green Bay's veteran locker room. But Rhodes was fired after one 8–8 season. Wolf visited team practice late in the 1999 season and believed that players had become too comfortable with Rhodes' style, and said the atmosphere resembled a country club.
In 2000, Wolf replaced Rhodes with Mike Sherman. Sherman had never been a head coach at any level of football and was relatively unknown in NFL circles. He had only coached in professional football for three years starting as the Packers' tight ends coach in 1997 and 1998. In 1999, he followed Mike Holmgren to Seattle and became the Seahawks' offensive coordinator, although Sherman did not call the plays during games. Despite Sherman's apparent anonymity, Wolf was blown away in the interview process by the coach's organizational skills and attention to detail. Sherman's inaugural season started slowly, but the Packers won their final four games to achieve a 9–7 record. Brett Favre praised the atmosphere Sherman had cultivated in Green Bay's locker room and fans were optimistic about the team's future. In the offseason, however, Wolf suddenly announced his own resignation as GM to take effect after the April 2001 draft. Packers President Bob Harlan was surprised by Wolf's decision and felt unsure of how to replace him. Harlan preferred the structure Green Bay had employed since 1991; a general manager who ran football operations and hired a subservient head coach. But with the momentum and locker room chemistry that was built during the 2000 season, Harlan was reluctant to bring in a new individual with a potentially different philosophy. Wolf recommended that Harlan give the job to Sherman. Though Harlan was wary of the structure in principle, he agreed with Wolf that it was the best solution. In 2001, Sherman assumed the duties of both GM and head coach.
From 2001 to 2004, Sherman coached the Packers to respectable regular-season success, led by the spectacular play of Brett Favre, Ahman Green, and a formidable offensive line. But Sherman's teams faltered in the playoffs. Prior to 2003, the Packers had never lost a home playoff game since the NFL instituted a post-season in 1933 (they were 13–0, with 11 of the wins at Lambeau and two more in Milwaukee.). That ended January 4, 2003, when the Atlanta Falcons defeated the Packers 27–7 in an NFC Wild Card game. The Packers would also lose at home in the playoffs to the Minnesota Vikings two years later.
By the end of the 2004 season, the Packers team depth appeared to be diminishing. Sherman also seemed overworked and reportedly had trouble communicating with players on the practice field with whom he was also negotiating contracts. Harlan felt the dual roles were too much for one man to handle and removed Sherman from the GM position in early 2005, while retaining him as a head coach. Harlan hired Seattle Seahawks Vice President of Operations Ted Thompson as the new Executive Vice President, General Manager and Director of Football Operations. The relationship between Thompson and Sherman appeared strained, as Thompson immediately began rebuilding Green Bay's roster. Following a dismal 4–12 season, Thompson fired Sherman. Thompson hired Mike McCarthy, the former offensive coordinator for the San Francisco 49ers and New Orleans Saints as his new head coach. McCarthy had also previously served as the quarterbacks coach for the Packers in 1999.
2007.
After missing the playoffs in 2006, Brett Favre announced that he would return for the 2007 season; it would turn out to be one of his best. The Packers won 10 of their first 11 games and finished 13–3, earning a first round bye in the playoffs. The Packers' passing offense, led by Favre and a very skilled wide receiver group, finished second in the NFC, behind the Dallas Cowboys, and third overall in the league. Running back Ryan Grant, acquired for a sixth-round draft pick from the New York Giants, became the featured back in Green Bay and rushed for 956 yards and 8 touchdowns in the final 10 games of the regular season. In the divisional playoff round, in a heavy snowstorm, the Packers beat the Seattle Seahawks 42–20. Grant rushed for three touchdowns and over 200 yards, while Favre tossed three touchdown passes and 1 snowball to receiver Donald Driver in celebration.
On January 20, 2008, Green Bay appeared in their first NFC Championship Game in 10 years facing the New York Giants in Green Bay. The game was lost 23–20 on an overtime field goal by Lawrence Tynes. This would be Brett Favre's final game as a Green Bay Packer with his final pass being an interception in overtime.
Mike McCarthy coached the NFC team during the 2008 Pro Bowl in Hawaii. Al Harris and Aaron Kampman were also picked to play for the NFC Pro Bowl team as starters. Donald Driver was named as a third-string wideout on the Pro Bowl roster. Brett Favre was named the first-string quarterback for the NFC, but he declined to play in the Pro Bowl and was replaced on the roster by Tampa Bay Buccaneers' quarterback Jeff Garcia. The Packers also had several first alternates, including Chad Clifton and Nick Barnett.
In December 2007, Ted Thompson was signed to a 5-year contract extension with the Packers, while it was announced on February 5, 2008 that head coach Mike McCarthy signed a 5-year contract extension as well.
2008–present: The Aaron Rodgers era.
2008.
On March 4, 2008, Brett Favre tearfully announced his retirement. Within five months, however, he filed for reinstatement with the NFL on July 29. Favre's petition was granted by Commissioner Roger Goodell, effective August 4, 2008. On August 6, 2008, it was announced that Brett Favre was traded to the New York Jets for a conditional draft pick in 2009.
The Packers began their 2008 season with their 2005 first-round draft pick, quarterback Aaron Rodgers, under center, as the first QB other than Favre to start for the Packers in 16 years. Rodgers played well in his first year starting for the Packers, throwing for over 4000 yards and 28 touchdowns. However, injuries plagued the Packers' defense, as they lost 7 close games by 4 points or less, finishing with a 6–10 record. After the season, eight assistant coaches were dismissed by McCarthy, including Bob Sanders, the team's defensive coordinator, who was replaced by Dom Capers.
2009.
In March 2009, the organization assured fans that Brett Favre's jersey number would be retired, but not during the 2009 season. In April 2009, the Packers selected defensive lineman B.J. Raji of Boston College as the team's first pick in the draft. The team then traded three draft picks (including the pick the Packers acquired from the Jets for Brett Favre) for another first-round pick, selecting linebacker Clay Matthews III of the University of Southern California.
During the 2009 NFL season, two match-ups between the franchise and its former legendary QB, Brett Favre, were highly anticipated after Favre's arrival with the division-rival Vikings in August. The first encounter took place in week 4, on a Monday Night Football game which broke several TV audience records. The scheduling of this game was made possible when Baseball Commissioner and Packer Board of Directors Member Bud Selig forced Baseball's Minnesota Twins to play 2 games within a 12-hour span. The Vikings won the game 30–23. Brett Favre threw 3 TDs, no interceptions, and had a passer rating of 135. The teams met for a second time in week 8, Favre leading the Vikings to a second win, 38–26, in Green Bay. Rodgers was heavily pressured in both games, being sacked 14 times total, but still played exceptionally well, throwing five touchdowns and only one interception. The next week, the Packers were upset by the win-less Tampa Bay Buccaneers. Following a players only meeting, the team started to roll and found some stability on the offensive line with the return of tackle Mark Tauscher bringing a minor halt to sacks to Rodgers and opening the running game to Ryan Grant and the other running backs. Green Bay finished the season strongly, winning 7 out of their last 8 games, including winning their 16th regular season finale in the past 17 seasons, and earning a NFC wild-card playoff bid with an 11–5 regular-season record. The Packers defense was ranked No. 2 and the offense was ranked No. 6 with rookies Brad Jones and Clay Matthews III becoming sensations at linebacker and young players like James Jones, Brandon Jackson, Jermichael Finley and Jordy Nelson becoming threats on offense. Rodgers also became the first quarterback in NFL history to throw for at least 4000 yards in each of his first two seasons as a starter. Also, cornerback Charles Woodson won NFL Defensive Player of the Year honors after recording 9 interceptions, forcing four fumbles, 3 touchdowns and registering 74 tackles and 2 sacks. In fact, Woodson's 9 interceptions were more than the 8 collected by all Packer opponents that season. Though the defense was ranked high, injuries to Al Harris, Tramon Williams, Will Blackmon, Atari Bigby and Brandon Underwood severely limited the depth of the secondary and teams like the Minnesota Vikings and Pittsburgh Steelers used that to their advantage by unleashing aerial assaults against inexperienced players with the NFL's best receivers. The season ended with an overtime loss in a wild card round shoot out at the Arizona Cardinals, 51–45.
2010: Super Bowl XLV championship.
After finishing the 2009 season with an 11–5 record, the Packers began the 2010 NFL Draft holding the 23rd pick. They selected Offensive tackle Bryan Bulaga from Iowa; with pick 2–56 they selected Defensive end Mike Neal from Purdue. They then traded picks 3–86 and 4–122 to the Philadelphia Eagles for pick 3–71, choosing Safety Morgan Burnett from Georgia Tech. With pick 5–154 they selected Tight end Andrew Quarless from Penn State. With their compensatory selection pick 5–169 they chose Offensive guard Marshall Newhouse from Texas Christian. With pick 6–193 they selected Running back James Starks from Buffalo. In their final selection, 7–230. they chose Defensive end C. J. Wilson of East Carolina.
The team lost Johnny Jolly to a season-long suspension after he violated the NFL drug policy. Their running corps suffered a blow when RB Ryan Grant sustained a season-ending ankle injury in week 1. By the end of the season, the tean had 16 people on injured reserve, including 7 starters: running back Ryan Grant, tight end Jermichael Finley, linebacker Nick Barnett, safety Morgan Burnett, linebacker Brandon Chillar, tackle Mark Tauscher, and linebacker Brad Jones.
After finishing the regular season 10–6 the Packers clinched the No. 6 seed in the NFC playoffs. They first faced No. 3 seeded Philadelphia, winning 21–16. In the Divisional round they defeated No. 1 seeded Atlanta 48–21. They then played the Chicago Bears at Soldier Field to reach the NFC Championship Game – only the second playoff meeting between the two storied rivals (the other a 33–14 Chicago victory which sent them to the 1941 NFL Championship Game). Green Bay won 21–14 to move on to Super Bowl XLV. On February 6, 2011, they defeated the AFC champion Pittsburgh Steelers 31–25, becoming the first No. 6 seed from the NFC to win a Super Bowl. Aaron Rodgers was named Super Bowl MVP.
2011 Season.
In 2011, the Packers went 15–1, a franchise best record in the regular season. However, the team faltered in the playoffs, losing to the New York Giants 37-20 in the second round after a first round bye.
2012 Season.
In 2012, the Packers went 11-5. They beat the Minnesota Vikings in the NFC Wildcard round 24-10, and lost in the Divisional round 45-31 to the San Francisco 49ers.
The Packers topped the first-ever AP Pro32 rankings, a new pro football version of the AP Top 25 college football and basketball polls.
2013 Season.
In 2013, the Packers went 8-7-1, losing to the San Francisco 49ers 20-23 in the first round of the playoffs.
2014.
The Packers recorded their 700th victory against the Bears in week four. Overall, the team went 12-4 and clinched the #2 seed in the NFC Playoffs. They advanced to the NFC Championship and faced the Seattle Seahawks. After leading throughout most of regulation they lost 28-22 in a historic overtime rally by Seattle.
Record 13 NFL world championships.
The Packers have been World Championships a record 13 times, topping their nearest rival, the Chicago Bears, by four. The first three were decided by league standing, the next six by the NFL Title Game, and the final four by Super Bowl victories. The Packers are also the only team to win three consecutive NFL titles, having accomplished this twice, 1929-30-31 under Lambeau, and 1965-66-67 under Lombardi.
Community ownership.
The Packers are the only community-owned franchise in American major league professional sports. Rather than being the property of an individual, partnership, or corporate entity, they are held in 2014 by 360,584 stockholders. No one is allowed to hold more than 200,000 shares, approximately 4% of the 5,011,557 shares currently outstanding. It is this broad-based community support and non-profit structure which has kept the team in Green Bay for nearly a century in spite of being the smallest market in all of North American professional sports.
The city of Green Bay had a population of only 104,057 as of the 2010 census, and only 600,000 in its television market, a fraction of the average NFL figures. The team, however, has long had an extended fan base throughout Wisconsin and parts of the Midwest, thanks in part to playing one pre-season and three regular-season home games each year in Milwaukee through 1995. It was only when baseball-only Miller Park preempted football there that the Packers' home slate became played entirely in Green Bay.
There have been five stock sales to fund Packer operations over the team's history, beginning with $5,000 being raised through 1,000 shares offered at $5 apiece in 1923. Most recently, $64 million was raised in 2011-2012 towards a $143-million Lambeau Field expansion. Demand exceeded expectations, and the original 250,000 share limit had to be increased before some 250,000 new buyers from all 50 U.S. states and Canada purchased 269,000 shares at $250 apiece, approximately 99% online.
Based on the original "Articles of Incorporation for the Green Bay Football Corporation" enacted in 1923, should the franchise to have been sold any post-expenses money would have gone to the Sullivan-Wallen Post of the American Legion to build "a proper soldier's memorial." This stipulation was included to ensure there could never be any financial inducement for shareholders to move the club from Green Bay. At the November 1997 annual meeting, shareholders voted to change the beneficiary from the Sullivan-Wallen Post to the Green Bay Packers Foundation, which makes donations to many charities and institutions throughout Wisconsin.
Even though it is referred to as "common stock" in corporate offering documents, a share of Packers stock does not share the same rights traditionally associated with common or preferred stock. It does not include an equity interest, does not pay dividends, can not be traded, has no securities-law protection, and brings no season ticket purchase privileges. All shareholders receive are voting rights, an invitation to the corporation's annual meeting, and an opportunity to purchase exclusive shareholder-only merchandise. Shares of stock cannot be resold, except back to the team for a fraction of the original price. While new shares can be given as gifts, transfers are technically allowed only between immediate family members once ownership has been established.
Green Bay is the only team with this form of ownership structure in the NFL, which is in direct violation of current league rules stipulating a maximum of 32 owners per team, with one holding a minimum 30% stake. The Packers' corporation was grandfathered when the NFL's current ownership policy was established in the 1980s. As a publicly held nonprofit, the Packers are also the only American major-league sports franchise to release its financial balance sheet every year.
Board of Directors.
Green Bay Packers, Inc., is governed by a seven-member Executive Committee elected from a 45-member board of directors. It consists of a president, vice president, treasurer, secretary and three members-at-large; only the president is compensated. Responsibilities include directing corporate management, approving major capital expenditures, establishing broad policy, and monitoring management performance.
The team's elected president normally represents the Packers in NFL owners meetings. During his time as coach Vince Lombardi generally represented the team at league meetings in his role as GM, except at owners-only meetings, where president Dominic Olejniczak appeared.
Green Bay Packers Foundation.
The team created the Green Bay Packers Foundation in December 1986. It assists in a wide variety of activities and programs benefiting education, civic affairs, health services, human services and youth-related programs.
At the team's 1997 annual stockholders meeting the foundation was designated in place of a Sullivan-Wallen Post soldiers memorial as recipient of any residual assets upon the team's sale or dissolution.
Fan base.
The Packers have an exceptionally loyal fan base. Regardless of team performance, every game played in Green Bay has been sold out since 1960. Despite the Packers having by far the smallest local TV market, the team consistently ranks as one of most popular in the NFL. They also have one of the longest season ticket waiting lists in professional sports, 86,000 names long, more than there are seats at Lambeau Field. The average wait is said to be over 30 years, but with only 90 or so tickets turned over annually it would be 955 years before the newest name on the list got theirs. As a result, season tickets are willed to next of kin and newborns placed optimistically on the waiting list.
Packers fans are often referred to as cheeseheads, a nickname for Wisconsin residents reflecting the state's bountiful cheese production first leveled as an insult at a 1987 game between the Chicago White Sox and Milwaukee Brewers. Instead, it came to be a statewide source of pride, and particularly since 1994 has been embraced by Packers fans. Bright orange triangular cheesehead hats are a fixture wherever the team plays.
During training camp in the summer months, held outside the Don Hutson Center, young Packers fans can bring their bikes and have their favorite players ride them from the locker room to practice at Ray Nitschke Field. This old tradition began around the time of Lambeau Field's construction in 1957. Gary Knafelc, a Packers end at the time, said, "I think it was just that kids wanted us to ride their bikes. I can remember kids saying, 'Hey, ride my bike.'" Each new generation of Packer fan delights at the opportunity.
The team holds an annual scrimmage called Family Night, typically an intra-squad affair, at Lambeau Field. During 2004 and 2005 sellout crowds of over 60,000 fans showed up, with an all-time mark of 62,492 set in 2005 when the Buffalo Bills appeared.
In August 2008, ESPN.com ranked Packers fans as second-best in the NFL. The team initially finished tied with the Pittsburgh Steelers (who finished ahead of the Packers) as having the best fans, but the tie was broken by ESPN's own John Clayton, a Pittsburgh native.
Nickname, logo, and uniforms.
Needing to outfit his new squad, team founder Curly Lambeau solicited funds from his employer, the Indian Packing Company. He was given $500 for uniforms and equipment in return for the team being named for its sponsor. An early newspaper article referred to the fledglings as "the Indians", but by the time they played their first game "Packers" had taken hold.
Indian Packing was purchased in 1920 by the Acme Packing Company. Acme continued to support the team, which played its first NFL season with "ACME PACKERS" emblazoned on its jerseys.
Lambeau, a Notre Dame alumnus, borrowed its Irish's navy blue and gold team colors, much as George Halas borrowed his Illinois alma mater's for the Chicago Bears. As a result, the early Packers were often referred to as the "Bays" or the "Blues" (and even occasionally as "the Big Bay Blues").
By 1950, Green Bay had changed its colors to hunter green and gold. Navy blue was kept as a secondary color, seen primarily on sideline capes, but was quietly dropped on all official materials shortly thereafter. The team's current uniform combination of forest green or white jerseys and metallic gold pants was adopted soon after Vince Lombardi arrived in 1959. However, to celebrate the NFL's 75th anniversary in 1994, the Packers joined in a league-wide donning of "throwback" jerseys, back to navy blue and gold. The team would go throwback again for two Thanksgiving Day games against the Detroit Lions, in blue and gold 1930s-era uniforms in 2001, and 1960s green and gold (only slightly different from the current ones) in 2003.
In 1951, the team finally stopped wearing leather helmets, adopting the metallic gold plastic headgear it has used ever since. The oval "G" logo was added in 1961 when Lombardi asked Packers equipment manager Gerald "Dad" Braisher to design a logo. Braisher tasked his assistant, St. Norbert College art student John Gordon. Satisfied with a football-shaped letter "G", the pair presented it to Lombardi, who then approved the addition. Tiki Barber falsely reported it to stand for "greatness" without a reliable source to back up his claims. Other reputable media outlets then published similar stories using Barber's false claim as a source. The Packers' Assistant Director of PR and Corporate Communications had the following to say: "There’s nothing in our history that suggests there's any truth to this. The Packers Hall of Fame archivist said the same thing." The team used a number of different logos prior to 1961, but the "G" is the only logo that has ever appeared on the helmet. The Packers hold the trademark on the "G" logo, and have granted limited permission to other organizations to utilize a similar logo, such as the University of Georgia and Grambling State University. Adopted in 1964, the Georgia "G", though different in design and color, was similar to the Packers' "G". Then-Georgia head coach Vince Dooley thought it best to clear the use of Georgia's new emblem with the Packers.
While several NFL teams choose to wear white jerseys at home early in the season due to white's ability to reflect the late summer sunrays, the Packers have done so only twice, during the opening two games of the 1989 season. Although alternate gold jerseys with green numbers are sold on a retail basis, the team currently has no plans to introduce such a jersey to be used in actual games.
During the 2010 season, the Packers paid tribute to their historical brethren with a third jersey modeled after that worn by the club in 1929, during its first world championship season. The jersey was navy blue, again making the Packers "the Blues."
Upon the NFL's switch of uniform suppliers in 2012 to Nike from Reebok, the Packers refused any changes to their uniform in any way outside of the required supplier's logo and new league uniform logos, declining all of Nike's "Elite 51" enhancements, including retaining the traditional striped collar of the jersey rather than Nike's new collar design.
Stadium history.
After their early seasons at Bellevue Park and Hagemeister Park the Packers played home games in City Stadium from 1925 to 1956. The team won its first six NFL world championships there.
By the 1950s the wooden 25,000 seat arena was considered outmoded. The NFL threatened to move the franchise to Milwaukee full-time unless it got a better stadium. The city responded by building a new 32,150 seat City Stadium for the team, the first built exclusively for an NFL team, which opened in time for the 1957 season. It was renamed Lambeau Field in 1965 to honor Curly Lambeau, who had died earlier in the year.
Expanded seven times before the end of the 1990s, Lambeau Field capacity reached 60,890. In 2003, it was extensively renovated to expand seating, modernize stadium facilities, and add an atrium area. Even with a current seating capacity of 72,928, ticket demand far outpaces supply, as all Packers games have been sold out since 1960. About 86,000 names are on the waiting list for season tickets.
The Packers played part of their home slate in Milwaukee starting in 1933, including two to three home games each year in Milwaukee's County Stadium from 1953 to 1994. Indeed, County Stadium had been built partly to entice the Packers to move to Milwaukee full-time. The Packers worked to capture their growing fan base in Milwaukee and the larger crowds. By the 1960s, threat of an American Football League franchise in Milwaukee prompted the Packers to stay, including scheduling a Western Conference Playoff in 1967.
County Stadium was built primarily as baseball stadium, and made only the bare minimum adjustments to accommodate football. At its height, it only seated 56,000 people, just barely above the NFL minimum; many of those seats were badly obstructed. The field was just barely large enough to fit a football field. Both teams shared the same sideline (separated by a piece of tape) and the end zones extended onto the warning track. By 1994, improvements and seating expansions at Lambeau, along with the Brewers preparing to campaign for their new stadium prompted the Packers to play their full slate in Green Bay for the first time in 62 years. Former season ticketholders for the Milwaukee package continue to receive preference for one pre-season and the second and fifth regular season games at Lambeau Field each season, along with playoff games through a lottery under the "Gold Package" plan.
Statistics and records.
Recent seasonal results.
"The results of the last seven completed Packer seasons, including current results, are listed below. For full season-by-season franchise results, see List of Green Bay Packers seasons."
Playoff record.
Overall record 30 wins, 19 losses
Notable players.
Pro Football Hall of Fame members.
The Packers have the second most members in the Pro Football Hall of Fame with 23. They trail only the Chicago Bears (with 27).
Players
Coaches and Executives
Retired numbers.
In nearly nine decades of Packers football, the Packers have formally retired 5 numbers. All five Packers are members of the Pro Football Hall of Fame and their numbers and names are displayed on the green facade of Lambeau Field's north endzone as well as in the Lambeau Field Atrium.
After Brett Favre stated his intent to retire in May 2008, the Packers announced that his No. 4 would be retired in a ceremony during the team's 2008 opening game against the Minnesota Vikings. The ceremony was cancelled following Favre's subsequent decision to return to the game, and he was traded to the New York Jets. In March 2009, the Packers indicated that the team still intends to retire Favre's number, but due to the circumstances surrounding his departure from the team, no timeline had been set. However, the Packers have not reissued the No. 4 jersey since Favre left the team, and it is understood no Packer will wear it again in the near future. On August 3, 2014, the Packers organization announced the retirement of the No. 4 jersey after the 2014 NFL season and the induction of Favre into the Packers Hall of Fame in 2015.
Radio, television and film.
The Packers are unique in having their market area cover two media markets, both Green Bay and Milwaukee. NFL blackout restrictions for the team apply within both areas. However, Packers games have not been blacked out locally since 1972 (the last year home game local telecasts were prohibited regardless of sellout status) due to strong home attendance and popularity. As mentioned above, every Packers home game—preseason, regular season and playoffs—has been sold out since 1960.
The flagship station of the Packers Radio Network is WTMJ in Milwaukee. WTMJ has aired Packers games since 1929, the longest association between a radio station and an NFL team to date, and the only rights deal in American professional sports where a station outside of the team's main metro area is the radio flagship; while this might be unusual, the station transmits a city-grade signal at all hours to Green Bay proper. Games air in Green Bay on WTAQ (1360/97.5) and WIXX-FM (101.1), and WAPL (105.7) and WHBY (1150) in Appleton and the Fox Cities. Wayne Larrivee is the play-by-play announcer and Larry McCarren is the color analyst. Larrivee joined the team after many years as the Chicago Bears' announcer. Jim Irwin and Max McGee were the longtime radio announcers before Larrivee and McCarren. When victory is assured for the Packers, either a game winning touchdown, interception or a crucial 4th down defensive stop, Larrivee's trademark declaration of "And there is your dagger!" signifies the event. In limited circumstances where the Milwaukee Brewers are in either playoff or post-season contention and their play-by-play takes priority, WTMJ's sister FM station WLWK-FM (94.5) currently airs Packer games to avert game conflicts.
The TV rights for pre-season games not nationally broadcast are held by Journal Broadcast Group stations WGBA-TV (Channel 26) in Green Bay and WTMJ-TV (Channel 4) in Milwaukee, along with Quincy Newspapers' six ABC stations in the central, northern and western parts of the state, KQDS-TV (Channel 21) in Duluth-Superior, and in Escanaba/Marquette, Michigan, WLUC-TV (Channel 6), along with their Fox subchannel. As such, these stations are all allowed to use the tagline "Your official Packers station" in their market area by the team, and also carry the weekly coach's show hosted by WTMJ-TV's Jessie Garcia, "The Mike McCarthy Show" on Tuesday evenings at 6:30 pm throughout the football season. Until the end of the 2011 season, the team's partner in Green Bay was WFRV-TV (Channel 5), and sister satellite WJMN-TV in Escanaba. As part of the 2012 deal, McCarren resigned his duties as sports director of WFRV to move to WTMJ/WGBA as a Packers analyst, becoming WGBA's official sports director on April 1, 2013 as his non-compete clause to appear as a sports anchor in Green Bay expired. WFRV/WJMN still airs any Packers regular season home games against an AFC team.
The 2012 TV rights deal expanded the team's preseason network further across the Midwest. Additional stations include the Quad Cities region of Iowa/Illinois where game coverage is carried by KLJB (Channel 18) in Davenport, Iowa and KGCW (Channel 26) in Burlington, Iowa, both owned by Grant Broadcasting System II, KCWI-TV (Channel 23) in Des Moines, KWWL (Channel 7) in Waterloo, Iowa, and in Omaha, Nebraska, KMTV-TV (Channel 3), a sister Journal station to WTMJ and WGBA. As part of a large package of preseason football from various team networks, KFVE (Channel 9) in Honolulu, Hawaii also carried Packers state network games in the 2012 preseason. The network also added its first affiliate with Spanish language play-by-play, Milwaukee's WYTU-LD (Channel 63/49.4), a Telemundo affiliate, which airs statewide on the cable systems of Charter Communications and throughout Time Warner Cable's eastern Wisconsin systems, including Green Bay.
Pre-season coverage is produced by CBS, formerly using the "NFL on CBS" graphics package until the last contract ended as a remnant of WFRV's former ownership by the CBS Corporation itself until 2007. In 2012, the pre-season coverage began to use the NBC Sports "Sunday Night Football" graphics package due to WTMJ/WGBA's NBC affiliation. The TV play-by-play announcer, Kevin Harlan (also on loan from CBS), is the son of former Packers president Bob Harlan, with Rich Gannon joining him as color commentator. Since the 2008 pre-season all Packers preseason games on the statewide network are produced and aired in high definition, with WTMJ-TV subcontracting the games to minor network affiliates in Milwaukee during Summer Olympics years due to mandatory non-preemption policies by their network, NBC (this was not done in 2012 as the pre-season opener was a national ESPN game).
ESPN "Monday Night Football" games, both pre-season and season, are broadcast over the air on ABC affiliates WBAY-TV (Channel 2) in Green Bay and WISN-TV (Channel 12) in Milwaukee, while the stations airing Packers games in the NFL Network "Thursday Night Football" package have varied. WBAY's evening news anchor Bill Jartz also serves as the public address system announcer for Lambeau Field.
The team's intra-squad Lambeau scrimmage at the beginning of the season, marketed as "Packers Family Night", is broadcast by WITI (Channel 6) in Milwaukee, and produced by WLUK (Channel 11) in Green Bay, both Fox affiliates which broadcast the bulk of the team's regular-season games. The scrimmage is also broadcast by the state's other Fox affiliates. It was aired for the first time in 2011 in high definition.
In 2015, five members of the Green Bay Packers made an appearance as an a cappella group in the comedy sequel of Pitch Perfect, Pitch Perfect 2.

</doc>
<doc id="12795" url="http://en.wikipedia.org/wiki?curid=12795" title="General election">
General election

A general election is an election in which all or most members of a given political body are chosen. The term is usually used to refer to elections held for a nation's primary legislative body, as distinguished from by-elections and local elections.
In presidential systems, the term refers to a regularly scheduled election where both the president, and either "a class" of or all members of the national legislature are elected at the same time. A general election day may also include elections for local officials.
The term originates in the Elections in the United Kingdom for the House of Commons.
In India.
The elections held to elect the members of the Lok Sabha after expiry of the normal term of five years are called the General Elections. Elections to some State Legislative Assemblies may be held along with the Parliamentary Elections. Earlier up to 1957 simultaneous elections were held for both the Lok Sabha and the State Assemblies. However, on account of early dismissal and mid-term elections the two got separated.
In the United Kingdom.
The term general election in the United Kingdom often refers to the elections held on the same day in all constituencies of their Members of Parliament (MPs) to the House of Commons. Under the terms of the Fixed-term Parliaments Act 2011, the period between one general election and the next is fixed at 5 years, unless the Commons passes a motion of no confidence in the Government sooner than that, or if the House of Commons, with the support of at least two thirds of its members, resolves that a general election should take place sooner. 
The term may also be used to refer to elections to any democratically elected body in which all of the members are up for election. Section 2 of the Scotland Act 1998, for example, specifically refers to ordinary elections to the Scottish Parliament as general elections.
Originally, British elections took place over a period of several weeks, with individual constituencies holding polling on separate days. The Parliament Act 1911 introduced the requirement that elections in all parliamentary constituencies be held on the same day. There has been a convention since the 1930s that general elections in Britain should take place on a Thursday; the last general election to take place on any other weekday was that of 1931.
The five-year limit on the time of a Parliament can be varied by an Act of Parliament implemented by several bodies. This was done during both World Wars; the Parliament elected in December 1910 was prolonged to November 1918, and that elected in November 1935 lasted until June 1945. The House of Lords has an absolute veto on any Bill to extend the life of Parliament.
In the United States.
In U.S. politics, general elections are elections held at any level (e.g. city, county, congressional district, state) that involve competition between at least two parties. General elections occur every four years and include the presidential election. Some parallels can be drawn between the general election in parliamentary systems and the biennial elections determining all House seats, although there is no analogue to "calling early elections" in the U.S., and the members of the elected U.S. Senate face elections of only one-third at a time at two-year intervals including during a general election.
In the State of Louisiana the expression "general election" means the runoff election which occurs between the two highest candidates as determined by the jungle primary.

</doc>
<doc id="12797" url="http://en.wikipedia.org/wiki?curid=12797" title="Gerard Hengeveld">
Gerard Hengeveld

Gerard Hengeveld (7 December 1910, Kampen - October 28, 2001, Bergen, North Holland) was a Dutch classical pianist, music composer and educationalist. He is especially known for his compositions of study material for piano. Other compositions include two piano concertos, a violin sonata, and a sonata for cello. Hengeveld was an able interpreter and performer of the music of Bach for piano and harpsichord. He gave regular concerts in the Concertgebouw in Amsterdam. Some of his concerts were captured on record.
Hengeveld died in 2001 at the age of 90, in Bergen.

</doc>
<doc id="12798" url="http://en.wikipedia.org/wiki?curid=12798" title="George William, Elector of Brandenburg">
George William, Elector of Brandenburg

George William (German: "Georg Wilhelm") (13 November 1595 – 1 December 1640), of the Hohenzollern dynasty, was margrave and elector of Brandenburg and duke of Prussia from 1619 until his death. His reign was marked by ineffective governance during the Thirty Years' War. He was the father of Frederick William, the "Great Elector".
Biography.
Early life.
Born in Cölln on the Spree, George William was the son of John Sigismund, Margrave of Brandenburg and Anna of Prussia. His maternal grandfather was Albert Frederick, Duke of Prussia.
In 1616 George William married Elisabeth Charlotte of the Palatinate. Their only son Frederick William later became known as the "Great Elector". Of his two daughters, the eldest, Louise Charlotte, married Jacob Kettler, Duke of Courland, and the younger, Hedwig Sophie, married William VI, Landgrave of Hesse-Kassel.
Rule.
In 1619 George William inherited the Margravate of Brandenburg and the Duchy of Prussia. He paid his feudal homage in person to the King of Poland, Zygmunt Waza, in September 1621 in Warsaw (the Duchy of Prussia was a fief of the Kingdom of Poland at the time). The homage was renewed in 1633 after the election of a new Polish king, Władysław IV Waza.
During the Thirty Years' War, George William tried to remain neutral between the Roman Catholic forces of the Holy Roman Empire and the Protestant principalities. As his sister Maria Eleonora of Brandenburg was queen of Sweden, George William had to maneuver between requests of assistance from his Protestant brother-in-law King Gustavus Adolphus of Sweden and his own Protestant counsellors on one side and his Catholic chancellor Count Adam von Schwarzenberg on the other. 
Despite his attempts at neutrality, George William was forced by Gustavus Adolphus to join the Protestant forces in 1631. His rule was largely weak and ineffective, as much of government responsibilities in Brandenburg-Prussia was turned over to Schwarzenberg as the country suffered greatly during the war. Protestant and Catholic troops alike burned and plundered the region and the population was decimated there as it was throughout the German states. 
With his brother-in-law Gustavus dead in 1632, George William maintained the Swedish alliance until after the Swedish defeat at the Battle of Nordlingen on 6 September 1634. At that point, George William withdrew Brandenburg from the war and signed the Peace of Prague with Emperor Ferdinand II on 30 May 1635. Leaving Schwarzenberg in charge of the government, George William withdrew in 1637 to the relatively safe region of the Duchy of Prussia, where he lived in retirement until his death at Königsberg in 1640.

</doc>
<doc id="12808" url="http://en.wikipedia.org/wiki?curid=12808" title="GSM">
GSM

GSM (Global System for Mobile Communications, originally Groupe Spécial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI) to describe protocols for second-generation (2G) digital cellular networks used by mobile phones, first deployed in Finland in July 1992. s of 2014[ [update]] it has become the default global standard for mobile communications - with over 90% market share, operating in over 219 countries and territories.
2G networks developed as a replacement for first generation (1G) analog cellular networks, and the GSM standard originally described a digital, circuit-switched network optimized for full duplex voice telephony. This expanded over time to include data communications, first by circuit-switched transport, then by packet data transport via GPRS (General Packet Radio Services) and EDGE (Enhanced Data rates for GSM Evolution or EGPRS).
Subsequently, the 3GPP developed third-generation (3G) UMTS standards followed by fourth-generation (4G) LTE Advanced standards, which do not form part of the ETSI GSM standard.
"GSM" is a trademark owned by the GSM Association. It may also refer to the (initially) most common voice codec used, Full Rate.
History.
In 1982, work began to develop a European standard for digital cellular voice telephony when the European Conference of Postal and Telecommunications Administrations (CEPT) created the Groupe Spécial Mobile committee and later provided a permanent technical support group based in Paris. Five years later, in 1987, 15 representatives from 13 European countries signed a memorandum of understanding in Copenhagen to develop and deploy a common cellular telephone system across Europe, and EU rules were passed to make GSM a mandatory standard. The decision to develop a continental standard eventually resulted in a unified, open, standard-based network which was larger than that in the United States.
In 1987 Europe produced the very first agreed GSM Technical Specification in February. Ministers from the four big EU countries cemented their political support for GSM with the Bonn Declaration on Global Information Networks in May and the GSM MoU was tabled for signature in September. The MoU drew-in mobile operators from across Europe to pledge to invest in new GSM networks to an ambitious common date. It got GSM up and running fast.
In this short 37-week period the whole of Europe (countries and industries) had been brought behind GSM in a rare unity and speed guided by four public officials Armin Silberhorn (Germany), Stephen Temple (UK), Philippe Dupuis (France), and Renzo Failli (Italy). In 1989, the Groupe Spécial Mobile committee was transferred from CEPT to the European Telecommunications Standards Institute (ETSI).
In parallel, France and Germany signed a joint development agreement in 1984 and were joined by Italy and the UK in 1986. In 1986 the European Commission proposed reserving the 900 MHz spectrum band for GSM. The world's first GSM call was made by the former Finnish prime minister Harri Holkeri to Kaarina Suonio (mayor in city of Tampere) on July 1, 1991, on a network built by Telenokia and Siemens and operated by Radiolinja. The following year in 1992, the first short messaging service (SMS or "text message") message was sent and Vodafone UK and Telecom Finland signed the first international roaming agreement.
Work began in 1991 to expand the GSM standard to the 1800 MHz frequency band and the first 1800 MHz network became operational in the UK by 1993. Also that year, Telecom Australia became the first network operator to deploy a GSM network outside Europe and the first practical hand-held GSM mobile phone became available.
In 1995, fax, data and SMS messaging services were launched commercially, the first 1900 MHz GSM network became operational in the United States and GSM subscribers worldwide exceeded 10 million. Also this year, the GSM Association was formed. Pre-paid GSM SIM cards were launched in 1996 and worldwide GSM subscribers passed 100 million in 1998.
In 2000, the first commercial GPRS services were launched and the first GPRS compatible handsets became available for sale. In 2001 the first UMTS (W-CDMA) network was launched, a 3G technology that is not part of GSM. Worldwide GSM subscribers exceeded 500 million. In 2002 the first Multimedia Messaging Service (MMS) were introduced and the first GSM network in the 800 MHz frequency band became operational. EDGE services first became operational in a network in 2003 and the number of worldwide GSM subscribers exceeded 1 billion in 2004.
By 2005, GSM networks accounted for more than 75% of the worldwide cellular network market, serving 1.5 billion subscribers. In 2005 the first HSDPA capable network also became operational. The first HSUPA network was launched in 2007. High-Speed Packet Access (HSPA) and its uplink and downlink versions are 3G technologies, not part of GSM. Worldwide GSM subscribers exceeded three billion in 2008.
The GSM Association estimated in 2010 that technologies defined in the GSM standard serve 80% of the global mobile market, encompassing more than 5 billion people across more than 212 countries and territories, making GSM the most ubiquitous of the many standards for cellular networks.
It is important to note that GSM is a second-generation (2G) standard employing Time-Division Multiple-Access (TDMA) spectrum-sharing, issued by the European Telecommunications Standards Institute (ETSI). The GSM standard does not include the 3G UMTS CDMA-based technology nor the 4G LTE OFDMA-based technology standards issued by the 3GPP.
Macau planned to phase out its 2G GSM networks as of June 4, 2015, making it the first region to decommission a GSM network.
Technical details.
Network structure.
The network is structured into a number of discrete sections:
Base station subsystem.
GSM is a cellular network, which means that cell phones connect to it by searching for cells in the immediate vicinity. There are five different cell sizes in a GSM network—macro, micro, pico, femto, and umbrella cells. The coverage area of each cell varies according to the implementation environment. Macro cells can be regarded as cells where the base station antenna is installed on a mast or a building above average rooftop level. Micro cells are cells whose antenna height is under average rooftop level; they are typically used in urban areas. Picocells are small cells whose coverage diameter is a few dozen metres; they are mainly used indoors. Femtocells are cells designed for use in residential or small business environments and connect to the service provider’s network via a broadband internet connection. Umbrella cells are used to cover shadowed regions of smaller cells and fill in gaps in coverage between those cells.
Cell horizontal radius varies depending on antenna height, antenna gain, and propagation conditions from a couple of hundred meters to several tens of kilometres. The longest distance the GSM specification supports in practical use is 35 km. There are also several implementations of the concept of an extended cell, where the cell radius could be double or even more, depending on the antenna system, the type of terrain, and the timing advance.
Indoor coverage is also supported by GSM and may be achieved by using an indoor picocell base station, or an indoor repeater with distributed indoor antennas fed through power splitters, to deliver the radio signals from an antenna outdoors to the separate indoor distributed antenna system. These are typically deployed when significant call capacity is needed indoors, like in shopping centers or airports. However, this is not a prerequisite, since indoor coverage is also provided by in-building penetration of the radio signals from any nearby cell.
GSM carrier frequencies.
GSM networks operate in a number of different carrier frequency ranges (separated into GSM frequency ranges for 2G and UMTS frequency bands for 3G), with most 2G GSM networks operating in the 900 MHz or 1800 MHz bands. Where these bands were already allocated, the 850 MHz and 1900 MHz bands were used instead (for example in Canada and the United States). In rare cases the 400 and 450 MHz frequency bands are assigned in some countries because they were previously used for first-generation systems.
Most 3G networks in Europe operate in the 2100 MHz frequency band. For more information on worldwide GSM frequency usage, see GSM frequency bands.
Regardless of the frequency selected by an operator, it is divided into timeslots for individual phones. This allows eight full-rate or sixteen half-rate speech channels per radio frequency. These eight radio timeslots (or burst periods) are grouped into a TDMA frame. Half-rate channels use alternate frames in the same timeslot. The channel data rate for all 8 channels is 270.833 kbit/s, and the frame duration is 4.615 ms.
The transmission power in the handset is limited to a maximum of 2 watts in GSM 850/900 and 1 watt in GSM 1800/1900.
Voice codecs.
GSM has used a variety of voice codecs to squeeze 3.1 kHz audio into between 6.5 and 13 kbit/s. Originally, two codecs, named after the types of data channel they were allocated, were used, called Half Rate (6.5 kbit/s) and Full Rate (13 kbit/s). These used a system based on linear predictive coding (LPC). In addition to being efficient with bitrates, these codecs also made it easier to identify more important parts of the audio, allowing the air interface layer to prioritize and better protect these parts of the signal.
As GSM was further enhanced in 1997 with the Enhanced Full Rate (EFR) codec, a 12.2 kbit/s codec that uses a full-rate channel. Finally, with the development of UMTS, EFR was refactored into a variable-rate codec called AMR-Narrowband, which is high quality and robust against interference when used on full-rate channels, or less robust but still relatively high quality when used in good radio conditions on half-rate channel.
Subscriber Identity Module (SIM).
One of the key features of GSM is the Subscriber Identity Module, commonly known as a SIM card. The SIM is a detachable smart card containing the user's subscription information and phone book. This allows the user to retain his or her information after switching handsets. Alternatively, the user can also change operators while retaining the handset simply by changing the SIM. Some operators will block this by allowing the phone to use only a single SIM, or only a SIM issued by them; this practice is known as SIM locking.
Phone locking.
Sometimes mobile network operators restrict handsets that they sell for use with their own network. This is called "locking" and is implemented by a software feature of the phone. A subscriber may usually contact the provider to remove the lock for a fee, utilize private services to remove the lock, or use software and websites to unlock the handset themselves.
In some countries (e.g., Bangladesh, Brazil, Chile, Germany, Hong Kong, India, Iran, Lebanon, Malaysia, Nepal, Pakistan, Singapore, South Africa) all phones are sold unlocked.
GSM service security.
GSM was designed with a moderate level of service security. The system was designed to authenticate the subscriber using a pre-shared key and challenge-response. Communications between the subscriber and the base station can be encrypted. The development of UMTS introduces an optional Universal Subscriber Identity Module (USIM), that uses a longer authentication key to give greater security, as well as mutually authenticating the network and the user, whereas GSM only authenticates the user to the network (and not vice versa). The security model therefore offers confidentiality and authentication, but limited authorization capabilities, and no non-repudiation.
GSM uses several cryptographic algorithms for security. The A5/1, A5/2, and A5/3 stream ciphers are used for ensuring over-the-air voice privacy. A5/1 was developed first and is a stronger algorithm used within Europe and the United States; A5/2 is weaker and used in other countries. Serious weaknesses have been found in both algorithms: it is possible to break A5/2 in real-time with a ciphertext-only attack, and in January 2007, The Hacker's Choice started the A5/1 cracking project with plans to use FPGAs that allow A5/1 to be broken with a rainbow table attack. The system supports multiple algorithms so operators may replace that cipher with a stronger one.
On 28 December 2009 German computer engineer announced that he had cracked the A5/1 cipher. According to Nohl, he developed a number of rainbow tables (static values which reduce the time needed to carry out an attack) and have found new sources for known plaintext attacks. He also said that it is possible to build "a full GSM interceptor...from open-source components" but that they had not done so because of legal concerns. Nohl claimed that he was able to intercept voice and text conversations by impersonating another user to listen to voicemail, make calls, or send text messages using a seven-year-old Motorola cellphone and decryption software available for free online.
New attacks have been observed that take advantage of poor security implementations, architecture, and development for smartphone applications. Some wiretapping and eavesdropping techniques hijack the audio input and output providing an opportunity for a third party to listen in to the conversation.
GSM uses General Packet Radio Service (GPRS) for data transmissions like browsing the web. The most commonly deployed GPRS ciphers were publicly broken in 2011.
The researchers revealed flaws in the commonly used GEA/1 and GEA/2 ciphers and published the open-source "gprsdecode" software for sniffing GPRS networks. They also noted that some carriers do not encrypt the data (i.e., using GEA/0) in order to detect the use of traffic or protocols they do not like (e.g., Skype), leaving customers unprotected. GEA/3 seems to remain relatively hard to break and is said to be in use on some more modern networks. If used with USIM to prevent connections to fake base stations and downgrade attacks, users will be protected in the medium term, though migration to 128-bit GEA/4 is still recommended.
Standards information.
The GSM systems and services are described in a set of standards governed by ETSI, where a full list is maintained.
GSM open-source software.
Several open-source software projects exist that provide certain GSM features:
Issues with patents and open source.
Patents remain a problem for any open-source GSM implementation, because it is not possible for GNU or any other free software distributor to guarantee immunity from all lawsuits by the patent holders against the users. Furthermore new features are being added to the standard all the time which means they have patent protection for a number of years.
The original GSM implementations from 1991 may now be entirely free of patent encumbrances, however patent freedom is not certain due to the United States' "first to invent" system that was in place until 2012. The "first to invent" system, coupled with "patent term adjustment" can extend the life of a U.S. patent far beyond 20 years from its priority date. It is unclear at this time whether OpenBTS will be able to implement features of that initial specification without limit. As patents subsequently expire, however, those features can be added into the open-source version. As of 2011, there have been no lawsuits against users of OpenBTS over GSM use.

</doc>
<doc id="13030" url="http://en.wikipedia.org/wiki?curid=13030" title="Gary Busey">
Gary Busey

Gary Busey (born June 29, 1944) is an American actor. He has appeared in films such as "Thunderbolt and Lightfoot" (1974),"The Gumball Rally" (1976), "The Buddy Holly Story" (1978), "Big Wednesday" (1978), "Silver Bullet" (1985), "Eye of the Tiger" (1986), "Lethal Weapon" (1987), "Predator 2" (1990), "Point Break" (1991), "Under Siege" (1992), "The Firm" (1993), "Rookie of the Year" (1994), "Surviving the Game" (1994), "Drop Zone" (1994), "Black Sheep" (1996), "Lost Highway" (1997), "Soldier" (1998), and "Fear and Loathing in Las Vegas" (1998). In addition he has had guest appearances on shows such as "Gunsmoke", "Walker, Texas Ranger", "Law & Order", "Scrubs", and "Entourage". He was nominated for an Academy Award for Best Actor in 1978 for his role in "The Buddy Holly Story".
Early life.
Busey was born in Goose Creek, Texas, the son of Sadie Virginia (née Arnett), a homemaker, and Delmer Lloyd Busey, a construction design manager. He graduated from Nathan Hale High School in Tulsa, Oklahoma in 1962. While attending Pittsburg State University in Pittsburg, Kansas on a football scholarship, he became interested in acting. He then transferred to Oklahoma State University in Stillwater, Oklahoma, where he quit school just one class short of graduation.
Career.
Early career.
Busey began his show business career as a drummer in The Rubber Band. He appears on several Leon Russell recordings, credited as playing drums under the names "Teddy Jack Eddy" and "Sprunk", a character he created when he was a cast member of a local television comedy show in Tulsa, Oklahoma, called "The Uncanny Film Festival and Camp Meeting" on station KTUL (which starred fellow Tulsan Gailard Sartain as "Dr. Mazeppa Pompazoidi"). For his skits on Uncanny Film Festival, Busey drew on his American Hero, belligerent, know-it-all character. When he told Gailard Sartain his character needed a name, Sartain replied, "Take three: Teddy, Jack and Eddy."
He played in a band called Carp, which released one album on Epic Records in 1969. Busey continued to play several small roles in both film and television during the 1970s. In 1975, as the character "Harvey Daley," he was the last person killed on the series "Gunsmoke" (in the third-to-last episode, No. 633 – "The Busters").
Rise to prominence.
In 1974, Busey made his major film debut with a supporting role in Michael Cimino's buddy action caper "Thunderbolt and Lightfoot", starring Clint Eastwood and Jeff Bridges.
In 1976, he was hired by Barbra Streisand and her producer-boyfriend Jon Peters to play Bobby Ritchie, road manager to Kris Kristofferson's character in the remake film "A Star is Born". On the DVD commentary of the film, Streisand says Busey was great and that she had seen him on a TV series and thought he had the right qualities to play the role.
In 1978, he starred as rock legend Buddy Holly in "The Buddy Holly Story" with Sartain as The Big Bopper. For his performance, Busey received the greatest critical acclaim of his career and the movie earned Busey an Academy Award nomination for Best Actor and the National Society of Film Critics' Best Actor award. In the film, he changes the lyrics to the song "Well All Right" and sings, "We're gonna love Teddy Jack..." a reference to his Teddy Jack Eddy persona. In the same year he also starred in the small yet acclaimed drama "Straight Time" and the surfing movie "Big Wednesday", which is now a minor cult classic.
In the 1980s, Busey's roles included the critically acclaimed western "Barbarosa" (1982), the comedies "D.C. Cab" (1983) and "Insignificance" (1985), and the Stephen King adaption "Silver Bullet" (1985). Perhaps most notably, he played one of the primary antagonists in the smash hit action comedy "Lethal Weapon" (1987).
In the 1990s, he had prominent supporting roles in successful action films such as "Predator 2" (1990), "Point Break" (1991) and "Under Siege" (1992), which remain some of his best-known roles today. He also appeared in a number of other notable films such as "Rookie of the Year" (1993), "The Firm" (1993), "Black Sheep" (1996), "Lost Highway" (1997), and "Fear and Loathing in Las Vegas" (1998), further establishing himself as a prolific actor in major films. Many of these characters are defined by their dynamic yet odd and offbeat personalities.
Busey sang the song "Stay All Night" on "Saturday Night Live" in March 1979 (season 4, episode 14), and on the "Late Show with David Letterman" in the 1990s.
2000s–present.
Since the turn of the century, Busey has appeared in very few mainstream films. In 2002, Busey voiced the character Phil Cassidy in ', then again in ' in 2006. In 2005, he also voiced himself on an episode of "The Simpsons" and appeared in the popular miniseries "Into the West". Busey controversially appeared in the 2006 Turkish nationalist film "," (Kurtlar Vadisi: Irak, in Turkish), which was accused of fascism, anti-Americanism and anti-Semitism.
In 2007, he appeared as himself in a prominent recurring role on HBO's "Entourage", in which he parodied his eccentric image, ultimately appearing on three episodes of the show.
In 2008, he joined the second season of the reality show "Celebrity Rehab with Dr. Drew". Busey returned to reality television in "Celebrity Apprentice 4", which premiered in March 2011, and appeared again in "Celebrity Apprentice 6". There, he briefly reprised his role as Buddy Holly by performing "Not Fade Away".
In a series of 2010 YouTube advertisements for Vitamin Water, Busey appeared as Norman Tugwater, a lawyer who defends professional athletes' entitlements to a cut from Fantasy Football team owners.
In 2014, he became a celebrity spokesperson for Amazon Fire TV. That August, he appeared in, and became the first American winner of the fourteenth series of the UK version of "Celebrity Big Brother".
Personal life.
In 1971, Busey's wife Judy Helkenberg gave birth to their son, William Jacob "Jake" Busey. Busey and Helkenberg divorced when Jake was 19 years old. Busey has a daughter named Alectra from a previous relationship. In February 2010, Busey's girlfriend Steffanie Sampson gave birth to their son, Luke Sampson Busey.
On December 4, 1988, Busey was severely injured in a motorcycle accident in which he was not wearing a helmet. His skull was fractured, and doctors feared he suffered permanent brain damage. During the filming of the second season of "Celebrity Rehab" in 2008, Busey was referred to psychiatrist Dr. Charles Sophy. Sophy suspected that Busey's brain injury has had a greater effect on him than realized. He described it as essentially weakening his mental "filters" and causing him to speak and act impulsively. Sophy recommended Busey take valproic acid (Depakote), with which Busey agreed.
In 1996, Busey publicly announced that he was a Christian, saying: "I am proud to tell Hollywood I am a Christian. For the first time I am now free to be myself."
In 1997, after recurring nosebleeds, he underwent surgery to remove a golf-ball-sized cancerous tumor from his sinus cavity, and after surgery he underwent radiation therapy.

</doc>
<doc id="13050" url="http://en.wikipedia.org/wiki?curid=13050" title="Guru Meditation">
Guru Meditation

Guru Meditation is an error notice displayed by early versions of the Commodore Amiga computer when they crashed. It is analogous to the "Blue Screen Of Death" in Microsoft Windows operating systems, or a kernel panic in Unix. It has later been used as a message for unrecoverable errors elsewhere, such as Varnish, a reverse proxy, and HTTP accelerator, and VirtualBox, a Hypervisor environment.
Description.
When a Guru Meditation is displayed, the options are to reboot by pressing the left mouse button, or to invoke ROMWack by pressing the right mouse button. (ROMWack is a minimalist debugger built into the operating system which is accessible by connecting a 9600 bit/s terminal to the serial port.)
The alert itself appears as a black rectangular box located in the upper portion of the screen. Its border and text are red for a normal Guru Meditation, or green/yellow for a Recoverable Alert, another kind of Guru Meditation. The screen goes black, and the power and disk-activity LEDs may blink immediately before the alert appears. In AmigaOS 1.x, programmed in ROMs known as Kickstart 1.1, 1.2 and 1.3, the errors are always red. In AmigaOS 2.x and 3.x, recoverable alerts are yellow, except for some very early versions of 2.x where they were green. Dead-end alerts are red in all OS versions.
The alert occurred when there was a fatal problem with the system. If the system had no means of recovery, it could display the alert, even in systems with numerous critical flaws. In extreme cases, the alert could even be displayed if the system's memory was completely exhausted.
The error is displayed as two fields, separated by a period. The format is #0000000x.yyyyyyyy in case of a CPU error, or #aabbcccc.dddddddd in case of a system software error. The first field is either the Motorola 68000 exception number that occurred (if a CPU error occurs) or an internal error identifier (such as an 'Out of Memory' code), in case of a system software error. The second can be the address of a "Task" structure, or the address of a memory block whose allocation or deallocation failed. It is never the address of the code that caused the error. If the cause of the crash is uncertain, this number is rendered as 48454C50, which stands for "HELP" in hexadecimal ASCII characters (48=H, 45=E, 4C=L, 50=P).
The text of the alert messages was completely baffling to most users. Only highly technically adept Amiga users would know, for example, that exception 3 was an address error, and meant the program was accessing a word on an unaligned boundary. Users without this specialized knowledge would have no recourse but to look for a "Guru" or to simply reboot the machine and hope for the best.
Guru Meditation handler.
There was a commercially available error handler for AmigaOS, before version 2.04, called GOMF (Get Outa My Face) made by Hypertek/Silicon Springs Development corp. It was able to deal with many kinds of errors and gave the user a choice to either remove the offending process and associated screen or allow the machine to show the Guru Meditation. In many cases removal of the offending process gave one the choice to save one's data and exit running programs before rebooting the system. When the damage was not extensive one was able to continue using the machine. However it did not save the user from all errors, as one may have still seen this error occasionally.
Recoverable Alerts.
Recoverable Alerts are non-critical crashes in the computer system. In most cases, it is possible to resume work and save files after a Recoverable Alert, while a normal, red Guru Meditation always results in an immediate reboot.
Many experts nevertheless recommend to reboot as soon as possible after encountering a Recoverable Alert because the system may be in an unpredictable state that can cause data corruption .
System software error codes.
The first byte specifies the area of the system affected. The top bit will be set if the error is a dead end alert.
Origins.
The term "Guru Meditation Error" originated as an in-house joke in Amiga's early days. The company had a product called the "Joyboard", a game controller much like a joystick but operated by one's feet, similar to the modern-day Wii Balance Board. Early in the development of the Amiga computer operating system, the company's developers became so frustrated with the system's frequent crashes that, as a relaxation technique, a game developed where a person would sit cross-legged on the joyboard, resembling an Indian guru. The player tried to remain extremely still; the winner of the game stayed still the longest. If the player moved too much, a "guru meditation" error occurred.
The final unlockable balance activity in Wii Fit represents a similar game. The same game is unlocked from the start in "Wii Fit Plus".

</doc>
<doc id="13057" url="http://en.wikipedia.org/wiki?curid=13057" title="Gatling gun">
Gatling gun

The Gatling gun is one of the best-known early rapid-fire weapons and a forerunner of the modern machine gun. Invented by Richard Gatling, it is known for its use by the Union forces during the American Civil War in the 1860s, which was the first time it was employed in combat. Later it was used in the Boshin War, the Anglo-Zulu War and still later in the assault on San Juan Hill during the Spanish–American War.
The Gatling gun's operation centered on a cyclic multi-barrel design which facilitated cooling and synchronized the firing-reloading sequence. Each barrel fired a single shot when it reached a certain point in the cycle, after which it ejected the spent cartridge, loaded a new round and, in the process, allowed the barrel to cool somewhat. This configuration allowed higher rates of fire to be achieved without the barrel overheating.
History.
The Gatling gun was designed by the American inventor Dr. Richard J. Gatling in 1861 and patented on November 4, 1862. Gatling wrote that he created it to reduce the size of armies and so reduce the number of deaths by combat and disease, and to show how futile war is.
Although the first Gatling gun was capable of firing continuously, it required a person to crank it; therefore it was not a true automatic weapon. The Maxim gun, invented and patented in 1883, was the first true fully automatic weapon, making use of the fired projectile's recoil force to reload the weapon. Nonetheless, the Gatling gun represented a huge leap in firearm technology.
Prior to the Gatling gun, the only weapons available to militaries, capable of firing many projectiles in a short space of time, were mass-firing volley weapons like the French Reffye mitrailleuse in 1870–1871, and field cannons firing canister shot, much like a very large shotgun. The latter were widely used during and after the Napoleonic Wars. Although the maximum rate of fire was increased by firing multiple projectiles simultaneously, these weapons still needed to be reloaded after each discharge, which for multi-barrel systems like the "mitrailleuse" was cumbersome and time-consuming. This negated much of the advantage of their high rate of fire per discharge, making them much less powerful on the battlefield. In comparison, the Gatling gun offered a rapid and continuous rate of fire without having to be manually reloaded by opening the breech.
The original Gatling gun was a field weapon which used multiple rotating barrels turned by a hand crank, and firing loose (no links or belt) metal cartridge ammunition using a gravity feed system from a hopper. The Gatling gun's innovation lay neither in the rotating chamber mechanism, first used by the Puckle gun nearly a century and a half before, nor in the use of multiple barrels to limit overheating (used by the "mitrailleuse" gun); rather, the innovation was the gravity feed reloading system, which allowed unskilled operators to achieve a relatively high rate of fire of 200 rounds per minute.
American Civil War and the Americas.
The Gatling gun was first used in warfare during the American Civil War. Twelve of the guns were purchased personally by Union commanders and used in the trenches during the siege of Petersburg, Virginia (June 1864 – April 1865). Eight other Gatling guns were fitted on gunboats. The gun was not accepted by the American Army until 1866, but a sales representative of the manufacturing company demonstrated it in combat.
Captain Germán Astete of the Peruvian Navy took with him dozens of Gatling guns from the United States to Peru in December 1879 during the Peru-Chile War of the Pacific. Gatling guns were used by the Peruvian Navy and Army, especially in the Battle of Tacna (May 1880) and the Battle of San Juan (January 1881) against the invading Chilean Army. Lieutenant A.L. Howard of the Connecticut National Guard had an interest in the company manufacturing Gatling guns, and took a personally owned Gatling gun to Saskatchewan, Canada, in 1885 for use with the Canadian military against the Métis during Louis Riel's North-West Rebellion.
Early multi-barrel guns were approximately the size and weight of artillery pieces, and were often perceived as a replacement for cannons firing grapeshot or canister shot. Gatling guns were even mounted aboard ships. Compared with earlier weapons such as the "mitrailleuse", which required manual reloading, the Gatling gun was more reliable and easier to operate, and had a lower, but continuous rate of fire. The large wheels required to move these guns around required a high firing position, which increased the vulnerability of their crews.
Sustained firing of gunpowder cartridges generated a cloud of smoke, making concealment impossible until smokeless powder became available in the late 19th century. When fighting troops of industrialized nations, Gatling guns could be engaged by artillery they could not reach and their crews could be targeted by snipers they could not see.
In Africa and Asia.
The Gatling gun was used most successfully to expand European colonial empires by killing warriors of non-industrialized societies mounting massed attacks, including the Matabele, the Zulu, the Bedouins, and the Mahdists. Imperial Russia purchased 400 Gatling guns and used them against Turkmen cavalry and other nomads of central Asia. The Royal Navy used Gatling guns against the Egyptians at Alexandria in 1882.
Spanish–American War.
Because of infighting within army ordnance, Gatling guns were again used by the U.S. Army during the Spanish–American War. A four-gun battery of Model 1895 ten-barrel Gatling guns in .30 Army made by Colt's Arms Company was formed into a separate detachment led by Lt. John "Gatling Gun" Parker. The detachment proved very effective, supporting the advance of American forces at the Battle of San Juan Hill, where three of the Gatlings with swivel mountings were used with great success against the Spanish defenders. During the American charge up San Juan and Kettle hills, the three guns fired a total of 18,000 .30 Army rounds in 8 1/2 minutes (an average of over 700 rounds per minute per gun) against Spanish troop positions along the crest of both hills, wreaking terrible carnage.
Despite this remarkable achievement, the Gatling's weight and cumbersome artillery carriage hindered its ability to keep up with infantry forces over difficult ground, particularly in Cuba, where roads were often little more than jungle footpaths. By this time, the U.S. Marines had been issued the modern tripod-mounted M1895 Colt–Browning machine gun using the 6mm Lee Navy round, which they employed to defeat the Spanish infantry at the battle of Cuzco Wells.
Basic design.
The Gatling gun was hand-crank operated with six barrels revolving around a central shaft, although some models had as many as ten. Early models had a fibrous matting stuffed in among the barrels which could be soaked with water to cool the barrels down. Later models eliminated the matting-filled barrels as being counterproductive.
The ammunition was initially a steel cylinder charged with black powder and primed with a percussion cap, because self-contained brass cartridges had not yet been fully developed and available. The shells were gravity-fed into the breech through a hopper or stick magazine on top of the gun. Each barrel had its own firing mechanism. After 1861, new brass cartridges similar to modern cartridges replaced the paper cartridge, but Gatling did not switch to them immediately.
The Model 1881 was designed to use the 'Bruce'-style feed system (U.S. Patents 247,158 and 343,532) that accepted two rows of .45-70 cartridges. While one row was being fed into the gun, the other could be reloaded, thus allowing sustained fire. The final gun required four operators. By 1876, the gun had a theoretical rate of fire of 1,200 rounds per minute which is 20 rounds per second, although 400 rounds per minute was more likely in combat.
Each barrel fires once per revolution at about the same position. The barrels, a carrier, and a lock cylinder were separate and all mounted on a solid plate revolving around a central shaft, mounted on an oblong fixed frame. The carrier was grooved and the lock cylinder was drilled with holes corresponding to the barrels. Each barrel had a single lock, working in the lock cylinder on a line with the barrel. The lock cylinder was encased and joined to the frame. The casing was partitioned, and through this opening the barrel shaft was journaled. In front of the casing was a cam with spiral surfaces. The cam imparted a reciprocating motion to the locks when the gun rotated. Also in the casing was a cocking ring with projections to cock and fire the gun.
Turning the crank rotated the shaft. Cartridges, held in a hopper, dropped individually into the grooves of the carrier. The lock was simultaneously forced by the cam to move forward and load the cartridge, and when the cam was at its highest point, the cocking ring freed the lock and fired the cartridge. After the cartridge was fired the continuing action of the cam drew back the lock bringing with it the spent cartridge which then dropped to the ground.
The grouped barrel concept had been explored by inventors since the 18th century, but poor engineering and the lack of a unitary cartridge made previous designs unsuccessful. The initial Gatling gun design used self-contained, reloadable steel cylinders with a chamber holding a ball and black-powder charge, and a percussion cap on one end. As the barrels rotated, these steel cylinders dropped into place, were fired, and were then ejected from the gun. The innovative features of the Gatling gun were its independent firing mechanism for each barrel and the simultaneous action of the locks, barrels, carrier and breech.
The smallest-caliber gun also had a Broadwell drum feed in place of the curved magazine of the other guns. The drum, named after L. W. Broadwell, an agent for Gatling's company, comprised twenty stick magazines arranged around a central axis, like the spokes of a wheel, each holding twenty cartridges with the bullet noses oriented toward the central axis. This invention was patented in U. S. 110,338. As each magazine emptied, the drum was manually rotated to bring a new magazine into use until all 400 rounds had been fired.
By 1893, the Gatling was adapted to take the new .30 Army smokeless cartridge. The new M1893 guns featured six barrels, and were capable of a maximum (initial) rate of fire of 800–900 rounds per minute. Dr. Gatling later used examples of the M1893 powered by electric motor and belt to drive the crank. Tests demonstrated the electric Gatling could fire bursts of up to 1,500 rpm.
The M1893, with minor revisions, became the M1895, and 94 guns were produced for the U.S. Army by Colt. Four M1895 Gatlings under Lt. John H. Parker saw considerable combat during the Santiago campaign in Cuba in 1898. The M1895 was designed to accept only the Bruce feeder. All previous models were unpainted, but the M1895 was painted olive drab (O.D.) green, with some parts left blued.
The Model 1900 was very similar to the model 1895, but with only a few components finished in O.D. green. The U.S. Army purchased a quantity of M1900s. All Gatling Models 1895–1903 could be mounted on an armored field carriage. In 1903, the Army converted its M1900 guns in .30 Army to fit the new .30-03 cartridge (standardized for the M1903 Springfield rifle) as the M1903. The later M1903-'06 was an M1903 converted to .30-06. This conversion was principally carried out at the Army's Springfield Armory arsenal repair shops. All models of Gatling guns were declared obsolete by the U.S. Army in 1911, after 45 years of service.
Development of modern Gatling-type guns.
After the Gatling gun was replaced in service by newer recoil- or gas-operated weapons, the approach of using multiple externally powered rotating barrels fell into disuse for many decades. However, some examples were developed during the interwar years, but only existed as prototypes, or were rarely used. The concept resurfaced after World War II with the development of the M61 Vulcan.
Modern Gatling guns.
Many other versions of the Gatling gun were built in the late 20th century-present. An example is the M61 Vulcan, built in 1956.

</doc>
<doc id="13077" url="http://en.wikipedia.org/wiki?curid=13077" title="Galileo (spacecraft)">
Galileo (spacecraft)

 
Galileo was an unmanned spacecraft which studied the planet Jupiter and its moons, as well as several other Solar System bodies. Named after the astronomer Galileo Galilei, it consisted of an orbiter and entry probe. It was launched on October 18, 1989, carried by Space Shuttle "Atlantis", on the STS-34 mission. "Galileo" arrived at Jupiter on December 7, 1995, after gravitational assist flybys of Venus and Earth, and became the first spacecraft to orbit Jupiter. It launched the first probe into Jupiter, directly measuring its atmosphere. Despite suffering major antenna problems, "Galileo" achieved the first asteroid flyby, of 951 Gaspra, and discovered the first asteroid moon, Dactyl, around 243 Ida. In 1994, "Galileo" observed Comet Shoemaker–Levy 9's collision with Jupiter. The spacecraft was an international effort by the United States of America and the Federal Republic of Germany.
Jupiter's atmospheric composition and ammonia clouds were recorded, the clouds possibly created by outflows from the lower depths of the atmosphere. Io's volcanism and plasma interactions with Jupiter's atmosphere were also recorded. The data "Galileo" collected supported the theory of a liquid ocean under the icy surface of Europa, and there were indications of similar liquid-saltwater layers under the surfaces of Ganymede and Callisto. Ganymede was shown to possess a magnetic field and the spacecraft found new evidence for exospheres around Europa, Ganymede, and Callisto. "Galileo" also discovered that Jupiter's faint ring system consists of dust from impacts on the four small inner moons. The extent and structure of Jupiter's magnetosphere was also mapped.
On September 21, 2003, after 14 years in space and 8 years in the Jovian system, "Galileo‍‍ '​‍s" mission was terminated by sending the orbiter into Jupiter's atmosphere at a speed of over 48 km per second, eliminating the possibility of contaminating local moons with terrestrial bacteria.
On December 11, 2013, NASA reported, based on results from the "Galileo" mission, the detection of "clay-like minerals" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa, moon of Jupiter. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.
Background.
In the early 1970s the first flyby's of Jupiter were achieved by Pioneer 10 and Pioneer 11, and before the decade was out it was also visited by the more advanced Voyager 1 and Voyager 2 spacecraft. Jupiter was rated as the number one priority in the Planetary Science Decadal Survey published in the summer of 1968. Jupiter is the largest planet in the solar system and its largest moon, Ganymede, is bigger than Mercury. (see also Moons of Jupiter)
Mission overview.
Work on the spacecraft began at JPL in 1977, while the "Voyager 1" and "2" missions were still being prepared for launch. Early plans called for a launch on Space Shuttle "Columbia" on what was then codenamed STS-23 in January 1982, but delays in the development of the Space Shuttle allowed more time for development of the probe. As the shuttle program got underway, "Galileo" was scheduled for launch in 1984, but this later slipped to 1985 and then to 1986. The mission was initially called the "Jupiter Orbiter Probe"; it was christened "Galileo" in 1978.
Once the spacecraft was complete, its launch was scheduled for STS-61-G on-board "Atlantis" in 1986. The Inertial Upper Stage booster was going to be used at first, but this changed to the Centaur booster, then back to IUS after "Challenger".
The Centaur-G liquid hydrogen-fueled booster stage allowed a direct trajectory to Jupiter. However, the mission was further delayed by the hiatus in launches that occurred after the Space Shuttle Challenger disaster. New safety protocols introduced as a result of the disaster prohibited the use of the Centaur-G stage on the Shuttle, forcing "Galileo" to use a lower-powered Inertial Upper Stage solid-fuel booster. The mission was re-profiled in 1987 to use several gravitational slingshots, referred to as the "VEEGA" or Venus Earth Earth Gravity Assist maneuvers, to provide the additional velocity required to reach its destination.
It was finally launched on October 18, 1989, by the Space Shuttle "Atlantis" on the STS-34 mission.
Venus was flown by at 05:58:48 UT on February 10, 1990 at a range of 16,106 km. Having gained 8,030 km per hour in speed, the spacecraft flew by Earth twice, the first time at a range of 960 km at 20:34:34 UT on 8 December 1990 before approaching the minor planet 951 Gaspra to a distance of 1,604 km at 22:37 UT on 29 October 1991. "Galileo" then performed a second flyby of Earth at 303.1 km at 15:09:25 UT on 8 December 1992, adding 3.7 km per second to its cumulative speed. "Galileo" performed close observation of a second asteroid, 243 Ida, at 16:51:59 UT on 28 August 1993 at a range of 2,410 km. The spacecraft discovered Ida has a moon Dactyl, the first discovery of a natural satellite orbiting an asteroid. In 1994, "Galileo" was perfectly positioned to watch the fragments of the comet Shoemaker-Levy 9 crash into Jupiter, whereas terrestrial telescopes had to wait to see the impact sites as they rotated into view. After releasing its atmospheric probe on 13 July 1995, the "Galileo" orbiter became the first man-made satellite of Jupiter at 00:27 UT on 8 December 1995 when it fired its main engine to enter a 198-day parking orbit.
"Galileo‍‍ '​‍s" prime mission was a two-year study of the Jovian system. The spacecraft traveled around Jupiter in elongated ellipses, each orbit lasting about two months. The differing distances from Jupiter afforded by these orbits allowed "Galileo" to sample different parts of the planet's extensive magnetosphere. The orbits were designed for close-up flybys of Jupiter's largest moons. Once the prime mission concluded, an extended mission started on December 7, 1997; the spacecraft made a number of flybys of Europa and Io. The closest approach was 180 km on October 15, 2001. The radiation environment near Io was very unhealthy for "Galileo"'s systems, and so these flybys were saved for the extended mission when loss of the spacecraft would be more acceptable.
"Galileo‍‍ '​‍s" cameras were deactivated on January 17, 2002, after they had sustained irreparable radiation damage. NASA engineers were able to recover the damaged tape recorder electronics, and "Galileo" continued to return scientific data until it was deorbited in 2003, performing one last scientific experiment —a measurement of the moon Amalthea's mass as the spacecraft swung by it.
Spacecraft.
The Jet Propulsion Laboratory built the "Galileo" spacecraft and managed the "Galileo" mission for NASA. Germany supplied the propulsion module. NASA's Ames Research Center managed the probe, which was built by Hughes Aircraft Company.
At launch, the orbiter and probe together had a mass of 2,564 kg and stood seven metres tall. One section of the spacecraft rotated at 3 rpm, keeping "Galileo" stable and holding six instruments that gathered data from many different directions, including the fields and particles instruments. The other section of the spacecraft was an antenna, and data were periodically transmitted to it. Back on the ground, the mission operations team used software containing 650,000 lines of programming code in the orbit sequence design process; 1,615,000 lines in the telemetry interpretation; and 550,000 lines of code in navigation.
Command and Data Handling (CDH).
The CDH subsystem was actively redundant, with two parallel data system buses running at all times. Each data system bus (a.k.a. string) was composed of the same functional elements, consisting of multiplexers (MUX), high-level modules (HLM), low-level modules (LLM), power converters (PC), bulk memory (BUM), data management subsystem bulk memory (DBUM), timing chains (TC), phase locked loops (PLL), Golay coders (GC), hardware command decoders (HCD) and critical controllers (CRC).
The CDH subsystem was responsible for maintaining the following functions:
The spacecraft was controlled by six RCA 1802 COSMAC microprocessor CPUs: four on the spun side and two on the despun side. Each CPU was clocked at about 1.6 MHz, and fabricated on sapphire (silicon on sapphire), which is a radiation-and static-hardened material ideal for spacecraft operation. This microprocessor was the first low-power CMOS processor chip, quite on a par with the 8-bit 6502 that was being built into the Apple II desktop computer at that time.
The Galileo Attitude and Articulation Control System (AACSE) was controlled by two Itek Advanced Technology Airborne Computers (ATAC), built using radiation-hardened 2901s.
The AACSE could be reprogrammed in flight by sending the new program through the Command and Data Subsystem.
"Galileo"'s attitude control system software was written in the HAL/S programming language, also used in the Space Shuttle program.
Memory capacity provided by each BUM was 16K of RAM, while the DBUMs each provided 8K of RAM. There were two BUMs and two DBUMs in the CDH subsystem and they all resided on the spun side of the spacecraft. The BUMs and DBUMs provided storage for sequences and contain various buffers for telemetry data and interbus communication.
Every HLM and LLM was built up around a single 1802 microprocessor and 32K of RAM (for HLMs) or 16K of RAM (for LLMs). Two HLMs and two LLMs resided on the spun side while two LLMs were on the despun side.
Thus, total memory capacity available to the CDH subsystem was 176K of RAM: 144K allocated to the spun side and 32K to the despun side.
Each HLM was responsible for the following functions:
Each LLM was responsible for the following functions:
The HCD receives command data from the modulation/demodulation subsystem, decodes these data and transfers them to the HLMs and CRCs.
The CRC controls the configuration of CDH subsystem elements. It also controls access to the two data system buses by other spacecraft subsystems. In addition, the CRC supplies signals to enable certain critical events (e.g. probe separation).
The GCs provide Golay encoding of data via hardware.
The TCs and PLLs establish timing within the CDH subsystem.
Propulsion.
The Propulsion Subsystem consisted of a 400 N main engine and twelve 10 N thrusters, together with propellant, storage and pressurizing tanks and associated plumbing. The 10 N thrusters were mounted in groups of six on two 2-meter booms. The fuel for the system was 925 kg of monomethylhydrazine and nitrogen tetroxide. Two separate tanks held another 7 kg of helium pressurant. The Propulsion Subsystem was developed and built by Daimler Benz Aero Space AG (DASA) (formerly Messerschmitt–Bölkow–Blohm (MBB)) and provided by Germany, the major international partner in Project "Galileo".
Electrical power.
At the time, Solar panels were not practical at Jupiter's distance from the Sun (it would have needed a minimum of 65 m2 of solar panels). Chemical batteries would likewise be prohibitively massive due to the technological limitations. The solution was two radioisotope thermoelectric generators (RTGs) which powered the spacecraft through the radioactive decay of plutonium-238. The heat emitted by this decay was converted into electricity through the solid-state Seebeck effect. This provided a reliable and long-lasting source of electricity unaffected by the cold environment and high-radiation fields in the Jovian system.
Each GPHS-RTG, mounted on a 5-meter long boom, carried 7.8 kg of 238Pu. Each RTG contained 18 separate heat source modules, and each module encased four pellets of plutonium dioxide, a ceramic material resistant to fracturing. The modules were designed to survive a range of hypothetical accidents: launch vehicle explosion or fire, re-entry into the atmosphere followed by land or water impact, and post-impact situations. An outer covering of graphite provided protection against the structural, thermal, and eroding environments of a potential re-entry. Additional graphite components provided impact protection, while iridium cladding of the fuel cells provided post-impact containment. The RTGs produced about 570 watts at launch. The power output initially decreased at the rate of 0.6 watts per month and was 493 watts when "Galileo" arrived at Jupiter.
As the launch of "Galileo" neared, anti-nuclear groups, concerned over what they perceived as an unacceptable risk to the public's safety from "Galileo's" RTGs, sought a court injunction prohibiting "Galileo's" launch. RTGs had been used for years in planetary exploration without mishap: the Lincoln Experimental Satellites 8/9, launched by the U.S. Department of Defense, had 7% more plutonium on board than "Galileo", and the two Voyager spacecraft each carried 80% as much plutonium as "Galileo" did. However, activists remembered the messy crash of the Soviet Union's nuclear-powered Cosmos 954 satellite in Canada in 1978, and though was not nuclear-powered, the 1986 Challenger accident raised public awareness about spacecraft failures. In addition, no RTGs had ever done a non-orbital swing past the Earth at close range and high speed, as "Galileo's" Venus-Earth-Earth Gravity Assist trajectory required it to do. This created a novel mission failure modality that might plausibly have entailed total dispersal of "Galileo's" plutonium in the Earth's atmosphere. Scientist Carl Sagan, for example, a strong supporter of the "Galileo" mission, said in 1989 that "there is nothing absurd about either side of this argument." 
After "Challenger", a study considered additional shielding but rejected it, in part because such a design significantly increased the overall risk of mission failure and only shifted the other risks around (for example, if a failure on orbit had occurred, additional shielding would have significantly increased the consequences of a ground impact).
Instrumentation overview.
Scientific instruments to measure fields and particles were mounted on the spinning section of the spacecraft, together with the main antenna, power supply, the propulsion module and most of "Galileo"'s computers and control electronics. The sixteen instruments, weighing 118 kg altogether, included magnetometer sensors mounted on an 11 m boom to minimize interference from the spacecraft; a plasma instrument for detecting low-energy charged particles and a plasma-wave detector to study waves generated by the particles; a high-energy particle detector; and a detector of cosmic and Jovian dust. It also carried the Heavy Ion Counter, an engineering experiment added to assess the potentially hazardous charged particle environments the spacecraft flew through, and an added Extreme Ultraviolet detector associated with the UV spectrometer on the scan platform.
The despun section's instruments included the camera system; the near infrared mapping spectrometer to make multi-spectral images for atmospheric and moon surface chemical analysis; the ultraviolet spectrometer to study gases; and the photo-polarimeter radiometer to measure radiant and reflected energy. The camera system was designed to obtain images of Jupiter's satellites at resolutions from 20 to 1,000 times better than "Voyagers best, because "Galileo" flew closer to the planet and its inner moons, and because the more modern CCD sensor in "Galileos camera was more sensitive and had a broader color detection band than the vidicons of "Voyager".
Instrumentation details.
The following information was taken directly from NASA's "Galileo" legacy site.
Despun section.
Solid State Imager (SSI).
The SSI was an 800-by-800-pixel solid state camera consisting of an array of silicon sensors called a "charge coupled device" (CCD). Galileo was one of the first spacecraft to be equipped with a CCD camera. The optical portion of the camera was built as a Cassegrain telescope. Light was collected by the primary mirror and directed to a smaller secondary mirror that channeled it through a hole in the center of the primary mirror and onto the CCD. The CCD sensor was shielded from radiation, a particular problem within the harsh Jovian magnetosphere. The shielding was accomplished by means of a 10 mm thick layer of tantalum surrounding the CCD except where the light enters the system. An eight-position filter wheel was used to obtain images at specific wavelengths. The images were then combined electronically on Earth to produce color images. The spectral response of the SSI ranged from about 0.4 to 1.1 micrometres. The SSI weighed 29.7 kilograms and consumed, on average, 15 watts of power.
Near-Infrared Mapping Spectrometer (NIMS).
The NIMS instrument was sensitive to 0.7-to-5.2-micrometre wavelength IR light, overlapping the wavelength range of the SSI. The telescope associated with NIMS was all reflective (using only mirrors and no lenses) with an aperture of 229 mm. The spectrometer of NIMS used a grating to disperse the light collected by the telescope. The dispersed spectrum of light was focused on detectors of indium antimonide and silicon. The NIMS weighed 18 kilograms and used 12 watts of power on average.
Ultraviolet Spectrometer / Extreme Ultraviolet Spectrometer (UVS/EUV).
The Cassegrain telescope of the UVS had a 250 mm aperture and collected light from the observation target. Both the UVS and EUV instruments used a ruled grating to disperse this light for spectral analysis. This light then passed through an exit slit into photomultiplier tubes that produced pulses or "sprays" of electrons. These electron pulses were counted, and these count numbers constituted the data that were sent to Earth. The UVS was mounted on "Galileo"'s scan platform and could be pointed to an object in inertial space. The EUV was mounted on the spun section. As "Galileo" rotated, EUV observed a narrow ribbon of space perpendicular to the spin axis. The two instruments combined weighed about 9.7 kilograms and used 5.9 watts of power.
Photopolarimeter-Radiometer (PPR).
The PPR had seven radiometry bands. One of these used no filters and observed all incoming radiation, both solar and thermal. Another band allowed only solar radiation through. The difference between the solar-plus-thermal and the solar-only channels gave the total thermal radiation emitted. The PPR also measured in five broadband channels that spanned the spectral range from 17 to 110 micrometres. The radiometer provided data on the temperatures of Jupiter's atmosphere and satellites. The design of the instrument was based on that of an instrument flown on the Pioneer Venus spacecraft. A 100 mm aperture reflecting telescope collected light and directed it to a series of filters, and, from there, measurements were performed by the detectors of the PPR. The PPR weighed 5.0 kilograms and consumed about 5 watts of power.
Spun section.
Dust Detector Subsystem (DDS).
The Dust Detector Subsystem (DDS) was used to measure the mass, electric charge, and velocity of incoming particles. The masses of dust particles that the DDS could detect go from 10−16 to 10−7 grams. The speed of these small particles could be measured over the range of 1 to 70 kilometers per second. The instrument could measure impact rates from 1 particle per 115 days (10 megaseconds) to 100 particles per second. Such data was used to help determine dust origin and dynamics within the magnetosphere. The DDS weighed 4.2 kilograms and used an average of 5.4 watts of power.
Energetic Particles Detector (EPD).
The Energetic Particles Detector (EPD) was designed to measure the numbers and energies of ions and electrons whose energies exceeded about 20 keV (3.2 fJ). The EPD could also measure the direction of travel of such particles and, in the case of ions, could determine their composition (whether the ion is oxygen or sulfur, for example). The EPD used silicon solid state detectors and a time-of-flight detector system to measure changes in the energetic particle population at Jupiter as a function of position and time. These measurements helped determine how the particles got their energy and how they were transported through Jupiter's magnetosphere. The EPD weighed 10.5 kilograms and used 10.1 watts of power on average.
Heavy Ion Counter (HIC).
The HIC was in effect a repackaged and updated version of some parts of the flight spare of the Voyager Cosmic Ray System. The HIC detected heavy ions using stacks of single crystal silicon wafers. The HIC could measure heavy ions with energies as low as 6 MeV (1 pJ) and as high as 200 MeV (32 pJ) per nucleon. This range included all atomic substances between carbon and nickel. The HIC and the EUV shared a communications link and, therefore, had to share observing time. The HIC weighed 8 kilograms and used an average of 2.8 watts of power.
Magnetometer (MAG).
The magnetometer (MAG) used two sets of three sensors. The three sensors allowed the three orthogonal components of the magnetic field section to be measured. One set was located at the end of the magnetometer boom and, in that position, was about 11 m from the spin axis of the spacecraft. The second set, designed to detect stronger fields, was 6.7 m from the spin axis. The boom was used to remove the MAG from the immediate vicinity of "Galileo" to minimize magnetic effects from the spacecraft. However, not all these effects could be eliminated by distancing the instrument. The rotation of the spacecraft was used to separate natural magnetic fields from engineering-induced fields. Another source of potential error in measurement came from the bending and twisting of the long magnetometer boom. To account for these motions, a calibration coil was mounted rigidly on the spacecraft to generate a reference magnetic field during calibrations. The magnetic field at the surface of the Earth has a strength of about 50,000 nT. At Jupiter, the outboard (11 m) set of sensors could measure magnetic field strengths in the range from ±32 to ±512 nT, while the inboard (6.7 m) set was active in the range from ±512 to ±16,384 nT. The MAG experiment weighed 7 kilograms and used 3.9 watts of power.
Plasma Subsystem (PLS).
The PLS used seven fields of view to collect charged particles for energy and mass analysis. These fields of view covered most angles from 0 to 180 degrees, fanning out from the spin axis. The rotation of the spacecraft carried each field of view through a full circle. The PLS measured particles in the energy range from 0.9 eV to 52 keV (0.1 aJ to 8.3 fJ). The PLS weighed 13.2 kilograms and used an average of 10.7 watts of power.
Plasma Wave Subsystem (PWS).
An electric dipole antenna was used to study the electric fields of plasmas, while two search coil magnetic antennas studied the magnetic fields. The electric dipole antenna was mounted at the tip of the magnetometer boom. The search coil magnetic antennas were mounted on the high-gain antenna feed. Nearly simultaneous measurements of the electric and magnetic field spectrum allowed electrostatic waves to be distinguished from electromagnetic waves. The PWS weighed 7.1 kilograms and used an average of 9.8 watts.
Jupiter science.
After arriving on December 7, 1995 and completing 35 orbits around Jupiter throughout a nearly eight-year mission, the "Galileo" Orbiter was destroyed during a controlled impact with Jupiter on September 21, 2003. During that intervening time, "Galileo" forever changed the way scientists saw Jupiter and provided a wealth of information on the moons orbiting the planet which will be studied for years to come. Culled from NASA's press kit, the top orbiter science results were:
Other science conducted by "Galileo".
Remote detection of life on Earth.
The astronomer Carl Sagan, pondering the question of whether life on Earth could be easily detected from space, devised a set of experiments in the late 1980s using "Galileo' "s remote sensing instruments during the mission's first Earth flyby in December 1990. After data acquisition and processing, Sagan et al. published a paper in "Nature" in 1993 detailing the results of the experiment. "Galileo" had indeed found what are now referred to as the "Sagan criteria for life". These included strong absorption of light at the red end of the visible spectrum (especially over continents) which was caused by absorption by chlorophyll in photosynthesizing plants, absorption bands of molecular oxygen which is also a result of plant activity, infrared absorption bands caused by the ~1 micromole per mole (µmol/mol) of methane in Earth's atmosphere (a gas which must be replenished by either volcanic or biological activity), and modulated narrowband radio wave transmissions uncharacteristic of any known natural source. "Galileo' "s experiments were thus the first ever controls in the newborn science of astrobiological remote sensing.
The "Galileo" optical experiment.
In December 1992, during "Galileo' "s second gravity-assist planetary flyby of Earth, another groundbreaking experiment was performed. Optical communications in space was assesed by detecting light pulses from powerful lasers with "Galileo' "s CCD. The experiment, dubbed Galileo OPtical EXperiment or GOPEX, used two separate sites to beam laser pulses to the spacecraft, one at Table Mountain Observatory in California and the other at the Starfire Optical Range in New Mexico. The Table Mountain site used a frequency doubled Neodymium-Yttrium-Aluminium Garnet (Nd:YAG) laser operating at 532 nm with a repetition rate of ~15 to 30 Hz and a pulse power (FWHM) in the tens of megawatts range, which was coupled to a 0.6 meter Cassegrain telescope for transmission to "Galileo"; the Starfire range site used a similar setup with a larger transmitting telescope (1.5 m). Long exposure (~0.1 to 0.8 s) images using "Galileo"'s 560 nm centered green filter produced images of Earth clearly showing the laser pulses even at distances of up to 6,000,000 km. Adverse weather conditions, restrictions placed on laser transmissions by the U.S. Space Defense Operations Center (SPADOC) and a pointing error caused by the scan platform acceleration on the spacecraft being slower than expected (which prevented laser detection on all frames with less than 400 ms exposure times) all contributed to the reduction of the number of successful detections of the laser transmission to 48 of the total 159 frames taken. Nonetheless, the experiment was considered a resounding success and the data acquired will likely be used in the future to design laser "downlinks" which will send large volumes of data very quickly, from spacecraft to Earth. The scheme is already being studied (as of 2004) for a data link to a future Mars orbiting spacecraft.
Star scanner.
"Galileo' "s star scanner was a small optical telescope that provided an absolute attitude reference. It also made several scientific discoveries serendipitously. In the prime mission, it was found that the star scanner was able to detect high-energy particles as a noise signal. This data was eventually calibrated to show the particles were predominantly >2 MeV electrons that were trapped in the Jovian magnetic belts.
A second discovery occurred in 2000. The star scanner was observing a set of stars which included the second magnitude star Delta Velorum. At one point, this star dimmed for 8 hours below the star scanner's detection threshold. Subsequent analysis of "Galileo" data and work by amateur and professional astronomers showed that Delta Velorum is the brightest known eclipsing binary, brighter at maximum than even Algol. It has a primary period of 45 days and the dimming is just visible with the naked eye.
A final discovery occurred during the last two orbits of the mission. When the spacecraft passed the orbit of Jupiter's moon Amalthea, the star scanner detected unexpected flashes of light that were reflections from moonlets. None of the individual moonlets were reliably sighted twice, hence no orbits were determined and the moonlets did not meet the International Astronomical Union requirements to receive designations. It is believed that these moonlets most likely are debris ejected from Amalthea and form a tenuous, and perhaps temporary, ring.
Asteroid encounters.
First asteroid encounter: 951 Gaspra.
On October 29, 1991, two months after entering the asteroid belt, "Galileo" performed the first asteroid encounter by a human spacecraft, passing approximately 1,600 km from 951 Gaspra at a relative speed of about 8 kilometers per second (18,000 mph). Several pictures of Gaspra were taken, along with measurements using the NIMS instrument to indicate composition and physical properties. The last two images were relayed back to Earth in November 1991 and June 1992. The imagery revealed a cratered and very irregular body, measuring about 19 by. The remainder of data taken, including low-resolution images of more of the surface, were transmitted in late November 1992.
Second asteroid encounter: 243 Ida and Dactyl.
On August 28, 1993, "Galileo" flew within 2,400 km of the asteroid 243 Ida. The probe discovered that Ida had a small moon, dubbed Dactyl, measuring around 1.4 km in diameter; this was the first asteroid moon discovered. Measurements using "Galileo"'s solid state imager, magnetometer and NIMS instrument were taken. From subsequent analysis of this data, Dactyl appears to be an SII subtype S type asteroid, and is spectrally different from 243 Ida. It is hypothesized that Dactyl may have been produced by partial melting within a Koronis parent body, while the 243 Ida region escaped such igneous processing.
Mission challenges.
Some of the mission challenges that had to be overcome included intense radiation at Jupiter and hardware wear-and-tear, as well as dealing with the unexpected conditions of discovery.
Radiation-related anomalies.
Jupiter's uniquely harsh radiation environment caused over 20 anomalies over the course of "Galileo"'s mission, in addition to the incidents expanded upon above. Despite exceeding its radiation design limit by at least a factor of three, the spacecraft survived all these anomalies – work-arounds were found eventually for all of these problems, and "Galileo" was never rendered entirely non-functional by Jupiter's radiation. The radiation limits for "Galileo's" computers were based off data returned from Pioneers 10 and 11, since much of the design work was underway before the two Voyagers arrived at Jupiter in 1979.
A typical effect of the radiation was that several of the science instruments suffered increased noise while within about 700,000 km of Jupiter. The SSI camera began producing totally white images when the spacecraft was hit by the exceptional 'Bastille Day' coronal mass ejection in 2000, and did so again on subsequent close approaches to Jupiter. The quartz crystal used as the frequency reference for the radio suffered permanent frequency shifts with each Jupiter approach. A spin detector failed, and the spacecraft gyro output was biased by the radiation environment.
The most severe effect of the radiation were current leakages somewhere in spacecraft's power bus, most likely across brushes at a spin bearing connecting rotor and stator sections of the orbiter. These current leakages triggered a reset of the onboard computer and caused it to go into safe mode. The resets occurred when the spacecraft was either close to Jupiter or in the region of space magnetically downstream of the Earth. A change to the software was made in April 1999 that allowed the onboard computer to detect these resets and autonomously recover, so as to avoid safe mode.
Main antenna problem.
"Galileo' "s high-gain antenna failed to fully deploy after its first flyby of Earth.
The antenna had 18 ribs, like an umbrella and when the driver motor started and put pressure on the ribs, they were supposed to pop out of the cup their tips were held in. Only 15 popped out, leaving the antenna looking like a lop-sided, half-open umbrella. Investigators concluded that during the 4.5 years that "Galileo" spent in storage after the 1986 "Challenger" disaster, the lubricants between the tips of the ribs and the cup evaporated and no one thought to renew them.
Engineers tried thermal-cycling the antenna, rotating the spacecraft up to its maximum spin rate of 10.5 rpm, and "hammering" the antenna deployment motor — turning it on and off repeatedly — over 13,000 times, but all attempts failed to open the high-gain antenna.
The associated problem mission managers faced was if one rib popped free, there would be increased pressure on the remaining two, and if one of them popped out the last would be under so much pressure it would never release.
The second part of the problem was due to Galileo's revised flight plan. The probe had never been intended to approach the Sun any closer than the orbit of Earth, but sending it to Venus would expose it to temperatures at least 50 degrees higher than at Earth distance. So the probe had to be protected from that extra heat, part of which involved adapting some of the computer functions. Forty-one drivers had been programmed into the computer, but with no room for any more, the mission planners had to decide which driver they could use in association with the heat protection. They chose the antenna motor reverse driver.
Even with the dry grease at the antenna rib tips, had the antenna motor been able to run backwards, as well as forwards, the ribs would have eventually popped out.
Fortunately, "Galileo" possessed an additional low-gain antenna that was capable of transmitting information back to Earth, although since it transmitted a signal isotropically, the low-gain antenna's bandwidth was significantly less than the high-gain antenna's would have been; the high-gain antenna was to have transmitted at 134 kilobits per second, whereas the low-gain antenna was only intended to transmit at about 8 to 16 bits per second. "Galileo' "s low-gain antenna transmitted with a power of about 15 to 20 watts, which, by the time it reached Earth, and had been collected by one of the large aperture (70 m) DSN antennas, had a total power of about −170 dBm or 10 zeptowatts (10 × 10−21 watts). Through the implementation of sophisticated technologies, the arraying of several Deep Space Network antennas and sensitivity upgrades to the receivers used to listen to "Galileo' "s signal, data throughput was increased to a maximum of 160 bits per second. By further using data compression, the effective data rate could be raised to 1,000 bits per second. The data collected on Jupiter and its moons was stored in the spacecraft's onboard tape recorder, and transmitted back to Earth during the long apoapsis portion of the probe's orbit using the low-gain antenna. At the same time, measurements were made of Jupiter's magnetosphere and transmitted back to Earth. The reduction in available bandwidth reduced the total amount of data transmitted throughout the mission, although 70% of "Galileo' "s science goals could still be met.
Tape recorder anomalies and remote repair.
The failure of "Galileo' "s high-gain antenna meant that data storage to the tape recorder for later compression and playback was absolutely crucial in order to obtain any substantial information from the flybys of Jupiter and its moons. In October 1995, "Galileo' "s four-track, 114-megabyte digital tape recorder, which was manufactured by Odetics Corporation, remained stuck in rewind mode for 15 hours before engineers learned what had happened and sent commands to shut it off. Though the recorder itself was still in working order, the malfunction possibly damaged a length of tape at the end of the reel. This section of tape was subsequently declared "off limits" to any future data recording, and was covered with 25 more turns of tape to secure the section and reduce any further stresses, which could tear it. Because it happened only weeks before "Galileo" entered orbit around Jupiter, the anomaly prompted engineers to sacrifice data acquisition of almost all of the Io and Europa observations during the orbit insertion phase, in order to focus solely on recording data sent from the Jupiter probe descent.
In November 2002, after the completion of the mission's only encounter with Jupiter's moon Amalthea, problems with playback of the tape recorder again plagued "Galileo". About 10 minutes after the closest approach of the Amalthea flyby, "Galileo" stopped collecting data, shut down all of its instruments, and went into safe mode, apparently as a result of exposure to Jupiter's intense radiation environment. Though most of the Amalthea data was already written to tape, it was found that the recorder refused to respond to commands telling it to play back data. After weeks of troubleshooting of an identical flight spare of the recorder on the ground, it was determined that the cause of the malfunction was a reduction of light output in three infrared Optek OP133 light emitting diodes located in the drive electronics of the recorder's motor encoder wheel. The GaAs LEDs had been particularly sensitive to proton-irradiation-induced atomic lattice displacement defects, which greatly decreased their effective light output and caused the drive motor's electronics to falsely believe the motor encoder wheel was incorrectly positioned. "Galileo' "s flight team then began a series of "annealing" sessions, where current was passed through the LEDs for hours at a time to heat them to a point where some of the crystalline lattice defects would be shifted back into place, thus increasing the LED's light output. After about 100 hours of annealing and playback cycles, the recorder was able to operate for up to an hour at a time. After many subsequent playback and cooling cycles, the complete transmission back to Earth of all recorded Amalthea flyby data was successful.
Probe parachute deployment.
The atmospheric probe deployed its parachute fifty-three seconds later than anticipated, resulting in a small loss of upper atmospheric readings. This was attributed to wiring problems with an accelerometer that determined when to begin the parachute deployment sequence.
End of mission and deorbit.
Years of Jupiter's intense radiation took its toll on the spacecraft's systems, and its fuel supply was running low in the early 2000s. Galileo had not been sterilized, so to prevent forward contamination of Jupiter's moons, a plan was formulated to send it directly into the planet. So "Galileo" was intentionally commanded to crash into Jupiter, which eliminated the possibility it would impact Jupiter's moons and seed them with bacteria.
In order to crash into Jupiter, "Galileo" flew by Amalthea on November 5, 2002, during its 34th orbit, allowing a measurement of the moon's mass as it passed within 163.0 km ± 11.7 km of its surface. On April 14, 2003, "Galileo" reached its greatest distance from Jupiter for the entire mission prior to orbital insertion, 26000000 km, before plunging back towards the gas giant for its final impact. At the completion of its 35th and final circuit around the Jovian system, "Galileo" impacted the gas giant in darkness just south of the equator on September 21, 2003, at 18:57 GMT. Its impact speed was approximately 173736 kph. The total mission cost was about US$1.4 billion.
Follow-on missions.
There was a spare Galileo spacecraft that the Europeans considered using for a mission to Saturn, however this was passed over in favor of a newer design. Even before Galileo concluded, NASA considered the Europa Orbiter, which was a mission to Jupiter's Moon Europa but it was canceled in 2002. The mission for an orbiter for Jupiter would be the Juno Jupiter Orbiter launched in 2011. (See also New Horizon's flyby en route to Pluto) The ESA also is planning to return to the Jovian system with the Jupiter Icy Moons Explorer (JUICE), which is designed to orbit Ganymede in the 2020s. There have been several other attempts at missions that were either dedicated to, or included the Jupiter system as part of their mission plan but did not make it out of the planning stages.
Galileo team.
Investigators and researchers came from the following institutions, and included famous scientists such as Carl Sagan and James Van Allen
Makers:
Crew of STS-34:

</doc>
<doc id="13135" url="http://en.wikipedia.org/wiki?curid=13135" title="Game show">
Game show

A game show is a type of radio, television, or internet programming genre in which contestants, television personalities or celebrities, sometimes as part of a team, play a game which involves answering questions or solving puzzles usually for money and/or prizes. Alternatively, a "gameshow" can be a demonstrative program about a game (while usually retaining the spirit of a rewards ceremony). In the former, contestants may be invited from a pool of public applicants. On some shows, contestants compete against other players or another team, while other shows involve contestants playing alone for a good outcome or a high score. Game shows often reward players with prizes such as cash, trips and goods and services provided by the show's sponsor prize suppliers, who in turn usually do so for the purposes of product placement.
History.
Television game shows descended from similar programs on radio. The very first television game show, "Spelling Bee", was broadcast in 1938. "Truth or Consequences" was the first game show to air on commercially licensed television. Its first episode aired in 1941 as an experimental broadcast.
Over the course of the 1950s, as television began to pervade the popular culture, game shows quickly became a fixture. Daytime game shows would be played for lower stakes to target stay-at-home housewives. Higher-stakes programs would air in prime time. During the late 1950s, high-stakes games such as "Twenty One" and "The $64,000 Question" began a rapid rise in popularity. However, the rise of quiz shows proved to be short-lived. In 1959, many of the higher stakes game shows were discovered to be rigged. Ratings declines led to most of the prime time games being canceled.
An early variant of the game show, the panel game, survived the quiz show scandals. On shows like "What's My Line?", "I've Got A Secret" and "To Tell The Truth", panels of celebrities would interview a guest in an effort to determine some fact about them; in others, celebrities would answer questions. Panel games had success in primetime until the late 1960s, when they were collectively dropped from television because of their perceived “low budget” nature. Panel games made a comeback in American daytime television (where the lower budgets were tolerated) in the 1970s through comedy-driven shows such as "Match Game" and "Hollywood Squares". In the UK, where commercial demographic pressures were not as prominent, panel shows were kept in primetime and have continued to thrive; they have transformed into showcases for the nation's top stand-up comedians on shows such as "Have I Got News For You", "Would I Lie to You?", "Mock The Week", "QI" and "8 out of 10 Cats", all of which put a heavy emphasis on comedy, leaving the points as mere formalities. The focus on quick-witted comedians has resulted in strong ratings, which, combined with low costs of production, have only spurred growth in the UK panel show phenomenon.
Game shows remained a fixture of US daytime television through the 1960s after the quiz show scandals. Lower-stakes games made a slight comeback in daytime in the early 1960s; examples include "Jeopardy!" which began in 1964 and the original version of "The Match Game" first aired in 1962. "Let's Make a Deal" began in 1963 and the 1960s also marked the debut of "Hollywood Squares", "Password", "The Dating Game" and "The Newlywed Game".
Though CBS gave up on daytime game shows in 1968, the other networks did not follow suit. Color television was introduced to the game show genre in the late 1960s on all three networks. The 1970s saw a renaissance of the game show as new games and massive upgrades to existing games made debuts on the major networks. "The New Price Is Right", an update of the 1950s-era game show "The Price Is Right", debuted in 1972 and marked CBS's return to the game show format in its effort to draw wealthier, suburban viewers. "The Match Game" became "Big Money" "Match Game 73", which proved popular enough to prompt a spin-off, "Family Feud", on ABC in 1976. "The $10,000 Pyramid" and its numerous higher-stakes derivatives also debuted in 1973, while the 1970s also saw the return of formerly disgraced producer and host Jack Barry, who debuted "The Joker's Wild" and a clean version of the previously rigged "Tic-Tac-Dough" in the 1970s. "Wheel of Fortune" debuted on NBC in 1975. The Prime Time Access Rule, which took effect in 1971, barred networks from broadcasting in the 7-8 p.m. time slot immediately preceding prime time, opening up time slots for syndicated programming. Most of the syndicated programs were "nighttime" adaptations of network daytime game shows; these game shows originally aired once a week, but by the late 1970s and early 1980s most of the games had transitioned to five days a week.
Game shows were the lowest priority of television networks, and were rotated out every thirteen weeks if unsuccessful. Most tapes were destroyed until the early 1980s. Over the course of the late 1980s and early 1990s as fewer new hits were produced, game shows lost their permanent place in the daytime lineup. ABC gave up on game shows in 1986. NBC lasted until 1991, but attempted to bring them back in 1993 before cancelling its game show block again in 1994. CBS phased out most of their game shows, except for "The Price Is Right", by 1993. To the benefit of the genre, the moves of "Wheel of Fortune" and a modernized revival of "Jeopardy!" to syndication in 1983 and 1984, respectively, was and remains highly successful; the two are, to this day, fixtures in the prime time "access period". 
Cable television also allowed for the debut of game shows such as "Supermarket Sweep" (Lifetime), "Trivial Pursuit" and "Family Challenge" (Family Channel), and "Double Dare" (Nickelodeon). It also opened up a previously underdeveloped market for game show reruns; general interest networks such as CBN Cable Network and USA Network had popular blocks for game show reruns from the mid-1980s to the mid-'90s before that niche was overtaken by Game Show Network in 1994.
After the popularity of game shows hit a nadir in the mid-1990s United States (at which point "The Price Is Right" was the only game show still on daytime network TV), the British game show "Who Wants to Be a Millionaire?" began distribution across the globe. Upon the show's American debut in 1999, it was an instant hit and became a regular part of ABC's prime time lineup until 2002. Several shorter-lived high-stakes games were attempted around the time of the millennium, both in the United States and the United Kingdom, such as "Winning Lines", "The Chair", "Greed" and "Shafted", leading to some dubbing this period as "The Million-Dollar Game Show Craze". These higher stakes contests also opened the door to reality television contests such as "Survivor" and "Big Brother", in which contestants win large sums of money for outlasting their peers in a given environment. Several game shows returned to daytime in syndication during this time as well, such as "Family Feud", "Hollywood Squares", and "Millionaire". 
The popularity of game shows in the United States was closely paralleled around the world. Reg Grundy Organisation, for instance, would buy the international rights for American game shows and reproduce them in other countries, especially in Grundy's native Australia. In the United Kingdom, game shows have had a more steady and permanent place in the television lineup and never lost popularity in the 1990s as they did in the United States, due in part to the fact that game shows were highly regulated by the Independent Broadcasting Authority in the 1980s and those restrictions were lifted in the 1990s, allowing for higher-stakes games to be played. Game shows have had an inconsistent place in Canadian television, with most homegrown game shows there being made for the French-speaking Quebecois market and the majority of English-language game shows in the country being rebroadcast from, or made with the express intent of export to, the United States; there have been exceptions to this (see, for instance, the long-running "Definition"). Unlike reality television franchises, international game show franchises generally only see Canadian adaptations in a series of specials, based heavily on the American versions but usually with a Canadian host to allow for Canadian content credits. The smaller markets and lower revenue opportunities for Canadian shows in general also affect game shows there, with Canadian games (especially Quebecois ones) often having very low budgets for prizes, unless the series is made for export. Canadian contestants are generally allowed to participate on American game shows, and there have been at least three Canadian game show hosts; Monty Hall, Jim Perry and Alex Trebek; that have gone on to long careers hosting American series.
In the US, CBS is currently the only major network airing daily national game shows. It still airs "The Price Is Right" and, as of 2009, is also airing a revival of "Let's Make a Deal". "Deal" airs on weekdays at a time chosen by each CBS affiliate, while "Price" airs weekdays at 10am or 11am in most markets. Although ABC does not air any national game shows, its syndication wing Disney-ABC Domestic Television distributes "Who Wants to Be a Millionaire?", and many of their local affiliates air it in syndication.
Of US radio stations, the oldest, continually aired quiz show was Simply Trivia (started in 1972) on WYSO until the station cancelled it in 2014.
Prizes.
Many of the prizes awarded on game shows are provided through product placement; although in some cases, they are provided by private organizations or purchased at either the full price or at a discount by the show. There is the widespread use of "promotional consideration", in which a game show receives a subsidy from an advertiser in return for awarding that manufacturer's product as a prize or consolation prize. Some products supplied by manufacturers may not be intended to be awarded at all, and are instead just used as part of the gameplay (such as the low-priced items used in several Pricing Games of "The Price Is Right").
For high-stakes games, a network may purchase prize indemnity insurance to avoid paying the cost of a rare but expensive prize out of pocket. If said prize is won too often, the insurance company may refuse to insure a show (this was a factor in the discontinuation of "The Price Is Right $1,000,000 Spectacular" series of prime-time specials; three contestants had won the top prize in a five-episode span after fifteen episodes without a winner, due in large part to a change in the rules, and the insurance companies had made it extremely difficult to get further insurance for the remaining episodes). A network or syndicator may also opt to distribute large cash prizes in the form of an annuity, spreading the cost of the prize out over several years or decades.
From about 1960 through the rest of the 20th century, American networks placed restrictions on the amount of money that could be given away on a game show, in an effort to avoid a repeat of the scandals of the 1950s. This usually took the form of an earnings cap that forced a player to retire once they won a certain amount of money or a limit of how many episodes (usually five) a player could appear on a show. The introduction of syndicated games, particularly in the 1980s, eventually allowed for more valuable prizes and extended runs on a particular show. Likewise, British television was under strict regulations, even more strict than those on the American side of the Atlantic Ocean, on prizes until the 1990s; these limits seriously restricted the value of prizes that could be given and did not allow games of chance to have an influence on the outcome of the game (thus the British version of "The Price Is Right" at first did not include the American version's "Showcase Showdown," in which contestants spun a large wheel to determine who would advance to the Showcase bonus round). The lifting of these restrictions in the 1990s was a major factor in the explosion of high-stakes game shows in the later part of that decade in both the U.S. and Britain (and, subsequently, around the world).
Bonus round.
A bonus round (also known as a bonus game or an end game) usually follows a main game as a bonus to the winner of that game. In the bonus round, the stakes are higher and the game is considered to be tougher. 
The game play of a bonus round usually varies from the standard game play of the front game, although there are often borrowed or related elements of the main game in the bonus round in order to ensure the entire show has a unified premise. Though some end games are referred to as "bonus rounds", many are not specifically referred to as such in games, though they fit the same general role.
There is no one formula for the format of a bonus round. There are differences in almost every bonus round, though there are many recurring elements from show to show. The bonus round is often played for the show's top prize.
Until the 1960s, most game shows did not offer a bonus round. In traditional two-player formats, the winner — if a game show's rules provided for this — became the champion and simply played a new challenger either on the next show or after the commercial break.
One of the earliest forms of bonus rounds was the Jackpot Round of the original series "Beat the Clock". After two rounds of performing stunts, the wife of the contestant couple would perform at a jackpot board for a prize. The contestant was shown a famous quotation or common phrase, and the words were scrambled. To win the announced bonus, the contestant had to unscramble the words within 20 seconds. The contestant received a consolation gift worth over $200 if she was unsuccessful.
Another early bonus round ended each episode of "You Bet Your Life" with the team who won the most money answering one final question for a jackpot which started at $1,000 and increased $500 each week until won.
Another early example was the Lightning Round on the word game "Password", starting in 1961. The contestant who won the front game played a quick-fire series of passwords within 60 seconds, netting $50 per correctly guessed word, for a maximum bonus prize of $250.
The bonus round came about after game show producer Mark Goodson was first presented "Password", contending that it was not enough to merely guess passwords during the show. "We needed something more, and that's how the Lightning Round was invited," said Howard Felsher, who produced "Password" and "Family Feud". "From that point on every game show had to have an end round. You'd bring a show to a network and they'd say, 'What's the endgame?' as if they had thought of it themselves."
The end game of "Match Game" served as the impetus for a completely new game show. The first part of "Match Game"'s bonus round, called Audience Match, asked contestants to guess how a studio audience responded to a question. In 1975, Goodson decided that this line of questioning would make a good game show of its own, and the concept eventually became "Family Feud".

</doc>
<doc id="13140" url="http://en.wikipedia.org/wiki?curid=13140" title="George Frederick, Margrave of Brandenburg-Ansbach">
George Frederick, Margrave of Brandenburg-Ansbach

George Frederick of Brandenburg-Ansbach (German: "Georg Friedrich der Ältere"; 5 April 1539 in Ansbach – 25 April 1603) was Margrave of Ansbach and Bayreuth, as well as Regent of Prussia. He was the son of George, Margrave of Brandenburg-Ansbach and a member of the House of Hohenzollern. He married firstly, in 1559, Elisabeth of Brandenburg-Küstrin (29 August 1540 – 8 March 1578). He married secondly, in 1579, Sophie of Brunswick-Lüneburg (30 October 1563 – 1639), daughter of William of Brunswick-Lüneburg and Dorothea of Denmark.
George Frederick reigned in his native Ansbach, Franconia and Jägerndorf, Upper Silesia since 1556 and, after the death of his cousin Albert Alcibiades in 1557, also in Kulmbach. He took over the administration of the Duchy of Prussia in 1577, when the then-reigning Duke Albert Frederick became ill.
He was the last of the older Franconia line of the House of Hohenzollern. Upon his death Ansbach and Kulmbach were inherited by younger princes of the Brandenburg line according to the House Treaty of Gera of 1598.
George Frederick rebuilt the palace and fortress Plassenburg, which was destroyed after the second margravian war (1552–1554), as one of the most impressive residences of the renaissance in the German empire. He also built the fortress Wülzburg and the old palace in Bayreuth.
During his reign between 1557 and 1603 in the Franconian territories of the Hohenzollern (Brandenburg-Ansbach and Brandenburg-Kulmbach) he kept peace, rebuilt cities and Castles, founded several schools and a University.

</doc>
<doc id="13158" url="http://en.wikipedia.org/wiki?curid=13158" title="Gilles Apap">
Gilles Apap

Gilles Apap (born 21 May 1963) is a French classical violinist. Born in Bougie, Algeria, he was raised in Nice, France. In 1985 he won first prize in the contemporary music category at the Yehudi Menuhin Competition. He served as concertmaster with the Santa Barbara Symphony in California for 10 years, but has since focused on his career as a soloist with orchestras around the world.
A virtuosic violinist, Gilles is well known for his love of, and respect for, traditional music from Eastern Europe to America and for his ability to convey an authentic sound to his audience, whether the music be gypsy, Irish, swing, or bluegrass.
He recorded three CDs in the 1990s with Sony Classical, then formed his own company, , with the help of supportive friends. Since 1999, Apapaziz has recorded eight Gilles Apap CDs, the latest, "Gypsy Tunes...California Style, that Is", with the Transylvanian Mountain Boys.

</doc>
<doc id="13162" url="http://en.wikipedia.org/wiki?curid=13162" title="Gelatin dessert">
Gelatin dessert

Gelatin desserts are desserts made with sweetened and flavored gelatin. They can be made by combining plain gelatin with other ingredients or by using a premixed blend of gelatin with additives. Fully prepared gelatin desserts are sold in a variety of forms, ranging from large decorative shapes to individual serving cups.
Brands.
Popular brands of premixed gelatin include:
History.
Before gelatin became widely available as a commercial product, the most typical gelatin dessert was "calf's foot jelly". As the name indicates, this was made by extracting and purifying gelatin from the foot of a calf. This gelatin was then mixed with fruit juice and sugar.
Preparation.
To make a gelatin dessert, gelatin is dissolved in hot liquid with the desired flavors and other additives. These latter ingredients usually include sugar, fruit juice, or sugar substitutes; they may be added and varied during preparation, or pre-mixed with the gelatin in a commercial product which merely requires the addition of hot water.
In addition to sweeteners, the prepared commercial blends generally contain flavoring agents and other additives, such as adipic acid, fumaric acid, sodium citrate, and artificial flavorings and food colors. Because the collagen is processed extensively, the final product is not categorized as a meat or animal product by the US federal government.
Prepared commercial blends may be sold as a powder or as a concentrated gelatinous block, divided into small squares. Either type is mixed with sufficient hot water to completely dissolve it, and then mixed with enough cold water to make the volume of liquid specified on the packet.
The solubility of powdered gelatin can be enhanced by sprinkling it into the liquid several minutes before heating, "blooming" the individual granules. The fully dissolved mixture is then refrigerated, slowly forming a colloidal gel as it cools.
Gelatin desserts may be enhanced in many ways, such as using decorative molds, creating multicolored layers by adding a new layer of slightly cooled liquid over the previously-solidified one, or suspending non-soluble edible elements such as marshmallows or fruit. Some types of fresh fruit and their unprocessed juices are incompatible with gelatin desserts; see the Chemistry section below.
When fully chilled, the most common ratios of gelatin to liquid (as instructed on commercial packaging) usually result in a custard-like texture which can retain detailed shapes when cold but melts back to a viscous liquid when warm. A recipe calling for the addition of additional gelatin to regular jelly gives a rubbery product that can be cut into shapes with cookie cutters and eaten with fingers (called "Knox Blox" by the Knox company, makers of unflavored gelatin). Higher gelatin ratios can be used to increase the stability of the gel, culminating in gummy candies which remain rubbery solids at room temperature (see Bloom (test)).
Gelatin shots.
A gelatin shot (usually called a Jell-O shot in North America and vodka jelly or jelly shot in the UK and Australia) is a shooter in which liquor, usually vodka, rum, tequila, or neutral grain spirit replaces some of the water or fruit juice that is used to congeal the gel.
The American satirist and mathematician Tom Lehrer has been rumored to have been the first to invent the gelatin shot in the 1950s while working for the National Security Agency, where he developed vodka gelatin as a way to circumvent a restriction of alcoholic beverages on base, but the claim that he was first is untrue. The earliest published recipe dates from 1862, found in "How to Mix Drinks, or The Bon Vivant's Companion" by Jerry Thomas: the recipe calls for gelatin, cognac, rum, and lemon juice.
Gelatin substitutes.
Other culinary gelling agents can be used instead of animal-derived gelatin. These plant-derived substances are more similar to pectin and other gelling plant carbohydrates than to gelatin proteins; their physical properties are slightly different, creating different constraints for the preparation and storage conditions. These other gelling agents may also be preferred for certain traditional cuisines or dietary restrictions.
Agar, a product made from red algae, is the traditional gelling agent in many Asian desserts. Agar is a popular gelatin substitute in quick jelly powder mix and prepared dessert gels that can be stored at room temperature. Compared to gelatin, agar preparations require a higher dissolving temperature, but the resulting gels congeal more quickly and remain solid at higher temperatures, 104 F, as opposed to 59 F for gelatin. Vegans and vegetarians can use agar to replace animal-derived gelatin.
Carrageenan is also derived from seaweed, and lacks agar's occasionally unpleasant smell during cooking. It sets more firmly than agar and is often used in kosher and halal cooking.
Konjac is a gelling agent used in many Asian foods, including the popular konnyaku fruit jelly candies.
Chemistry.
Gelatin consists of partially hydrolyzed collagen, a protein which is highly abundant in animal tissues such as bone and skin. Although many gelatin desserts incorporate fruit, some fresh fruits contain proteolytic enzymes; these enzymes cut the gelatin molecule into peptides (protein fragments) too small to form a firm gel. The use of such fresh fruits in a gelatin recipe results in a dessert that never "sets".
Specifically, pineapple contains the protease (protein cutting enzyme) bromelain, kiwi fruit contains actinidin, figs contain ficain, and papaya contains papain. Cooking or canning denatures and deactivates the proteases, so canned pineapple, for example, works fine in a gelatin dessert.
Safety.
Although eating tainted beef can lead to New Variant Creutzfeldt Jakob Disease (the human variant of mad-cow disease, bovine spongiform encephalopathy), there is no known case of BSE having been transmitted through collagen products such as gelatin.

</doc>
<doc id="13210" url="http://en.wikipedia.org/wiki?curid=13210" title="History of the ancient Levant">
History of the ancient Levant

The Levant is a geographical term that refers to a large area in Southwest Asia, south of the Taurus Mountains, bounded by the Mediterranean Sea in the west, the Arabian Desert in the south, and Mesopotamia in the east. It stretches 400 miles north to south from the Taurus Mountains to the Sinai desert, and 70 to 100 miles east to west between the sea and the Arabian desert. The term is also sometimes used to refer to modern events or states in the region immediately bordering the eastern Mediterranean Sea: Cyprus, Israel, Palestine, Jordan, Lebanon, and Syria.
The term normally does not include Anatolia (although at times Cilicia may be included), the Caucasus Mountains, Mesopotamia or any part of the Arabian Peninsula proper. The Sinai Peninsula is sometimes included, though it is more considered an intermediate, peripheral or marginal area forming a land bridge between the Levant and northern Egypt.
Stone Age.
Anatomically modern Homo sapiens are demonstrated at the area of Mount Carmel, during the Middle Paleolithic dating from about c. 90,000 BC. This move out of Africa seems to have been unsuccessful and by c. 60,000 BC in Palestine/Israel/Syria, especially at Amud, classic Neanderthal groups seem to have profited from the worsening climate to have replaced Homo sapiens, who seem to have been confined once more to Africa.
A second move out of Africa is demonstrated by the Boker Tachtit Upper Paleolithic culture, from 52–50,000 BC, with humans at Ksar Akil XXV level being modern humans. This culture bears close resemblance to the Badoshan Aurignacian culture of Iran, and the later Sebilian I Egyptian culture of c. 50,000 BC. Stephen Oppenheimer suggests that this reflects a movement of modern human (possibly Caucasian) groups back into North Africa, at this time.
It would appear this sets the date by which Homo sapiens Upper Paleolithic cultures begin replacing Neanderthal Levalo-Mousterian, and by c. 40,000 BC Palestine was occupied by the Levanto-Aurignacian Ahmarian culture, lasting from 39–24,000 BC. This culture was quite successful spreading as the Antelian culture (late Aurignacian), as far as Southern Anatolia, with the Atlitan culture.
After the Late Glacial Maxima, a new Epipaleolithic culture appears in Southern Palestine. Extending from 18–10,500 BC, the Kebaran culture shows clear connections to the earlier Microlithic cultures using the bow and arrow, and using grinding stones to harvest wild grains, that developed from the c. 24,000–17,000 BC Halfan culture of Egypt, that came from the still earlier Aterian tradition of the Sahara. Some linguists see this as the earliest arrival of Nostratic languages in the Middle East.
Kebaran culture was quite successful, and may have been ancestral to the later Natufian culture (10,500–8500 BC), which extended throughout the whole of the Levantine region. These people pioneered the first sedentary settlements, and may have supported themselves from fishing, and from the harvest of wild grains plentiful in the region at that time.
Natufian culture also demonstrates the earliest domestication of the dog, and the assistance of this animal in hunting and guarding human settlements may have contributed to the successful spread of this culture. In the northern Syrian, eastern Anatolian region of the Levant, Natufian culture at Cayonu and Mureybet developed the first fully agricultural culture with the addition of wild grains, later being supplemented with domesticated sheep and goats, which were probably domesticated first by the Zarzian culture of Northern Iraq and Iran (which like the Natufian culture may have also developed from Kebaran).
By 8500–7500 BC, the Pre-Pottery Neolithic A (PPNA) culture developed out of the earlier local tradition of Natufian in Southern Palestine, dwelling in round houses, and building the first defensive site at Jericho (guarding a valuable fresh water spring). This was replaced in 7500 BC by Pre-Pottery Neolithic B (PPNB), dwelling in square houses, coming from Northern Syria and the Euphrates bend.
During the period of 8500–7500 BC, another hunter-gatherer group, showing clear affinities with the cultures of Egypt (particularly the Outacha retouch technique for working stone) was in Sinai. This Harifian culture may have adopted the use of pottery from the Isnan culture and Helwan culture of Egypt (which lasted from 9000 to 4500 BC), and subsequently fused with elements from the PPNB culture during the climatic crisis of 6000 BC to form what Juris Zarins calls the Syro-Arabian pastoral technocomplex, which saw the spread of the first Nomadic pastoralists in the Ancient Near East. These extended southwards along the Red Sea coast and penetrating the Arabian bifacial cultures, which became progressively more Neolithic and pastoral, and extending north and eastwards, to lay the foundations for the tent-dwelling Martu and Akkadian peoples of Mesopotamia.
In the Amuq valley of Syria, PPNB culture seems to have survived, influencing further cultural developments further south. Nomadic elements fused with PPNB to form the Minhata Culture and Yarmukian Culture which were to spread southwards, beginning the development of the classic mixed farming Mediterranean culture, and from 5600 BC were associated with the Ghassulian culture of the region, the first chalcolithic culture of the Levant. This period also witnessed the development of megalithic structures, which continued into the Bronze Age.
Bronze Age.
The first cities started developing in southern Mesopotamia during the 4th millennium BC. With these cities, ties of religion began to replace ties of kinship as the basis for society. During the Uruk phase, colonists and traders from Southern Iraq established important quarters in settlements throughout the northern part of the Levantine region (e.g. Amuq). In Southern Iraq each city had a patron god, worshipped in a massive central temple called a "ziggurat", and was ruled by a priest-king ("ishakku"). Society became more segmented and specialized and capable of coordinate projects like irrigation and warfare.
Along with cities came a number of advances in technology. By around the 31st century BC, writing, the wheel, and other such innovations had been introduced. By then, the Sumerian Peoples of south Mesopotamia were all organized into a variety of independent City-states, such as Ur and Uruk, which by around 26th century BC had begun to coalesce into larger political units. By accommodating the conquered people's gods, religion became more polytheistic and government became somewhat more secular; the title of "lugal", big man, appears alongside the earlier religious titles, although his primary duty is still the worship of the state gods.
This process came to its natural conclusion with the development of the first empires around the 24th century BC. A people called the Akkadians invaded the valley under Sargon I and established their supremacy over the Sumerians, and extended their control into Syria as far as the coast. The Ebla archive mentions the cities of Hazor and Jerusalem amongst other sites of the region. They were followed by the extension of Khirbet Kerak ware cultures, showing affinities with the Caucasus, and possibly linked to the later appearance of the Hurrians. This was synchronous with the empires of Ur during the 22nd and 21st centuries BC and the Old Babylonian Kingdom during the 18th and 17th centuries BC, both of which did not extend as far as the Levant. During this time the Kingdom of Yamkhad on the Euphrates, and of Qatna on the Orontes, were important city states of the Syrian region.
Parallel developments were meanwhile occurring in Egypt, which by the 32nd century BC had been unified to form the Old Kingdom of Egypt, and amongst the peoples of the Indus Valley in north-western India. All of these civilizations lie in fertile river valleys where agriculture is relatively easy once dams and irrigation are constructed to control the flood waters.
This started to change around the end of the 3rd millennium BC as cities started to spread to the nearby hilly country: among the Assyrians in north Mesopotamia, the Canaanites in Eastern Mediterranean, to the Minoans in Crete, and to the Hittites in eastern Anatolia. Around this same time various immigrants, such as the Hittites in Anatolia and Mycenaean Greeks, started appearing around the peripheries of civilization.
These groups are associated with the appearance of the light two-wheeled war chariot and typically with Indo-European languages. Horses and chariots require a lot of time and upkeep, so their use was mainly confined to a small nobility. These are the "heroic" societies familiar to us from epics like the "Iliad" and the "Ramayana".
Around the 17th and 16th centuries BC most of the older centres had been overrun. Babylonia was conquered by the Kassites, and the civilization of the Indus Valley was on decline probably due to climate. Another group, the Mitanni, subjugated Assyria and for a time menaced the Hittite kingdom, but were defeated by the two around the middle of the 14th. Various Achaean kingdoms developed in Greece, most notably that of Mycenae, and by the 15th century BC were dominant over the older Minoan cities. And the Semitic Hyksos used the new technologies to occupy Egypt, but were expelled, leaving the empire of the New Kingdom to develop in their wake. From 1550 until 1100, much of the Levant was conquered by Egypt, which in the latter half of this period contested Syria with the Hittite Empire.
At the end of the 13th century BC, all of these powers suddenly collapsed. Cities all around the eastern Mediterranean were sacked within a span of a few decades by assorted raiders. The Achaean kingdoms disappeared, and the Hittite empire was destroyed. Egypt repelled its attackers with only a major effort, and over the next century shrank to its territorial core, its central authority permanently weakened. Only Assyria and Egypt escaped significant damage.
Iron Age.
The destruction at the end of the Bronze Age left a number of tiny kingdoms and City-states behind. A few Hittite centres remained in northern Syria, along with some Phoenician ports in Canaan that escaped destruction and developed into great commercial powers. The Israelites emerged as a rural culture (possibly from the displaced Canaanite refugees escaping the Bronze Age Collapse to Judea and Samaria alongside groups like the Shasu and the Habiru) mainly in the Canaanite hill-country and the Eastern Galilee, quickly spreading through the land and forming an alliance in the struggle for the land against the Philistines to the West, Moab and Ammon to the East and Edom to the South. In the 12th century BC, most of the interior, as well as Babylonia, was overrun by Arameans, while the shoreline around today's Gaza Strip was settled by Philistines.
In this period a number of technological innovations spread, most notably iron working and the Phoenician alphabet, developed by the Phoenicians or the Canaanites around the 16th century BC.
During the 9th century BC, the Assyrians began to reassert themselves against the incursions of the Aramaeans, and over the next few centuries developed into a powerful and well-organised empire. Their armies were among the first to employ cavalry, which took the place of chariots, and had a reputation for both prowess and brutality. At their height, the Assyrians dominated all of the Levant, Egypt, and Babylonia. However, the empire began to collapse toward the end of the 7th century BC, and was obliterated by an alliance between a resurgent New Kingdom of Babylonia and the Iranian Medes.
The subsequent balance of power was short-lived, though. In the 550s BC the Persians revolted against the Medes and gained control of their empire, and over the next few decades annexed to it the realms of Lydia in Anatolia, Damascus, Babylonia, and Egypt, as well as consolidating their control over the Iranian plateau nearly as far as India. This vast kingdom was divided up into various satrapies and governed roughly according to the Assyrian model, but with a far lighter hand. Around this time Zoroastrianism became the predominant religion in Persia.
Classical empires.
From 492–449 BC, the Persians made a series of unsuccessful attempts to conquer Greece. The civilization that had developed there since the end of the Bronze Age was organized along entirely different lines than those of the Middle East, consisting of numerous small City-States fielding citizen militias. Nonetheless they banded together and proved quite capable of dealing with the massive armies of their foe.
By the 4th century BC, Persia had fallen into decline. The campaigns of Xenophon illustrated how very vulnerable it had become to attack by an army organized along Greek lines, but the Greek city-states had weakened each other irreparably through in-fighting. However, in 338 BC, the rising power of Macedon overcame Greece, and under Alexander the Great turned its attention eastward. Alexander conquered Persia in little more than a decade.
Alexander did not live long enough to consolidate his realm, and in the half-century following his death (323 BC) it was carved up by his feuding generals. The Antigonids established themselves in Macedon, the Ptolemies in Egypt, and various small principalities appeared in northern Anatolia. The greater share of the east went to the descendants of Seleucus I Nicator. This period saw great innovations in mathematics, science, architecture, and the like, and Greeks founded cities throughout the east, some of which grew to be the world's first major metropolises. Their culture did not, however, reach very far into the countryside.
The Seleucids adopted a pro-western stance that alienated both the powerful eastern satraps and the Greeks who had migrated to the east. During the 2nd century BC, Greek culture lost ground there, and the empire began to break apart. The province of Bactria revolted, and Parthia was conquered by the semi-nomadic Parni. By 141 BC, the Parthians had established themselves as an empire, after the Seleucid model, and had conquered all of Iran and Mesopotamia. The Seleucid kingdom continued to decline and its remaining provinces were annexed by the Roman Republic in 64 BC as Iudaea Province.
The Parthian nobility reacted against growing Roman influences around the start of new era. Throughout the next century, there was a strong expansion of national culture and a dissolution in central authority. In 114 AD, Trajan temporarily occupied Mesopotamia, and with the end of Hadrian's 40-year peace the two powers were at almost constant hostilities. Mesopotamia was occupied again, but the Parthians recovered and pillaged the Roman provinces. Shortly thereafter, though, the province of Persia rose up in revolt, and defeated the last Parthian emperor in 224 AD.
The new Persian dynasty, the Sassanids, restored central authority. In this period Zoroastrianism developed into an organized religion with close ties to the new state. Various sects of Christianity also spread throughout Iran, and Manichaeism developed from the two religions; these were initially tolerated but later persecuted as the Romans followed the opposite route. Conflicts with Rome, and later with the Byzantine Empire, continued intermittently.
In 391, the Byzantine era began with the permanent division of the Roman Empire into East and Western halves. The last true Roman Emperor in the West was unseated in 476, by which time it had been completely overrun by Germanic nations; however, the Eastern half, known as the Byzantine Empire, lasted much longer, persevering in one form or another until 1453. Byzantine control over the sites of Israel and Judah and other parts of the Levant lasted until 636, when it was conquered by Arabs and became a part of the Caliphate.
The Byzantines reached their lowest point under Phocas, with the Sassanids occupying the whole of the eastern Mediterranean. In 610, though, Heraclius took the throne of Constantinople and began a successful counter-attack, expelling the Persians and invading Media and Assyria. Unable to stop his advance, Khosrau II was assassinated and the Sassanid empire fell into anarchy. Weakened by their quarrels, neither empire was prepared to deal with the onslaught of the Arabs, newly unified under the banners of Islam and anxious to expand their faith. By 650, Arab forces had conquered all of Persia, Syria, and Egypt.

</doc>
<doc id="13219" url="http://en.wikipedia.org/wiki?curid=13219" title="Howard Hawks">
Howard Hawks

Howard Winchester Hawks (May 30, 1896 – December 26, 1977) was an American film director, producer and screenwriter of the classic Hollywood era. He is popular for his films from a wide range of genres such as "Scarface" (1932), "Bringing Up Baby" (1938), "Only Angels Have Wings" (1939),
"His Girl Friday" (1940), "Sergeant York" (1941), "To Have and Have Not" (1944), "The Big Sleep" (1946), "Red River" (1948), "The Thing from Another World" (1951), "Gentlemen Prefer Blondes" (1953), and "Rio Bravo" (1959).
In 1942, Hawks was nominated for the Academy Award for Best Director for "Sergeant York," and in 1975 he was awarded an Honorary Academy Award as "a master American filmmaker whose creative efforts hold a distinguished place in world cinema."
Early life and education.
Howard Winchester Hawks was born in Goshen, Indiana, the first-born child of Frank W. Hawks (1865–1950), a wealthy paper manufacturer, and his wife, Helen (née Howard; 1872–1952), the daughter of a wealthy industrialist. Hawks's family on his father's side were American pioneers and his ancestor John Hawks had emigrated from England to Massachusetts in 1630. The family eventually settled in Goshen and by the 1890s was one of the wealthiest families in the Midwest, due mostly to the highly profitable Goshen Milling Company.
Hawks's maternal grandfather, C. W. Howard (1845–1916), had homesteaded in Neenah, Wisconsin in 1862 at age 17. Within 15 years he had made his fortune in the town's paper mill and other industrial endeavors. Frank Hawks and Helen Howard met in the early 1890s and married in 1895. Howard Hawks was the eldest of five children and his birth was followed by Kenneth Neil Hawks (August 12, 1899 – January 2, 1930), William Bellinger Hawks (January 29, 1901 – January 10, 1969), Grace Louise Hawks (October 17, 1903 – December 23, 1927) and Helen Bernice Hawks (1906-May 4, 1911). In 1898 the family moved to Neenah, Wisconsin where Frank Hawks began working for his father-in-law's Howard Paper Company.
Between 1906 and 1909 the Hawks family began to spend more time in Pasadena, California during the cold Wisconsin winters in order to improve Helen Hawks's ill health. Gradually they began to spend only their summers in Wisconsin before permanently moving to Pasadena in 1910. The family settled in a house down the street from Throop Polytechnic Institute (which would eventually become California Institute of Technology), and the Hawks children began attending the school's Polytechnic Elementary School in 1907.
Hawks was an average student at school and did not excel in sports, but by 1910 had discovered coaster racing, an early form of soapbox racing. In 1911, Hawks's youngest sibling Helen died suddenly of food poisoning. From 1910 to 1912 Hawks attended Pasadena High School, where he was again an average student. But in 1912 the Hawks family moved to nearby Glendora, California, where Frank Hawks owned orange groves. Hawks finished his junior year of high school at Citrus Union High School in Glendora., and was then sent to Phillips Exeter Academy in New Hampshire from 1913 to 1914. Again, he was an average student and his family's wealth may have influenced his acceptance to the elite private school. While in New England, Hawks often attended theatrical shows in nearby Boston. In 1914 Hawks returned to Glendora and graduated from Pasadena High School in 1914. In 1914 Hawks was accepted to Cornell University in Ithaca, New York, where he majored in mechanical engineering and was a member of Delta Kappa Epsilon. As always, Hawks was an average student and college friend Ray S. Ashbury remembered him as spending more of his time playing craps and drinking alcohol than studying, although Hawks was also known to be a voracious reader of popular American and English novels in college.
In 1916, Hawks' grandfather, C.W. Howard, bought him a Mercer race car and Hawks began racing and working on his new car during the summer vacation in California. It was at this time that Hawks first met Victor Fleming, allegedly when the two men raced on a dirt track and caused an accident. Fleming had been an auto mechanic and early aviator when his old friend Marshall Neilan recommended him to film director Allan Dwan as a good mechanic. Fleming went on to impress Dwan by quickly fixing both his car and a faulty film camera and by 1916 had worked his way up to the position of cinematographer.
Meeting Fleming lead to Hawks's first job in the film industry as a prop boy on the Douglas Fairbanks film "In Again, Out Again" for Famous Players-Lasky, which Fleming was employed on as the cinematographer. According to Hawks, a new set was in need of quickly being built when the studios set designer was unavailable and Hawks volunteered to do the job himself, much to Fairbanks's satisfaction. He was next employed as a prop boy and general assistant on an unspecified film directed by Cecil B. DeMille (Hawks never named the film in later interviews and DeMille made five films roughly in that time period). While breaking into the film industry in the summer of 1916, Hawks also unsuccessfully attempted to transfer to Stanford University, and then returned to Cornell in September 1916. Hawks left Cornell in April 1917 when the United States declared war on Germany and entered World War I. Like many college students who joined the Armed services during the war, he received a degree in absentia in 1918. Before Hawks was called for active duty, he took the opportunity to go back to Hollywood and by the end of April 1917 was working on Cecil B. DeMille's "The Little American", where he met and befriended the then eighteen-year-old slate boy James Wong Howe. Hawks next worked on the Mary Pickford film "The Little Princess", directed by Marshall Neilan. According to Hawks, Neilan did not show up to work one day and the resourceful Hawks offered to direct a scene himself, which Pickford agreed to allow.
Hawks began directing at age 21 after he and cinematographer Charles Rosher filmed a double exposure dream sequence with Mary Pickford. Hawks worked with Pickford and Neilan again on "Amarilly of Clothes-Line Alley" before joining the United States Army Air Service. Hawks's military records were destroyed in the 1973 Military Archive Fire, so the only account of his military service is his own. According to Hawks, he spent fifteen weeks in basic training at the University of California in Berkeley where he was trained to be a squadron commander. When Pickford visited Hawks at basic training, his superior officers were so impressed that they promoted him to flight instructor and sent him to Texas to teach new recruits. Due to boredom, Hawks attempted to get a transfer during the first half of 1918 before finally being sent to Fort Monroe, Virginia. But the Armistice was signed in November of that year and Hawks was discharged as a Second Lieutenant without having seen active duty.
Early film career.
After the War, Hawks was eager to return to Hollywood. After having also served in the Air Force, his brother Kenneth Hawks graduated from Yale University in 1919 and the brothers moved to Hollywood together to pursue their careers. They quickly made friends with Hollywood insider (and fellow Ivy Leaguer) Allan Dwan, but Hawks landed his first important job when he used his family's wealth to loan money to studio head Jack Warner. Warner quickly paid the loan back and hired Hawks as a producer to "oversee" the making of a new series of one-reel comedies starring the Italian comedian Monty Banks. Hawks stated that he personally directed "three or four" of the shorts, however no documentation exists to confirm this. The films were profitable, but Hawks soon left the series and proceeded to form his own production company using his family wealth and connections to secure financing. The company was named "Associated Producers" and was a joint venture between Hawks, Allan Dwan, Marshall Neilan and director Allen Holubar, with a distribution deal through First National. The company made fourteen films between 1920 and 1923, with eight directed by Neilan, three by Dwan and three by Holubar. More of a "boy's club" than a production company, the four men gradually drifted apart and went their separate ways by 1923, at which time Hawks decided that he wanted to direct instead of produce.
Beginning in the early 1920, Hawks lived in rented houses in Hollywood with the group of friends he was accumulating. This rowdy group of mostly macho, risk-taking men included his brother Kenneth Hawks, Victor Fleming, Jack Conway, Harold Rosson, Richard Rosson, Arthur Rosson and Eddie Sutherland. During this time period Hawks first met Irving Thalberg, the frail and sickly vice-President in charge of production at Metro-Goldwyn-Mayer. Eventually many of the young men in this group would become successful at MGM under Thalberg, and Hawks admired his intelligence and sense of story. At the same time, Hawks was becoming friends with barn stormers and pioneer aviators at Rogers Airport in Los Angeles, getting to know men like Moye Stephens. In 1923, Famous Players-Lasky president Jesse Lasky was looking for a new Production Editor in the Story department of his studio, and Thalberg suggested the Ivy-League Hawks. Hawks accepted and was immediately put in charge of over forty productions, including many literary acquisitions that included works by Joseph Conrad, Jack London and Zane Grey. Hawks worked on the scripts for all of the films produced, but had his first official screenplay credit in 1924 on "Tiger Love". Hawks was the Story Editor at Famous Players (later Paramount Pictures) almost two years, and occasionally edited such films as "Heritage of the Desert". Although Hawks signed a new one-year contract with Famous-Players in the fall of 1924, he broke his contract to become a story editor for Thalberg at MGM with the promise that Thalberg would make him a director in a year. But in 1925 when Thalberg hesitated to follow through on his promise, Hawks broke his contract at MGM.
Career as a film director.
Silent films: 1925-1929.
In October 1925 Sol Wurtzel, William Fox's studio superintendent at the Fox Film Corporation, invited Hawks to join his company with the promise of letting Hawks direct. Over the next three years, Hawks directed his first eight films (six silent, two "talkies"). A few months after joining Fox Kenneth Hawks was also hired and eventually became one of Fox's top production supervisors. Hawks reworked the scripts of most of the films he directed without always taking official credit for his work. He also worked on the scripts for "Honesty- The Best Policy" in 1926 and Joseph von Sternberg's "Underworld" in 1927, famous for being one of the first gangster films. In 1926 Hawks was introduced to Athole Shearer by his friend Victor Fleming, who was dating Athole's sister Norma Shearer at the time. Shearer's first marriage to writer John Ward was unhappy and she and Hawks began dating throughout 1927 until Shearer asked Ward for a divorce in 1928. Shearer had a young son with Ward named Peter. At the same time, Kenneth Hawks began dating actress Mary Astor, and Hawks's youngest brother Bill began dating actress Bessie Love. Kenneth Hawks and Mary Astor eventually married in February 1928, while Bill Hawks and Bessie Love married in December 1929. On December 23, 1927 Hawks's sister Grace died of tuberculosis after Hawks's mother refused to allow medical treatment because of her Christian Science beliefs. Hawks and Athole Shearer finally married on May 28, 1928 and they honeymooned in Hawaii. Hawks's contract with Fox ended in May 1929, and he never again signed a long-term contract with a major studio but managed to remain an independent producer-director for the rest of his long career. In October 1929, Hawks and Shearer had their first child, David Hawks.
"The Road to Glory" (1926)
Hawks's first film "The Road to Glory" was based on a thirty-five page treatment that Hawks wrote and is one of only two Hawks films that are lost films. The film stars May McAvoy as a young woman who is gradually going blind and tries to spare the two men in her life from the burden of her illness. The two men are her boyfriend Rockliffe Fellowes and her father Ford Serling, with Leslie Fenton playing the greedy rich man whom she agrees to live with in order to get away from her father and lover. The film contained religious iconography and messages that would never again be seen in a Hawks film. It was shot from December 1925 to January 1926 and premiered in April. It received good reviews from film critics. In later interviews, Hawks said "It didn't have any fun in it. It was pretty bad. I don't think anybody enjoyed it except a few critics." Hawks was dissatisfied with the film after being certain that dramatic films would establish his reputation, but realized what he had done wrong when Sol Wurtzel told Hawks "Look, you've shown you can make a picture, but for God's sake, go out and make entertainment."
"Fig Leaves" (1926)
Immediately after completing "The Road to Glory" Hawks began writing his next film, "Fig Leaves", his first (and, until 1935, only) comedy. The film portrays a married couple by juxtaposing them in the Garden of Eden and in modern New York City. The Garden of Eden humorously depicts Adam and Eve awoken by a Flintstones-like coconut alarm clock and Adam reading the morning paper off of giant stone tablets. Then in the modern day the biblical serpent is replaced by Eve's gossiping neighbor and Eve becomes a sexy flapper and fashion model when Adam is at work. The film starred George O'Brien as Adam and Olive Borden as Eve and received positive reviews, particularly for the art direction and costume designs. It was released in July 1926 and was Hawks's first hit as a director. Although he mainly dismissed his early work, Hawks praised this film in later interviews.
"Paid to Love" (1927)
"Paid to Love" is notable in Hawks's filmography as being the only time that he made a highly stylized, experimental film. German film director F. W. Murnau had recently made "The Last Laugh" and "Sunrise" and was the most critically acclaimed director in Hollywood, and Hawks's attempted to imitate Murnau's style with this film. The film includes atypical tracking shots, expressionistic lighting and stylistic film editing that was inspired by German Expressionist films. In a later interview Hawks commented "It isn't my type of stuff, at least I got it over in a hurry. You know the idea of wanting the camera to do those things: Now the camera's somebody's eyes." Hawks worked on the script with Seton I. Miller, with whom he would go on to collaborate with on seven more films. The film stars George O'Brien as the introverted Crown Prince Michael, William Powell as his happy-go-lucky brother and Virginia Valli as Michael's flapper love interest Dolores. Valli's character was an early yet incomplete example of the Hawksian woman archetype as the sexually aggressive showgirl, while O'Brien's Michael portrayal of a shy man not interested in sex is a character later elaborated upon by Cary Grant and Gary Cooper in later Hawks films. "Paid to Love" was completed by September 1926, but remained unreleased until July 1927, when it was unsuccessful financially.
"Cradle Snatchers" (1927)
"Cradle Snatchers" was based on a 1925 hit stage play by Russell G. Medcraft and Norma Mitchell about three unhappy, middle-aged housewives who teach their adulterous husbands a lesson by starting affairs with college-aged young men during the jazz age. The film starred Louise Fazenda, Dorothy Phillips and Ethel Wales and was shot in early 1927. The film was released in May 1927 and was a minor hit. For many years it was believed to be a lost film until film director Peter Bogdanovich discovered a print in 20th Century Fox's film vaults, although the print was missing part of reel three and all of reel four.
"Fazil" (1928)
In March 1927 Hawks signed a new one-year, three picture contract with Fox and was assigned to direct "Fazil", based on the play "L'Insoumise" by Pierre Frondaie. Hawks again worked with Seton Miller on the script about a Middle Eastern prince who has an affair with a Parisienne showgirl and cast Charles Farrell as the prince and Greta Nissen as Fabienne. Hawks went both over schedule and over budget on the film, which began the rift between him and Sol Wurtzel that would eventually lead to Hawks leaving Fox. Although finished in August 1927, the film was not released until June 1928.
"A Girl in Every Port" (1928)
"A Girl in Every Port" is considered by film scholars to be the most important film of Hawks's silent career because it is his first film to introduce many of the Hawksian themes and characters that would continue until his final films. It was his first "love story between two men," with two men bonding over their duty, skills and careers and considering their friendship to be more important than relationships with women. Hawks wrote the original story and developed the screenplay with James Kevin McGuinness and Seton Miller. In the film Victor McLaglen and Robert Armstrong play two merchant seamen who are arch rivals when it comes to women, with McLaglen constantly finding Armstrong's tattoo on the women that he is attempting to seduce. Eventually the two rivals become friends. Louise Brooks plays the cabaret singer with whom the two men fall in love. The film was shot from October to December 1927 and released in February 1928. It was successful in the US, and a hit in Europe where Louise Brooks was developing a cult following. In France, Henri Langlois called Hawks "the Gropius of the cinema" and Swiss novelist and poet Blaise Cendrars said that the film "definitely marked the first appearance of contemporary cinema." However, Hawks once again went over budget with this film and his relationship with Sol Wurtzel became worse. After an advance screening that received positive reviews, Wurtzel told Hawks "This is the worst picture Fox has made in years."
"The Air Circus" (1928)
"The Air Circus" is Hawks' first film centered around aviation, one of his early loves. In 1928, Charles Lindbergh was the world's most famous person and "Wings" was one of the most popular films of the year. Wanting to capitalize on the country's aviation craze, Fox immediately bought Hawks's original story for "The Air Circus", a variation of the male friendship plot of "A Girl in Every Port" about two young pilots. Officially, the original story is credited to Graham Baker and Andrew Bennison, while the screenplay was written by Seton Miller and Norman Z. McLeod. In the film, Arthur Lake and David Rollins play two eager young pilots at flight school who compete over their flight instructors aviatrix sister played by Sue Carol. The film was shot from April to June 1928, but Fox ordered an additional 15 minutes of dialogue footage to be shot so the film could compete with the new "talkies" being released. Hawks hated the new dialogue written by Hugh Herbert and he refused to participate in the re-shoots. The film was released in September 1928 and was a moderate hit. It is one of two filmed directed by Hawks's that is a lost film.
"Trent's Last Case" (1929)
"Trent's Last Case" is an adaptation of British author E. C. Bentley's 1913 novel of the same name, and had already been adapted to film in England in 1920. Hawks considered the novel to be "one of the greatest detective stories of all time" and was eager to make it his first sound film. He cast Raymond Griffith in the lead role of Phillip Trent. Griffith's throat had been damaged by poison gas during World War I and his voice was a hoarse whisper, prompting Hawks to later state "I thought he ought to be great in talking pictures "because" of that voice." However, after shooting only a few scenes, Fox shut Hawks down and ordered him to make a silent film, both because of Griffith's voice and because they only owned the legal rights to make a silent film. The film did have a musical score and synchronized sound effects, but no dialogue. Due to the failing business of silent films, it was never released in the US and only briefly screened in England where film critics hated it. The film was believed lost until the mid-1970s and was screened for the first time in the US at a Hawks retrospective in 1974. Hawks was in attendance of the screening and attempted to have the only print of the film destroyed.
Early sound films: 1930-1934.
By 1930, Hollywood was in upheaval over the coming of "talkies" and many careers (both actors and directors) were ruined. Many Hollywood studios were recruiting stage actors and directors that they believed were better suited for sound films. After having worked in the industry for 14 years and directing many financially successful films, Hawks found himself having to re-prove himself as being an asset to the studios. Leaving Fox on sour terms didn't help his reputation, but Hawks was one of the few people in Hollywood who never backed down from fights with studio heads. After several months of unemployment, Hawks renewed his career with his first sound film in 1930.
On January 2, 1930, Hawks's brother Kenneth Hawks died while shooting the film "Such Men Are Dangerous". Kenneth's career as a director was quickly rising after his debut film "Big Time" in 1929. This sound film starred Lee Tracy and Mae Clarke and was an early example of the "fast-talking" sound films that Howard Hawks would later make one of his signatures. "Such Men Are Dangerous" was based on the life Alfred Lowenstein, a Belgian captain who either jumped or fell out of his plane in 1928. On January 2, Kenneth Hawks and his crew flew three planes (two with cameras, one with a stunt actor) over Santa Monica Bay when the two camera planes crashed into each other, killing ten men. The crash was the first major on-set accident in Hollywood history and made national news. Mary Astor kept her distance from the Hawks family after Kenneth's death.
"The Dawn Patrol" (1930)
Hawks's first all sound film was "The Dawn Patrol", based on an original story by John Monk Saunders and (unofficially) Hawks. Saunders was a flight instructor during World War I and had written "Wings". He was considered one of the most talented writers in Hollywood and was often compared to F. Scott Fitzgerald and Ernest Hemingway. Accounts vary on who first came up with the idea of the film, but Hawks and Saunders developed the story together and tried to sell it to several studios before First National agreed to produce the film. Saunders received solo screen credit for the original story and won an Academy Award for Best Story in 1930. The screenplay was written by Hawks, Seton Miller and Dan Totheroh and starred Richard Barthelmess and Douglas Fairbanks, Jr. Shooting began in late February 1930, about the same time that Howard Hughes was finally finishing his epic World War I aviation epic "Hell's Angels" after being in production since September 1927. Hawks schrewdly began to hire many of the aviation experts and cameramen that had been employed by Hughes, including Elmer Dyer, Harry Reynolds and Ira Reed. When Hughes found out about the rival film, he did everything he could to sabotage "The Dawn Patrol" by harassing Hawks and other studio personal, hiring a spy that was quickly caught and finally suing First National for copyright infringement. Hughes finally dropped the lawsuit in late 1930, and he and Hawks became good friends during the legal battle. In the film, Barthelmess and Fairbanks play two Royal Flying Corps pilots during World War I who deal with the pressure of wartime combat and constant death by drinking and fighting with their commanding officer. Filming was finished in late May 1930 and premiered in July, setting a first week box office record at the Winter Garden Theatre in New York. The film became one of the biggest hits of 1930.
"The Criminal Code" (1931)
Hawks did not get along with Warner Brothers executive Hal B. Wallis and his contract allowed him to be loaned out to other studios. Hawks took the opportunity to accept a directing offer from Harry Cohn at Columbia Pictures: "The Criminal Code", based on a successful play by Martin Flavin. Hawks and Seton Miller worked on the script with Flavin for a month and filming began in September 1930. The film starred Walter Huston, Phillips Holmes, Constance Cummings and Boris Karloff. Huston plays a prison warden who wants to reform the conditions of the inmates and Holmes plays a wrongly convicted prisoner who learns about the "code" of not ratting on other inmates. Hawks called Huston "the greatest actor [he] ever worked with". Hawks also worked with his old friend James Wong Howe for the first time as a cinematographer. The film opened in January 1931 and was a hit. Hawks encountered a minor amount of censorship when the film was banned in Chicago, which would deal with even further on his next film.
"Scarface" (1932)
In 1930 Howard Hughes hired Hawks to direct "Scarface", a gangster film loosely based on the life of Chicago mobster Al Capone. The film starred Paul Muni in the title role, with memorable supporting performances from George Raft, Boris Karloff, and Osgood Perkins. The film was completed in September 1931, but the censorship of the Hays Code prevented it from being released as Hawks and Hughes had originally intended, and the two men fought the Hays Office (and made compromises) for over a year until the film was released in 1932, after such other pivotal early gangster films as "The Public Enemy" and "Little Caesar". "Scarface" was the first film in which Hawks worked with screenwriter Ben Hecht, who became a close friend and collaborator for twenty years.
"The Crowd Roars" (1932)
After filming was complete on "Scarface", Hawks left Hughes to fight the legal battles and returned to First National to fulfill his contract, this time with producer Darryl F. Zanuck. For his next film, Hawks wanted to make a film about his childhood passion: car racing. "The Crowd Roars" is loosely based on the play "The Barker: A Play of Carnival Life" by Kenyon Nicholson. Hawks developed the script with Seton Miller for their eighth and final collaboration and the script was by Miller, Kubec Glasmon, John Bright and Niven Busch. The film starred James Cagney, Eric Linden, Joan Blondell and Ann Dvorak. In the film, Cagney plays a race car driver who tries to "protect" his younger brother Linden, who is also a driver, from being distracted by his girlfriend, Blondell. At the same time, Cagney must hide his own neurotic girlfriend, played by Dvorak, from his younger brother. Blondell and Dvorak were initially cast in each other's roles but swapped after a few days of shooting. Shooting began on December 7, 1931 at Legion Ascot Speedway and wrapped on February 1, 1932. Hawks used real race car drivers in the film, including the 1930 Indianapolis 500 winner Billy Arnold. The film was released in March and became a hit.
"Tiger Shark" (1932)
Later in 1932 he directed "Tiger Shark" starring Edward G. Robinson as a tuna fisherman. In these early films, Hawks established the prototypical "Hawksian Man", which film critic Andrew Sarris described as "upheld by an instinctive professionalism."
"Today We Live" (1933)
In 1933 Hawks signed a three-picture deal at Metro-Goldwyn-Mayer Studios and his first film was there "Today We Live" in 1933, starring Joan Crawford and Gary Cooper. The World War I film was based on a short story by author William Faulkner, who Hawks got to know personally during the shooting of the film and remained friends with for over twenty years.
Hawks's next two films at MGM were the boxing drama "The Prizefighter and the Lady" and the bio-pic "Viva Villa!", starring Wallace Beery as Mexican Revolutionary leader Pancho Villa. But because of studio interference on both films, Hawks walked out on his MGM contract without completing either film himself.
Later sound films.
In 1934 Hawks went to Columbia Pictures to make his first screwball comedy, "Twentieth Century", starring John Barrymore and Hawks's distant cousin Carole Lombard. The film was based on a stage play by Ben Hecht and Charles MacArthur, and along with Frank Capra's "It Happened One Night" (released the same year) is considered to be the defining film of the screwball comedy genre. In 1935 Hawks made "Barbary Coast" with Edward G. Robinson and Miriam Hopkins. In 1936 he made the aviation adventure "Ceiling Zero" with James Cagney and Pat O'Brien. Also in 1936, Hawks began filming "Come and Get It", starring Edward Arnold, Joel McCrea, Frances Farmer and Walter Brennan. But he was fired by Samuel Goldwyn in the middle of shooting and the film was completed by William Wyler.
In 1938 Hawks made the screwball comedy "Bringing Up Baby" for RKO Pictures. The film starred Cary Grant and Katharine Hepburn and has been called "the screwiest of the screwball comedies" by film critic Andrew Sarris. In the film, Grant plays a near-sighted paleontologist who suffers one humiliation after another due to the lovestruck socialite played by Hepburn. The film was unsuccessful when initially released but has gradually become regarded as Hawks's masterpiece. Hawks followed this with the aviation drama "Only Angels Have Wings", again starring Cary Grant and made in 1939 for Columbia Pictures. The film also starred Jean Arthur, Thomas Mitchell, Rita Hayworth and Richard Barthelmess.
In 1940 Hawks returned to the screwball comedy genre with "His Girl Friday", starring Cary Grant and Rosalind Russell. The film was an adaptation of the hit Broadway play "The Front Page" by Ben Hecht and Charles MacArthur, which had already been made into a film in 1931, and again with some of the plotlines reworked by the same screenwriters and transplanted to Rudyard Kipling's India for "Gunga Din" the year before "His Girl Friday", with Cary Grant in that script's counterpart to the same role. In 1941 Hawks made "Sergeant York", starring Gary Cooper as a pacifist farmer who becomes a decorated World War I soldier. The film was the highest-grossing film of 1941 and won two Academy Awards (Best Actor and Best Editing), as well as earning Hawks his only nomination for Best Director. Later that year Hawks re-teamed with Cooper for "Ball of Fire", also starring Barbara Stanwyck. The film was written by Billy Wilder and Charles Brackett and is playfully based on "Snow White and the Seven Dwarfs". In the film, Cooper plays a sheltered, intellectual linguist who is writing an encyclopedia with six other scientists, and hires street-wise Stanwyck to help them with modern slang terms. In 1941 Hawks began work on the Howard Hughes produced (and later directed) film "The Outlaw", based on the life of Billy the Kid and starring Jane Russell. Hawks completed initial shooting of the film in early 1941, but due to perfectionism and battles with the Hollywood Production Code, Hughes continued to re-shoot and re-edit the film until it was finally released in 1943, with Hawks uncredited as director.
After making the World War II film "Air Force" in 1943 starring John Garfield, Hawks made two films with Hollywood and real life lovers Humphrey Bogart and Lauren Bacall. "To Have and Have Not", made in 1944, stars Bogart, Bacall and Walter Brennan and is based on a short story by Ernest Hemingway. Hawks was a close friend of Hemingway and made a bet with the author that he could make a good film out of Hemingway's "worst book". Hawks, William Faulkner and Jules Furthman collaborated on the script about a French fishing boat captain and various situations of espionage during the Fall of France in 1940. Bogart and Bacall fell in love on the set of the film and married soon afterwards. Hawks re-teamed with the newlyweds in 1946 with "The Big Sleep", based on the Philip Marlowe detective novel by Raymond Chandler.
In 1948, Hawks made "Red River", an epic western reminiscent of "Mutiny on the Bounty" starring John Wayne and Montgomery Clift in his first film. Later that year, Hawks remade his earlier film "Ball of Fire" as "A Song Is Born", this time starring Danny Kaye and Virginia Mayo. This version of the film follows the same plot but pays more attention to popular jazz music and includes such jazz legends as Tommy Dorsey, Benny Goodman, Louis Armstrong, Lionel Hampton, and Benny Carter playing themselves. In 1949 Hawks re-teamed with Cary Grant in the screwball comedy "I Was a Male War Bride", also starring Ann Sheridan.
In 1951, he produced - and some believe essentially directed - the science fiction film "The Thing from Another World". He followed this with the 1952 western film "The Big Sky", starring Kirk Douglas. Later in 1952 Hawks re-teamed with Cary Grant for the fifth and final time in the screwball comedy "Monkey Business", also starring Marilyn Monroe and Ginger Rogers. Grant plays a scientist reminiscent of his character in "Bringing up Baby", who creates a formula that increases his vitality. Film critic John Belton called the film Hawks's "most organic comedy." Hawks's third film of 1952 was a contribution to the omnibus film "O. Henry's Full House", which includes short films based on the stories by the writer O. Henry made by various directors. Hawks's short film "The Ransom of Red Chief" starred Fred Allen, Oscar Levant and Lee Aaker.
In 1953, Hawks made "Gentlemen Prefer Blondes", which featured Marilyn Monroe famously singing "Diamonds Are a Girl's Best Friend." The film starred Monroe and Jane Russell as two gold digging, cabaret performer best friends that many critics point out is Hawks's only female version of his celebrated "buddy film" genre. In 1955 Hawks made an atypical "Land of the Pharaohs", a Sword-and-sandal epic about ancient Egypt and starring Jack Hawkins and Joan Collins. The film was Hawks's final collaboration with longtime friend William Faulkner before the author's death. In 1959 Hawks re-teamed with John Wayne in "Rio Bravo", also starring Dean Martin, Ricky Nelson and Walter Brennan as four lawmen "defending the fort" of their local jail where a local criminal is awaiting a trial and his family attempt to break him out. Film critic Robin Wood has said if he "were asked to choose a film that would justify the existence of Hollywood...it would be "Rio Bravo"."
In 1962 Hawks made "Hatari!", again with John Wayne as a big game hunter in Africa. In 1964 Hawks made his final comedy, "Man's Favorite Sport?", starring Rock Hudson (since Cary Grant felt he was too old for the role) and Paula Prentiss. Hawks then returned to his childhood passion for car races with "Red Line 7000" in 1965. the film starred a young James Caan in his first leading role. Hawks's final two films were both Western remakes of "Rio Bravo" starring John Wayne. In 1966 Hawks directed "El Dorado", starring Wayne, Robert Mitchum and James Caan, which was released the following year. In 1970 he made "Rio Lobo", with Wayne, Jorge Rivero and Jack Elam.
Personal life.
Hawks was married three times:
His brothers were director/writer Kenneth Neil Hawks and film producer William Bettingger Hawks.
Hawks died on December 26, 1977, aged 81 from complications from falling over his dog several weeks earlier at his home in Palm Springs, California at the time he was working with his last protege discovery, Larraine Zax.
Style.
Hawks was versatile as a director, filming comedies, dramas, gangster films, science fiction, film noir, and Westerns. Hawks's own functional definition of what constitutes a "good movie" is revealing of his no-nonsense style: "Three great scenes, no bad ones."
Hawks also defined a good director as "someone who doesn't annoy you".
While Hawks was not sympathetic to feminism, he popularized the Hawksian woman archetype, which has been cited as a prototype of the post-feminist movement.
Orson Welles in an interview with Peter Bogdanovich said of Howard Hawks in comparison to John Ford "Hawks is great prose; Ford is poetry".
Despite Hawks's work in a variety of Hollywood genres he still retained an independent sensibility. Film critic David Thomson wrote of Hawks in "The New Biographical Dictionary of Film" "Far from being the meek purveyor of Hollywood forms, he always chose to turn them upside down, "To Have and Have Not" and "The Big Sleep", ostensibly an adventure and a thriller, are really love stories. "Rio Bravo", apparently a Western - everyone wears a cowboy hat - is a comedy conversation piece. The ostensible comedies are shot through with exposed emotions, with the subtlest views of the sex war, and with a wry acknowledgment of the incompatibility of men and women." As David Boxwell states "It’s a body of work that has been accused of ahistorical and adolescent escapism, but Hawks’ fans rejoice in his oeuvre‘s remarkable avoidance of Hollywood’s religiosity, bathos, flag-waving, and sentimentality.
Legacy.
His directorial style and the use of natural, conversational dialogue in his films were cited a major influence on many noted filmmakers, including Robert Altman, John Carpenter, and Quentin Tarantino. His work is admired by many notable directors including Peter Bogdanovich, Martin Scorsese, François Truffaut, Michael Mann and Jacques Rivette.
Brian De Palma dedicated his version of "Scarface" to Hawks and Ben Hecht.
Although his work was not initially taken seriously by British critics of the "Sight and Sound" circle, he was venerated by French critics associated with "Cahiers du cinéma", who intellectualized his work in a way Hawks himself was moderately amused by, (his work was promoted in France by The Studio des Ursulines cinema) and he was also admired by more independent British writers such as Robin Wood. Wood named the Hawks-directed "Rio Bravo" as his top film of all time.
Jean-Luc Godard called Hawks "the greatest American artist".
Critic Leonard Maltin labeled Hawks "the greatest American director who is not a household name," noting that, while his work may not be as well known as Ford, Welles, or DeMille, he is no less a talented filmmaker.
Andrew Sarris in his influential book of film criticism "The American Cinema: Directors and Directions 1929-1968" included Hawks in the "pantheon" of the 14 greatest film directors who had worked in the United States.
In the 2012 Sight & Sound's Greatest Film Poll, Howard Hawks had six films he directed in the Critic's Top 250 Films: "Rio Bravo" (number 63), "Bringing Up Baby" (number 110), "Only Angels Have Wings" (number 154), "His Girl Friday" (number 171), "The Big Sleep" (number 202) and "Red River" (number 235)
Hawks was nicknamed "The Gray Fox" by members of the Hollywood community.
Awards.
He was nominated for Academy Award for Best Director in 1942 for "Sergeant York", but he received his only Oscar in 1975 as an Honorary Award from the Academy.
His films "The Big Sleep", "Bringing Up Baby", "His Girl Friday", "Red River", "Scarface", "Sergeant York", "The Thing from Another World" and "Twentieth Century" were rated "culturally significant" by the United States Library of Congress and inducted into the National Film Registry.
"Bringing Up Baby" (1938) was listed number 97 on the American Film Institute's AFI's 100 Years... 100 Movies. On the AFI's AFI's 100 Years... 100 Laughs "Bringing Up Baby" was listed number 14, "His Girl Friday" (1940) was listed number 19 and "Ball of Fire" (1941) was listed number 92.
For his contribution to the motion picture industry, Howard Hawks has a star on the Hollywood Walk of Fame at 1708 Vine Street.

</doc>
<doc id="13260" url="http://en.wikipedia.org/wiki?curid=13260" title="Hee Haw">
Hee Haw

Hee Haw is an American television variety show featuring country music and humor with fictional rural Kornfield Kounty as a backdrop. It aired on CBS-TV from 1969–1971 before a 20-year run in local syndication. The show was inspired by "Rowan & Martin's Laugh-In", the major difference being that "Hee Haw" was far less topical, and was centered on country music and rural Southern culture. Hosted by country artists Buck Owens and Roy Clark for most of the series' run, the show was equally well known for its voluptuous, scantily-clad women in stereotypical farmer's daughter outfits and country-style minidresses (a group that came to be known as the "Hee Haw Honeys"), and its cornpone humor.
"Hee Haw"'s appeal, however, was not limited to a rural audience. It was successful in all of the major markets, including New York, Los Angeles and Chicago. Other niche programs such as "The Lawrence Welk Show" (which targeted older audiences) and "Soul Train" (which targeted African-American audiences) also rose to prominence in syndication during the era. Like "Laugh-In", the show minimized production costs by taping all of the recurring sketches for a season in batches, setting up for the Cornfield one day, the Joke Fence another, etc. At the height of its popularity, an entire year's worth of shows would be taped in two separate week-long sessions, then individual shows would be assembled from edited sections. Only musical performances were taped with a live audience; a laugh track was added to all other segments.
The series was taped for the CBS Television Network at its network affiliate WLAC-TV (now WTVF) in downtown Nashville, and later at Opryland USA in East Nashville. The show was produced by Yongestreet Productions through the mid-1980s; it was later produced by Gaylord Entertainment, which distributed the show in syndication. The show's name was coined by show business talent manager and producer Bernie Brillstein and derives from a common English onomatopoeia used to describe the braying sound that a donkey makes.
Creation and syndication.
Much of "Hee Haw's" origin was Canadian. The series' creators, comedy writers Frank Peppiatt and John Aylesworth, were from Canada. From 1969 until the late 1980s, "Hee Haw" was produced by Yongestreet Productions, named after Yonge Street, a major thoroughfare in Toronto. Gordie Tapp and Don Harron, both writer/performers on the show, were also Canadian.
"Hee Haw" started on CBS-TV as a summer 1969 replacement for "The Smothers Brothers Comedy Hour". Though the show had respectable ratings (it sat at #16 for the 1970-71 season), it was dropped in July 1971 by CBS as part of the so-called "Rural Purge" (along with fellow country-themed shows "The Beverly Hillbillies", "Mayberry R.F.D." and "Green Acres"), owing to network executives' feeling that its viewers reflected a less appealing, aging demographic (e.g. rural, somewhat older, less affluent, less prone to buy).
Undaunted, the producers put together a syndication deal for the show, which continued in roughly the same format for 20 more years (though Owens departed in 1986). After Owens left, Clark was assisted each week by a country music celebrity co-host.
During the show's peak in popularity, "Hee Haw" often competed in syndication against "The Lawrence Welk Show", a long-running ABC program which had also been canceled in 1971, also in an attempt to purge the networks of older demographic-leaning programs. Like "Hee Haw", "Lawrence Welk" was picked up for syndication in the fall of 1971, and there were some markets where the same station aired both programs. (The success of "Hee Haw" and "Lawrence Welk" in syndication, and the network decisions that led to their respective cancellations, were the inspiration for a novelty song called "The Lawrence Welk-Hee Haw Counter-Revolution Polka," performed by Clark; the song became a top 10 hit on the "Billboard" Hot Country Singles chart in the fall of 1972.)
Mirroring the long downward trend in the popularity of variety shows in general that had taken place in the 1970s, ratings began to decline for "Hee Haw" by the mid-1980s, a trend that continued into the early 1990s. In the fall of 1991, in an attempt to win back viewers and attract a younger audience, the show's format and setting underwent a dramatic overhaul. The changes included a new title ("The Hee Haw Show"), more pop-oriented country music, and the barnyard-cornfield setting replaced by a city street and shopping mall set. The first of the new shows aired in January 1992.
Despite the attempt to keep the show fresh, the changes alienated many of its longtime viewers while failing to gain the hoped-for younger viewers, and the ratings continued their decline.
During the summer of 1992, a decision was made to end first-run production, and instead air highlights of the show's earlier years in a revamped program called "Hee Haw Silver" (as part of celebrating the show's 25th year). Under the new format, Clark hosted a mixture of classic clips and new footage.
The "Hee Haw Silver" episodes spotlighted many of the classic comedy skits and moments from the show, with a series of retrospective looks at performers who had since died, such as David "Stringbean" Akeman, Archie Campbell, Junior Samples, and Kenny Price. According to the show's producer, Sam Lovullo, the ratings showed improvement with these classic reruns; however, the series was finally canceled in 1993 at the conclusion of its 25th season. "Hee Haw" continued to pop up in reruns (see below for details) throughout the 1990s and later during the following decade, in a series of successful DVD releases from Time Life.
Reruns.
After the show's syndication run ended, reruns aired on The Nashville Network from 1993 until 1996. Upon the cancellation of reruns in 1996 the program resurfaced, in reruns, the following year for a limited run on the same network. Its 21 years in TV syndication (1971–1992) was the record for the longest-running U.S. syndicated TV program, until "Soul Train" surpassed it in 1993; "Hee Haw" remains the fifth longest-running syndicated American TV program, though the longest-running of its genre.
During the 2006–07 season CMT aired a series of reruns and TV Land also recognized the series with an award presented by k.d. lang; in attendance were Roy Clark, Gunilla Hutton, Barbi Benton, the Hager twins, Linda Thompson, Misty Rowe and others. It was during this point, roughly between the years of 2004 and 2007, that Time Life began selling selected episodes of the show on DVD. Among the DVD content offered was the 1978 10th anniversary special that hadn't been seen since its original airing. CMT sporadically aired the series, usually in graveyard slots, and primarily held the rights in order to be able to air the musical performances as part of their music video library (such as during the "Pure Vintage" block on CMT Pure Country).
Reruns of "Hee Haw" began airing on RFD-TV in September 2008, where it currently remains, anchoring the network's Sunday night lineup, although beginning in January 2014 the show airs on Saturday afternoon and is rerun on Sunday night. In 2011 the network began re-airing the earliest episodes from 1969–70 on Thursday evenings. In the summer of 2011 a lot of the surviving cast and an ensemble of country artists taped a "Country's Family Reunion" special, entitled "Salute to the Kornfield", which aired on RFD-TV in January 2012. The special is also part of "Country's Family Reunion's" DVD series. Concurrent with the special was the unveiling of a "Hee Haw" exhibit, titled "Pickin' and Grinnin"', at the Oklahoma History Center in Oklahoma City.
Cast members.
Two rural-style comedians, already well known in their native Canada, gained their first major U.S. exposure. Gordie Tapp and Don Harron (whose KORN Radio character, newscaster Charlie Farquharson, had been a fixture of Canadian television since 1952 and later appeared on "The Red Green Show").
Other cast members over the years included, but were not limited to:
Roy Acuff,
Cathy Baker (as the show's emcee),
Billy Jim Baker,
Barbi Benton,
Kelly Billingsley,
Vicki Bird,
Jennifer Bishop,
Archie Campbell,
Phil Campbell,
Harry Cole (Weeping Willie),
Mackenzie Colt,
John Henry Faulk,
Marianne Gordon (Rogers),
Jim and Jon Hager,
Victoria Hallman,
Diana Goodman,
Gunilla Hutton,
Linda Johnson,
Grandpa Jones,
Zella Lehr (the "unicycle girl"),
George Lindsey (reprising his "Goober" character from "The Andy Griffith Show"),
Jimmy Little,
Irlene Mandrell,
Dawn McKinley,
Patricia McKinnon,
Sherry Miles,
Rev. Grady Nutt,
Minnie Pearl,
Claude "Jackie" Phelps,
Slim Pickens,
Kenny Price,
Anne Randall,
Chase Randolph,
Susan Raye,
Jimmie Riddle,
Jeannine Riley,
Alice Ripley,
Lulu Roman,
Misty Rowe,
Junior Samples,
Ray Sanders,
Terry Sanders,
Gailard Sartain,
Diana Scott,
Gerald Smith (the "Georgia Quacker"),
Jeff Smith,
Donna Stokes,
Dennis Stone,
Roni Stoneman,
Mary Taylor,
Nancy Taylor,
Linda Thompson,
Lisa Todd,
Pedro Tomas,
Nancy Traylor, Buck Trent,
Jackie Waddell, Pat Woodell and
Jonathan Winters, among many others.
The Buckaroos (Buck Owens' band) initially served as the house band on the show and consisted of members Don Rich, Jim Shaw, Jerry Brightman, Jerry Wiggins, Rick Taylor, Doyle Singer (Doyle Curtsinger), Don Lee, Ronnie Jackson, Terry Christoffersen, Doyle Holly, and in later seasons fiddle player Jana Jae, and Victoria Hallman, who replaced Don Rich on harmony vocals (Rich was killed in a motorcycle accident in 1974). In later seasons, harmonica player Charlie McCoy joined the cast and became the show's music director, forming the "Hee Haw Band", which became the house band for the rest of the series' run. The Nashville Edition, a four-member (two male, two female) singing group, served as the background singers for most of the musical performances.
Some of the cast members made national headlines: Lulu Roman was twice charged with drug possession in 1971, David "Stringbean" Akeman and his wife were murdered in November 1973 during a robbery at their home; and as mentioned above, Buck Owens' lead guitarist and harmony singer Don Rich of the Buckaroos was killed in a motorcycle crash in 1974.
Some cast members, such as Charlie McCoy and Tennessee Ernie Ford, originally appeared on the show as guest stars.
After Buck Owens left the show, a different country music artist would accompany Roy Clark as a guest co-host each week until the final season ("Hee Haw Silver"), which Clark hosted alone.
Recurring sketches and segments.
Some of the most popular sketches and segments on "Hee Haw" included:
Guest stars often participated in some of the sketches (mostly the "PFFT! You Was Gone" and "The Cornfield" sketches); however, this did not occur until later seasons.
Musical legacy.
Hee Haw was a premiere showcase on commercial television during the 1970s and early 1980s for country, bluegrass, gospel, and other styles of American traditional music, featuring hundreds of elite musical performances that were paramount to the success, popularity and legacy of the series for a broad audience of Southern, rural and purely music fans alike.
Some of the most popular music-based segments on the show (other than guest stars' performances) included:
In addition to hosts Buck Owens and Roy Clark, who would perform at least one song each week, other cast members—such as Gunilla Hutton, Misty Rowe, Victoria Hallman, Grandpa Jones (sometimes with his wife Ramona), Kenny Price, Archie Campbell, Barbi Benton, The Nashville Edition, Vicki Bird, and Diana Goodman—would occasionally perform a song on the show; and the show would almost always open with a song performed by the entire cast.
Lovullo also has made the claim the show presented "what were, in reality, the first musical videos." Lovullo said his videos were conceptualized by having the show's staff go to nearby rural areas and film animals and farmers, before editing the footage to fit the storyline of a particular song. "The video material was a very workable production item for the show," he wrote. "It provided picture stories for songs. However, some of our guests felt the videos took attention away from their live performances, which they hoped would promote record sales. If they had a hit song, they didn't want to play it under comic barnyard footage." The concept's mixed reaction eventually spelled an end to the "video" concept on "Hee Haw". However, several of co-host Owens' songs – including "Tall, Dark Stranger," "Big in Vegas" and "I Wouldn't Live in New York City (If They Gave Me the Whole Dang Town)" – aired on the series and have since aired on Great American Country and CMT as part of their classic country music programming blocks.
Guest appearances.
"Hee Haw" featured at least two, and sometimes three or four, guest celebrities each week. While most of the guest stars were country music artists, a wide range of other famous luminaries were featured. Also, several clogging groups frequently performed on the show, and occasionally the show featured child singers who would perform top country songs of the day.
Sheb Wooley, one of the original cast members, wrote the show's theme song. After filming the initial 13 episodes, other professional demands caused him to leave the show, but he returned from time to time as a guest.
Loretta Lynn was the first guest star of "Hee Haw" and made more guest appearances than any other artist. She also co-hosted the show more than any other guest co-host and therefore appears on more of the DVD releases for retail sale than any other guest star.
From 1990–92, country superstar Garth Brooks appeared on the show four times. In 1992, producer Sam Lovullo tried unsuccessfully to contact Brooks because he wanted him for the final show. Brooks surprised Lovullo by showing up last minute, ready to don his overalls and perform for the final episode.
Stage settings.
A barn interior set was used as the main stage for most of the musical performances from the show's premiere until the debut of the "Hee Haw Honky Tonk" sketch in the early 1980s. Afterwards, the "Hee Haw Honky Tonk" set would serve as the main stage for the rest of the series' run. Buck Owens began using the barn interior set for his performances after it was replaced by the "Hee Haw Honky Tonk" set and was named "Buck's Place" (an obvious nod to one of Owens' hits, "Sam's Place"). Other settings for the musical performances throughout the series' run included a haystack (where the entire cast performed songs), the living room of a Victorian house, the front porch and lawn of the Samuel B. Sternwheeler home, a grist mill (where Roy Clark performed many of his songs in earlier seasons), and a railroad depot, where Buck Owens performed many of his songs in earlier seasons.
The Elvis connection.
Elvis Presley was a fan of "Hee Haw" and wanted to appear as a guest on the program in the 1970s, but his manager, Colonel Tom Parker, would not allow him to do so. Two of the Hee Haw Honeys dated Presley long before they joined the cast: Most notably Linda Thompson in the early 1970s, whom Presley also had a long-lasting, steady relationship with after his divorce from Priscilla; and Diana Goodman shortly afterwards. Shortly after Presley's death, his father, Vernon Presley, made an appearance on the show and paid tribute to his late son, noting how much he enjoyed watching the show.
Episode closings.
At the end of the show, hosts Clark and Owens, backed by the entire cast, sang the original closing song with the following lyrics:
And then (spoken):
"Hee Haw Honeys" (spin-off series).
"Hee Haw" produced a short-lived spin-off series, "Hee Haw Honeys" (not to be confused with "Hee Haw's" female cast members), for the 1979 television season. The musical sitcom starred Kathie Lee Johnson (Gifford) along with "Hee Haw" regulars Misty Rowe, Gailard Sartain, Lulu Roman, and Kenny Price as a family who owned a truck stop restaurant (likely inspired by the "Lulu's Truck Stop" sketch on "Hee Haw"). Their restaurant included a bandstand, where guest country artists would perform a couple of their hits of the day, sometimes asking the cast to join them. Cast members would also perform songs occasionally; and the Nashville Edition, "Hee Haw's" backup singing group, frequently appeared on the show, portraying regular patrons of the restaurant.
Legacy.
"Hee Haw" continues to remain beloved and popular with its long-time fans and those who've discovered the program through DVD releases or its reruns on RFD-TV. In spite of the loving support of the series by its fans, the program has never been a favorite of television critics or reviewers.
On at least four episodes of the animated Fox series "Family Guy", when the storyline hits a dead-end, a cutaway to Conway Twitty performing a song is inserted. The handoff is done in "Hee Haw" style, and often uses actual footage from the show.
Lulu Roman released a new album titled "At Last" on January 15, 2013. The album features Lulu's versions of 12 classics and standards including guest appearances by Dolly Parton, T. Graham Brown, Linda Davis, and Georgette Jones (daughter of George Jones and Tammy Wynette).

</doc>
<doc id="13276" url="http://en.wikipedia.org/wiki?curid=13276" title="Historiography">
Historiography

Historiography refers to both the study of the methodology of historians and the development of history as a discipline, and also to a body of historical work on a particular subject. The historiography of a specific topic covers how historians have studied that topic using particular sources, techniques, and theoretical approaches. Scholars discuss historiography topically – such as the "historiography of the British Empire," the "historiography of early Islam", or the "historiography of China" – as well as different approaches and genres, such as political history or social history. Beginning in the nineteenth century, with the ascent of academic history, a body of historiographic literature developed. The extent to which historians are influenced by their own groups and loyalties—such as to their nation state—is a much debated question.
The research interests of historians change over time, and in recent decades there has been a shift away from traditional diplomatic, economic and political history toward newer approaches, especially social and cultural studies. From 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5,723 faculty members, 1,644 (29%) identified themselves with social history while political history came next with 1,425 (25%).
Terminology.
In the early modern period, the term "historiography" tended to be used in a more basic sense, to mean simply "the writing of history". "Historiographer" therefore meant "historian", and it is in this sense that certain official historians were given the title "Historiographer Royal", in Sweden (from 1618), England (from 1660), and Scotland (from 1681). The Scottish post is still in existence.
Furay and Salevouris (1988) define historiography as "the study of the way history has been and is written – the history of historical writing... When you study 'historiography' you do not study the events of the past directly, but the changing interpretations of those events in the works of individual historians."
Premodern history.
Understanding the past appears to be a universal human need, and the telling of history has emerged independently in civilisations around the world. What constitutes history is a philosophical question (see philosophy of history). The earliest chronologies date back to Mesopotamia and ancient Egypt, though no historical writers in these early civilizations were known by name. For the purposes of this article, history is taken to mean written history recorded in a narrative format for the purpose of informing future generations about events. Some experts have advised against the tendency to extrapolate trends for historical patterns that do not align with expectations about the future.
East Asia.
Ancient China.
In China, the earliest history was recorded in oracle bone script which was deciphered and may date back to around late 2nd millennium BCE. The "Zuo Zhuan", attributed to Zuo Qiuming in the 5th century, is the earliest written of narrative history in the world and covers the period from 722 to 468. The "Classic of History" is one of the Five Classics of Chinese classic texts and one of the earliest narratives of China. The "Spring and Autumn Annals", the official chronicle of the State of Lu covering the period from 722 to 481, is among the earliest surviving historical texts to be arranged on annalistic principles in the world. It is traditionally attributed to Confucius(551–479 BCE). "Zhan Guo Ce" was a renowned ancient Chinese historical compilation of sporadic materials on the Warring States period compiled between the 3rd and 1st centuries.
Sima Qian (around 100) was the first in China to lay the groundwork for professional historical writing. His written work was the "Shiji" ("Records of the Grand Historian"), a monumental lifelong achievement in literature. Its scope extends as far back as the 16th century, and it includes many treatises on specific subjects and individual biographies of prominent people, and also explores the lives and deeds of commoners, both contemporary and those of previous eras. His work influenced every subsequent author of history in China, including the prestigious Ban family of the Eastern Han Dynasty era.
Traditional Chinese historiography describes history in terms of dynastic cycles. In this view, each new dynasty is founded by a morally righteous founder. Over time, the dynasty becomes morally corrupt and dissolute. Eventually, the dynasty becomes so weak as to allow its replacement by a new dynasty.
Korea.
The tradition of Korean historiography was established with the "Samguk Sagi", a history of Korea from its allegedly earliest times. It was compiled by Goryeo court historian Kim Busik after its commission by King Injong of Goryeo (r. 1122 - 1146). It was completed in 1145 and relied not only on earlier Chinese histories for source material, but also on the "Hwarang Segi" written by the Silla historian Kim Daemun in the 8th century. The latter work is now lost.
Japan.
The earliest works of history produced in Japan were the "Rikkokushi", a corpus of six national histories covering the history of Japan from its mythological beginnings until the 9th century. The first of these works were the "Nihon Shoki", compiled by Prince Toneri in 720.
Hellenic world.
The earliest known systematic historical thought emerged in ancient Greece, a development which would be an important influence on the writing of history elsewhere around the Mediterranean region. Greek historians greatly contributed to the development of historical methodology. The earliest known critical historical works were "The Histories", composed by Herodotus of Halicarnassus (484–425 BCE) who became known as the "father of history.". Herodotus attempted to distinguish between more and less reliable accounts, and personally conducted research by travelling extensively, giving written accounts of various Mediterranean cultures. Although Herodotus' overall emphasis lay on the actions and characters of men, he also attributed an important role to divinity in the determination of historical events.
The generation following Herodotus witnessed a spate of local histories of the individual city-states ("poleis"), written by the first of the local historians who employed the written archives of city and sanctuary. Dionysius of Halicarnassus characterized these historians as the forerunners of Thucydides, and these local histories continued to be written into Late Antiquity, as long as the city-states survived. Two early figures stand out: Hippias of Elis, who produced the lists of winners in the Olympic Games that provided the basic chronological framework as long as the pagan classical tradition lasted, and Hellanicus of Lesbos, who compiled more than two dozen histories from civic records, all of them now lost.
Thucydides largely eliminated divine causality in his account of the war between Athens and Sparta, establishing a rationalistic element which set a precedent for subsequent Western historical writings. He was also the first to distinguish between cause and immediate origins of an event, while his successor Xenophon (c. 431 – 355) introduced autobiographical elements and character studies in his Anabasis.
The proverbial Philippic attacks of the Athenian orator Demosthenes (384–322) on Philip II of Macedon marked the height of ancient political agitation. The now lost history of Alexander's campaigns by the diadoch Ptolemy I (367–283) may represent the first historical work composed by a ruler. Polybius (c. 203 – 120) wrote on the rise of Rome to world prominence, and attempted to harmonize the Greek and Roman points of view.
The Chaldean priest Berossus ( 3rd century) composed a Greek-language "History of Babylonia" for the Seleucid king Antiochus I, combining Hellenistic methods of historiography and Mesopotamian accounts to form a unique composite. Reports exist of other near-eastern histories, such as that of the Phoenician historian Sanchuniathon; but he is considered semi-legendary and writings attributed to him are fragmentary, known only through the later historians Philo of Byblos and Eusebius, who asserted that he wrote before even the Trojan war.
Roman world.
The Romans adopted the Greek tradition, writing at first in Greek, but eventually chronicling their history in a freshly non-Greek language. While early Roman works were still written in Greek, the "Origines", composed by the Roman statesman Cato the Elder (234–149), was written in Latin, in a conscious effort to counteract Greek cultural influence. It marked the beginning of Latin historical writings. Hailed for its lucid style, Julius Caesar's (100–44) "Bellum Gallicum" exemplifies autobiographical war coverage. The politician and orator Cicero (106–43) introduced rhetorical elements in his political writings.
Strabo (63 – c. 24) was an important exponent of the Greco-Roman tradition of combining geography with history, presenting a descriptive history of peoples and places known to his era. Livy (59 – 17) records the rise of Rome from city-state to empire. His speculation about what would have happened if Alexander the Great had marched against Rome represents the first known instance of alternate history.
Biography, although popular throughout antiquity, was introduced as a branch of history by the works of Plutarch (c. 46 – 127) and Suetonius (c. 69 – after 130) who described the deeds and characters of ancient personalities, stressing their human side. Tacitus (c. 56 – c. 117) denounces Roman immorality by praising German virtues, elaborating on the topos of the Noble savage.
Christendom.
Christian historiography began early, perhaps as early as Luke-Acts, which is the primary source for the Apostolic Age, though its historical reliability is disputed. In the first Christian centuries, the New Testament canon was developed. The growth of Christianity and its enhanced status in the Roman Empire after Constantine I (see State church of the Roman Empire) led to the development of a distinct Christian historiography, influenced by both Christian theology and the nature of the Christian Bible, encompassing new areas of study and views of history. The central role of the Bible in Christianity is reflected in the preference of Christian historians for written sources, compared to the classical historians' preference for oral sources and is also reflected in the inclusion of politically unimportant people. Christian historians also focused on development of religion and society. This can be seen in the extensive inclusion of written sources in the "Ecclesiastical History" written by Eusebius of Caesarea around 324 and in the subjects it covers. Christian theology considered time as linear, progressing according to divine plan. As God's plan encompassed everyone, Christian histories in this period had a universal approach. For example, Christian writers often included summaries of important historical events prior to the period covered by the work.
Writing history was popular among Christian monks and clergy in the Middle Ages. They wrote about the history of Jesus Christ, that of the Church and that of their patrons, the dynastic history of the local rulers. In the Early Middle Ages historical writing often took the form of annals or chronicles recording events year by year, but this style tended to hamper the analysis of events and causes. An example of this type of writing is the Anglo-Saxon Chronicles, which were the work of several different writers: it was started during the reign of Alfred the Great in the late 9th century, but one copy was still being updated in 1154. Some writers in the period did construct a more narrative form of history. These included Gregory of Tours, and more successfully Bede who wrote both secular and ecclesiastical history and is known for writing the "Ecclesiastical History of the English People".
During the Renaissance, history was written about states or nations. The study of history changed during the Enlightenment and Romanticism. Voltaire described the history of certain ages that he considered important, rather than describing events in chronological order. History became an independent discipline. It was not called "philosophia historiae" anymore, but merely history ("historia").
Islamic world.
Muslim historical writings first began to develop in the 7th century, with the reconstruction of the Prophet Muhammad's life in the centuries following his death. With numerous conflicting narratives regarding Muhammad and his companions from various sources, it was necessary to verify which sources were more reliable. In order to evaluate these sources, various methodologies were developed, such as the "science of biography", "science of hadith" and "Isnad" (chain of transmission). These methodologies were later applied to other historical figures in the Islamic civilization. Famous historians in this tradition include Urwah (d. 712), Wahb ibn Munabbih (d. 728), Ibn Ishaq (d. 761), al-Waqidi (745–822), Ibn Hisham (d. 834), Muhammad al-Bukhari (810–870) and Ibn Hajar (1372–1449). Historians of the medieval Islamic world also developed an interest in world history.
Islamic historical writing eventually culminated in the works of the Arab Muslim historian Ibn Khaldun (1332–1406), who published his historiographical studies in the "Muqaddimah" (translated as "Prolegomena") and "Kitab al-I'bar" ("Book of Advice"). His work was forgotten until it was rediscovered in the late 19th century.
Enlightenment.
During the Age of Enlightenment, the modern development of historiography through the application of scrupulous methods began.
Voltaire.
French "philosophe" Voltaire (1694–1778) had an enormous influence on the art of history writing. His best-known histories are "The Age of Louis XIV" (1751), and "Essay on the Customs and the Spirit of the Nations" (1756). "My chief object," he wrote in 1739, "is not political or military history, it is the history of the arts, of commerce, of civilization – in a word, – of the human mind." He broke from the tradition of narrating diplomatic and military events, and emphasized customs, social history and achievements in the arts and sciences. The "Essay on Customs" traced the progress of world civilization in a universal context, thereby rejecting both nationalism and the traditional Christian frame of reference. Influenced by Bossuet's "Discourse on the Universal History" (1682), he was the first scholar to make a serious attempt to write the history of the world, eliminating theological frameworks, and emphasizing economics, culture and political history. He treated Europe as a whole, rather than a collection of nations. He was the first to emphasize the debt of medieval culture to Arab civilization, but otherwise was weak on the Middle Ages. Although he repeatedly warned against political bias on the part of the historian, he lost few opportunities to expose what he saw as the intolerance and frauds of the church over the ages. Voltaire advised scholars that anything contradicting the normal course of nature was not to be believed. Although he found evil in the historical record, he fervently believed reason and educating the illiterate masses would lead to progress. Voltaire explains his view of historiography in his article on "History" in Diderot's "Encyclopédie":
Voltaire's histories used the values of the Enlightenment to evaluate the past. He helped free historiography from antiquarianism, Eurocentrism, religious intolerance and a concentration on great men, diplomacy, and warfare. Peter Gay says Voltaire wrote "very good history," citing his "scrupulous concern for truths," "careful sifting of evidence," "intelligent selection of what is important," "keen sense of drama," and "grasp of the fact that a whole civilization is a unit of study."
David Hume.
At the same time, philosopher David Hume was having a similar impact on history in Great Britain. In 1754 he published the "History of England", a 6-volume work which extended "From the Invasion of Julius Caesar to the Revolution in 1688". Hume adopted a similar scope to Voltaire in his history; as well as the history of Kings, Parliaments, and armies, he examined the history of culture, including literature and science, as well. His short biographies of leading scientists explored the process of scientific change and he developed new ways of seeing scientists in the context of their times by looking at how they interacted with society and each other – he paid special attention to Francis Bacon, Robert Boyle, Isaac Newton and William Harvey.
He also argued that the quest for liberty was the highest standard for judging the past, and concluded that after considerable fluctuation, England at the time of his writing had achieved "the most entire system of liberty, that was ever known amongst mankind."
William Robertson.
William Robertson, a Scottish historian, and the Historiographer Royal published the "History of Scotland 1542 - 1603", in 1759 and his most famous work, "The history of the reign of Charles V" in 1769. His scholarship was painstaking for the time and he was able to access a large number of documentary sources that had previously been unstudied. He was also one of the first historians who understood the importance of general and universally applicable ideas in the shaping of historical events.
Edward Gibbon.
The apex of Enlightenment history was reached with Edward Gibbon's monumental six-volume work, "The History of the Decline and Fall of the Roman Empire", published on 17 February 1776. Because of its relative objectivity and heavy use of primary sources, its methodology became a model for later historians. This has led to Gibbon being called the first "modern historian". The book sold impressively, earning its author a total of about £9000. Biographer Leslie Stephen wrote that thereafter, "His fame was as rapid as it has been lasting."
Gibbon's work has been praised for its style, its piquant epigrams and its effective irony. Winston Churchill memorably noted, "I set out upon...Gibbon's "Decline and Fall of the Roman Empire" [and] was immediately dominated both by the story and the style. ...I devoured Gibbon. I rode triumphantly through it from end to end and enjoyed it all." Gibbon was pivotal in the secularizing and 'desanctifying' of history, with his fiercely polemical attacks on Christianity. Unusually for an 18th-century historian, Gibbon was never content with secondhand accounts when the primary sources were accessible (though most of these were drawn from well-known printed editions). "I have always endeavoured," he says, "to draw from the fountain-head; that my curiosity, as well as a sense of duty, has always urged me to study the originals; and that, if they have sometimes eluded my search, I have carefully marked the secondary evidence, on whose faith a passage or a fact were reduced to depend." In this insistence upon the importance of primary sources, Gibbon is considered by many to have broken new ground in the methodical study of history:
In accuracy, thoroughness, lucidity, and comprehensive grasp of a vast subject, the 'History' is unsurpassable. It is the one English history which may be regarded as definitive. ...Whatever its shortcomings the book is artistically imposing as well as historically unimpeachable as a vast panorama of a great period.
19th century.
The tumultuous events surrounding the French Revolution inspired much of the historiography and analysis of the early 19th century. Interest in the 1688 Glorious Revolution was also rekindled by the Great Reform Act of 1832 in England.
Thomas Carlyle.
Thomas Carlyle published his three-volume "", in 1837. The first volume was accidentally burned by John Stuart Mill's maid. Carlyle rewrote it from scratch. Carlyle's style of historical writing stressed the immediacy of action, often using the present tense. He emphasised the role of forces of the spirit in history and thought that chaotic events demanded what he called 'heroes' to take control over the competing forces erupting within society. He considered the dynamic forces of history as being the hopes and aspirations of people that took the form of ideas, and were often ossified into ideologies. Carlyle's "The French Revolution" was written in a highly unorthodox style, far removed from the neutral and detached tone of the tradition of Gibbon. Carlyle presented the history as dramatic events unfolding in the present as though he and the reader were participants on the streets of Paris at the [famous events. Carlyle's invented style was epic poetry combined with philosophical treatise. It is rarely read or cited in the last century.
Thomas Macaulay.
Thomas Macaulay produced his most famous work of history, "The History of England from the Accession of James the Second", in 1848. His writings are famous for their ringing prose and for their confident, sometimes dogmatic, emphasis on a progressive model of British history, according to which the country threw off superstition, autocracy and confusion to create a balanced constitution and a forward-looking culture combined with freedom of belief and expression. This model of human progress has been called the Whig interpretation of history.
His legacy continues to be controversial; Gertrude Himmelfarb wrote that "most professional historians have long since given up reading Macaulay, as they have given up writing the kind of history he wrote and thinking about history as he did." However, J. R. Western wrote that: "Despite its age and blemishes, Macaulay's "History of England" has still to be superseded by a full-scale modern history of the period".
French historians: Michelet and Taine.
In his main work "Histoire de France", French historian Jules Michelet coined the term Renaissance (meaning "Rebirth" in French language), as a period in Europe's cultural history that represented a break from the Middle Ages, creating a modern understanding of humanity and its place in the world.
The nineteen volume work covered French history from Charlemagne to the outbreak of the Revolution.
Michelet was one of the first historians to shift the emphasis of history to the common people, rather than the leaders and institutions of the country. He devoted himself to writing a picturesque history of the Middle Ages, and his account is still one of the most vivid that exists. His inquiry into manuscript and printed authorities was most laborious, but his lively imagination, and his strong religious and political prejudices, made him regard all things from a singularly personal point of view.
Hippolyte Taine was the chief theoretical influence of French naturalism, a major proponent of sociological positivism and one of the first practitioners of historicist criticism. He was unable to secure an academic position. He pioneered The idea of "milieu" as an active historical force which amalgamated geographical, psychological and social factors. Historical writing for him was a search for general laws. His brilliant style kept his writing in circulation long after his theoretical approaches were passe.
Cultural and constitutional history.
One of the major progenitors of the history of culture and art, was the Swiss historian Jacob Burckhardt Siegfried Giedion described Burckhardt's achievement in the following terms: "The great discoverer of the age of the Renaissance, he first showed how a period should be treated in its entirety, with regard not only for its painting, sculpture and architecture, but for the social institutions of its daily life as well." Burckhardt's best known work is "The Civilization of the Renaissance in Italy" (1860).
His most famous work was "The Civilization of the Renaissance in Italy", published in 1860; it was the most influential interpretation of the Italian Renaissance in the nineteenth century and is still widely read. According to John Lukacs, he was the first master of cultural history, which seeks to describe the spirit and the forms of expression of a particular age, a particular people, or a particular place. His innovative approach to historical research stressed the importance of art and its inestimable value as a primary source for the study of history. He was one of the first historians to rise above the narrow nineteenth-century notion that "history is past politics and politics current history.
By the mid-19th century, scholars were beginning to analyse the history of institutional change, particularly the development of constitutional government. William Stubbs's "Constitutional History of England" (3 vols., 1874–78) was an important influence on this developing field. The work traced the development of the English constitution from the Teutonic invasions of Britain until 1485, and marked a distinct step in the advance of English historical learning. He argued that the theory of the unity and continuity of history should not remove distinctions between ancient and modern history. He believed that, though work on ancient history is a useful preparation for the study of modern history, either may advantageously be studied apart. He was a good palaeographer, and excelled in textual criticism, in examination of authorship, and other such matters, while his vast erudition and retentive memory made him second to none in interpretation and exposition.
Von Ranke and Professionalization in Germany.
The modern academic study of history and methods of historiography were pioneered in 19th-century German universities, especially the University of Göttingen. Leopold von Ranke was a pivotal influence in this regard, and is considered as the founder of modern source-based history. According to Caroline Hoefferle, “Ranke was probably the most important historian to shape historical profession as it emerged in Europe and the United States in the late 19th century.”
Specifically, he implemented the seminar teaching method in his classroom, and focused on archival research and analysis of historical documents. Beginning with his first book in 1824, the "History of the Latin and Teutonic Peoples from 1494 to 1514", Ranke used an unusually wide variety of sources for a historian of the age, including "memoirs, diaries, personal and formal missives, government documents, diplomatic dispatches and first-hand accounts of eye-witnesses". Over a career that spanned much of the century, Ranke set the standards for much of later historical writing, introducing such ideas as reliance on primary sources, an emphasis on narrative history and especially international politics ("aussenpolitik"). Sources had to be solid, not speculations and rationalizations. His credo was to write history the way it was. He insisted on primary sources with proven authenticity.
Ranke also rejected the 'teleological approach' to history, which traditionally viewed each period as inferior to the period which follows. In Ranke's view, the historian had to understand a period on its own terms, and seek to find only the general ideas which animated every period of history. In 1831 and at the behest of the Prussian government, Ranke founded and edited the first historical journal in the world, called "Historisch-Politische Zeitschrift".
Another important German thinker was Georg Wilhelm Friedrich Hegel, whose theory of historical progress ran counter to Ranke's approach. In Hegel's own words, his philosophical theory of "World history... represents the development of the spirit's consciousness of its own freedom and of the consequent realization of this freedom.". This realization is seen by studying the various cultures that have developed over the millennia, and trying to understand the way that freedom has worked itself out through them:
World history is the record of the spirit's efforts to attain knowledge of what it is in itself. The Orientals do not know that the spirit or man as such are free in themselves. And because they do not know that, they are not themselves free. They only know that One is free... The consciousness of freedom first awoke among the Greeks, and they were accordingly free; but, like the Romans, they only knew that Some, and not all men as such, are free... The Germanic nations, with the rise of Christianity, were the first to realize that All men are by nature free, and that freedom of spirit is his very essence.
Karl Marx introduced the concept of historical materialism into the study of world historical development. In his conception, the economic conditions and dominant modes of production determined the structure of society at that point. In his view five successive stages in the development of material conditions would occur in Western Europe. The first stage was primitive communism where property was shared and there was no concept of "leadership". This progressed to a slave society where the idea of class emerged and the State developed. Feudalism was characterized by an aristocracy working in partnership with a theocracy and the emergence of the Nation-state. Capitalism appeared after the bourgeois revolution when the capitalists (or their merchant predecessors) overthrew the feudal system and established a market economy, with
private property and Parliamentary democracy. Marx then predicted the eventual proletarian revolution that would result in the attainment of socialism, followed by Communism, where property would be communally owned.
Previous historians had focused on cyclical events of the rise and decline of rulers and nations. Process of nationalization of history, as part of national revivals in the 19th century, resulted with separation of "one's own" history from common universal history by such way of perceiving, understanding and treating the past that constructed history as history of a nation. A new discipline, sociology, emerged in the late 19th century and analyzed and compared these perspectives on a larger scale.
20th century.
Macaulay and Whig history.
The term Whig history, coined by Herbert Butterfield in his short book "The Whig Interpretation of History" in 1931, means the approach to historiography which presents the past as an inevitable progression towards ever greater liberty and enlightenment, culminating in modern forms of liberal democracy and constitutional monarchy. In general, Whig historians emphasized the rise of constitutional government, personal freedoms and scientific progress. The term has been also applied widely in historical disciplines outside of British history (the history of science, for example) to criticize any teleological (or goal-directed), hero-based, and transhistorical narrative.
Paul Rapin de Thoyras's history of England, published in 1723, became "the classic Whig history" for the first half of the 18th century. It was later supplanted by the immensely popular "The History of England" by David Hume. Whig historians emphasized the achievements of the Glorious Revolution of 1688. This included James Mackintosh's "History of the Revolution in England in 1688", William Blackstone's "Commentaries on the Laws of England" and Henry Hallam's "Constitutional History of England".
The most famous exponent of 'Whiggery' was Thomas Babington Macaulay, who published the first volumes of his "The History of England from the Accession of James II" in 1848. It proved an immediate success and replaced Hume's history to become the new orthodoxy. His 'Whiggish convictions' are spelled out in his first chapter:
This consensus was steadily undermined during the post-World War I re-evaluation of European history, and Butterfield's critique exemplified this trend. Intellectuals no longer believed the world was automatically getting better and better. Subsequent generations of academic historians have similarly rejected Whig history because of its presentist and teleological assumption that history is driving toward some sort of goal. Other criticized 'Whig' assumptions included viewing the British system as the apex of human political development, assuming that political figures in the past held current political beliefs (anachronism), considering British history as a march of progress with inevitable outcomes and presenting political figures of the past as heroes, who advanced the cause of this political progress, or villains, who sought to hinder its inevitable triumph. J. Hart says "a Whig interpretation requires human heroes and villains in the story."
France: Annales School.
The French Annales School radically changed the focus of historical research in France during the 20th century by stressing long-term social history, rather than political or diplomatic themes. The school emphasized the use of quantification and the paying of special attention to geography.
The "Annales d'histoire économique et sociale" journal was founded in 1929 in Strasbourg by Marc Bloch and Lucien Febvre. These authors, the former a medieval historian and the latter an early modernist, quickly became associated with the distinctive "Annales" approach, which combined geography, history, and the sociological approaches of the Année Sociologique (many members of which were their colleagues at Strasbourg) to produce an approach which rejected the predominant emphasis on politics, diplomacy and war of many 19th and early 20th-century historians as spearheaded by historians whom Febvre called Les Sorbonnistes. Instead, they pioneered an approach to a study of long-term historical structures ("la longue durée") over events and political transformations. Geography, material culture, and what later Annalistes called "mentalités," or the psychology of the epoch, are also characteristic areas of study. The goal of the Annales was to undo the work of the Sorbonnistes, to turn French historians away from the narrowly political and diplomatic toward the new vistas in social and economic history.
An eminent member of this school, Georges Duby, described his approach to history as one that relegated the sensational to the sidelines and was reluctant to give a simple accounting of events, but strived on the contrary to pose and solve problems and, neglecting surface disturbances, to observe the long and medium-term evolution of economy, society and civilisation. The Annalistes, especially Lucien Febvre, advocated a "histoire totale", or "histoire tout court", a complete study of a historic problem.
The second era of the school was led by Fernand Braudel and was very influential throughout the 1960s and 1970s, especially for his work on the Mediterranean region in the era of Philip II of Spain. Braudel developed the idea, often associated with Annalistes, of different modes of historical time: "l'histoire quasi immobile" (motionless history) of historical geography, the history of social, political and economic structures ("la longue durée"), and the history of men and events, in the context of their structures. His 'longue durée' approach stressed slow, and often imperceptible effects of space, climate and technology on the actions of human beings in the past. The "Annales" historians, after living through two world wars and incredible political upheavals in France, were deeply uncomfortable with the notion that multiple ruptures and discontinuities created history. They preferred to stress inertia and the longue durée. Special attention was paid to geography, climate, and demography as long-term factors. They believed the continuities of the deepest structures were central to history, beside which upheavals in institutions or the superstructure of social life were of little significance, for history lies beyond the reach of conscious actors, especially the will of revolutionaries.
Noting the political upheavals in Europe and especially in France in 1968, Eric Hobsbawm argued that "in France the virtual hegemony of Braudelian history and the "Annales" came to an end after 1968, and the international influence of the journal dropped steeply." Multiple responses were attempted by the school. Scholars moved in multiple directions, covering in disconnected fashion the social, economic, and cultural history of different eras and different parts of the globe. By the time of crisis the school was building a vast publishing and research network reaching across France, Europe, and the rest of the world. Influence indeed spread out from Paris, but few new ideas came in. Much emphasis was given to quantitative data, seen as the key to unlocking all of social history. However, the Annales ignored the developments in quantitative studies underway in the U.S. and Britain, which reshaped economic, political and demographic research.
Marxist historiography.
Marxist historiography developed as a school of historiography influenced by the chief tenets of Marxism, including the centrality of social class and economic constraints in determining historical outcomes. Friedrich Engels wrote "The Peasant War in Germany", which analysed social warfare in early Protestant Germany in terms of emerging capitalist classes. Although it lacked a rigorous engagement with archival sources, it indicated an early interest in history from below and class analysis, and it attempts a dialectical analysis. Another treatise of Engels, "The Condition of the Working Class in England in 1844", was salient in creating the socialist impetus in British politics from then on, e.g. the Fabian Society.
R. H. Tawney was an early historian working in this tradition. "The Agrarian Problem in the Sixteenth Century" (1912) and "Religion and the Rise of Capitalism" (1926), reflected his ethical concerns and preoccupations in economic history. He was profoundly interested in the issue of the enclosure of land in the English countryside in the sixteenth and seventeenth centuries and in Max Weber's thesis on the connection between the appearance of Protestantism and the rise of capitalism. His belief in the rise of the gentry in the century before the outbreak of the Civil War in England provoked the 'Storm over the Gentry' in which his methods were subjected to severe criticisms by Hugh Trevor-Roper and John Cooper.
A circle of historians inside the Communist Party of Great Britain (CPGB) formed in 1946 and became a highly influential cluster of British Marxist historians, who contributed to history from below and class structure in early capitalist society. While some members of the group (most notably Christopher Hill and E. P. Thompson) left the CPGB after the 1956 Hungarian Revolution, the common points of British Marxist historiography continued in their works. They placed a great emphasis on the subjective determination of history.
Christopher Hill's studies on 17th-century English history were widely acknowledged and recognised as representative of this school. His books include "Puritanism and Revolution" (1958), "Intellectual Origins of the English Revolution" (1965 and revised in 1996), "The Century of Revolution" (1961), "AntiChrist in 17th-century England" (1971), "The World Turned Upside Down" (1972) and many others.
E. P. Thompson pioneered the study of history from below in his work, "The Making of the English Working Class", published in 1963. It focused on the forgotten history of the first working-class political left in the world in the late-18th and early-19th centuries. In his preface to this book, Thompson set out his approach to writing history from below:
I am seeking to rescue the poor stockinger, the Luddite cropper, the "obsolete" hand-loom weaver, the "Utopian" artisan, and even the deluded follower of Joanna Southcott, from the enormous condescension of posterity. Their crafts and traditions may have been dying. Their hostility to the new industrialism may have been backward-looking. Their communitarian ideals may have been fantasies. Their insurrectionary conspiracies may have been foolhardy. But they lived through these times of acute social disturbance, and we did not. Their aspirations were valid in terms of their own experience; and, if they were casualties of history, they remain, condemned in their own lives, as casualties.
Thompson's work was also significant because of the way he defined "class." He argued that class was not a structure, but a relationship that changed over time. He opened the gates for a generation of labor historians, such as David Montgomery and Herbert Gutman, who made similar studies of the American working classes.
Other important Marxist historians included Eric Hobsbawm, C. L. R. James, Raphael Samuel, A. L. Morton and Brian Pearce.
Although Marxist historiography made important contributions to the history of the working class, oppressed nationalities, and the methodology of history from below, its chief problematic aspect was its argument on the nature of history as "determined" or "dialectical"; this can also be stated as the relative importance of subjective and objective factors in creating outcomes. It increasingly fell out of favour in the 1960s and '70s. Geoffrey Elton was important in undermining the case for a Marxist historiography, about which he argued was presenting seriously flawed interpretations of the past. In particular, Elton was opposed to the idea that the English Civil War was caused by socioeconomic changes in the 16th and 17th centuries, arguing instead that it was due largely to the incompetence of the Stuart kings.
In dealing with the era of the Second World War, Addison notes that in Britain by the 1990s, labour history was, "in sharp decline," because:
Biography.
Biography has been a major form of historiography since the days when Plutarch wrote the parallel lives of great Roman and Greek leaders. It is a field especially attractive to nonacademic historians, and often to the wives or children of famous men who have access to the trove of letters and documents. Academic historians tend to downplay biography because it pays too little attention to broad social, cultural, political and economic forces, and perhaps too much attention to popular psychology. The “Great Man” tradition in Britain originated in the multi-volume "Dictionary of National Biography" (which originated in 1882 and issued updates into the 1970s); it continues to this day in the new "Oxford Dictionary of National Biography." In the United States, the "Dictionary of American Biography" was planned in the late 1920s and appeared with numerous supplements into the 1980s. It has now been displaced by the "American National Biography" as well as numerous smaller historical encyclopedias that give thorough coverage to Great Persons. Bookstores do a thriving business in biographies, which sell far more copies than the esoteric monographs based on post-structuralism, cultural, racial or gender history. Michael Holroyd says the last forty years “may be seen as a golden age of biography” but nevertheless calls it the "shallow end of history." Nicolas Barker argues that “more and more biographies command an ever larger readership,” as he speculates that biography has come “to express the spirit of our age."
British debates.
Marxist historian E. H. Carr developed a controversial theory of history in his 1961 book "What Is History?", which proved to be one of the most influential books ever written on the subject. He presented a middle-of-the-road position between the empirical or (Rankean) view of history and R. G. Collingwood's idealism, and rejected the empirical view of the historian's work being an accretion of "facts" that he or she has at their disposal as nonsense. He maintained that there is such a vast quantity of information that the historian always chooses the "facts" he or she decides to make use of. In Carr's famous example, he claimed that millions had crossed the Rubicon, but only Julius Caesar's crossing in 49 BC is declared noteworthy by historians. For this reason, Carr argued that Leopold von Ranke's famous dictum "wie es eigentlich gewesen" (show what actually happened) was wrong because it presumed that the "facts" influenced what the historian wrote, rather than the historian choosing what "facts of the past" he or she intended to turn into "historical facts". At the same time, Carr argued that the study of the facts may lead the historian to change his or her views. In this way, Carr argued that history was "an unending dialogue between the past and present".
Carr is held by some critics to have had a deterministic outlook in history. Others have modified or rejected this use of the label 'determinist'. He took a hostile view of those historians who stress the workings of chance and contingency in the workings of history. In Carr's view, no individual is truly free of the social environment in which they live, but contended that within those limitations, there was room, albeit very narrow room for people to make decisions that have an impact on history. Carr emphatically contended that history was a social science, not an art, because historians like scientists seek generalizations that helped to broaden the understanding of one's subject.
One of Carr's most forthright critics was Hugh Trevor-Roper, who argued that Carr's dismissal of the "might-have-beens of history" reflected a fundamental lack of interest in examining historical causation. Trevor-Roper asserted that examining possible alternative outcomes of history was far from being a "parlour-game" was rather an essential part of the historians' work, as only by considering all possible outcomes of a given situation could a historian properly understand the period.
The controversy inspired Sir Geoffrey Elton to write his 1967 book "The Practice of History". Elton criticized Carr for his "whimsical" distinction between the "historical facts" and the "facts of the past", arguing that it reflected "...an extraordinarily arrogant attitude both to the past and to the place of the historian studying it". Elton, instead, strongly defended the traditional methods of history and was also appalled by the inroads made by postmodernism. Elton saw the duty of historians as empirically gathering evidence and objectively analyzing what the evidence has to say. As a traditionalist, he placed great emphasis on the role of individuals in history instead of abstract, impersonal forces. Elton saw political history as the highest kind of history. Elton had no use for those who seek history to make myths, to create laws to explain the past, or to produce theories such as Marxism.
American approaches.
In the historiography of the United States, there were a series of major approaches in the 20th century. In 2009-2012, there were an average of 16,000 new academic history books published in the U.S. every year.
Progressive historians.
From 1910 to the 1940s, "Progressive" historiography was dominant, especially in political studies. It stressed the central importance of class conflict in American history. Important leaders included Vernon L. Parrington, Carl L. Becker, Arthur M. Schlesinger, Sr., John Hicks, and C. Vann Woodward. The movement established a strong base at the History Department at the University of Wisconsin with Curtis Nettels, William Hesseltine, Merle Curti, Howard K. Beale, Merrill Jensen, Fred Harvey Harrington (who became the university president), William Appleman Williams, and a host of graduate students. Charles A. Beard was the most prominent representative with his "Beardian" approach that reached both scholars and the general public.
In covering the Civil War, Charles and Mary Beard did not find it useful to examine nationalism, unionism, states' rights, slavery, abolition or the motivations of soldiers in battle. Instead, they proclaimed it was a:
Arthur Schlesinger, Jr. wrote the "Age of Jackson" (1945), one of the last major books from this viewpoint. Schlesinger made Jackson a hero for his successful attacks on the Second Bank of the United States. His own views were clear enough: "Moved typically by personal and class, rarely by public, considerations, the business community has invariably brought national affairs to a state of crisis and exasperated the rest of society into dissatisfaction bordering on revolt."
Consensus history.
Consensus history emphasizes the basic unity of American values and downplays conflict as superficial. It was especially attractive in the 1950s and 1960s. Prominent leaders included Richard Hofstadter, Louis Hartz, Daniel Boorstin, Allan Nevins, Clinton Rossiter, Edmund Morgan, and David M. Potter. In 1948 Hofstadter made a compelling statement of the consensus model of the American political tradition:
New Left history.
Consensus history was rejected by New Left viewpoints that attracted a younger generation of radical historians in the 1960s. These viewpoints stress conflict and emphasize the central roles of class, race and gender. The history of dissent, and the experiences of racial minorities and disadvantaged classes was central to the narratives produced by New Left historians.
New social and political history.
Social history, often called the new social history, is a broad branch that studies the experiences of ordinary people in the past. In its "golden age" it was a major growth field in the 1960s and 1970s, and still is well represented in history departments. However after 1980 the "cultural turn" directed the next generation to new topics. In the two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. It growth was inspired by the social sciences, computers, statistics, new data sources such as individual census information, and summer training programs at the Newberry Library and the University of Michigan. The "New Political History" saw the application of social history methods to politics, as the focus shifted from politicians and legislation to voters and elections.
The Cultural turn.
The "cultural turn" of the 1980s and 1990s affected scholars in most areas of history. Inspired largely by anthropology, it turned away from leaders, ordinary people and famous events to look at the use of language and cultural symbols to represent the changing values of society.
The British historian Peter Burke finds that cultural studies has numerous spinoffs, or topical themes it has strongly influenced. The most important include gender studies and postcolonial studies, as well as memory studies, and film studies.
Diplomatic historian Melvyn P. Leffler finds that the problem with the "cultural turn" is that the culture concept is imprecise, and may produce excessively broad interpretations, because it:
Memory studies.
Memory studies is a new field, focused on how nations and groups (and historians) construct and select their memories of the past in order to celebrate (or denounce) key features, thus making a statement of their current values and beliefs. Historians have played a central role in shaping the memories of the past as their work is diffused through popular history books and school textbooks. French sociologist Maurice Halbwachs, opened the field with "La mémoire collective" (Paris: 1950).
Many historians examine the how the memory of the past has been constructed, memorialized or distorted. Historians examine how legends are invented. For example there are numerous studies of the memory of atrocities from World War II, notably the Holocaust in Europe and Japanese behavior in Asia. British historian Heather Jones argues that the historiography of the First World War in recent years has been reinvigorated by the cultural turn. Scholars have raised entirely new questions regarding military occupation, radicalization of politics, race, and the male body.
Representative of recent scholarship is a collection of studies on the "Dynamics of Memory and Identity in Contemporary Europe" SAGE has published the scholarly journal "Memory Studies" since 2008, and the book series ‘Memory Studies’ was launched by Palgrave Macmillan in 2010 with 5-10 titles a year.
World history.
World history, as a distinct field of historical study, emerged as an independent academic field in the 1980s. It focused on the examination of history from a global perspective and looked for common patterns that emerged across all cultures. The basic thematic approach of this field was to analyse two major focal points: integration - (how processes of world history have drawn people of the world together), and difference - (how patterns of world history reveal the diversity of the human experience).
Arnold J. Toynbee's ten-volume "A Study of History", written between 1933 and 1954, was an important influence on this developing field. He took a comparative topical approach to 26 independent civilizations and demonstrated that they displayed striking parallels in their origin, growth, and decay. He proposed a universal model to each of these civilizations, detailing the stages through which they all pass: genesis, growth, time of troubles, universal state, and disintegration. With his endless output of papers, articles, speeches and presentations, and numerous books translated into many languages, Toynbee was perhaps the world’s most read and discussed scholar in the 1940s and 1950s. Yet Toynbee's work lost favor among both the general public and scholars by the 1960s, due to the religious and spiritual outlook that permeates the largest part of his work. His work has seldom been read or cited in recent decades.
Chicago historian William H. McNeill wrote "The Rise of the West" (1965) to improve upon Toynbee by showing how the separate civilizations of Eurasia interacted from the very beginning of their history, borrowing critical skills from one another, and thus precipitating still further change as adjustment between traditional old and borrowed new knowledge and practice became necessary. He then discusses the dramatic effect of Western civilization on others in the past 500 years of history. McNeill took a broad approach organized around the interactions of peoples across the globe. Such interactions have become both more numerous and more continual and substantial in recent times. Before about 1500, the network of communication between cultures was that of Eurasia. The term for these areas of interaction differ from one world historian to another and include "world-system" and "ecumene." His emphasis on cultural fusions had a major impact on historical theory.
Scholarly journals.
The historical journal, a forum where academic historians could exchange ideas and publish newly discovered information, came into being in the 19th century. The early journals were similar to those for the physical sciences, and were seen as a means for history to become more professional. Journals also helped historians to establish various historiographical approaches, the most notable example of which was "Annales. Économies. Sociétés. Civilisations.", a publication of the Annales School in France. Journals now typically have one or more editors and associate editors, an editorial board, and a pool of scholars to whom articles that are submitted are sent for confidential evaluation. The editors will send out new books to recognized scholars for reviews that usually run 500 to 1000 words. The vetting and publication process often takes months or longer. Publication in a prestigious journal (which accept 10% or fewer of the articles submitted) is an asset in the academic hiring and promotion process. Publication demonstrates that the author is conversant with the scholarly field. Page charges and fees for publication are uncommon in history. Journals are subsidized by universities or historical societies, scholarly associations, and subscription fees from libraries and scholars. Increasingly they are available through library pools that allow many academic institutions to pool subscriptions to online versions. Most libraries have a system for obtaining specific articles through inter-library loan.
Narrative.
According to Lawrence Stone, narrative has traditionally been the main rhetorical device used by historians. In 1979, at a time when the new Social History was demanding a social-science model of analysis, Stone detected a move back toward the narrative. Stone defined narrative as follows: it is organized chronologically; it is focused on a single coherent story; it is descriptive rather than analytical; it is concerned with people not abstract circumstances; and it deals with the particular and specific rather than the collective and statistical. He reported that, "More and more of the 'new historians' are now trying to discover what was going on inside people's heads in the past, and what it was like to live in the past, questions which inevitably lead back to the use of narrative."
Historians committed to a social science approach, however, have criticized the narrowness of narrative and its preference for anecdote over analysis, and its use of clever examples rather than statistically verified empirical regularities.
Topics studied.
Some of the common topics in historiography are:
Approaches.
How a historian approaches historical events is one of the most important decisions within historiography. It is commonly recognised by historians that, in themselves, individual historical facts dealing with names, dates and places are not particularly meaningful. Such facts will only become useful when assembled with other historical evidence, and the process of assembling this evidence is understood as a particular historiographical approach.
The most influential historiographical approaches are:
Scholars typically specialize in a particular theme and region. see:
Related fields.
Important related fields include:

</doc>
<doc id="13288" url="http://en.wikipedia.org/wiki?curid=13288" title="Holland">
Holland

Holland is a region and former province on the western coast of the Netherlands. The name "Holland" is also frequently used to informally refer to the whole of the country of the Netherlands. This usage is commonly accepted in other countries, but in the Netherlands and particularly in other regions of the country it could be found undesirable or even insulting.
From the 10th to the 16th century, Holland proper was a unified political region within the Holy Roman Empire as a county ruled by the Counts of Holland. By the 17th century, Holland had risen to become a maritime and economic power, dominating the other provinces of the newly independent Dutch Republic.
Today, the former County of Holland roughly consists of the two Dutch provinces of North Holland and South Holland, which together include the Netherlands' three largest cities: the capital city of Amsterdam; Rotterdam, home of Europe's largest port; and the seat of government of The Hague.
Etymology and terminology.
The name "Holland" first appeared in sources in 866 for the region around Haarlem, and by 1064 was being used as the name of the entire county. By this time, the inhabitants of Holland were referring to themselves as "Hollanders". "Holland" is derived from the Middle Dutch term "holtland" ("wooded land"). This spelling variation remained in use until around the 14th century, at which time the name stabilised as "Holland" (alternative spellings at the time were "Hollant" and "Hollandt"). A popular folk etymology holds that "Holland" is derived from "hol land" ("hollow land") and was inspired by the low-lying geography of Holland.
The proper name of the area in both Dutch and English is "Holland". Holland is a part of the Netherlands. "Holland" is informally used in English and other languages, including sometimes the Dutch language itself, to mean the whole of the modern country of the Netherlands. This example of "pars pro toto" or synecdoche is similar to the tendency in the past to refer to the United Kingdom as "England".
The people of Holland are referred to as "Hollanders" in both Dutch and English. Today this refers specifically to people from the current provinces of North Holland and South Holland. Strictly speaking, the term "Hollanders" does not refer to people from the other provinces in the Netherlands, but colloquially "Hollanders" is sometimes used in this wider sense.
In Dutch, the Dutch word ""Hollands" is the adjectival form for "Holland". The Dutch word "Hollands" is also colloquially and occasionally used by some Dutch people in the sense of "Nederlands"" (Dutch), but then often with the intention of contrasting with other types of Dutch people or language, for example Limburgish, the Belgian form of the Dutch language ("Flemish"), or even any southern variety of Dutch within the Netherlands itself.
However, in English there is no commonly used adjective for "Holland". "Dutch" refers to the Netherlands as a whole, not just the region of Holland. "Hollands" is ordinarily expressed in English in two ways:
The following usages apply in certain limited situations but do not ordinarily serve as the English equivalent of the commonly used Dutch adjective "Hollands".
History.
Each of the provinces in the Netherlands has a history that deserves full attention on its own. However, to a certain extent at least, the history of Holland is the history of the Netherlands, and vice versa. See the article on "History of the Netherlands" for a more detailed history. The article here focuses on those points that are specific to Holland itself or that highlight the nature of the role played by Holland in the Netherlands as a whole.
County of Holland.
Until the 9th century, the inhabitants of the area that became Holland were Frisians. The area was part of Frisia. At the end of the 9th century, Holland became a separate county in the Holy Roman Empire. The first Count of Holland known about with certainty was Dirk I, who ruled from 896 to 931. He was succeeded by a long line of counts in the House of Holland (who were in fact known as counts of Frisia until 1101). When John I, count of Holland, died childless in 1299, the county was inherited by John II of Avesnes, count of Hainaut. By the time of William V (House of Wittelsbach; 1354–1388) the count of Holland was also the count of Hainaut and Zealand.
At the time a part of Frisia, West Friesland, was conquered (as a result, most provincial institutions, including the States of Holland and West Frisia, would for centuries refer to "Holland and West Frisia" as a unit). The Hook and Cod wars started around this time and ended when the countess of Holland, Jacoba or Jacqueline was forced to give up Holland to the Burgundian Philip III, known as Philip the Good, in 1432.
In 1432, Holland became part of the Burgundian Netherlands and since 1477 of the Habsburg Seventeen Provinces. In the 16th century the county became the most densely urbanised region in Europe, with the majority of the population living in cities. Within the Burgundian Netherlands, Holland was the dominant province in the north; the political influence of Holland largely determined the extent of Burgundian dominion in that area. The last count of Holland was Philip III, better known as Philip II king of Spain. He was deposed in 1581 by the Act of Abjuration, although the kings of Spain continued to carry the titular appellation of Count of Holland until the Peace of Münster signed in 1648.
Dutch Republic.
In the Dutch Rebellion against the Habsburgs during the Eighty Years' War, the naval forces of the rebels, the Watergeuzen, established their first permanent base in 1572 in the town of Brill. In this way, Holland, now a sovereign state in a larger Dutch confederation, became the centre of the rebellion. It became the cultural, political and economic centre of the United Provinces (Dutch: "Verenigde Provinciën"), in the 17th century, the Dutch Golden Age, the wealthiest nation in the world. After the King of Spain was deposed as the count of Holland, the executive and legislative power rested with the States of Holland, which was led by a political figure who held the office of Grand Pensionary.
The largest cities in the Dutch Republic were in the province of Holland, such as Amsterdam, Rotterdam, Leiden, Alkmaar, The Hague, Delft, Dordrecht and Haarlem. From the great ports of Holland, Hollandic merchants sailed to and from destinations all over Europe, and merchants from all over Europe gathered to trade in the warehouses of Amsterdam and other trading cities of Holland.
Many Europeans thought of the United Provinces first as "Holland" rather than as the "Republic of the Seven United Provinces of the Netherlands". A strong impression of "Holland" was planted in the minds of other Europeans, which then was projected back onto the Republic as a whole. Within the provinces themselves, a gradual slow process of cultural expansion took place, leading to a "Hollandification" of the other provinces and a more uniform culture for the whole of the Republic. The dialect of urban Holland became the standard language.
Under French rule.
The formation of the Batavian Republic, inspired by the French revolution, led to a more centralised government. Holland became a province of a unitary state. Its independence was further reduced by an administrative reform in 1798, in which its territory was divided into several departments called "Amstel", "Delf", "Texel", and part of "Schelde en Maas".
From 1806 to 1810 Napoleon styled his vassal state, governed by his brother Louis Napoleon and shortly by the son of Louis, Napoleon Louis Bonaparte, as the "Kingdom of Holland". This kingdom encompassed much of what would become the modern Netherlands. The name reflects how natural at the time it had become to equate Holland with the non-Belgian Netherlands as a whole.
During the period the Low Countries were annexed by the French Empire and actually incorporated into France (from 1810 to 1813), Holland was divided into départements Zuyderzée, and Bouches-de-la-Meuse. From 1811 to 1813 Charles-François Lebrun, duc de Plaisance served as governor-general. He was assisted by Antoine de Celles, Goswin de Stassart and François Jean-Baptiste d'Alphonse.
Kingdom of the Netherlands.
After 1813, Holland was restored as a province of the United Kingdom of the Netherlands. Holland was divided into the present provinces North Holland and South Holland in 1840, after the Belgian Revolution of 1830. This reflected a historical division of Holland along the IJ into a Southern Quarter ("Zuiderkwartier") and a Northern Quarter ("Noorderkwartier"), but the actual division is different from the old division. From 1850, a strong process of nation formation took place, the Netherlands being culturally unified and economically integrated by a modernisation process, with the cities of Holland as its centre.
Geography.
Holland is situated in the west of the Netherlands. A maritime region, Holland lies on the North Sea at the mouths of the Rhine and the Meuse (Maas). It has numerous rivers and lakes and an extensive inland canal and waterway system. To the south is Zealand. The region is bordered on the east by the IJsselmeer and four different provinces of the Netherlands.
Holland is protected from the sea by a long line of coastal dunes. Most of the land area behind the dunes consists of polder landscape lying well below sea level. At present the lowest point in Holland is a polder near Rotterdam, which is about seven meters below sea level. Continuous drainage is necessary to keep Holland from flooding. In earlier centuries windmills were used for this task. The landscape was (and in places still is) dotted with windmills, which have become a symbol of Holland.
Holland is 7,494 square kilometres (land and water included), making it roughly 13% of the area of the Netherlands. Looking at land alone, it is 5,488 square kilometres in size. The combined population is 6.1 million.
The main cities in Holland are Amsterdam, Rotterdam and The Hague. Amsterdam is formally the capital of the Netherlands and its largest city. The Port of Rotterdam is Europe's largest and most important harbour and port. The Hague is the seat of government of the Netherlands. These cities, combined with Utrecht and other smaller municipalities, effectively form a single metroplex—a conurbation called Randstad.
The Randstad area is one of the most densely populated regions of Europe, but still relatively free of urban sprawl. There are strict zoning laws. Population pressures are enormous, property values are high, and new housing is constantly under development on the edges of the built-up areas. Surprisingly, much of the province still has a rural character. The remaining agricultural land and natural areas are highly valued and protected. Most of the arable land is used for intensive agriculture, including horticulture and greenhouse agri-businesses.
Reclamation of the land.
The land that is now Holland had never been stable. Over the millennia the geography of the region had been dynamic. The western coastline shifted up to thirty kilometres to the east and storm surges regularly broke through the row of coastal dunes. The Frisian Isles, originally joined to the mainland, became detached islands in the north. The main rivers, the Rhine and the Meuse (Maas), flooded regularly and changed course repeatedly and dramatically.
The people of Holland found themselves living in an unstable, watery environment. Behind the dunes on the coast of the Netherlands a high peat plateau had grown, forming a natural protection against the sea. Much of the area was marsh and bog. By the tenth century the inhabitants set about cultivating this land by draining it. However, the drainage resulted in extreme soil shrinkage, lowering the surface of the land by up to fifteen metres.
To the south of Holland, in Zeeland, and to the north, in Frisia, this development led to catastrophic storm floods literally washing away entire regions, as the peat layer disintegrated or became detached and was carried away by the flood water. From the Frisian side the sea even flooded the area to the east, gradually hollowing Holland out from behind and forming the Zuiderzee (the present IJsselmeer). This inland sea threatened to link up with the "drowned lands" of Zealand in the south, reducing Holland to a series of narrow dune barrier islands in front of a lagoon. Only drastic administrative intervention saved the county from utter destruction. The counts and large monasteries took the lead in these efforts, building the first heavy emergency dikes to bolster critical points. Later special autonomous administrative bodies were formed, the "waterschappen" ("water control boards"), which had the legal power to enforce their regulations and decisions on water management. As the centuries went by, they eventually constructed an extensive dike system that covered the coastline and the polders, thus protecting the land from further incursions by the sea.
However, the Hollanders did not stop there. Starting around the 16th century, they took the offensive and began land reclamation projects, converting lakes, marshy areas and adjoining mudflats into polders. This continued right into the 20th century. As a result, historical maps of mediaeval and early modern Holland bear little resemblance to the maps of today.
This ongoing struggle to master the water played an important role in the development of Holland as a maritime and economic power and in the development of the character of the people of Holland.
Culture.
Holland tends to be associated with a particular image. The stereotypical image of Holland is an artificial amalgam of tulips, windmills, clogs, cheese and traditional dress ("klederdracht"). As is the case with many stereotypes, this is far from the truth and reality of life in Holland. This can at least in part be explained by the active exploitation of these stereotypes in promotions of Holland and the Netherlands. In fact only in a few of the more traditional villages, such as Volendam and locations in the Zaan area, are the different costumes with wooden shoes still worn by some inhabitants.
The predominance of Holland in the Netherlands has resulted in regionalism on the part of the other provinces. This is a reaction to the perceived threat that Holland poses to the identities and local cultures of the other provinces. The other provinces have a strong, and often negative, image of Holland and the Hollanders, to whom certain qualities are ascribed within a mental geography, a conceptual mapping of spaces and their inhabitants. On the other hand, some Hollanders take Holland's cultural dominance for granted and treat the concepts of "Holland" and the "Netherlands" as coincidental. Consequently, they see themselves not primarily as "Hollanders", but simply as "Dutch" ("Nederlanders"). This phenomenon has been called "hollandocentrism".
Language.
The predominant language spoken in Holland is Dutch. Hollanders sometimes refer to the Dutch language as "Hollands", instead of the standard term "Nederlands". Inhabitants of Belgium and other provinces of the Netherlands refer to "Hollands" to indicate someone speaking in a Hollandic dialect, or strong accent.
Standard Dutch was historically largely based on the dialect of the County of Holland, incorporating many traits derived from the dialects of the previously more powerful Duchy of Brabant and County of Flanders. Strong dialectal variation still exists throughout the Low Countries. Today, Holland-proper is the region where the original dialects are least spoken, in many areas having been completely replaced by standard Dutch, and the Randstad has the largest influence on the developments of the standard language—with the exception of the Dutch spoken in Belgium.
Despite this correspondence between standard Dutch and the Dutch spoken in the Randstad, there are local variations within Holland itself that differ from standard Dutch. The main cities each have their own modern urban dialect, that can be considered a sociolect. A small number of people, especially in the area north of Amsterdam, still speak the original dialect of the county, Hollandic. The Hollandic dialect is present in the north: Volendam and Marken and the area around there, West Friesland and the Zaanstreek; and in a south-eastern fringe bordering on the provinces of North Brabant and Utrecht. In the south on the island of Goeree-Overflakkee, Zealandic is spoken.
New Holland.
The province of Holland gave its name to a number of colonial settlements and discovered regions that were called "Nieuw Holland" or New Holland. The most extensive of these was the island continent presently known as Australia: New Holland was first applied to Australia in 1644 by the Dutch seafarer Abel Tasman as a Latin "Nova Hollandia", and remained in international use for 190 years. On the same voyage he named New Zealand after the Dutch province of Zeeland. In the Netherlands "Nieuw Holland" would remain the usual name of the continent until the end of the 19th century; it is now no longer in use there, the Dutch name today being "Australië".

</doc>
<doc id="13305" url="http://en.wikipedia.org/wiki?curid=13305" title="History of the Republic of Turkey">
History of the Republic of Turkey

The Republic of Turkey was created after the overthrow of Sultan Mehmet VI Vahdettin by the new Republican Parliament in 1922. This new regime delivered the "coup de grâce" to the Ottoman state which had been practically wiped away from the world stage following the First World War.
Single-party period, 1923–1946.
The history of modern Turkey begins with the foundation of the republic on October 29, 1923, with Mustafa Kemal (Atatürk) as its first president. The government was formed from the Ankara-based revolutionary group, led by Mustafa Kemal Atatürk and his colleagues. The second constitution was ratified by the Grand National Assembly on April 20, 1924.
For about the next 10 years, the country saw a steady process of secular Westernization through Atatürk's Reforms, which included the unification of education; the discontinuation of religious and other titles; the closure of Islamic courts and the replacement of Islamic canon law with a secular civil code modeled after Switzerland's and a penal code modeled after the Italian Penal Code; recognition of the equality between the sexes and the granting of full political rights to women on 5 December 1934; the language reform initiated by the newly founded Turkish Language Association; replacement of the Ottoman Turkish alphabet with the new Turkish alphabet derived from the Latin alphabet; the dress law (the wearing of a fez, is outlawed); the law on family names; and many others.
Chronology of Major Kemalist Reforms: 
The first party to be established in the newly formed republic was the Women's Party (Kadınlar Halk Fırkası). It was founded by Nezihe Muhiddin and several other women but was stopped from its activities, since during the time women were not yet legally allowed to engage in politics. The actual passage to multi-party period was first attempted with the Liberal Republican Party by Ali Fethi Okyar. The Liberal Republican Party was dissolved on 17 November 1930 and no further attempt for a multi-party democracy was made until 1945. Turkey was admitted to the League of Nations in July 1932.
Atatürk's successor after his death on November 10, 1938 was İsmet İnönü. He started his term in the office as a respected figure of the Independence War but because of internal fights between power groups and external events like the World War which caused a lack of goods in the country, he lost some of his popularity and support.
World War II.
After failing in 1939 to get a defensive alliance against Germany with Britain, Turkey maintained neutrality during the war (1939–45). Ambassadors from the Axis powers and Allies intermingled in Ankara. İnönü signed a non-aggression treaty with Nazi Germany on June 18, 1941, 4 days before the Axis powers invaded the Soviet Union.
Nationalist magazines Bozrukat and Chinar Altu called for the declaration of war against the Soviet Union. In July 1942, Bozrukat published a map of Greater Turkey, which included Soviet controlled Caucasus and central Asian republics.
In August 1942, during talks with the German ambassador, Turkish prime minister Şükrü Saracoğlu stated: "The Russian problem can only be solved in case half the Russian population is exterminated."
In the summer of 1942, Turkish high command considered war with the Soviet Union almost unavoidable. An operation was planned, with Baku being the initial target.
Turkey traded with both sides and purchased arms from both sides. The Allies tried to stop German purchases of chrome (used in making better steel). Inflation was high as prices doubled.
By August 1944, the Axis was clearly losing the war and Turkey broke off relations. Only in February 1945, Turkey declared war on Germany and Japan, a symbolic move that allowed Turkey to join the future United Nations.
On October 24, 1945 Turkey signed the United Nations Charter as one of the fifty-one original members.
In 1946, İnönü's government organized multi-party elections, which were won by his party. He remained as the president of the country until 1950. He is still remembered as one of the key figures of Turkey.
Multi-party period, 1946–present.
The real multi-party period begins with the election of the Democratic Party government in 1950.
The government of Adnan Menderes (1950-1960) proved very popular at first, relaxing the restrictions on Islam and presiding over a booming economy. In the latter half of the 1950s, however, the economy began to fail and the government introduced censorship laws limiting dissent. The government became plagued by high inflation and a massive debt.
On May 27, 1960, General Cemal Gürsel led a military coup d'état, removing President Celal Bayar and Prime Minister Menderes, the second of whom was executed. The system returned to civilian control in October 1961. A fractured political system emerged in the wake of the 1960 coup, producing a series of unstable government coalitions in parliament alternating between the Justice Party of Süleyman Demirel on the right and the Republican People's Party of İsmet İnönü and Bülent Ecevit on the left.
The army issued a memorandum warning the civilian government in 1971, leading to another coup which resulted in the fall of the Demirel government and the establishment of interim governments.
In 1974, under Prime Minister Ecevit in coalition with the religious National Salvation Party, Turkey carried out an invasion of Cyprus.
The governments of the National Front, a series of coalitions between rightist parties, followed as Ecevit was not able to remain in office despite ranking first in the elections. The fractured political scene and poor economy led to mounting violence between ultranationalists and communists in the streets of Turkey's cities, resulting in some 5,000 deaths during the late 1970s.
A military coup d'état, headed by General Kenan Evren, took place in 1980. Martial law was extended from 20 to all then existing 67 provinces of Turkey. Within two years, the military returned the government to civilian hands, although retaining close control of the political scene. The political system came under one-party governance under the Motherland Party (ANAP) of Turgut Özal (Prime Minister from 1983 to 1989). The Motherland Party combined a globally oriented economic program with the promotion of conservative social values. Under Özal, the economy boomed, converting towns like Gaziantep from small provincial capitals into mid-sized economic boomtowns. Military rule began to be phased out at the end of 1983. In particular in provinces in the south-east of Turkey it was replaced by a state of emergency. In 1985 the government established village guards (local paramilitary militias) to oppose separatist Kurdish groups.
Starting in July 1987, the South-East was submitted to state of emergency legislation, a measure which lasted until November 2002. With the turn of the 1990s, political instability returned. The 1995 elections brought a short-lived coalition between Mesut Yılmaz's ANAP and the True Path Party, now with Tansu Çiller at the helm.
In 1997, the military, citing his government's support for religious policies deemed dangerous to Turkey's secular nature, sent a memorandum to Prime Minister Necmettin Erbakan requesting that he resign, which he did. This was named a postmodern coup. Shortly thereafter, the Welfare Party (RP) was banned and reborn as the Virtue Party (FP). A new government was formed by ANAP and Ecevit's Democratic Left Party (DSP) supported from the outside by the center-left Republican People's Party (CHP), led by Deniz Baykal. The DSP became the largest parliamentary party in the 1999 elections. Second place went to the far-right Nationalist Movement Party (MHP). These two parties, alongside Yılmaz's ANAP formed a government. The government was somewhat effective, if not harmonious, bringing about much-needed economic reform, instituting human rights legislation, and bringing Turkey ever closer to the European Union.
AK Party government, 2002–present.
A series of economic shocks led to new elections in 2002, bringing into power the conservative Justice and Development Party (AK Party) of the former mayor of Istanbul, Recep Tayyip Erdoğan. The political reforms of the AK Party has ensured the beginning of the negotiations with the European Union. The AK Party again won the 2007 elections, which followed the controversial August 2007 presidential election, during which AK Party member Abdullah Gül was elected President at the third round. Recent developments in Iraq (explained under positions on terrorism and security), secular and religious concerns, the intervention of the military in political issues, relations with the EU, the United States, and the Muslim world were the main issues. The outcome of this election, which brought the Turkish and Kurdish ethnic/nationalist parties (MHP and DTP) into the parliament, will affect Turkey's bid for the European Union membership, as Turkish perceptions of the current process (or lack thereof) affected the results and will continue to affect policy making in coming years.
AKP is the only government in Turkish political history that has managed to win three general elections in a row with an increasing amount of votes received in each one. The AKP has positioned itself in the midpoint of the Turkish political scene, much thanks to the stability brought by steady economic growth since they came to power in 2002. A large part of the population have welcomed the end of the political and economic instability of the 1990s, often associated with coalition governments - see Economic history of Turkey. 2011 figures showed a 9% GDP growth for Turkey.
Alleged members of a clandestine group called Ergenekon were detained in 2008 as part of a long and complex trial. Members are accused of terrorism and of plotting to overthrow the civilian government.
On 22 February 2010 more than 40 officers were arrested and formally charged with attempting to overthrow the government with respect to so-called "Sledgehammer" plot. The accused included four admirals, a general and two colonels, some of them retired, including former commanders of the Turkish navy and air force (three days later, the former commanders of the navy and air force were released).
Although the 2013 protests in Turkey started as a response against the removal of Taksim Gezi Park in Istanbul, they have sparked riots across the country in cities such as Izmir and Ankara as well.

</doc>
<doc id="13428" url="http://en.wikipedia.org/wiki?curid=13428" title="Economy of Hungary">
Economy of Hungary

The economy of Hungary is a medium-sized, upper-middle-income, structurally, politically and institutionally open economy in Central Europe and is part of the European Union's (EU) single market. The economy of Hungary experienced market liberalization in the early 1990s as part of the transition from a socialist economy to a market economy, similarly to most countries in the former Eastern Bloc. Hungary is a member of the Organisation for Economic Co-operation and Development (OECD) since 1995, a member of the World Trade Organization (WTO) since 1996, and a member of the European Union since 2004.
The private sector accounts for more than 80% of the Hungarian gross domestic product (GDP). Foreign ownership of and investment in Hungarian firms are widespread, with cumulative foreign direct investment worth more than $70 billion. Hungary's main industries are mining, metallurgy, construction materials, processed foods, textiles, chemicals (especially pharmaceuticals), and motor vehicles. Hungary's main agricultural products are wheat, corn, sunflower seed, potatoes, sugar beets; pigs, cattle, poultry, and dairy products.
History of the Hungarian economy.
Árpád Age.
At the age of feudalism the key factor of the economic life was the land. The new economic and social orders formed the private ownership of the lands. There are three forms of existence: the royal, ecclesiastical and secular private estate. The royal estate of the Árpád dynasty had evolved from the tribal lands.
The origin of the secular private holdings dates back to the conquest tribal common estates, which are increasingly in charge of the society and grows over private ownership of the becoming leaders.
However, from the founding of the state the royal gift also entered the multiplying factors secular private property line. This organization developed a feudal estate, which had two elements: the ancient estate and the possessions which were awarded by Saint Stephen I, and then the royal donations. Over the holder unrestricted right granted by the latter lineal heir almost returned to the king. In the Order of the laws changed in 1351, which abolished the nobility's possessions for free disposal. It forbidden the nobility to sale their inherited land.
The Carpathian Basin was more suitable for agriculture than large livestock grazing, and therefore increased steadily in the former weight. In the 11-12 th centuries natural farming and soil changer tillage systems met: grazing the animals, and they used the fertilized land until depletion. The most important tools for the agriculture were the plow and the ox.
Hungarian economy prior to the transition.
 
The Hungarian economy prior to World War II was primarily oriented toward agriculture and small-scale manufacturing. Hungary's strategic position in Europe and its relative high lack of natural resources also have dictated a traditional reliance on foreign trade. For instance, its largest car manufacturer, Magomobil (maker of the "Magosix"), produced a total of a few thousand units. In the early 1920s the textile industry began to expand rapidly, by 1928 it became the most important industry in the foreign trade of Hungary exporting textile goods worth more than 60 million pengős in that year. Companies like MÁVAG exported locomotives to India and South-America, its locomotive no. 601 was the largest and most powerful in Europe at the time.
From the late 1940s, the Communist government started to nationalise the industry. At first only factories with more than 100 workers were nationalized, later this limit was reduced to only 10. In the agriculture, the government started a disastrous programme of collectivization. From the early 1950s more and more new factories were built. This rapid and forced industrialization followed the standard Stalinist pattern in an effort to encourage a more self-sufficient economy. Most economic activity was conducted by state-owned enterprises or cooperatives and state farms. In 1968, Stalinist self-sufficiency was replaced by the "New Economic Mechanism", which reopened Hungary to foreign trade, gave limited freedom to the workings of the market, and allowed a limited number of small businesses to operate in the services sector. 
Although Hungary enjoyed one of the most liberal and economically advanced economies of the former Eastern Bloc, both agriculture and industry began to suffer from a lack of investment in the 1970s, and Hungary's net foreign debt rose significantly—from $1 billion in 1973 to $15 billion in 1993—due largely to consumer subsidies and unprofitable state enterprises. In the face of economic stagnation, Hungary opted to try further liberalization by passing a joint venture law, instating an income tax, and joining the International Monetary Fund (IMF) and the World Bank. By 1988, Hungary had developed a two-tier banking system and had enacted significant corporate legislation which paved the way for the ambitious market-oriented reforms of the post-communist years.
Transition to a market economy.
After the fall of communism, the former Eastern Bloc had to transition from a one-party, centrally planned economy to a market economy with a multi-party political system. With the collapse of the Soviet Union, the Eastern Bloc countries suffered a significant loss in both markets for goods, and subsidizing from the Soviet Union. Hungary, for example, "lost nearly 70% of its export markets in Eastern and Central Europe." The loss of external markets in Hungary left "800,000 unemployed people because all the unprofitable and unsalvageable factories had been closed." Another form of Soviet subsidizing that greatly affected Hungary after the fall of communism was the loss of social welfare programs. Because of the lack of subsidies and a need to reduce expenditures, many social programs in Hungary had to be cut in an attempt to lower spending. As a result, many people in Hungary suffered incredible hardships during the transition to a market economy. Following privatization and tax reductions on Hungarian businesses, unemployment suddenly rose to 12% in 1991 (it was 1.7% in 1990 ), gradually decreasing until 2001. Economic growth, after a fall in 1991 to −11.9%, gradually grew until the end of the 1990s at an average annual rate of 4.2%. With the stabilization of the new market economy, Hungary has experienced growth in foreign investment with a "cumulative foreign direct investment totaling more than $60 billion since 1989." 
The Antall government of 1990–94 began market reforms with price and trade liberation measures, a revamped tax system, and a nascent market-based banking system. By 1994, however, the costs of government overspending and hesitant privatization had become clearly visible. Cuts in consumer subsidies led to increases in the price of food, medicine, transportation services, and energy. Reduced exports to the former Soviet bloc and shrinking industrial output contributed to a sharp decline in GDP. Unemployment rose rapidly to about 12% in 1993. The external debt burden, one of the highest in Europe, reached 250% of annual export earnings, while the budget and current account deficits approached 10% of GDP. The devaluation of the currency (in order to support exports), without effective stabilization measures, such as indexation of wages, provoked an extremely high inflation rate, that in 1991 reached 35% and slightly decreased until 1994, growing again in 1995. In March 1995, the government of Prime Minister Gyula Horn implemented an austerity program, coupled with aggressive privatization of state-owned enterprises and an export-promoting exchange raw regime, to reduce indebtedness, cut the current account deficit, and shrink public spending. By the end of 1997 the consolidated public sector deficit decreased to 4.6% of GDP—with public sector spending falling from 62% of GDP to below 50%—the current account deficit was reduced to 2% of GDP, and government debt was paid down to 94% of annual export earnings. 
The Government of Hungary no longer requires IMF financial assistance and has repaid all of its debt to the fund. Consequently, Hungary enjoys favorable borrowing terms. Hungary's sovereign foreign currency debt issuance carries investment-grade ratings from all major credit-rating agencies, although recently the country was downgraded by Moody's, S&P and remains on negative outlook at Fitch. In 1995 Hungary's currency, the Forint (HUF), became convertible for all current account transactions, and subsequent to OECD membership in 1996, for almost all capital account transactions as well. Since 1995, Hungary has pegged the forint against a basket of currencies (in which the U.S. dollar is 30%), and the central rate against the basket is devalued at a preannounced rate, originally set at 0.8% per month, the Forint is now an entirely free-floating currency. The government privatization program ended on schedule in 1998: 80% of GDP is now produced by the private sector, and foreign owners control 70% of financial institutions, 66% of industry, 90% of telecommunications, and 50% of the trading sector. 
After Hungary's GDP declined about 18% from 1990 to 1993 and grew only 1%–1.5% up to 1996, strong export performance has propelled GDP growth to 4.4% in 1997, with other macroeconomic indicators similarly improving. These successes allowed the government to concentrate in 1996 and 1997 on major structural reforms such as the implementation of a fully funded pension system (partly modelled after Chile's pension system with major modifications), reform of higher education, and the creation of a national treasury. Remaining economic challenges include reducing fiscal deficits and inflation, maintaining stable external balances, and completing structural reforms of the tax system, health care, and local government financing. Recently, the overriding goal of Hungarian economic policy has been to prepare the country for entry into the European Union, which it joined in late 2004. 
Prior to the change of regime in 1989, 65% of Hungary's trade was with Comecon countries. By the end of 1997, Hungary had shifted much of its trade to the West. Trade with EU countries and the OECD now comprises over 70% and 80% of the total, respectively. Germany is Hungary's single most important trading partner. The US has become Hungary's sixth-largest export market, while Hungary is ranked as the 72nd largest export market for the U.S. Bilateral trade between the two countries increased 46% in 1997 to more than $1 billion. The U.S. has extended to Hungary most-favored-nation status, the Generalized System of Preferences, Overseas Private Investment Corporation insurance, and access to the Export-Import Bank. 
With about $18 billion in foreign direct investment (FDI) since 1989, Hungary has attracted over one-third of all FDI in central and eastern Europe, including the former Soviet Union. Of this, about $6 billion came from American companies. Foreign capital is attracted by skilled and relatively inexpensive labor, tax incentives, modern infrastructure, and a good telecommunications system. 
By 2006 Hungary’s economic outlook had deteriorated. Wage growth had kept up with other nations in the region; however, this growth has largely been driven by increased government spending. This has resulted in the budget deficit ballooning to over 10% of GDP and inflation rates predicted to exceed 6%. This prompted Nouriel Roubini, a White House economist in the Clinton administration, to state that "Hungary is an accident waiting to happen."
Privatization in Hungary.
In January 1990, the State Privatization Agency (SPA, "Állami Vagyonügynökség") was established to manage the first steps of privatization. Because of Hungary's $21.2 billion foreign debt, the government decided to sell state property instead of distributing it to the people for free. The SPA was attacked by populist groups because several companies' management had the right to find buyers and discuss sale terms with them thus "stealing" the company. Another reason for discontent was that the state offered large tax subsidies and environmental investments, which sometimes cost more than the selling price of the company. Along with the acquisition of companies, foreign investors launched many "greenfield investments".
The center-right Hungarian Democratic Forum government of 1990–1994 decided to demolish agricultural co-operatives by splitting them up and giving machinery and land to their former members. The government also introduced a Recompensation Law which offered vouchers to people who had owned land before it was nationalized in 1948. These people (or their descendants) could exchange their vouchers for land previously owned by agricultural co-operatives, who were forced to give up some of their land for this purpose.
Small stores and retail businesses were privatized between 1990 and 1994, however, greenfield investments by foreign retail companies like Tesco, Cora and IKEA had a much bigger economic impact. Many public utilities, including the national telecommunications company Matáv, the national oil and gas conglomerate MOL Group, and electricity supply and production companies were privatized as well.
Though most banks were sold to foreign investors, the largest bank, National Savings Bank (OTP), remained Hungarian-owned. 20%-20% of the shares were sold to foreign institutional investors and given to the Social Security organizations, 5% were bought by employees, and 8% was offered at the Budapest Stock Exchange.
Hungary's economy since 1990.
Reaching 1995, Hungary's fiscal indices deteriorated: foreign investment fell as well as judgement of foreign analysts on economic outlook. Due to high demand in import goods, Hungary also had a high trade deficit and budget gap, and it could not reach an agreement with the IMF, either.
After not having a minister of finance for more than a month, prime minister Gyula Horn appointed Lajos Bokros as Finance Minister on 1 March 1995. He introduced a string of austerity measures (the "Bokros Package") on 12 March 1995 which had the following key points: one-time 9% devaluation of the forint, introducing a constant sliding devaluation, 8% additional customs duty on all goods except for energy sources, limitation of growth of wages in the public sector, simplified and accelerated privatization. The package also included welfare cutbacks, including abolition of free higher education and dental service; reduced family allowances, child-care benefits, and maternity payments depending on income and wealth; lowering subsidies of pharmaceuticals, and raising retirement age.
These reforms not only increased investor confidence, but they were also supported by the IMF and the World Bank, however, they were not welcome widely by the Hungarians; Bokros broke the negative record of popularity: 9% of the population wanted to see him in an "important political position" and only 4% were convinced that the reforms would "improve the country's finances in a big way" 
In 1996, the Ministry of Finance introduced a new pension system instead of the fully state-backed one: private pension savings accounts were introduced, which were 50% social security based and 50% funded.
In 2006 Prime Minister Ferenc Gyurcsány was reelected on a platform promising economic “reform without austerity.”
However, after the elections in April 2006, the Socialist coalition under Gyurcsány unveiled a package of austerity measures which were designed to reduce the budget deficit to 3% of GDP by 2008.
Because of the austerity program, the economy of Hungary slowed down in 2007.
2008-2009 financial crisis.
Declining exports, reduced domestic consumption and fixed asset accumulation hit Hungary hard during the financial crisis of 2008, making the country enter a severe recession of -6.4%, one of the worst economic contractions in its history.
On 27 October 2008, Hungary reached an agreement with the IMF and EU for a rescue package of US$25 billion, aiming to restore financial stability and investors' confidence.
Because of the uncertainty of the crisis, banks gave less loans which led to a decrease in investment. This along with price-awareness and fear of bankruptcy led to a fallback in consumption which then increased job losses and decreased consumption even further. Inflation did not rise significantly, but real wages decreased.
The fact that the euro and the Swiss franc are worth a lot more in forints than they were before affected a lot of people. According to The Daily Telegraph, "statistics show that more than 60 percent of Hungarian mortgages and car loans are denominated in foreign currencies". After the election in 2010 of the new Fidesz-party government of Prime Minister Viktor Orbán, Hungarian banks were forced to allow the conversion of foreign-currency mortgages to the forint. The new government also nationalised $13 billion of private pension-fund assets, which could then be used to support the government debt position.
Present-day Hungarian economy.
The economy showed signs of recovery in 2011 with decreasing tax rates and a moderate 1.7 percent GDP growth.
From November 2011 to January 2012, all three major credit rating agencies downgraded Hungarian debt to a non-investment speculative grade, commonly called "junk status". In part this is because of political changes creating doubts about the independence of the Hungarian National Bank. 
European Commission President José Manuel Barroso wrote to Prime Minister Viktor Orbán stating that new central bank regulations, allowing political intervention, "seriously harm" Hungary's interests, postponing talks on a financial aid package. Orbán responded "If we don’t reach an agreement, we’ll still stand on our own feet."
The European Commission launched legal proceedings against Hungary on 17 January 2012. The procedures concern Hungary’s central bank law, the retirement age for judges and prosecutors and the independence of the data protection office, respectively. One day later Orbán indicated in a letter his willingness to find solutions to the problems raised in the infringement proceedings. On 18 January he participated in plenary session of the European Parliament which also dealt with the Hungarian case. He said "Hungary has been renewed and reorganised under European principles". He also said that the problems raised by the European Union can be resolved “easily, simply and very quickly”. He added that none of the EC’s objections affected Hungary’s new constitution.
Physical properties.
Natural resources.
Hungary's total land area is 93,030 km2 along with 690 km2 of water surface area which altogether makes up 1% of Europe's area.
Nearly 75% of Hungary's landscape consists of flat plains. Additional 20% of the country's area consists of foothills whose altitude is 400 m at the most; higher hills and water surface makes up the remaining 5%.
The two flat plains that take up three quarters of Hungary's area are the Great Hungarian Plain and the Little Hungarian Plain. Hungary's most significant natural resource is arable land. About 83% of the country's total territory is suitable for cultivation; of this portion, 75% (around 50% of the country's area) is covered by arable land, which is an outstanding ratio compared to other EU countries. Hungary lacks extensive domestic sources of energy and raw materials needed for further industrial development.
19% of the country is covered by forests. These are located mainly in the foothills such as the North Hungarian and the Transdanubian Mountains, and the Alpokalja. The composition of forests is various; mostly oak or beech, but the rest include fir, willow, acacia and plane.
In European terms, Hungary's underground water reserve is one of the largest. Hence the country is rich in brooks and hot springs as well as medicinal springs and spas; as of 2003, there are 1250 springs that provide water warmer than 30 degrees C. 90% of Hungary's drinking water is mostly retrieved from such sources.
The major rivers of Hungary are the Danube and the Tisza. The Danube also flows through parts of Germany, Austria, Slovakia, Serbia, and Romania. It is navigable within Hungary for 418 km. The Tisza River is navigable for 444 km in the country. Hungary has three major lakes. Lake Balaton, the largest, is 78 km long and from 3 to 14 km wide, with an area of 592 km2. Lake Balaton is Central Europe's largest lake and a prosperous tourist spot and recreation area. Its shallow waters offer summer bathing and during the winter its frozen surface provides facilities for winter sports. Smaller bodies of water include Lake Velence (26 km2) in Fejér County and Lake Fertő (82 km2 within Hungary).
Infrastructure.
Transport.
Hungary has 31 058 km of roads and motorways of 1118 km. The total length of motorways has doubled in the last ten years with the most (106) kilometers built in 2006. Budapest is directly connected to the Austrian, Slovenian, Croatian and Serbian borders via motorways.
Due to its location and geographical features, several transport corridors cross Hungary. Pan-European corridors no. IV, V, and X, and European routes no. E60, E71, E73, E75, and E77 go through Hungary. Thanks to its radial road system, all of these routes touch Budapest.
There are five international, four domestic, four military and several non-public airports in Hungary. The largest airport is the Budapest Ferihegy International Airport (BUD) located at the southeastern border of Budapest. In 2008, the airport had 3,866,452 arriving and 3,970,951 departing passengers.
In 2006, the Hungarian railroad system was 7685 km long, 2791 km of it electrified.
Public utilities.
Electricity is available in every settlement in Hungary.
Piped gas is available in 2873 settlements, 91.1% of all of them. To avoid gas shortages due to Ukrainian pipeline shutdowns like the one in January 2009, Hungary participates both in the Nabucco and the South Stream gas pipeline projects. Hungary also has strategical gas reserves: the latest reserve of 1.2 billion cubic meters was opened in October 2009.
In 2008, 94.9% of households had running water. Though it is the responsibility of municipal governments to provide people with healthy water supply, the Hungarian government and the European Union offer subsidies to those who wish to develop water supplies or sewage systems. Partly because of these subsidies, 71.3% of all dwellings are connected to the sewage system, up from 50,1% in 2000.
Internet penetration has been rising significantly over the past few years: the ratio of households having an internet connection has risen from 22.1% (49% of which was broadband) in 2005 to 48.4% (87.3% of which was broadband) in 2008.
The Ministry of Economy and Transport introduced the eHungary program in 2004 aiming to provide every person in Hungary with internet access by setting up "eHungary points" in public spaces like libraries, schools and cultural centers. The program also includes "the introduction of the eCounsellor network – a service through which professionals provide assistance for citizens in the effective usage of electronic information, services and knowledge".
Sectors.
Agriculture.
Agriculture accounted for 4.3% of GDP in 2008 and along with the food industry occupied roughly 7.7% of the labor force. These two figures represent only the primary agricultural production: along with related businesses, agriculture makes up about 13% of the GDP. Hungarian agriculture is virtually self-sufficient and due to traditional reasons export-oriented: exports related to agriculture make up 20-25% of the total. About half of Hungary’s total land area is agricultural area under cultivation; this ratio is prominent among other EU members. This is due to the country's favourable conditions including continental climate and the plains that make up about half of Hungary’s landscape. The most important crops are wheat, corn, sunflower, potato, sugar beet, canola and a wide variety of fruits (notably apple, peach, pear, grape, watermelon, plum etc.). Hungary has several wine regions producing among others the worldwide famous white dessert wine Tokaji and the red Bull’s Blood. Another traditional world-famous alcoholic drink is the fruit brandy "pálinka".
Mainly cattle, pigs, poultry and sheep are raised in the country. The livestock includes the Hungarian Grey Cattle which is a major tourist attraction in the Hortobágy National Park. An important component of the country’s gastronomic heritage is foie gras with about 33000 farmers engaged in the industry. Hungary is the second largest world producer and the biggest exporter of foie gras (exporting mainly to France).
Another symbol of Hungarian agriculture and cuisine is the "paprika" (both sweet and hot types). The country is one of the leading paprika producers of the world with Szeged and Kalocsa being the centres of production.
Health care.
Hungary has a tax-funded universal healthcare system, organized by the state-owned National Healthcare Fund (Hungarian: "Országos Egészségbiztosítási Pénztár (OEP)"). Health insurance is not directly paid for by children, mothers or fathers with baby, students, pensioners, people with socially poor background, handicapped people (including physical and mental disorders), priests and other church employees. Health in Hungary can be described with a rapidly growing life expectancy and a very low infant mortality rate (4.9 per 1,000 live births in 2012). Hungary spent 7.4% of the GDP on health care in 2009 (it was 7.0% in 2000), lower than the average of the OECD. Total health expenditure was 1,511 US$ per capita in 2009, 1,053 US$ governmental-fund (69.7%) and 458 US$ private-fund (30.3%).
Industry.
The main sectors of Hungarian industry are heavy industry (mining, metallurgy, machine and steel production), energy production, mechanical engineering, chemicals, food industry and automobile production. The industry is leaning mainly on processing industry and (including construction) accounted for 29,32% of GDP in 2008. Due to the sparse energy and raw material resources, Hungary is forced to import most of these materials to satisfy the demands of the industry. Following the transition to market economy, the industry underwent restructuring and remarkable modernization. The leading industry is machinery, followed by chemical industry (plastic production, pharmaceuticals), while mining, metallurgy and textile industry seemed to be losing importance in the past two decades. In spite of the significant drop in the last decade, food industry is still giving up to 14% of total industrial production and amounts to 7-8% of the country's exports.
Nearly 50% of energy consumption is dependent on imported energy sources. Gas and oil are transported through pipelines from Russia forming 72% of the energy structure, while nuclear power produced by the nuclear power station of Paks accounts for 12%.
Automobile production.
Hungary is a favoured destination of foreign investors of automotive industry resulting in the presence of General Motors (Szentgotthárd), Magyar Suzuki (Esztergom), Mercedes-Benz (Kecskemét), and Audi factory (Győr) in Central Europe.
17% of the total Hungarian exports comes from the exports of Audi, Opel and Suzuki. The sector employs about 90.000 people in more than 350 car component manufacturing companies.
Audi has built the largest engine manufacturing plant of Europe (third largest in the world) in Győr becoming Hungary's largest exporter with total investments reaching over € 3,300 million until 2007. Audi's workforce assembles the Audi TT, the Audi TT Roadster and the A3 Cabriolet in Hungary. The plant delivers engines to carmakers Volkswagen, Skoda, Seat and also to Lamborghini.
Daimler-Benz invests € 800 million ($1.2 billion) and creates up to 2,500 jobs at a new assembly plant in Kecskemét, Hungary with capacity for producing 100,000 Mercedes-Benz compact cars a year.
Opel produced 80,000 Astra and 4,000 Vectra cars from March 1992 until 1998 in Szentgotthárd, Hungary. Today, the plant produces about half million engines and cylinder heads a year.
Services.
The tertiary sector accounted for 64% of GDP in 2007 and its role in the Hungarian economy is steadily growing due to constant investments into transport and other services in the last 15 years. Located in the heart of Central-Europe, Hungary’s geostrategic location plays a significant role in the rise of the service sector as the country’s central position makes it suitable and rewarding to invest.
The total value of imports was 68,62 billion euros, the value of exports was 68,18 billion euros in 2007. The external trade deficit decreased by 12,5% since the previous year, easing down from 2,4 billion to 308 million euros in 2007. In the same year, 79% of Hungary’s export and 70% of the imports were transacted inside the EU.
Tourism.
Tourism employs nearly 150 thousand people and the total income from tourism was 4 billion euros in 2008. One of Hungary’s top tourist destinations is Lake Balaton, the largest freshwater lake in Central Europe, with a number of 1,2 million visitors in 2008. The most visited region is Budapest, the Hungarian capital attracted 3,61 million visitors in 2008.
Hungary was the world’s 24th most visited country in 2011. The Hungarian spa culture is world-famous, with thermal baths of all sorts and over 50 spa hotels located in many towns, each of which offer the opportunity of a pleasant, relaxing holiday and a wide range of quality medical and beauty treatments.
Currency.
The currency of Hungary is the Hungarian forint (HUF, Ft) since 1 August 1946. A forint consists of 100 fillérs, however, these have not been in circulation since 1999, they are only used in accounting.
There are six coins (5, 10, 20, 50, 100, 200) and six banknotes (500, 1000, 2000, 5000, 10000 and 20000). The 1 and 2 forint coins were withdrawn in 2008, yet prices remained the same as stores follow the official rounding scheme for the final price. The 200 forint note was withdrawn on 16 November 2009.
As a member of the European Union, the long term aim of the Hungarian government is to replace the forint with the euro. "See also: Fiscal policy."
You can check current exchange rates with graphs of past rates at .
Socio-economic characteristics.
Human capital.
Education in Hungary is free and compulsory from the age of 5 to 16. The state provides free pre-primary schooling for all children, 8 years of general education and 4 years of upper secondary level general or vocational education. Higher education system follows the three-cycle structure and the credit system of the bologna process. Governments aim to reach European standards and encourage international mobility by putting emphasis on digital literacy, and enhancing foreign language studies: all secondary level schools teach foreign languages and at least one language certificate is needed for the acquisition of a diploma. Over the past decade, this resulted in a drastic increase in the number of people speaking at least one foreign language.
Hungary's most prestigious universities are:
Financial sources for education are mainly provided by the state (making up 5.1-5.3% of the annual GDP). In order to improve the quality of higher education, the government encourages contributions by students and companies. Another important contributor is the EU.
The system has weaknesses, the most important being segregation and unequal access to quality education. The 2006 PISA report concluded that while students from comprehensive schools did better than the OECD average, pupils from vocational secondary schools did much worse. Another problem is of the higher education’s: response to regional and labour market needs is insufficient. Government plans include improving the career guidance system and establishing a national digital network that will enable the tracking of jobs and facilitate the integration into the labour market.
Social stratification.
As most post-communist countries, Hungary’s economy is affected by its social stratification in terms of income and wealth, age, gender and racial inequalities.
Hungary’s Gini coefficient of 0.269 ranks 11th in the world. The graph on the right shows that Hungary is close in equality to the world-leader Denmark. The highest 10% of the population gets 22.2% of the incomes. According to the business magazine "Napi Gazdaság", the owner of the biggest fortune, 300 billion HUF, is Sándor Demján. On the other hand, the lowest 10% gets 4% of the incomes. Considering the standard EU indicators (Percentage of the population living under 60% of the per capita median income), 13% of the Hungarian population is stricken by poverty. According to the Human Development Report, the country’s HPI-1 value is 2.2% (3rd among 135 countries), and its HDI value is 0.879 (43rd out of 182).
The fertility rate in Hungary, just like in many European countries, is very low: 1.34 children/women (205th in the world) Life expectancy at birth is 73.3 years., while the expected number of healthy years is 57.6 for females and 53.5 for males. The average life expectancy overall is 73.1 years.
Hungary’s GDI (gender-related development index) value of 0.879 is 100% of its HDI value (3rd best in the world). 55.5% of the female population (between 15 and 64) participate in the labour force, and the ratio of girls to boys in primary and secondary education is 99%.
Racial inequality, which strikes primarily Roma in Hungary, is a serious problem. Although the definition of the Roma identity is controversial, qualitative studies prove that the Roma employment rate decreased significantly following the fall of Communism: due to the tremendous layoffs of unskilled workers during the transition years, more than one-third of Roma were excluded from the labour market. Therefore, this ethnic conflict is inherently interconnected with the income inequalities in the country – at least two-thirds of the poorest 300,000 people in Hungary are Romas. Furthermore, ethnic discrimination is outstandingly high, 32% of Romas experience discrimination when looking for work. Consequently, new Roma entrants to the labour market are rarely able to find employment, which creates a motivation deficit and further reinforces segregation and unemployment.
Institutional quality.
Twenty years after the change of the regime, corruption remains to be a severe issue in Hungary. According to Transparency International Hungary, almost one-third of top managers claim they regularly bribe politicians. Most people (42%) in Hungary think that the sector most affected by bribery is the political party system. Bribery is common in the healthcare system in the form of gratitude payment–92% of all people think that some payment should be made to the head surgeon conducting a heart operation or an obstetrician for a child birth.
Another problem is the administrative burden: in terms of the ease of doing business, Hungary ranks 47th out of 183 countries in the world. The five days’ time required to start a new business ranks 29th, and the country is 122nd concerning the ease of paying taxes.
In accordance with the theory of the separation of powers, the judicial system is independent from the legislative and the executive branches. Consequently, courts and prosecutions are not influenced by the government. However, the legal system is slow and overburdened, which makes proceedings and rulings lengthy and inefficient. Such a justice system is hardly capable of prosecuting corruption and protecting the country’s financial interests.
State participation.
Monetary policy.
The Hungarian organization responsible for controlling the country's monetary policy is the Hungarian National Bank (Hungarian: "Magyar Nemzeti Bank", MNB) which is the central bank in Hungary. According to the Hungarian Law of National Bank (which became operative in 2001. – LVIII. Law about The Hungarian National Bank), the primary objective of MNB is to achieve and maintain price stability. This aim is in line with the European and international practice.
Price stability means achieving and maintaining a basically low, but positive inflation rate. This level is around 2-2.5% according to international observations, while the European Central Bank "aims at inflation rates of below, but close to 2% over the medium term". Since Hungary is in the process of catching up (Balassa-Samuelson effect), the long-term objective is a slightly higher figure, around 2.3-3.2%. Therefore the medium term inflation target of the Hungarian National Bank is 3%.
Concerning the exchange rate system, the floating exchange rate system is in use since 26 February 2008, as a result of which HUF is fluctuating in accordance with the effects of the market in the face of the reference currency, the euro.
The chart on the right shows forint exchange rates for the British pound (GBP), euro (EUR), Swiss franc (CHF), and the U.S. dollar (USD) from June 2008 to September 2009. It indicates that a relatively strong forint weakened since the beginning of the financial crisis, and that its value has recently taken an upward turn.
Compared to the euro the forint was at peak on 18 June 2008 when 1000 Ft was €4.36 and €1 was 229.11Ft. The forint was worth the least on 6 March 2009; this day 1000 Ft was €3.16 and €1 was 316Ft).
Compared to USD, most expensive/cheapest dates are 22 June 2008 and 6 March 2009 with 1000HUF/USD rates 6.94 and 4.01 respectively.
On March 24, 2015 the Euro was at 299.1450 and USD was at 274.1650,
Fiscal policy.
In Hungary, state revenue makes up 44% and expenditure makes up 45% of the GDP which is relatively high compared to other EU members. This can be traced back to historical reasons such as socialist economic tradition as well as cultural characteristics that endorse paternalist behaviour on the state’s part, meaning that people have a habitual reflex that make them call for state subsidies. Some economists dispute this point, claiming that expenditure ran up to today’s critical amount from 2001, during two left-wing government cycles.
Along with joining the EU the country undertook the task of joining the Eurozone as well. Therefore, the Maastricht criteria which forms the condition of joining the Eurozone acts as an authoritative guideline to Hungarian fiscal politics. Although there has been remarkable progress, recent years’ statistics still point at significant discrepancies between the criteria and fiscal indices. The target date for adapting the Euro has not been fixed, either.
General government deficit has shown a drastic decline to -3.4% (2008) from -9.2% (2006). According to an MNB forecast however, until 2011, the deficit will by a small margin fall short of the 3.0% criterion.
Another criterion that is found lacking is the ratio of gross government debt to GDP which, since 2005, exceeds the allowed 60%. According to an ESA95 figure, in 2008 the ratio increased from 65.67% to 72.61%, which primarily results from the requisition of an IMF-arranged financial assistance package.
Hungary’s balance of payments on its current account has been negative since 1995, around 6-8% in the 2000s reaching a negative peak 8.5% in 2008. Still, current account deficit will expectedly decrease in the following period, as imports will diminish compared to exports as an effect of the financial crisis.
Tax system.
In Hungary, the 1988 reform of taxes introduced a comprehensive tax system which mainly consists of central and local taxes, including a personal income tax, a corporate income tax and a value added tax. Among the total tax income the ratio of local taxes is solely 5% while the EU average is 30%. Until 2010, the taxation of an individual was progressive, determining the tax rate based on the individual’s income: with earning up to 1,900,000 forints a year, the tax was 18%, the tax on incomes above this limit was 36% since 1 of July, 2009.
 Based on the new one-rate tax regime introduced January 2011, the overall tax rate for all income-earnings bands has been 16%. According to the income-tax returns of 2008, 14,6% of taxpayers was charged for 64,5% of the total tax burdens. Before the new Corporate income tax regime, the Corporate tax was fixed at 16% of the positive rateable value, with an additional tax called solidarity tax of 4%, the measure of which is calculated based on the result before tax of the company (the solidarity tax has been in use since September, 2006). The actual rateable value might be different is the two cases. Form January 2011, under the new Corporate income tax regime the tax rate was divided into two parts (i) corporations having income before tax below 500 million HUF (appr. USD 2.5 million) was lowered to 10% and (ii) 16% remained for all other companies until 2013. After this, the unified corporate income tax rate will be 10%, irrespectively from the size of the net income before tax. The rate of value added tax in Hungary is 27%, the highest in Europe, since 1 of January, 2012.
Miscellaneous data.
"'Households with access to fixed and mobile telephony "]
Broadband penetration rate
Individuals using computer and internet
External relations.
The EU.
Hungary joined the European Union on 05/01/2004 after a successful referendum among the EU-10. The EU's free trade system helps Hungary, as it is a relatively small country and thus needs export and import.
After the accession to the EU, Hungarian workers could immediately go to work to Ireland, Sweden and the United Kingdom. Other countries imposed restrictions.
Foreign trade.
In 2007, 25% of all exports of Hungary were of high technology, which is the 5th largest ratio in the European Union after Malta, Cyprus, Ireland, and the Netherlands. The EU10 average was 17.1% and the Eurozone average was 16% in 2007.

</doc>
<doc id="13499" url="http://en.wikipedia.org/wiki?curid=13499" title="Transclusion">
Transclusion

In computer science, transclusion is the inclusion of part or all of an electronic document into one or more other documents by reference.
For example, an article giving a overview of a country might include a chart (or paragraph, image, or another type of data) describing that country's agricultural exports; that chart could be from a different article about agriculture in general, with sub-sections about agriculture in different countries. Rather than copying the included data and storing it in two places, a transclusion enables modular design, by allowing the data to be stored only once (and perhaps corrected and updated if the link type supports doing so) and to be viewed in different contexts. The reference also serves to link both articles.
Transclusion is usually performed on demand at the time one document referencing another is opened, and is normally automatic and transparent to the end user. The result appears to be a single integrated document, even though its parts were assembled on-the-fly from several separate documents which may have come from different computers located in different places.
The term was coined by hypertext pioneer Ted Nelson in 1963.
Technical considerations.
Context neutrality.
Transclusion works better when transcluded sections of text are self-contained, so that the meaning and validity of the text is independent of the context in which it appears. For example, formulations like "as explained in the previous section" are problematic, because the transcluded section may appear in a different context, causing confusion. What constitutes "context neutral" text varies, but often includes things like company information or boilerplate.
Parameterization.
Under some circumstances, and in some technical contexts, transcluded sections of text may not require strict adherence to the "context neutrality" principle, because the transcluded sections are capable of "parameterization". Parameterization implies the ability to modify certain portions or subsections of a transcluded text depending on exogenous variables that can be changed independently. This is customarily done by supplying a transcluded text with one or more "substitution placeholders". These placeholders are then replaced with the corresponding variable values prior to rendering the final transcluded output in context.
History and implementation by Project Xanadu.
Ted Nelson (who had also originated the words "hypertext" and "hypermedia") coined the term "transclusion" in his 1982 book, "Literary Machines". Part of his proposal was the idea that micropayments could be automatically exacted from the reader for all the text, no matter how many snippets of content are taken from various places.
However, according to Nelson, the concept of transclusion had already formed part of his 1965 description of hypertext; 
Nelson defines transclusion as "the same content knowably in more than one place", setting it apart from more special cases such as the inclusion of content stored in a different location (which he calls "transdelivery") or "explicit quotation which remains connected to its origins" (which he calls "transquotation").
Some hypertext systems, including Ted Nelson's own Xanadu Project, support transclusion.
Nelson has delivered a demonstration of Web transclusion, the Little Transquoter (programmed to Nelson's specification by Andrew Pam in 2004-2005). It creates a new format built on portion addresses from Web pages; when dereferenced, each portion on the resulting page remains click-connected to its original context.
Implementation on the Web.
HTTP, as a transmission protocol, has rudimentary support for transclusion via byte serving (specifying a byte range in an HTTP request message).
Transclusion can occur either before (server-side) or after (client-side) transmission. For example:
Publishers of web content may object to the transclusion of material from their own web sites into other web sites, or they may require an agreement to do so. Critics of the practice may refer to various forms of inline linking as bandwidth theft or leeching. 
Other publishers may seek specifically to have their materials transcluded into other web sites, as in the form of web advertising, or as widgets like a hit counter or web bug.
Mashups make use of transclusion to assemble resources or data into a new application, as by placing geo-tagged photos on an interactive map, or by displaying business metrics in an interactive dashboard.
Client-side HTML.
HTML defines elements for client-side transclusion of images, scripts, stylesheets, other documents, and other types of media. 
Through techniques such as Ajax, scripts associated with an HTML document can instruct a web browser to modify the document in-place. Such scripts may transclude elements or documents from a server after the web browser has rendered the page, in response to user input or changing conditions, for example.
Future versions of HTML may support deeper transclusion of portions of documents using XML technologies such as entities, XPointer document referencing, and XSLT manipulations. (XPointer is patent-encumbered.)
An interesting use of Transclusion is found in the single-page application TiddlyWiki, http://tiddlywiki.com/#Transclusion.
Proxy servers may employ transclusion to reduce redundant transmissions of commonly-requested resources.
A popular Front End Framework known as AngularJS developed and maintained by Google has a directive callend ng-transclude that marks the insertion point for the transcluded DOM of the nearest parent directive that uses transclusion.
Server-side transclusion.
Transclusion can be accomplished on the server side, as through Server Side Includes and markup entity references resolved by the server software. It is a feature of substitution templates. 
Transclusion of source code.
The transclusion of source code into software design or reference materials allows the source code to be presented within the document, but not interpreted as part of the document, preserving the semantic consistency of the inserted code in relation to its source codebase.

</doc>
<doc id="13501" url="http://en.wikipedia.org/wiki?curid=13501" title="Source tracking">
Source tracking

Source tracking pertains to the ability of some hypertext systems to rigorously track the exact source of every document or partial document included in the system; that is, they remember who entered the information, when it was entered, when it was updated and by whom, and so on. This allows determining the exact history of every document (and even small parts of documents).
Present HTML and HTTP do not have this feature, but certain systems on the World Wide Web (such as WikiWiki and Everything Engine) may have limited versions of the capability.

</doc>
<doc id="13509" url="http://en.wikipedia.org/wiki?curid=13509" title="H. P. Lovecraft">
H. P. Lovecraft

Howard Phillips Lovecraft (; August 20, 1890 – March 15, 1937), known as H. P. Lovecraft, was an American author who achieved posthumous fame through his influential works of horror fiction. Virtually unknown and only published in pulp magazines before he died in poverty, he is now regarded as one of the most significant 20th-century authors in his genre.
Lovecraft was born in Providence, Rhode Island, where he spent most of his life. His father was confined to a mental institution when Lovecraft was three years old. His grandfather, a wealthy businessman, enjoyed storytelling and was an early influence. Intellectually precocious but sensitive, Lovecraft began composing rudimentary horror tales by the age of eight, but suffered from overwhelming feelings of anxiety. He encountered problems with classmates in school, and was kept at home by his highly strung and overbearing mother for illnesses that may have been psychosomatic. In high school, Lovecraft was able to better connect with his peers and form friendships. He also involved neighborhood children in elaborate make-believe projects, only regretfully ceasing the activity at seventeen years old. Despite leaving school in 1908 without graduating—he found mathematics particularly difficult—Lovecraft had developed a formidable knowledge of his favored subjects, such as history, linguistics, chemistry, and astronomy.
Although he seems to have had some social life, attending meetings of a club for local young men, Lovecraft, in early adulthood, was established in a reclusive "nightbird" lifestyle without occupation or pursuit of romantic adventures. In 1913 his conduct of a long running controversy in the letters page of a story magazine led to his being invited to participate in an amateur journalism association. Encouraged, he started circulating his stories; he was 31 at the time of his first publication in a professional magazine. Lovecraft contracted a marriage to an older woman he had met at an association conference. By age 34, he was a regular contributor to the newly founded "Weird Tales" magazine; he turned down an offer of the editorship.
Lovecraft returned to Providence from New York in 1926 and, over the next nine months, he produced some of his most celebrated tales, including "The Call of Cthulhu", canonical to the "Cthulhu Mythos". Never able to support himself from earnings as author and editor, Lovecraft saw commercial success increasingly elude him in this latter period, partly because he lacked the confidence and drive to promote himself. He subsisted in progressively straitened circumstances in his last years; an inheritance was completely spent by the time he died at the age of 46.
Early life.
Family.
Lovecraft was born on August 20, 1890, in his family home at 194 (later 456) Angell Street in Providence, Rhode Island. (The house was demolished in 1961.) He was the only child of Winfield Scott Lovecraft, a traveling salesman of jewelry and precious metals, and Sarah Susan Phillips Lovecraft, who could trace her ancestry to the Massachusetts Bay Colony in 1631. Both of his parents were of entirely English ancestry, all of which had been in New England since the colonial period. His parents married when they were in their thirties, unusually late in life for the time period. In 1893, when Lovecraft was three, his father became acutely psychotic and was placed in the Providence psychiatric institution, Butler Hospital, where he remained until his death in 1898. H. P. Lovecraft maintained throughout his life that his father had died in a condition of paralysis brought on by "nervous exhaustion". Although it has been suggested his father's mental illness may have been caused by syphilis, neither the younger Lovecraft nor his mother (who also died in Butler Hospital) seems to have shown signs of being infected with the disease.
After his father's hospitalization, Lovecraft was raised by his mother, his two aunts (Lillian Delora Phillips and Annie Emeline Phillips), and his maternal grandfather, Whipple Van Buren Phillips, an American businessman. All five resided together in the family home. Lovecraft was a prodigy, reciting poetry at the age of three, and writing complete poems by six. His grandfather encouraged his reading, providing him with classics such as "The Arabian Nights", "Bulfinch's Age of Fable", and children's versions of the "Iliad" and the "Odyssey". His grandfather also stirred the boy's interest in the weird by telling him his own original tales of Gothic horror.
Upbringing.
Lovecraft was frequently ill as a child. Because of his sickly condition, he barely attended school until he was eight years old, and then was withdrawn after a year. He read voraciously during this period and became especially enamored of chemistry and astronomy. He produced several hectographed publications with a limited circulation, beginning in 1899 with "The Scientific Gazette". Four years later, he returned to public school at Hope High School. Beginning in his early life, Lovecraft is believed to have suffered from night terrors, a form of parasomnia; he believed himself to be assaulted at night by horrific "night gaunts". Much of his later work is thought to have been directly inspired by these terrors. (Indeed, "Night Gaunts" became the subject of a poem he wrote of the same name, in which they were personified as devil-like creatures without faces.)
His grandfather's death in 1904 greatly affected Lovecraft's life. Mismanagement of his grandfather's estate left his family in a poor financial situation, and they were forced to move into much smaller accommodations at 598 (now a duplex at 598–600) Angell Street. In 1908, prior to his high school graduation, he said to have suffered what he later described as a "nervous breakdown", and consequently never received his high school diploma (although he maintained for most of his life that he did graduate). S. T. Joshi suggests in his biography of Lovecraft that a primary cause for this breakdown was his difficulty in higher mathematics, a subject he needed to master to become a professional astronomer.
Adulthood.
Reclusion.
The adult Lovecraft was gaunt with dark eyes set in a very pale face (he rarely went out before nightfall). For five years after leaving school, he lived an isolated existence with his mother, writing primarily poetry without seeking employment or new social contacts. This changed in 1913 when he wrote a letter to "The Argosy", a pulp magazine, complaining about the insipidness of the love stories in the publication by writer Fred Jackson. The ensuing debate in the magazine's letters column caught the eye of Edward F. Daas, president of the United Amateur Press Association (UAPA), who invited Lovecraft to join the organization in 1914.
Writing.
The UAPA reinvigorated Lovecraft and incited him to contribute many poems and essays; in 1916, his first published story, "The Alchemist", appeared in the "United Amateur". The earliest commercially published work came in 1922, when he was thirty-one. By this time he had begun to build what became a huge network of correspondents. His lengthy and frequent missives would make him one of the great letter writers of the century. Among his correspondents were Robert Bloch ("Psycho"), Clark Ashton Smith, and Robert E. Howard ("Conan the Barbarian" series). Many former aspiring authors later paid tribute to his mentoring and encouragement through the correspondence.
His oeuvre is sometimes seen as consisting of three periods: an early Edgar Allan Poe influence; followed by a Lord Dunsany inspired Dream Cycle; and finally the "Cthulhu Mythos" stories. However, many distinctive ideas and entities present in the third period were introduced in the earlier works, such as the 1917 story "Dagon", and the threefold classification is partly overlapping.
Death of mother.
In 1919, after suffering from hysteria and depression for a long period of time, Lovecraft's mother was committed to the mental institution—Butler Hospital—where her husband had died. Nevertheless, she wrote frequent letters to Lovecraft, and they remained close until her death on May 24, 1921, the result of complications from gall bladder surgery.
Marriage and New York.
A few days after his mother's death, Lovecraft attended a convention of amateur journalists in Boston, Massachusetts, where he met and became friendly with Sonia Greene, owner of a successful hat shop and seven years his senior. Lovecraft's aunts disapproved of the relationship. Lovecraft and Greene married on March 3, 1924, and relocated to her Brooklyn apartment; she thought he needed to get out of Providence in order to flourish and was willing to support him financially. Greene, who had been married before, later said Lovecraft had performed satisfactorily as a lover, though she had to take the initiative in all aspects of the relationship. She attributed Lovecraft's passive nature to a stultifying upbringing by his mother. Lovecraft's weight increased to 90 kg (200 lb) on his wife's home cooking.
He was enthralled by New York, and, in what was informally dubbed the Kalem Club, he acquired a group of encouraging intellectual and literary friends who urged him to submit stories to "Weird Tales"; editor Edwin Baird accepted many otherworldly 'Dream Cycle' Lovecraft stories for the ailing publication, though they were heavily criticized by a section of the readership. Established informally some years before Lovecraft lived in New York, the core Kalem Club members were boys' adventure novelist Henry Everett McNeil; the lawyer and anarchist writer James Ferdinand Morton, Jr.; and the poet Reinhardt Kleiner. In 1925 these four regular attendees were joined by Lovecraft along with his protégé Frank Belknap Long, bookseller George Willard Kirk, and Lovecraft's close friend Samuel Loveman. Loveman was Jewish, but was unaware of Lovecraft's nativist attitudes. Conversely, it has been suggested Lovecraft, who disliked mention of sexual matters, was unaware that Loveman and some of his other friends were homosexual.
Financial difficulties.
Not long after the marriage, Greene lost her business and her assets disappeared in a bank failure; she also became ill. Lovecraft made efforts to support his wife through regular jobs, but his lack of previous work experience meant he lacked proven marketable skills. After a few unsuccessful spells as a low level clerk, his job-seeking became desultory. The publisher of "Weird Tales" attempted to put the loss-making magazine on a business footing and offered the job of editor to Lovecraft, who declined, citing his reluctance to relocate to Chicago; "think of the tragedy of such a move for an aged antiquarian," the 34-year-old writer declared. Baird was replaced with Farnsworth Wright, whose writing Lovecraft had criticized. Lovecraft's submissions were often rejected by Wright. (This may have been partially due to censorship guidelines imposed in the aftermath of a "Weird Tales" story that hinted at necrophilia, although after Lovecraft's death Wright accepted many of the stories he had originally rejected.)
Red Hook.
Greene, moving where the work was, relocated to Cincinnati, and then to Cleveland; her employment required constant travel. Added to the daunting reality of failure in a city with a large immigrant population, Lovecraft's single room apartment in the run down area of Red Hook was burglarized, leaving him with only the clothes he was wearing. In August 1925 he wrote "The Horror at Red Hook" and "He", in the latter of which the narrator says "My coming to New York had been a mistake; for whereas I had looked for poignant wonder and inspiration ... I had found instead only a sense of horror and oppression which threatened to master, paralyze, and annihilate me". It was at around this time he wrote the outline for "The Call of Cthulhu" with its theme of the insignificance of all humanity. In the bibliographical study ', Michel Houellebecq suggested that the misfortunes fed Lovecraft's central motivation as a writer, which he said was racial resentment. With a weekly allowance Greene sent, Lovecraft moved to a working class area of Brooklyn Heights where he subsisted in a tiny apartment. He had lost 40 lb of bodyweight by 1926, when he left for Providence.
Return to Providence.
Back in Providence, Lovecraft lived in a "spacious brown Victorian wooden house" at 10 Barnes Street until 1933. The same address is given as the home of Dr. Willett in Lovecraft's "The Case of Charles Dexter Ward". The period beginning after his return to Providence—the last decade of his life—was Lovecraft's most prolific; in that time he produced short stories, as well as his longest work of fiction "The Case of Charles Dexter Ward" and "At the Mountains of Madness". He frequently revised work for other authors and did a large amount of ghost-writing, including "The Mound", "Winged Death", "The Diary of Alonzo Typer". Client Harry Houdini was laudatory, and attempted to help Lovecraft by introducing him to the head of a newspaper syndicate. Plans for a further project were ended by Houdini's death.
Although he was able to combine his distinctive style (allusive and amorphous description by horrified though passive narrators) with the kind of stock content and action Weird Tales's editor wanted—Wright paid handsomely to snap up "The Dunwich Horror" which proved very popular with readers—Lovecraft increasingly produced work that brought him no remuneration. Affecting a calm indifference to the reception of his works, Lovecraft was in reality extremely sensitive to criticism and easily precipitated into withdrawal. He was known to give up trying to sell a story after it had been once rejected. Sometimes, as with "The Shadow Over Innsmouth" (which included a rousing chase that supplied action) he wrote a story that might have been commercially viable, but did not try to sell it. Lovecraft even ignored interested publishers. He failed to reply when one inquired about any novel Lovecraft might have ready, although he had completed such a work: "The Case of Charles Dexter Ward;" it was never typed up.
Last years.
Throughout his life, selling stories and paid literary work for others did not provide enough to cover Lovecraft's basic expenses. Living frugally, he subsisted on an inheritance that was nearly depleted by the time of his last years. He sometimes went without food to afford the cost of mailing letters. Eventually, he was forced to move to smaller and meaner lodgings with his surviving aunt. He was also deeply affected by the suicide of his correspondent Robert E. Howard. In early 1937, Lovecraft was diagnosed with cancer of the small intestine, and suffered from malnutrition as a result. He lived in constant pain until his death on March 15, 1937, in Providence.
In accordance with his lifelong scientific curiosity, he kept a diary of his illness until close to the moment of his death.
Lovecraft was listed along with his parents on the Phillips family monument (). That was not enough for his fans, who in 1977 raised the money to buy him a headstone of his own in Swan Point Cemetery, on which they had inscribed Lovecraft's name, the dates of his birth and death, and the phrase "I AM PROVIDENCE", a line from one of his personal letters.
Groups of enthusiasts annually observe the anniversaries of Lovecraft's death at Ladd Observatory and of his birth at his grave site. In July 2013, the Providence City Council designated the intersection of Angell and Prospect streets near the author's former residences as "H. P. Lovecraft Memorial Square" and installed a commemorative sign.
Appreciation.
Within genre.
According to Joyce Carol Oates, Lovecraft – as with Edgar Allan Poe in the 19th century – has exerted "an incalculable influence on succeeding generations of writers of horror fiction". Horror, fantasy, and science fiction author Stephen King called Lovecraft "the twentieth century's greatest practitioner of the classic horror tale." King has made it clear in his semi-autobiographical non-fiction book "Danse Macabre" that Lovecraft was responsible for King's own fascination with horror and the macabre, and was the single largest figure to influence his fiction writing.
Literary.
Early efforts to revise an established literary view of Lovecraft as an author of 'pulp' were resisted by some eminent critics; in 1945 Edmund Wilson expressed the opinion that "the only real horror in most of these fictions is the horror of bad taste and bad art". But "Mystery and Adventure" columnist Will Cuppy of the "New York Herald Tribune" recommended to readers a volume of Lovecraft's stories, asserting that "the literature of horror and macabre fantasy belongs with mystery in its broader sense." In 2005 the status of classic American writer conferred by a Library of America edition was accorded to Lovecraft with the publication of "Tales", a collection of his weird fiction stories.
Philosophical.
According to scholar S. T Joshi: "There is never an entity in Lovecraft that is not in some fashion material". Graham Harman said that the leading figures at the initial speculative realism conference were astounded to discover that though they shared no philosophical heroes, all were enthusiastic readers of Lovecraft.
Seeing Lovecraft as having a unique—though implicit—anti-reductionalist ontology, Harman says "No other writer is so perplexed by the gap between objects and the power of language to describe them, or between objects and the qualities they possess."
Themes.
Several themes recur in Lovecraft's stories:
Forbidden knowledge.
Forbidden, darkly esoterically veiled knowledge is a central theme in many of Lovecraft's works. Many of his characters are driven by curiosity or scientific endeavor, and in many of his stories the knowledge they uncover proves Promethean in nature, either filling the seeker with regret for what they have learned, destroying them psychically, or completely destroying the person who holds the knowledge.
Some critics argue that this theme is a reflection of Lovecraft's contempt of the world around him, causing him to search inwardly for knowledge and inspiration.
Non-human influences on humanity.
The beings of Lovecraft's mythos often have human servants; Cthulhu, for instance, is worshiped under various names by cults amongst both the Greenland Inuit and voodoo circles of Louisiana, and in many other parts of the world.
These worshippers served a useful narrative purpose for Lovecraft. Many beings of the Mythos were too powerful to be defeated by human opponents, and so horrific that direct knowledge of them meant insanity for the victim. When dealing with such beings, Lovecraft needed a way to provide exposition and build tension without bringing the story to a premature end. Human followers gave him a way to reveal information about their "gods" in a diluted form, and also made it possible for his protagonists to win paltry victories. Lovecraft, like his contemporaries, envisioned "savages" as closer to supernatural knowledge unknown to civilized man.
Inherited guilt.
Another recurring theme in Lovecraft's stories is the idea that descendants in a bloodline can never escape the stain of crimes committed by their forebears, at least if the crimes are atrocious enough. Descendants may be very far removed, both in place and in time (and, indeed, in culpability), from the act itself, and yet, they may be haunted by the revenant past, e.g. "The Rats in the Walls", "The Lurking Fear", "Arthur Jermyn", "The Alchemist", "The Shadow Over Innsmouth", "The Doom that Came to Sarnath" and "The Case of Charles Dexter Ward".
Fate.
Often in Lovecraft's works the protagonist is not in control of his own actions, or finds it impossible to change course. Many of his characters would be free from danger if they simply managed to run away; however, this possibility either never arises or is somehow curtailed by some outside force, such as in "The Colour Out of Space" and "The Dreams in the Witch House". Often his characters are subject to a compulsive influence from powerful malevolent or indifferent beings. As with the inevitability of one's ancestry, eventually even running away, or death itself, provides no safety ("The Thing on the Doorstep", "The Outsider", "The Case of Charles Dexter Ward", etc.). In some cases, this doom is manifest in the entirety of humanity, and no escape is possible ("The Shadow Out of Time").
Civilization under threat.
Lovecraft was familiar with the work of the German conservative-revolutionary theorist Oswald Spengler, whose pessimistic thesis of the decadence of the modern West formed a crucial element in Lovecraft's overall anti-modern worldview. Spenglerian imagery of cyclical decay is present in particular in "At the Mountains of Madness". S. T. Joshi, in "H. P. Lovecraft: The Decline of the West", places Spengler at the center of his discussion of Lovecraft's political and philosophical ideas.
Lovecraft wrote to Clark Ashton Smith in 1927: "It is my belief, and was so long before Spengler put his seal of scholarly proof on it, that our mechanical and industrial age is one of frank decadence". Lovecraft was also acquainted with the writings of another German philosopher of decadence: Friedrich Nietzsche.
Lovecraft frequently dealt with the idea of civilization struggling against dark, primitive barbarism. In some stories this struggle is at an individual level; many of his protagonists are cultured, highly educated men who are gradually corrupted by some obscure and feared influence.
In such stories, the "curse" is often a hereditary one, either because of interbreeding with non-humans (e.g., "Facts Concerning the Late Arthur Jermyn and His Family" (1920), "The Shadow over Innsmouth" (1931) or through direct magical influence ("The Case of Charles Dexter Ward"). Physical and mental degradation often come together; this theme of 'tainted blood' may represent concerns relating to Lovecraft's own family history, particularly the death of his father due to what Lovecraft must have suspected to be a syphilitic disorder.
In other tales, an entire society is threatened by barbarism. Sometimes the barbarism comes as an external threat, with a civilized race destroyed in war (e.g., "Polaris"). Sometimes, an isolated pocket of humanity falls into decadence and atavism of its own accord (e.g., "The Lurking Fear"). But most often, such stories involve a civilized culture being gradually undermined by a malevolent underclass influenced by inhuman forces.
There is a lack of analysis as to whether England's gradual loss of prominence and related conflicts (Boer War, India, World War I) had an influence on Lovecraft's worldview. It is likely that the "roaring twenties" left Lovecraft disillusioned as he was still obscure and struggling with the basic necessities of daily life, combined with seeing non-Western European immigrants in New York City.
Race, ethnicity, and class.
Racism is the most controversial aspect of Lovecraft's works which "does not endear Lovecraft to the modern reader," and it comes across through many disparaging remarks against the various non-Anglo-Saxon races and cultures within his work. As he grew older, the more "jagged" aspects of his original Anglo-Saxon racial worldview softened into a universal classism or elitism regarding any fellow human being of self-ennobled high culture as of metaphorical "superior race." Lovecraft did not from the start hold all white people in uniform high regard, but rather he held English people and people of English descent, above all others. However some groups, such as people of Spanish descent are praised in his work as well. An example of this is the short story "Cool Air" in which a Latino doctor of Spanish descent in New York who is central to the story is described as being "A Spanish physician" of "striking intelligence and superior blood and breeding", and he is described as "short but exquisitely proportioned", with a "high-bred face of masterful though not arrogant expression", "a short iron-grey full beard", "full, dark eyes" and "an aquiline nose". While his racist perspective is undeniable, many critics argue this does not detract from his ability to create compelling philosophical worlds which have inspired many artists and readers. In his early published essays, private letters and personal utterances, he argued for a strong color line, for the purpose of preserving race and culture. These arguments occurred through direct statements against different races in his journalistic work and personal correspondence, or perhaps allegorically in his work using non-human races. Some have interpreted his racial attitude as being more "cultural" than brutally biological: Lovecraft showed sympathy to others who pacifically assimilated into Western culture, to the extent of even marrying a Jewish woman whom he viewed as "well assimilated." While Lovecraft's racial attitude has been seen as directly influenced by the time, a reflection of the New England society he grew up in, his racism appeared stronger than the popular viewpoints held at that time. Some researchers also note that his views failed to change in the face of increased social change of that time. 
However, perceptions concerning Lovecraft's views on race are being challenged by some 21st-century literary scholars. Some scholars such as Gail Bederman have noted that other contemporary authors such as Edgar Rice Burroughs who are subject to similar criticism were in fact no way unusual for their time, and were in fact completely typical of not just average early 20th century Americans but in fact were typical of the overwhelming majority of early 20th century "intellectuals" including Charlotte Perkins Gilman, Theodore Roosevelt, G. Stanley Hall, and others. This is true both of Lovecraft's racism as well as his ideas about cultural "degeneration" and his xenophobic anxiety in general. In "Black Christ and His Invisible Brother on the Cross: Race and Religion in H.P. Lovecraft's 'The Dunwich Horror'," Historian Michael Gurnow argues that the author suggests empathy toward minority plights and viewpoints. In "The Dunwich Horror," the two monstrous brothers of the tale are mulatto, thus part African American, and become martyrs by the story's climax.
Risks of a scientific era.
At the turn of the 20th century, man's increased reliance upon science was both opening new worlds and solidifying the manners by which he could understand them. Lovecraft portrays this potential for a growing gap of man's understanding of the universe as a potential for horror. Most notably in "The Colour Out of Space", the inability of science to comprehend a contaminated meteorite leads to horror.
In a letter to James F. Morton in 1923, Lovecraft specifically points to Einstein's theory on relativity as throwing the world into chaos and making the cosmos a jest; in a letter to Woodburn Harris in 1929, he speculates that technological comforts risk the collapse of science. Indeed, at a time when men viewed science as limitless and powerful, Lovecraft imagined alternative potential and fearful outcomes. In "The Call of Cthulhu", Lovecraft's characters encounter architecture which is "abnormal, non-Euclidean, and loathsomely redolent of spheres and dimensions apart from ours". Non-Euclidean geometry is the mathematical language and background of Einstein's general theory of relativity, and Lovecraft references it repeatedly in exploring alien archaeology.
Religion.
Lovecraft's works are ruled by several distinct pantheons of deities (actually aliens who are worshiped by humans as deities) who are either indifferent or actively hostile to humanity. Lovecraft's actual philosophy has been termed "cosmic indifferentism" and this is expressed in his fiction. Several of Lovecraft's stories of the Old Ones (alien beings of the Cthulhu Mythos) propose alternate mythic human origins in contrast to those found in the creation stories of existing religions, expanding on a natural world view. For instance, in Lovecraft's "At the Mountains of Madness" it is proposed that humankind was actually created as a slave race by the Old Ones, and that life on Earth as we know it evolved from scientific experiments abandoned by the Elder Things. Protagonist characters in Lovecraft are usually educated men, citing scientific and rationalist evidence to support their non-faith. "Herbert West–Reanimator" reflects on the atheism common within academic circles. In "The Silver Key", the character Randolph Carter loses the ability to dream and seeks solace in religion, specifically Congregationalism, but does not find it and ultimately loses faith.
Lovecraft himself adopted the stance of atheism early in his life. In 1932 he wrote in a letter to Robert E. Howard: "All I say is that I think it is damned unlikely that anything like a central cosmic will, a spirit world, or an eternal survival of personality exist. They are the most preposterous and unjustified of all the guesses which can be made about the universe, and I am not enough of a hairsplitter to pretend that I don't regard them as arrant and negligible moonshine. In theory I am an agnostic, but pending the appearance of radical evidence I must be classed, practically and provisionally, as an atheist."
Influences on Lovecraft.
Some of Lovecraft's work was inspired by his own nightmares. His interest started from his childhood days when his grandfather would tell him Gothic horror stories.
Lovecraft's most significant literary influence was Edgar Allan Poe. He had a British writing style due to his love of British literature. Like Lovecraft, Poe's work was out of step with the prevailing literary trends of his era. Both authors created distinctive, singular worlds of fantasy and employed archaisms in their writings. This influence can be found in such works as his novella "The Shadow Over Innsmouth" where Lovecraft references Poe's story ‘’The Imp of the Perverse’’ by name in Chapter 3, and in his poem "Nemesis", where the “"…ghoul-guarded gateways of slumber"" suggest the "…ghoul-haunted woodland of Weir" found in Poe's "Ulalume". A direct quote from the poem and a reference to Poe's only novel "The Narrative of Arthur Gordon Pym of Nantucket" is alluded to in Lovecraft's magnum opus "At the Mountains of Madness". Both authors shared many biographical similarities as well, such as the loss of their fathers at young ages and an early interest in poetry.
He was influenced by Arthur Machen's carefully constructed tales concerning the survival of ancient evil into modern times in an otherwise realistic world and his beliefs in hidden mysteries which lay behind reality. Lovecraft was also influenced by authors such as Oswald Spengler and Robert W. Chambers. Chambers was the writer of "The King in Yellow", of whom Lovecraft wrote in a letter to Clark Ashton Smith: "Chambers is like Rupert Hughes and a few other fallen Titans – equipped with the right brains and education but wholly out of the habit of using them". Lovecraft's discovery of the stories of Lord Dunsany, with their pantheon of mighty gods existing in dreamlike outer realms, moved his writing in a new direction, resulting in a series of imitative fantasies in a "Dreamlands" setting.
Lovecraft also cited Algernon Blackwood as an influence, quoting "The Centaur" in the head paragraph of "The Call of Cthulhu". He declared Blackwood's story "The Willows" to be the single best piece of weird fiction ever written.
Another inspiration came from a completely different source: scientific progress in biology, astronomy, geology, and physics. His study of science contributed to Lovecraft's view of the human race as insignificant, powerless, and doomed in a materialistic and mechanistic universe. Lovecraft was a keen amateur astronomer from his youth, often visiting the Ladd Observatory in Providence, and penning numerous astronomical articles for local newspapers. His astronomical telescope is now housed in the rooms of the August Derleth Society.
Lovecraft's materialist views led him to espouse his philosophical views through his fiction; these philosophical views came to be called cosmicism. Cosmicism took on a dark tone with his creation of what is today often called the Cthulhu Mythos, a pantheon of alien extra-dimensional deities and horrors which predate humanity, and which are hinted at in eons-old myths and legends. The term "Cthulhu Mythos" was coined by Lovecraft's correspondent and fellow author, August Derleth, after Lovecraft's death; Lovecraft jocularly referred to his artificial mythology as "Yog-Sothothery".
Lovecraft considered himself a man best suited to the early 18th century. His writing style, especially in his many letters, owes much to Augustan British writers of the Enlightenment like Joseph Addison and Jonathan Swift.
Among the books found in his library (as evidenced in "Lovecraft's Library" by S. T. Joshi) was "The Seven Who Were Hanged" by Leonid Andreyev and "A Strange Manuscript Found in a Copper Cylinder" by James De Mille.
Lovecraft's style has often been criticized by unsympathetic critics, yet scholars such as S. T. Joshi have shown that Lovecraft consciously utilized a variety of literary devices to form a unique style of his own – these include conscious archaism, prose-poetic techniques combined with essay-form techniques, alliteration, anaphora, crescendo, transferred epithet, metaphor, symbolism, and colloquialism.
Influence on culture.
Lovecraft was relatively unknown during his own time. While his stories appeared in the pages of prominent pulp magazines such as "Weird Tales" (eliciting letters of outrage as often as letters of praise from regular readers of the magazines), not many people knew his name. He did, however, correspond regularly with other contemporary writers, such as Clark Ashton Smith and August Derleth, people who became good friends of his, even though they never met in person. This group of writers became known as the "Lovecraft Circle", since they all freely borrowed elements of Lovecraft's stories – the mysterious books with disturbing names, the pantheon of ancient alien entities, such as Cthulhu and Azathoth, and eldritch places, such as the New England town of Arkham and its Miskatonic University – for use in their own works with Lovecraft's encouragement.
After Lovecraft's death, the Lovecraft Circle carried on. August Derleth in particular added to and expanded on Lovecraft's vision. However, Derleth's contributions have been controversial. While Lovecraft never considered his pantheon of alien gods more than a mere plot device, Derleth created an entire cosmology, complete with a war between the good Elder Gods and the evil Outer Gods, such as Cthulhu and his ilk. The forces of good were supposed to have won, locking Cthulhu and others up beneath the earth, in the ocean, and so forth. Derleth's Cthulhu Mythos stories went on to associate different gods with the traditional four elements of fire, air, earth and water—an artificial constraint which required rationalizations on Derleth's part as Lovecraft himself never envisioned such a scheme.
Lovecraft's fiction has been grouped into three categories by some critics. While Lovecraft did not refer to these categories himself, he did once write: "There are my 'Poe' pieces and my 'Dunsany pieces'—but alas—where are any Lovecraft pieces?"
Lovecraft's writing, particularly the so-called Cthulhu Mythos, has influenced fiction authors including modern horror and fantasy writers such as Stephen King, Ramsey Campbell, Bentley Little, Joe R. Lansdale, Alan Moore, Junji Ito, F. Paul Wilson, Brian Lumley, Caitlín R. Kiernan, William S. Burroughs, and Neil Gaiman, have cited Lovecraft as one of their primary influences. Beyond direct adaptation, Lovecraft and his stories have had a profound impact on popular culture. Some influence was direct, as he was a friend, inspiration, and correspondent to many of his contemporaries, such as August Derleth, Robert E. Howard, Robert Bloch and Fritz Leiber. Many later figures were influenced by Lovecraft's works, including author and artist Clive Barker, prolific horror writer Stephen King, comics writers Alan Moore, Neil Gaiman and Mike Mignola, film directors John Carpenter, Stuart Gordon, Guillermo Del Toro and artist H. R. Giger. Japan has also been significantly inspired and terrified by Lovecraft's creations and thus even entered the manga and anime media. Chiaki J. Konaka is an acknowledged disciple and has participated in Cthulhu Mythos, expanding several Japanese versions. He is an anime scriptwriter who tends to add elements of cosmicism, and is credited for spreading the influence of Lovecraft among anime base. Manga artist Junji Ito has also been inspired by Lovecraft. Novelist and manga author, Hideyuki Kikuchi, incorporated a number of locations, beings and events from the works of Lovecraft into the manga Taimashin.
Argentine writer Jorge Luis Borges wrote his short story "There Are More Things" in memory of Lovecraft. Contemporary French writer Michel Houellebecq wrote a literary biography of Lovecraft called "". Prolific American writer Joyce Carol Oates wrote an introduction for a collection of Lovecraft stories. The Library of America published a volume of Lovecraft's work in 2005, essentially declaring him a canonical American writer. French philosophers Gilles Deleuze and Félix Guattari refer to Lovecraft in "A Thousand Plateaus", calling the short story "Through the Gates of the Silver Key" one of his masterpieces.
Music.
Lovecraft's fictional Mythos has influenced a number of musicians. The psychedelic rock band H. P. Lovecraft (who shortened their name to Lovecraft and then Love Craft in the 1970s) released the albums "H. P. Lovecraft" and "H. P. Lovecraft II" in 1967 and 1968 respectively; their titles included "The White Ship". Metallica recorded a song inspired by "The Call of Cthulhu", an instrumental titled "The Call of Ktulu", and another song based on "The Shadow Over Innsmouth" titled "The Thing That Should Not Be", and another based on Frank Belknap Long's "The Hounds of Tindalos", titled "All Nightmare Long". Black Sabbath's "Behind the Wall of Sleep" appeared on their 1970 debut album and is based on Lovecraft's short story "Beyond the Wall of Sleep". The Darkest of the Hillside Thickets entire repertoire is Lovecraft-based. Melodic death metal band The Black Dahlia Murder produced "Throne of Lunacy" and "Thy Horror Cosmic" based on the Cthulhu Mythos. Progressive metal band Dream Theater's song "The Dark Eternal Night" is based on Lovecraft's story "Nyarlathotep". UK anarcho-punk band Rudimentary Peni make repeated references in their song titles, lyrics and artwork, including in the album "Cacophony", all 30 songs of which are inspired by the life and writings of Lovecraft. In the Iron Maiden album "Live After Death", the band mascot, Eddie, is rising from a grave inscribed with the name "H. P. Lovecraft" and a quotation from "The Nameless City": "That is not dead which can eternal lie yet with strange aeons even death may die." German metal group Mekong Delta made an album called "The Music of Erich Zann".
Games.
Lovecraft has also influenced gaming. Chaosium's role-playing game "Call of Cthulhu" (currently in its seventh major edition) has been in print for 30 years. The board games "Arkham Horror", "Eldritch Horror", and dice game "Elder Sign" are derived from mechanisms first introduced in the Call of Cthulhu RPG. Two collectible card games are "Mythos" and "Call of Cthulhu, the Living Card Game". Several video games are based on or influenced heavily by Lovecraft such as ', ', "Shadow of the Comet", "Prisoner of Ice", "Shadowman", "Alone in the Dark", "Chzo Mythos", ', "Cthulhu Saves the World", ', ', ', ', "Bloodborne", "Dead Space", "Splatterhouse", ', ', "Penumbra", "Blood" (according to Nick Newhard, its designer), "The Last Door", the megami tensei franchise and "Quake". The MMORPG "The Secret World" is heavily based on Lovecraftian lore. In ' the Daedric Prince "Hermaeus Mora" and his realm of Oblivion, Apocrypha, are both heavily influenced by Lovecraft.
In the MMORPG "World of WarCraft", the names of the Old Gods and other non-player character names are heavily influenced by H. P. Lovecraft's character names.
The board game "The Doom that Came to Atlantic City" created by Lee Moyer and Keith Baker is a game of Lovecraftian Great Old Ones fighting to destroy Atlantic City. With playing pieces by sculptor Paul Komoda, the game is currently in production through Cryptozoic Entertainment.
Lovecraft as a character in fiction.
Aside from his thinly veiled appearance in Robert Bloch's "The Shambler from the Stars", Lovecraft continues to be used as a character in supernatural fiction. An early version of Ray Bradbury's "The Exiles" uses Lovecraft as a character, who makes a brief, 600-word appearance eating ice cream in front of a fire and complaining about how cold he is. Lovecraft and some associates are included at length in Robert Anton Wilson and Robert Shea's "The Illuminatus! Trilogy" (1975). Lovecraft makes an appearance as a rotting corpse in "The Chinatown Death Cloud Peril" by Paul Malmont, a novel with fictionalized versions of a number of period writers.
Other notable works with Lovecraft as a character include Richard Lupoff's "Lovecraft's Book" (1985), "Cast a Deadly Spell" (1991), ' (1993), "Witch Hunt" (1994), ' (1998) and ' (2007). Lovecraft also appears in the Season 6, Episode 21 episode "Let it Bleed" of the TV show "Supernatural". A satirical version of Lovecraft named "H. P. Hatecraft" appeared as a recurring character on the Cartoon Network television series "Scooby-Doo! Mystery Incorporated". A character based on Lovecraft also appears in the visual novel ', under the name "Howard Phillips" (or "Mr. Howard" to most of the main characters).. He appears as a minor character in Brian Clevinger's comic book series Atomic Robo, as an aquintance and fellow scientist of Nikola Tesla, having been driven insane by his involvement in the Tunguska Event which exposed him to the hidden horrors of the wider universe. He is eventually killed when his body becomes host to an extradimensional being infecting the timestream.
Editions and collections of Lovecraft's work.
For most of the 20th century, the definitive editions (specifically "At the Mountains of Madness and Other Novels", "Dagon and Other Macabre Tales", "The Dunwich Horror and Others", and "The Horror in the Museum and Other Revisions") of his prose fiction were published by Arkham House, a publisher originally started with the intent of publishing the work of Lovecraft, but which has since published a considerable amount of other literature as well. Penguin Classics has at present issued three volumes of Lovecraft's works: "The Call of Cthulhu and Other Weird Stories", "The Thing on the Doorstep and Other Weird Stories", and most recently "The Dreams in the Witch House and Other Weird Stories". They collect the standard texts as edited by S. T. Joshi, most of which were available in the Arkham House editions, with the exception of the restored text of "The Shadow Out of Time" from "The Dreams in the Witch House", which had been previously released by small-press publisher Hippocampus Press. In 2005 the prestigious Library of America canonized Lovecraft with a volume of his stories edited by Peter Straub, and Random House's Modern Library line have issued the "definitive edition" of Lovecraft's "At the Mountains of Madness" (also including "Supernatural Horror in Literature").
Lovecraft's poetry is collected in "The Ancient Track: The Complete Poetical Works of H. P. Lovecraft" (Night Shade Books, 2001), while much of his juvenilia, various essays on philosophical, political and literary topics, antiquarian travelogues, and other things, can be found in "Miscellaneous Writings" (Arkham House, 1989). Lovecraft's essay "Supernatural Horror in Literature", first published in 1927, is a historical survey of horror literature available with endnotes as "The Annotated Supernatural Horror in Literature".
Letters.
Although Lovecraft is known mostly for his works of weird fiction, the bulk of his writing consists of voluminous letters about a variety of topics, from weird fiction and art criticism to politics and history. Lovecraft's biographer L. Sprague de Camp estimates that Lovecraft wrote 100,000 letters in his lifetime, a fifth of which are believed to survive.
He sometimes dated his letters 200 years before the current date, which would have put the writing back in U.S. colonial times, before the American Revolution (a war that offended his Anglophilia). He explained that he thought that the 18th and 20th centuries were the "best", the former being a period of noble grace, and the latter a century of science.
Lovecraft was not an active letter-writer in youth. In 1931 he admitted: "In youth I scarcely did any letter-writing — thanking anybody for a present was so much of an ordeal that I would rather have written a two hundred fifty-line pastoral or a twenty-page treatise on the rings of Saturn." (SL 3.369–70). The initial interest in letters stemmed from his correspondence with his cousin Phillips Gamwell but even more important was his involvement in the amateur journalism movement, which was initially responsible for the enormous number of letters Lovecraft produced.
Despite his light letter-writing in youth, in later life his correspondence was so voluminous that it has been estimated that he may have written around 30,000 letters to various correspondents, a figure which places him second only to Voltaire as an epistolarian. Lovecraft's later correspondence is primarily to fellow weird fiction writers, rather than to the amateur journalist friends of his earlier years.
Lovecraft clearly states that his contact to numerous different people through letter-writing was one of the main factors in broadening his view of the world: "I found myself opened up to dozens of points of view which would otherwise never have occurred to me. My understanding and sympathies were enlarged, and many of my social, political, and economic views were modified as a consequence of increased knowledge." (SL 4.389).
Today there are five publishing houses that have released letters from Lovecraft, most prominently Arkham House with its five-volume edition "Selected Letters." (Those volumes, however, severely abridge the letters they contain). Other publishers are Hippocampus Press ("Letters to Alfred Galpin" "et al."), Night Shade Books ("Mysteries of Time and Spirit: The Letters of H. P. Lovecraft and Donald Wandrei" "et al.".), Necronomicon Press ("Letters to Samuel Loveman and Vincent Starrett" et al.), and University of Tampa Press ("O Fortunate Floridian: H. P. Lovecraft's Letters to R. H. Barlow"). S.T. Joshi is supervising an ongoing series of volumes collecting Lovecraft's unabridged letters to particular correspondents.
"Lord of a Visible World: An Autobiography in Letters" was published in 2000, in which his letters are arranged according to themes, such as adolescence and travel.
Copyright.
There is controversy over the copyright status of many of Lovecraft's works, especially his later works. Lovecraft had specified that the young R. H. Barlow would serve as executor of his literary estate, but these instructions had not been incorporated into his will. Nevertheless his surviving aunt carried out his expressed wishes, and Barlow was given charge of the massive and complex literary estate upon Lovecraft's death.
Barlow deposited the bulk of the papers, including the voluminous correspondence, with the John Hay Library, and attempted to organize and maintain Lovecraft's other writing. August Derleth, an older and more established writer than Barlow, vied for control of the literary estate. One result of these conflicts was the legal confusion over who owned what copyrights.
All works published before 1923 are public domain in the U.S. However, there is some disagreement over who exactly owns or owned the copyrights and whether the copyrights apply to the majority of Lovecraft's works published post-1923.
Questions center over whether copyrights for Lovecraft's works were ever renewed under the terms of the United States Copyright Act of 1976 for works created prior to January 1, 1978. The problem comes from the fact that before the Copyright Act of 1976 the number of years a work was copyrighted in the U.S. was based on "publication" rather than life of the author plus a certain number of years and that it was good for only 28 years. After that point, a new copyright had to be filed, and any work that did not have its copyright renewed fell into the public domain. The Copyright Act of 1976 retroactively extended this renewal period for all works to a period of 47 years and the Sonny Bono Copyright Term Extension Act of 1998 added another 20 years to that, for a total of 95 years from publication. If the works were renewed, the copyrights would still be valid in the United States.
The European Union Copyright Duration Directive of 1993 extended the copyrights to 70 years after the author's death. So, all works of Lovecraft published during his lifetime, became public domain in all 27 European Union countries on January 1, 2008. In those Berne Convention countries who have implemented only the minimum copyright period, copyright expires 50 years after the author's death.
Lovecraft protégés and part owners of Arkham House, August Derleth and Donald Wandrei, often claimed copyrights over Lovecraft's works. On October 9, 1947, Derleth purchased all rights to "Weird Tales". However, since April 1926 at the latest, Lovecraft had reserved all second printing rights to stories published in "Weird Tales". Hence, "Weird Tales" may only have owned the rights to at most six of Lovecraft's tales. Again, even if Derleth did obtain the copyrights to Lovecraft's tales, no evidence as yet has been found that the copyrights were renewed. Following Derleth's death in 1971, his attorney proclaimed that all of Lovecraft's literary material was part of the Derleth estate and that it would be "[protected] to the fullest extent possible."
S. T. Joshi concludes in his biography, "H. P. Lovecraft: A Life", that Derleth's claims are "almost certainly fictitious" and that most of Lovecraft's works published in the amateur press are most likely now in the public domain. The copyright for Lovecraft's works would have been inherited by the only surviving heir of his 1912 will: Lovecraft's aunt, Annie Gamwell. Gamwell herself perished in 1941 and the copyrights then passed to her remaining descendants, Ethel Phillips Morrish and Edna Lewis. Morrish and Lewis then signed a document, sometimes referred to as the Morrish-Lewis gift, permitting Arkham House to republish Lovecraft's works but retaining the copyrights for themselves. Searches of the Library of Congress have failed to find any evidence that these copyrights were then renewed after the 28-year period and hence, it is likely that these works are now in the public domain.
Chaosium, publishers of the "Call of Cthulhu" role-playing game, have a trademark on the phrase "The Call of Cthulhu" for use in game products. TSR, Inc., original publisher of the "Advanced Dungeons & Dragons" role-playing game, included a section on the Cthulhu Mythos in one of the game's earlier supplements, "Deities & Demigods" (originally published in 1980 and later renamed to "Legends & Lore"). TSR later agreed to remove this section at Chaosium's request.
In 2009, the H. P. Lovecraft Literary Estate established Lovecraft Holdings, LLC, a company based out of Providence, which has filed trademark claims for Lovecraft's name and silhouette, and also claims ownership over Lovecraft's stories, letters, and essays.
Regardless of the legal disagreements surrounding Lovecraft's works, Lovecraft himself was extremely generous with his own works and encouraged others to borrow ideas from his stories and build on them, particularly with regard to his Cthulhu Mythos. He encouraged other writers to reference his creations, such as the "Necronomicon", Cthulhu and Yog-Sothoth. After his death, many writers have contributed stories and enriched the shared mythology of the Cthulhu Mythos, as well as making numerous references to his work.
Locations featured in Lovecraft stories.
Lovecraft drew extensively from his native New England for settings in his fiction. Numerous real historical locations are mentioned, and several fictional New England locations make frequent appearances.
References.
</dl>

</doc>
<doc id="13562" url="http://en.wikipedia.org/wiki?curid=13562" title="Habermas">
Habermas

Habermas is a surname. Notable people with the surname include:

</doc>
<doc id="13563" url="http://en.wikipedia.org/wiki?curid=13563" title="Herman Brood">
Herman Brood

Hermanus 'Herman' Brood (]; 5 November 1946 – 11 July 2001) was a Dutch musician and painter. As a musician he achieved artistic and commercial success in the 1970s and 1980s, and was called "the Netherlands' greatest and only rock 'n' roll star". Later in life he started a successful career as a painter.
Known for his hedonistic lifestyle of "sex, drugs and rock 'n roll," Brood was an "enfant terrible" and a cultural figure whose suicide, apparently caused by a failure to kick his drug and alcohol habit, only strengthened his controversial status. His suicide, according to a poll organized to celebrate fifty years of Dutch popular music, was the most significant event in its history.
Musical career.
Herman Brood was born in Zwolle, and started playing the piano at age 12. He founded beat band The Moans in 1964, which would later become Long Tall Ernie and the Shakers. Brood was asked to play with Cuby and the Blizzards, but was removed by management when the record company discovered he used drugs. For a number of years Brood was in jail (for dealing LSD), or abroad, and had a number of short-term engagements (with The Studs, the Flash & Dance Band, Vitesse).
In 1976, Brood started his own group, Herman Brood & His Wild Romance, (and started work with photographer Anton Corbijn) initially with Ferdi Karmelk (guitar), Gerrit Veen (bass), Peter Walrecht (drums), and Ellen Piebes and Ria Ruiters (vocals). They played the club and bar circuit, first in Groningen, In 1977 the band released their first album, "Street".
The band now played all over the Netherlands, playing as many gigs as possible. And Herman's drug habit became public domain: In 1977 for instance the Wild Romance played a gig in a high school in Almelo, the Christelijk Lyceum; during the break Brood was caught on the toilet taking heroine or speed (there are different reports on the type of drug, but it is a well-known story amongst former students), the rest of the concert was cancelled, and this also was the last time a rock concert took place at this school for many years.
They are still best known for their second album, "Shpritsz"—a play on the German word for syringe—from 1978. This album contained Brood anthems like "Dope Sucks," "Rock & Roll Junkie," and their first Dutch hit single, "Saturday Night." The band went through many personnel changes over the years; the best-known formation was Freddy Cavalli (bass), Dany Lademacher (guitar) (later replaced with David Hollestelle), and Cees 'Ani' Meerman (drums). A frequent contributor was Bertus Borgers (saxophone).
Brood's outspoken statements in the press about sex and drug use brought him into the Dutch public arena even more than his music. He was romantically involved with the German singer Nina Hagen, with whom he appeared in the 1979 film "Cha-Cha". He is reputed to be the subject of her song "Herrmann Hiess Er" (English title "Herrmann Was His Name") from the 1979 "Unbehagen" album, a song about a drug addict. Brood relished the media attention and became the most famous hard drug user in the Netherlands. "It is quite common for an artist to use drugs, but not for him to tell everybody. I admit that it scared me that my popularity could make people start using drugs," he once said in an interview.
In the summer of 1979, Brood tried to enter the American market, where he toured as a support-act for The Kinks, The Cars, and Foreigner. A re-recorded version of "Saturday Night" peaked at number 35 in the "Billboard" Hot 100, but the big break Brood hoped for didn't happen. When he returned to the Netherlands in October 1979, his band had begun to fall apart, and soon his popularity went downhill. "Go Nutz", the album Brood had recorded while in the States, and the movie "Cha-Cha", which finally premiered in December 1979, were considered artistic failures, even though "Go Nutz" produced three charting singles in the Netherlands and the "Cha Cha" soundtrack attained platinum status. The 1980 album "Wait a Minute..." was a minor success, but the follow-up albums "Modern Times Revive" (1981) and "Frisz & Sympatisz" (1982) failed to make the Dutch album charts.
Brood continued to record throughout the 1980s and had a few hits—a top-10 single, "Als Je Wint" with Henny Vrienten, and a minor hit with a reggae song, "Tattoo Song," but he spent more and more time on his art work. At the end of the '80s he made a comeback of sorts; "Yada Yada" (1988), produced by George Kooymans, was well-received, and he toured Germany with a renewed Wild Romance (which saw the return of Dany Lademacher). In 1990, he won the BV Popprijs, one of the highest Dutch awards for popular music, and recorded "Freeze" with Clarence Clemons of the E Street Band and Tejano accordion player Flaco Jiménez. A live "best of" album, "Saturday Night Live", appeared in 1992. His 50th birthday, in 1996, was celebrated with a show in Paradiso, Amsterdam, and the album (of duets) was released the same year.
Visual arts career.
After his career in music, Brood turned to painting and became a well-known character in Amsterdam art circles. His art is best described as pop-art, often very colorful and graffiti-inspired screen prints, and he achieved some commercial success and notoriety by, for instance, creating murals in various public spaces in and around Amsterdam. He continued to remain in the public eye, by appearing in the media and by his cooperation with biographical films such as 1994's "Rock'n Roll Junkie".
Suicide and legacy.
Toward the end of his life, Brood vowed to abstain from most drugs, reducing his drug use to alcohol and a daily shot of speed ("2 grams per day"). In 2001, depressed by the failure of his drug rehabilitation program and facing serious medical problems because of his prolonged drug use, he committed suicide on 11 July by jumping from the roof of the Amsterdam Hilton Hotel at the age of 54. He left a note, stating "Party on. I'll be seeing you." Extensively covered by the national media, his cremation took place five days later. Before the cremation, Brood's casket was driven from the Hilton hotel to Paradiso, Amsterdam, the streets lined with thousands of spectators. A commemorative concert was held in Paradiso, with performances by Hans Dulfer, André Hazes, and Jules Deelder, and leading Dutch music magazine "Muziekkrant OOR" devoted an entire issue to him. His ashes were placed at Zorgvlied cemetery.
Soon after his suicide, Brood's version of "My Way" spent three weeks as number one in the Dutch singles charts; the market value of his art work also increased greatly. A characteristic note is that Brood's paintings were already targeted often by vandals during his life, while after his death they were stolen for their value. His popularity (or notoriety) was verified by the fact that his name turned out to be the strongest brand of the year.
When U2 performed in the Netherlands three weeks after Brood's suicide, they paid tribute to him at each of the three shows. They dedicated an acoustic version of Duke Ellington's "Jump for Joy" to him, a song they never performed at any other time of their career. At the third show in Arnhem they also dedicated their own "Gone" to him and had his version of "My Way" played over the PA as outro music. In the middle of the show Bono delivered an emotional eulogy to Brood before the band performed "In a Little While".
On 5 November 2006 the Groninger Museum opened an exposition devoted to Herman Brood's life and work, comprising paintings, lyrics, and poetry, portraits by photographer Anton Corbijn, a collection of private pictures (from the family album), and concert photos and videos. The exhibition was on show until 28 January. It was centered on Herman's atelier (studio) where he created most of his paintings. The atelier had been entirely re-built in the museum. During the 90s, Herman Brood's studio was located on the second floor of the gallery in the Spuistraat in Amsterdam and has remained untouched since his death.
In 2007 the film "Wild Romance" premiered in the Netherlands, a movie about Brood's life. Brood was portrayed by Daniël Boissevain. He continues to inspire other artists: the 2007 album "Bluefinger" by Black Francis is based on the life and works of Brood. A tribute band called the Brood Roosters ("bread toasters") was active in the Netherlands until they split up in early 2009. Another tribute band called Yada Yada is still active in The Netherlands, often appearing with original members of the Wild Romance (Dany Lademacher, Ramon Rambeaux) .
In 2010, the Catastrophic Theatre Company collaborated with Frank Black on a rock opera based on the "Bluefinger" album. The opera's first performance, with Matt Kelly portraying Brood, was 12 November 2010 in Houston, Texas.

</doc>
<doc id="13572" url="http://en.wikipedia.org/wiki?curid=13572" title="Henry VII">
Henry VII

Henry VII may refer to:

</doc>
<doc id="13574" url="http://en.wikipedia.org/wiki?curid=13574" title="Herodotus">
Herodotus

Herodotus (; Ancient Greek: Ἡρόδοτος "Hēródotos", ]) was a Greek historian who was born in Halicarnassus, Caria (modern-day Bodrum, Turkey) and lived in the fifth century BC ( 484–425 BC). Widely referred to as "The Father of History" (first conferred by Cicero), he was the first historian known to collect his materials systematically and critically, and then to arrange them into a historiographic narrative. "The Histories"—his masterpiece and the only work he is known to have produced—is a record of his "inquiry" (or ἱστορία "historía", a word that passed into Latin and acquired its modern meaning of "history"), being an investigation of the origins of the Greco-Persian Wars and including a wealth of geographical and ethnographical information. Although some of his stories were fanciful and others inaccurate, he states he was reporting only what was told to him and was still often very close to right. Little is known of his personal history.
Place in history.
Herodotus announced the size and scope of his work at the beginning of his "Researches" or "Histories":
 This is the showing-forth of the inquiry of Herodotus of Halicarnassus, so that neither what has come to be from man in time might become faded, nor that great and wondrous deeds, those shown forth by Greeks and those by barbarians, might be without their glory; and together with all this, also through what cause they warred with each other.
His record of the achievements of others was an achievement in itself, though the extent of it has been debated. His place in history and his significance may be understood according to the traditions within which he worked. His work is the earliest Greek prose to have survived intact. However, Dionysius of Halicarnassus, a literary critic of Augustan Rome, listed seven predecessors of Herodotus, describing their works as simple, unadorned accounts of their own and other cities and people, Greek or foreign, including popular legends, sometimes melodramatic and naive, often charming—all traits that can be found in the work of Herodotus himself. Modern historians regard the chronology as uncertain. According to the ancient account, these predecessors included Dionysius of Miletus, Charon of Lampsacus, Hellanicus of Lesbos, Xanthus of Lydia and, the best attested of them all, Hecataeus of Miletus. Of these only fragments of Hecataeus's work survive (and the authenticity of these is debatable) yet they allow us glimpses into the kind of tradition within which Herodotus wrote his own "Histories", as in the introduction to Hecataeus's work, "Genealogies":
 Hecataeus the Milesian speaks thus: I write these things as they seem true to me; for the stories told by the Greeks are various and in my opinion absurd.
This points forward to the 'folksy' yet 'international' outlook typical of Herodotus. Yet, one modern scholar has described the work of Hecataeus as "a curious false start to history" because, despite its critical spirit, it failed to liberate history from myth. Herodotus mentions Hecataeus in his "Histories", on one occasion mocking him for his naive genealogy and, on another occasion, quoting Athenian complaints against his handling of their national history. It is possible that Herodotus borrowed much material from Hecataeus, as stated by Porphyry in a quote recorded by Eusebius. In particular, it is possible that he copied descriptions of the crocodile, hippopotamus and phoenix from Hecataeus's 'Circumnavigation of the Known World' ("Periegesis"/"Periodos ges"), even mis-representing the source as 'Heliopolitans' ("Histories" 2.73). But unlike Herodotus, Hecataeus did not record events that had occurred in living memory, nor did he include the oral traditions of Greek history within the larger framework of oriental history. There is no proof that Herodotus derived the ambitious scope of his own work, with its grand theme of civilizations in conflict, from any predecessor, despite much scholarly speculation about this in modern times. Herodotus claims to be better informed than his predecessors, relying on empirical observation to correct their excessive schematism. For example, he argues for continental asymmetry as opposed to the older theory of a perfectly circular earth with Europe and Asia/Africa equal in size ("Hist." 4.36 and 4.42). Yet, he retains idealising tendencies, as in his symmetrical notions of the Danube and Nile.
His debt to previous authors of prose 'histories' might be questionable but there is no doubt that he owed much to the example and inspiration of poets and story-tellers. For example, Athenian tragic poets provided him with a world-view of a balance between conflicting forces, upset by the hubris of kings, and they provided his narrative with a model of episodic structure. His familiarity with Athenian tragedy is demonstrated in a number of passages echoing Aeschylus's "Persae", including the epigrammatic observation that the defeat of the Persian navy at Salamis caused the defeat of the land army ("Hist." 8.68 ~ "Persae" 728). The debt may have been repaid by Sophocles because there appear to be echoes of "The Histories" in his plays, especially a passage in "Antigone" that resembles Herodotus's account of the death of Intaphernes ("Histories" 3.119 ~ "Antigone" 904-20)—this, however, is one of the most contentious issues in modern scholarship.
Homer was another inspirational source.
Just as Homer drew extensively on a tradition of oral poetry, sung by wandering minstrels, so Herodotus appears to have drawn on an Ionian tradition of story-telling, collecting and interpreting the oral histories he chanced upon in his travels. These oral histories often contained folk-tale motifs and demonstrated a moral, yet they also contained substantial facts relating to geography, anthropology and history, all compiled by Herodotus in an entertaining style and format. It is on account of the many strange stories and the folk-tales he reported that his critics in early modern times branded him 'The Father of Lies'. Even his own contemporaries found reason to scoff at his achievement. In fact one modern scholar has wondered if Herodotus left his home in Asiatic Greece, migrating westwards to Athens and beyond, because his own countrymen had ridiculed his work, a circumstance possibly hinted at in an epitaph said to have been dedicated to Herodotus at Thuria (one of his three supposed resting places):
 Herodotus the son of Sphynx
Lies; in Ionic history without peer; <br>
A Dorian born, who fled from Slander's brand <br>
And made in Thuria his new native land.
Yet it was in Athens where his most formidable contemporary critics could be found. In 425 BC, which is about the time that Herodotus is thought by many scholars to have died, the Athenian comic dramatist, Aristophanes, created "The Acharnians", in which he blames The Peloponnesian War on the abduction of some prostitutes—a mocking reference to Herodotus, who reported the Persians' account of their wars with Greece, beginning with the rapes of the mythical heroines Io, Europa, Medea and Helen. Similarly, the Athenian historian Thucydides dismissed Herodotus as a 'logos-writer' or story-teller. Thucydides, who had been trained in rhetoric, became the model for subsequent prose-writers as an author who seeks to appear firmly in control of his material, whereas Herodotus with his frequent digressions appeared to minimize (or possibly disguise) his auctorial control. Moreover, Thucydides developed a historical topic more in keeping with the Greek lifestyle—the polis or city-state—whereas the interplay of civilizations was more relevant to Asiatic Greeks (such as Herodotus himself), for whom life under foreign rule was a recent memory.
 Before the Persian crisis history had been represented among the Greeks only by local or family traditions. The Wars of Liberation had given to Herodotus the first genuinely historical inspiration felt by a Greek. These wars showed him that there was a corporate life, higher than that of the city, of which the story might be told; and they offered to him as a subject the drama of the collision between East and West. With him, the spirit of history was born into Greece; and his work, called after the nine Muses, was indeed the first utterance of Clio.
 — Richard Claverhouse Jebb.
Life.
Modern scholars generally turn to Herodotus's own writing for reliable information about his life, supplemented with ancient yet much later sources, such as the Byzantine "Suda":
 The data are so few—they rest upon such late and slight authority; they are so improbable or so contradictory, that to compile them into a biography is like building a house of cards, which the first breath of criticism will blow to the ground. Still, certain points may be approximately fixed...
 — George Rawlinson.
Modern accounts of his life typically, go something like this: Herodotus was born at Halicarnassus around 484 BC. There is no reason to disbelieve the Suda's information about his family, that it was influential and that he was the son of Lyxes and Dryo, and the brother of Theodorus, and that he was also related to Panyassis, an epic poet of the time. The town was within the Persian empire at that time and maybe the young Herodotus heard local eye-witness accounts of events within the empire and of Persian preparations for the invasion of Greece, including the movements of the local fleet under the command of Artemisia. Inscriptions recently discovered at Halicarnassus indicate that her grandson Lygdamis negotiated with a local assembly to settle disputes over seized property, which is consistent with a tyrant under pressure, and his name is not mentioned later in the tribute list of the Athenian Delian League, indicating that there might well have been a successful uprising against him sometime before 454 BC. Herodotus reveals affection for the island of Samos (III,39–60) and this is an indication that he might have lived there in his youth. So it is possible that his family was involved in an uprising against Lygdamis, leading to a period of exile on Samos and followed by some personal hand in the tyrant's eventual fall.
As Herodotus himself reveals, Halicarnassus, though a Dorian city, had ended its close relations with its Dorian neighbours after an unseemly quarrel (I,144), and it had helped pioneer Greek trade with Egypt (II,178). It was therefore an outward-looking, international-minded port within the Persian Empire and the historian's family could well have had contacts in countries under Persian rule, facilitating his travels and his researches. His eye-witness accounts indicate that he traveled in Egypt probably sometime after 454 BC or possibly earlier in association with Athenians, after an Athenian fleet had assisted the uprising against Persian rule in 460–454 BC. He probably traveled to Tyre next and then down the Euphrates to Babylon. For some reason, probably associated with local politics, he subsequently found himself unpopular in Halicarnassus and, sometime around 447 BC, he migrated to Periclean Athens, a city for whose people and democratic institutions he declares his open admiration (V,78) and where he came to know not just leading citizens such as the Alcmaeonids, a clan whose history features frequently in his writing, but also the local topography (VI,137; VIII,52–5).
According to Eusebius and Plutarch, Herodotus was granted a financial reward by the Athenian assembly in recognition of his work and there may be some truth in this. It is possible that he applied for Athenian citizenship—a rare honour after 451 BC, requiring two separate votes by a well-attended assembly—but was unsuccessful. In 443 BC, or shortly afterwards, he migrated to Thurium as part of an Athenian-sponsored colony. Aristotle refers to a version of "The Histories" written by 'Herodotus of Thurium' and indeed some passages in the "Histories" have been interpreted as proof that he wrote about southern Italy from personal experience there (IV,15,99; VI,127). Intimate knowledge of some events in the first years of the Peloponnesian War (VI,91; VII,133,233; IX,73) indicate that he might have returned to Athens, in which case it is possible that he died there during an outbreak of the plague. Possibly he died in Macedonia instead after obtaining the patronage of the court there or else he died back in Thurium. There is nothing in the "Histories" that can be dated with any certainty to later than 430, and it is generally assumed that he died not long afterwards, possibly before his sixtieth year.
Herodotus wrote his 'Histories' in the Ionian dialect yet he was born in Halicarnassus, originally a Dorian settlement. According to the "Suda" (an 11th-century encyclopaedia of Byzantium which possibly took its information from traditional accounts), Herodotus learned the Ionian dialect as a boy living on the island of Samos, whither he had fled with his family from the oppressions of Lygdamis, tyrant of Halicarnassus and grandson of Artemisia I of Caria. The Suda also informs us that Herodotus later returned home to lead the revolt that eventually overthrew the tyrant. However, thanks to recent discoveries of some inscriptions on Halicarnassus, dated to about that time, we now know that the Ionic dialect was used there even in official documents, so there was no need to assume like the Suda that he must have learned the dialect elsewhere. Moreover, the fact that the Suda is the only source we have for the heroic role played by Herodotus, as liberator of his birthplace, is itself a good reason to doubt such a romantic account.
It was conventional in Herodotus's day for authors to 'publish' their works by reciting them at popular festivals. According to Lucian, Herodotus took his finished work straight from Asia Minor to the Olympic Games and read the entire "Histories" to the assembled spectators in one sitting, receiving rapturous applause at the end of it. According to a very different account by an ancient grammarian, Herodotus refused to begin reading his work at the festival of Olympia until some clouds offered him a bit of shade, by which time, however, the assembly had dispersed—thus the proverbial expression "Herodotus and his shade" to describe someone who misses an opportunity through delay. Herodotus's recitation at Olympia was a favourite theme among ancient writers and there is another interesting variation on the story to be found in the Suda, Photius and Tzetzes, in which a young Thucydides happened to be in the assembly with his father and burst into tears during the recital, whereupon Herodotus observed prophetically to the boy's father: "Thy son's soul yearns for knowledge."
Eventually, Thucydides and Herodotus became close enough for both to be interred in Thucydides's tomb in Athens. Such at least was the opinion of Marcellinus in his "Life of Thucydides". According to the Suda, he was buried in Macedonian Pella and in the agora in Thurium.
Reliability.
While "The Histories" were occasionally criticized in antiquity, modern historians and philosophers generally take a positive view. Despite the controversy, Herodotus still serves as the primary, and often only, source for events in the Greek world, Persian Empire, and the region generally in the two centuries leading up until his own day. Herodotus, like many ancient historians, preferred an element of show to purely analytic history, aiming to give pleasure with “exciting events, great dramas, bizarre exotica.” As such, certain passages have been the subject of controversy and even some doubt, both in antiquity and today.
The accuracy of the works of Herodotus has been controversial since his own era. Cicero, Aristotle, Josephus, Duris of Samos Harpocration and Plutarch all commented on this controversy. Generally, though, he was then, and especially is now, regarded as reliable. Many scholars (Aubin, A. H. L. Heeren, Davidson, Cheikh Anta Diop, Poe, Welsby, Celenko, Volney, Pierre Montet, Bernal, Jackson, DuBois, Strabo), ancient and modern, routinely cite Herodotus. Many of these scholars (Welsby, Heeren, Aubin, Diop, etc.) explicitly mention the reliability of Herodotus's work and demonstrate corroboration of Herodotus's writings by modern scholars. A.H.L. Heeren quoted Herodotus throughout his work and provided corroboration by scholars regarding several passages (source of the Nile, location of Meroe, etc.). To further his work on the Egyptians and Assyrians, Aubin uses Herodotus's accounts in various passages and defends Herodotus's position. Aubin said Herodotus was "the author of the first important narrative history of the world". Diop provides several examples (the inundations of the Nile) that he argues support his view that Herodotus was "quite scrupulous, objective, scientific for his time." Diop argues that Herodotus "always distinguishes carefully between what he has seen and what he has been told." Diop also notes that Strabo corroborated Herodotus's ideas about the Black Egyptians, Ethiopians, and Colchians.
The reliability of Herodotus is sometimes criticized when writing about Egypt. Alan B. Lloyd argues that as a historical document, the writings of Herodotus are seriously defective, and that he was working from "inadequate sources". Nielsen writes that: "Though we cannot entirely rule out the possibility of Herodotus having been in Egypt, it must be said that his narrative bears little witness to it." German historian Detlev Fehling questions whether Herodotus ever traveled up the Nile River, and considers almost everything he says about Egypt and Ethiopia doubtful. About the claim of Herodotus that the Pharaoh Sesostris campaigned in Europe, and that he left a colony in Colchia, Fehling states that "there is not the slightest bit of history behind the whole story".
Herodotus provides much information about the nature of the world and the status of science during his lifetime, often engaging in private speculation. For example, he reports that the annual flooding of the Nile was said to be the result of melting snows far to the south, and he comments that he cannot understand how there can be snow in Africa, the hottest part of the known world, offering an elaborate explanation based on the way that desert winds affect the passage of the Sun over this part of the world (2:18ff). He also passes on dismissive reports from Phoenician sailors that, while circumnavigating Africa, they "saw the sun on the right side while sailing westwards". Owing to this brief mention, which is included almost as an afterthought, it has been argued that Africa was indeed circumnavigated by ancient seafarers, for this is precisely where the sun ought to have been. His accounts of India are among the oldest records of Indian civilization by an outsider.
Discoveries made since the end of the 19th century have generally added to his credibility. His description of Gelonus, located in Scythia, as a city thousands of times larger than Troy was widely disbelieved until it was rediscovered in 1975. The archaeological study of the now-submerged ancient Egyptian city of Heracleion and the recovery of the so-called "Naucratis stela" give credibility to Herodotus's previously unsupported claim that Heracleion was founded during the Egyptian New Kingdom.
After journeys to India and Pakistan, French ethnologist Michel Peissel claimed to have discovered an animal species that may illuminate one of the most bizarre passages in Herodotus's Histories. In Book 3, passages 102 to 105, Herodotus reports that a species of fox-sized, furry "ants" lives in one of the far eastern, Indian provinces of the Persian Empire. This region, he reports, is a sandy desert, and the sand there contains a wealth of fine gold dust. These giant ants, according to Herodotus, would often unearth the gold dust when digging their mounds and tunnels, and the people living in this province would then collect the precious dust. Peissel reports that in an isolated region of northern Pakistan, on the Deosai Plateau in Gilgit–Baltistan province, there is a species of marmot - the Himalayan marmot, a type of burrowing squirrel - that may have been what Herodotus called giant ants. Much like the province that Herodotus describes, the ground of the Deosai Plateau is rich in gold dust. According to Peissel, he interviewed the Minaro tribal people who live in the Deosai Plateau, and they have confirmed that they have, for generations, been collecting the gold dust that the marmots bring to the surface when they are digging their underground burrows. Later authors like Pliny the Elder mentioned this story in the gold mining section of his Naturalis Historia. Peissel offers the theory that Herodotus may have confused the old Persian word for "marmot" with the word for "mountain ant". Research suggests that Herodotus probably did not know any Persian (or any other language except his native Greek) and was forced to rely on a many local translators when travelling in the vast multilingual Persian Empire. Herodotus did not claim to have personally seen the creatures he described. Herodotus did, though, follow up in passage 105 of Book 3, with the claim that the "ants" are said to chase and devour full-grown camels.
Some "calumnious fictions" were written about Herodotus in a work titled "On the Malice of Herodotus", by Plutarch, a Chaeronean by birth, (or it might have been a Pseudo-Plutarch, in this case "a great collector of slanders"), including the allegation that the historian was prejudiced against Thebes because the authorities there had denied him permission to set up a school. Similarly, in a "Corinthian Oration", Dio Chrysostom (or yet another pseudonymous author) accused the historian of prejudice against Corinth, sourcing it in personal bitterness over financial disappointments—an account also given by Marcellinus in his "Life of Thucydides". In fact Herodotus was in the habit of seeking out information from empowered sources within communities, such as aristocrats and priests, and this also occurred at an international level, with Periclean Athens becoming his principal source of information about events in Greece. As a result, his reports about Greek events are often coloured by Athenian bias against rival states—Thebes and Corinth in particular.
Although "The Histories" were sometimes criticized in antiquity, modern historians and philosophers take a more positive view of Herodotus's methodology, especially those searching for a paradigm of objective historical writing. A few modern scholars have argued that Herodotus exaggerated the extent of his travels and invented his sources yet his reputation continues largely intact: "The Father of History is also the father of comparative anthropology", "the father of ethnography", and he is "more modern than any other ancient historian in his approach to the ideal of total history".
Herodotus and myth.
Although Herodotus considered his "inquiries" a serious pursuit of knowledge, he was not above relating entertaining tales derived from the collective body of myth, but he did so judiciously with regard for his historical method, by corroborating the stories through enquiry and testing their probability. While the gods never make personal appearances in his account of human events, Herodotus states emphatically that "many things prove to me that the gods take part in the affairs of man" (IX, 100).
In Book One, passages 23 and 24, Herodotus relates the story of Arion, the renowned harp player, "second to no man living at that time," who was saved by a dolphin. Herodotus prefaces the story by noting that "a very wonderful thing is said to have happened," and alleges its veracity by adding that the "Corinthians and the Lesbians agree in their account of the matter." Having become very rich while at the court of Periander, Arion conceived a desire to sail to Italy and Sicily. He hired a vessel crewed by Corinthians, whom he felt he could trust, but the sailors plotted to throw him overboard and seize his wealth. Arion discovered the plot and begged for his life, but the crew gave him two options: that either he kill himself on the spot or jump ship and fend for himself in the sea. Arion flung himself into the water, and a dolphin carried him to shore.
Translations.
Several English translations of "The Histories of Herodotus" are readily available in multiple editions. The most readily available are those translated by:

</doc>
<doc id="13614" url="http://en.wikipedia.org/wiki?curid=13614" title="Heretic II">
Heretic II

Heretic II is a dark fantasy action-adventure game developed by Raven Software and published by Activision in 1998 continuing the story of Corvus, the main character from its predecessor, "Heretic".
Using a modified Quake II engine, the game features a mix of a third-person camera with a first-person shooter's action, making for a new gaming experience at the time. While progressive, this was a controversial design decision among fans of the original title, a well-known first-person shooter built on the Doom engine. The music was composed by Kevin Schilder. Gerald Brom contributed conceptual work to characters and creatures for the game. This is the only Heretic/Hexen video game that is unrelated to id Software, apart from its role as engine licenser.
"Heretic II" was later ported to Linux by Loki Software and to the Amiga by Hyperion Entertainment and Macintosh by MacPlay.
Gameplay.
Players control Corvus from a camera fixed behind the player in 3rd-person perspective. Players are able to use a combination of both melee and ranged attacks, similar to its predecessor. Defensive spells are also available, and they draw from a separate ammunition pool. The game consists of a wide variety of high fantasy medieval backdrops to Corvus's adventure. The third-person perspective and three-dimensional game environment allowed developers to introduce a wide variety of gymnastic moves, like pole vaulting, in a much more dynamic environment than the original game's engine could produce. Both games invite comparison with their respective game-engine namesake: the original Heretic was built on the "Doom" engine, and Heretic II was built using the Quake II engine, later known as id Tech 2. Heretic II was favorably received at release because it took a different approach to its design.
Plot.
After Corvus returns from his banishment, he finds that a mysterious plague has swept the land of Parthoris, taking the sanity of those it does not kill. Corvus, the protagonist of the first game, is forced to flee his hometown of Silverspring after the infected attack him, but not before he is infected himself. The effects of the disease are held at bay in Corvus’ case because he holds one of the Tomes of Power, but he still must find a cure before he succumbs.
His quest leads him through the city and swamps to a jungle palace, then through a desert canyon and insect hive, followed by a dark network of mines and finally a to a castle on a high mountain where he finds an ancient Seraph named Morcalavin. Morcalavin is trying to reach immortality using the seven Tomes of Power, but he uses a false tome, as Corvus has one of them. This has caused Morcalavin to go insane and create the plague. During a battle between Corvus and Morcalavin, Corvus switches the false tome for his real one, curing Morcalavin’s insanity and ending the plague.

</doc>
<doc id="13625" url="http://en.wikipedia.org/wiki?curid=13625" title="Holden">
Holden

GM Holden Ltd, commonly designated Holden, is an Australian automaker that operates in Australasia and is headquartered in Port Melbourne, Victoria. The company was founded in 1856 as a saddlery manufacturer in South Australia. In 1908 it moved into the automotive field, before becoming a subsidiary of the United States-based General Motors (GM) in 1931. After becoming a subsidiary of GM, the company was named General Motors-Holden's Ltd, becoming Holden Ltd in 1998—the current name was adopted in 2005.
Holden is responsible for GM's vehicle operations in Australasia, and on their behalf, held partial ownership of GM Daewoo in South Korea between 2002 and 2009. Holden has offered a broad range of locally produced vehicles, supplemented by imported GM models. Holden has offered the following badge engineered models in sharing arrangements: Chevrolet, Isuzu, Nissan, Opel, Suzuki, Toyota and Vauxhall Motors. As of 2013, the vehicle lineup consists of models from GM Korea, GM Thailand, GM in the US, and self-developed Commodore, Caprice, and Ute. Holden also distributed the European Opel brand in Australia in 2012 until the brand's Australian demise in mid-2013.
All Australian-built Holden vehicles are manufactured at Elizabeth, South Australia, and engines are produced at the Fishermans Bend plant in Port Melbourne, Victoria. Historically, production or assembly plants were operated in all mainland states of Australia, with GM's New Zealand subsidiary Holden New Zealand operating a plant until 1990. The consolidation of car production at Elizabeth was completed in 1988, but some assembly operations continued at Dandenong until 1994. 
Although Holden's involvement in exports has fluctuated since the 1950s, the declining sales of large cars in Australia has led the company to look to international markets to increase profitability. Holden announced on 11 December 2013 that local manufacturing would cease by the end of 2017 and that the company would continue to have a large and ongoing presence in Australia importing and selling cars as national sales company. Holden will retain their design center, but with reduced staffing. In the last few years, Holden incurred losses due to the strong Australian dollar, with government grants being reduced in future. In May 2014 GM reversed their decision to abandon the Lang Lang Proving Ground and have decided to keep it as part of their engineering capability in Australia.
History of the company.
Early history.
In 1852, James Alexander Holden emigrated to South Australia from Walsall, England and in 1856 established "J.A. Holden & Co", a saddlery business in Adelaide. In 1885 German-born Henry Frederick Frost joined the business as a junior partner and J.A. Holden & Co became "Holden & Frost Ltd". Edward Holden, James' grandson, joined the firm in 1905 with an interest in automobiles. From there, the firm evolved through various partnerships and, in 1908, Holden & Frost moved into the business of minor repairs to car upholstery. The company began to produce complete motorcycle sidecar bodies in 1913, and Edward experimented with fitting bodies to different types of carriages. After 1917, wartime trade restrictions led the company to start full-scale production of vehicle body shells. J.A. Holden founded a new company in 1919, "Holden's Motor Body Builders Ltd" (HMBB) specialising in car bodies and utilising a facility on King William Street in Adelaide. 
By 1923, HMBB were producing 12,000 units per year. During this time, HMBB was the first company to assemble bodies for Ford Australia until their Geelong, plant was completed. From 1924, HMBB became the exclusive supplier of car bodies for GM in Australia, with manufacturing taking place at the new Woodville, South Australia plant. These bodies were made to suit a number of chassis imported from manufacturers such as Chevrolet and Dodge. In 1926 General Motors (Australia) was established with assembly plants at Newstead, Queensland; Marrickville, New South Wales; City Road, Melbourne, Victoria; Birkenhead, South Australia; and Cottesloe, Western Australia utilizing bodies produced by Holden Motor Body Builders and imported complete knock down (CKD) chassis. The Great Depression led to a substantial downturn in production by Holden, from 34,000 units annually in 1930 to just 1,651 units one year later. In 1931 General Motors purchased Holden Motor Body Builders and merged it with General Motors (Australia) Pty Ltd to form General Motors-Holden's Ltd (GM-H). 
Throughout the 1920s Holden also supplied tramcars to Melbourne and Metropolitan Tramways Board. Several have been preserved in both Australia and New Zealand.
1940s.
Holden's second full-scale car factory, located in Fishermans Bend (Port Melbourne), was completed in 1936, with construction beginning in 1939 on a new plant in Pagewood, New South Wales. However, World War II delayed car production with efforts shifted to the construction of vehicle bodies, field guns, aircraft and engines. Before the war ended, the Australian Government took steps to encourage an Australian automotive industry. Both GM and Ford provided studies to the Australian Government outlining the production of the first Australian-designed car. Ford's proposal was the government's first choice, but required substantial financial assistance. GM's study was ultimately chosen because of its low level of government intervention. After the war, Holden returned to producing vehicle bodies, this time for Buick, Chevrolet, Pontiac and Vauxhall. The Oldsmobile Ace was also produced from 1946 to 1948.
From here, Holden continued to pursue the goal of producing an Australian car. This involved compromise with GM, as Holden's managing director, Laurence Hartnett, favoured development of a local design, while GM preferred to see an American design as the basis for "Australia's Own Car". In the end, the design was based on a previously rejected post-war Chevrolet proposal. The Holden was launched in 1948, creating long waiting lists extending through 1949 and beyond. The name "Holden" was chosen in honour of Sir Edward Holden, the company's first chairman and grandson of J.A. Holden. Other names considered were "GeM", "Austral", "Melba", "Woomerah", "Boomerang", "Emu" and "Canbra", a phonetic spelling of Canberra. Although officially designated "48-215", the car was marketed simply as the "Holden". The unofficial usage of the name "FX" originated within Holden, referring to the updated suspension on the 48-215 of 1953.
1950s.
During the 1950s, Holden dominated the Australian car market. GM invested heavily in production capacity, which allowed the company to meet increased post-war demand for motor cars. Less expensive four-cylinder cars did not offer Holden's ability to deal with rugged rural areas. 48-215 sedans were produced in parallel with the 50-2106 coupé utility from 1951; the latter was known colloquially as the "ute" and became ubiquitous in Australian rural areas as the workhorse of choice. Production of both the utility and sedan continued with minor changes until 1953, when they were replaced by the facelifted FJ model, introducing a third panel van body style. The FJ was the first major change to the Holden since its 1948 introduction. Over time it gained iconic status and remains one of Australia's most recognisable automotive symbols. A new horizontally slatted grille dominated the front-end of the FJ, which received various other trim and minor mechanical revisions. In 1954 Holden began exporting the FJ to New Zealand. Although little changed from the 48-215, marketing campaigns and price cuts kept FJ sales steady until a completely redesigned model was launched. At the 2005 Australian International Motor Show in Sydney, Holden paid homage to the FJ with the Efijy concept car.
Holden's next model, the FE, launched in 1956; offered in a new station wagon body style dubbed "Station Sedan" in the company's sales literature. In the same year Holden commenced exports to Malaya, Thailand and North Borneo. Strong sales continued in Australia, and Holden achieved a market share of more than 50 percent in 1958 with the revised FC model. This was the first Holden to be tested on the new "Holden Proving Ground" based in Lang Lang, Victoria. 1957 saw Holden's export markets grow to 17 countries, with new additions including Indonesia, Hong Kong, Singapore, Fiji, Sudan, the East Africa region and South Africa. The opening of the Dandenong, Victoria, production facility in 1956 brought further jobs; by 1959 Holden employed 19,000 workers country-wide. In 1959 complete knock down assembly began in South Africa and Indonesia.
1960s.
In 1960, Holden introduced its third major new model, the FB. The car's style was inspired by 1950s Chevrolets, with tailfins and a wrap-around windshield with "dog leg" A-pillars. By the time it was introduced, many considered the appearance dated. Much of the motoring industry at the time noted that the adopted style did not translate well to the more compact Holden. The FB became the first Holden that was adapted for left-hand-drive markets, enhancing its export potential, and as such was exported to New Caledonia, New Hebrides, the Philippines, and Hawaii.
In 1960, Ford unveiled the new Falcon in Australia, only months after its introduction in the United States. To Holden's advantage, the Falcon was not durable, particularly in the front suspension, making it ill-suited for Australian conditions. In response to the Falcon, Holden introduced the facelifted EK series in 1961; the new model featured two-tone paintwork and optional "Hydramatic" automatic transmission. A restyled EJ series came in 1962, debuting the new luxury oriented Premier model. The EH update came a year later bringing the new "Red" motor, providing better performance than the previous "Grey" motor. The HD series of 1965 saw the introduction of the "Powerglide" automatic transmission. At the same time, an "X2" performance option with a more powerful version of the 179 cuin six-cylinder engine was made available. In 1966, the HR was introduced, including changes in the form of new front and rear styling and higher-capacity engines. More significantly, the HR fitted standard front seat belts; Holden thus became the first Australian automaker to provide the safety device as standard equipment across all models. This coincided with the completion of the production plant in Acacia Ridge, Queensland. By 1963, Holden was exporting cars to Africa, the Middle East, South-East Asia, the Pacific Islands, and the Caribbean.
Holden began assembling the compact HA series Vauxhall Viva in 1964. This was superseded by the Holden Torana in 1967, a development of the Viva ending Vauxhall production in Australia. Holden offered the LC, a Torana with new styling, in 1969 with the availability of Holden's six-cylinder engine. In the development days, the six-cylinder Torana was reserved for motor racing, but research had shown that there was a business case for such a model. The LC Torana was the first application of Holden's new three-speed "Tri-Matic" automatic transmission. This was the result of Holden's A$16.5 million transformation of the Woodville, South Australia factory for its production.
Holden's association with the manufacture of Chevrolets and Pontiacs ended in 1968, coinciding with the year of Holden's next major new model, the HK . This included Holden's first V8 engine, a Chevrolet engine imported from Canada. Models based on the HK series included an extended-length prestige model, the Brougham, and a two-door coupé, the Monaro. The mainstream Holden Special was rebranded the Kingswood, and the basic fleet model, the Standard, became the Belmont. On 3 March 1969 Alexander Rhea, managing director of General Motors-Holden's at the time, was joined by press photographers and the Federal Minister of Shipping and Transport, Ian Sinclair as the two men drove the two millionth Holden, an HK Brougham off the production line. This came just over half a decade since the one millionth car, an EJ Premier sedan rolled off the Dandenong line on 25 October 1962. Following the Chevrolet V8 fitted to the HK, the first Australian-designed and mass-produced V8, the Holden V8 engine debuted in the Hurricane concept of 1969 before fitment to facelifted HT model. This was available in two capacities: 253 cuin and 308 cuin. Late in HT production, use of the new "Tri-Matic" automatic transmission, first seen in the LC Torana was phased in as "Powerglide" stock was exhausted, but Holden's official line was that the HG of 1971 was the first full-size Holden to receive it.
Despite the arrival of serious competitors—namely, the Ford Falcon, Chrysler Valiant, and Japanese cars—in the 1960s, Holden's locally produced large six- and eight-cylinder cars remained Australia's top-selling vehicles. Sales were boosted by exporting the Kingswood sedan, station wagon, and utility body styles to Indonesia, Trinidad and Tobago, Pakistan, the Philippines, and South Africa in complete knock down form.
1970s.
Holden launched the new HQ series in 1971. At this time, the company was producing all of its passenger cars in Australia, and every model was of Australian design; however, by the end of the decade, Holden was producing cars based on overseas designs. The HQ was thoroughly re-engineered, featuring a perimeter frame and semi-monocoque (unibody) construction. Other firsts included an all-coil suspension and an extended wheelbase for station wagons, while the utilities and panel vans retained the traditional coil/leaf suspension configuration. The series included the new prestige Statesman brand, which also had a longer wheelbase, replacing the Brougham. The Statesman remains noteworthy because it was not marketed as a "Holden", but rather a "Statesman".
The HQ framework led to a new generation of two-door Monaros, and, despite the introduction of the similar sized competitors, the HQ range became the top-selling Holden of all time, with 485,650 units sold in three years. 14,558 units were exported and 72,290 CKD kits were constructed. The HQ series was facelifted in 1974 with the introduction of the HJ, heralding new front panel styling and a revised rear fascia. This new bodywork was to remain, albeit with minor upgrades through the HX and HZ series. Detuned engines adhering to government emission standards were brought in with the HX series, whilst the HZ brought considerably improved road handling and comfort with the introduction of "Radial Tuned Suspension" (RTS). As a result of GM's toying with the Wankel rotary engine, as used by Mazda of Japan, an export agreement was initiated in 1975. This involved Holden exporting with powertrains, HJ, and later, HX series Premiers as the Mazda Roadpacer AP. Mazda then fitted these cars with the "13B" rotary engine and three-speed automatic transmission. Production ended in 1977, after just 840 units sold.
During the 1970s, Holden ran an advertising jingle "", based on the "Baseball, Hot Dogs, Apple Pies and Chevrolet" jingle used by Chevrolet in the United States. Also, development of the Torana continued in with the larger mid-sized LH series released in 1974, offered only as a four-door sedan. The LH Torana was one of the few cars worldwide engineered to occupy four-, six-and eight-cylinder engines. This trend continued until Holden introduced the Sunbird in 1976; essentially the four-cylinder Torana with a new name. Designated LX, both the Sunbird and Torana introduced a three-door hatchback variant. A final UC update appeared in 1978. During its production run, the Torana achieved legendary racing success in Australia, achieving victories at the Mount Panorama Circuit in Bathurst, New South Wales.
In 1975, Holden introduced the compact Gemini, the Australian version of the "T-car", based on the Opel Kadett C. The Gemini was an overseas design developed jointly with Isuzu, GM's Japanese affiliate; and was powered by a 1.6-litre four-cylinder engine. Fast becoming a popular car, the Gemini rapidly attained sales leadership in its class, and the nameplate lived on until 1987.
Holden's most popular car to date, the Commodore, was introduced in 1978 as the VB. The new family car was loosely based on the Opel Rekord E body shell, but with the front from the Opel Senator grafted to accommodate the larger Holden six-cylinder and V8 engines. Initially, the Commodore maintained Holden's sales leadership in Australia. However, some of the compromises resulting from the adoption of a design intended for another market hampered the car's acceptance. In particular, it was narrower than its predecessor and its Falcon rival, making it less comfortable for three rear-seat passengers. With the abandonment of left-hand drive markets, Holden exported almost 100,000 Commodores to markets such as New Zealand, Thailand, Hong Kong, Malaysia, Indonesia, Malta and Singapore.
Holden discontinued the Torana in 1979 and the Sunbird in 1980. After the 1978 introduction of the Commodore, the Torana became the "in-between" car, surrounded by the smaller and more economical Gemini and the larger, more sophisticated Commodore. The closest successor to the Torana was the Camira, released in 1982 as Australia's version of GM's medium-sized "J-car".
1980s.
The 1980s were challenging for Holden and the Australian automotive industry. The Australian Government tried to revive the industry with the Button car plan, which encouraged car makers to focus on producing fewer models at higher, more economical volumes, and to export cars. The decade opened with the shut-down of the Pagewood, New South Wales production plant and introduction of the light commercial Rodeo, sourced from Isuzu in Japan. The Rodeo was available in both two- and four-wheel drive chassis cab models with a choice of petrol and diesel powerplants. The range was updated in 1988 with the TF series, based on the Isuzu TF. Other cars sourced from Isuzu during the 1980s were the four-wheel drive Jackaroo (1981), the Shuttle (1982) van and the Piazza (1986) three-door sports hatchback. The second generation Holden Gemini from 1985 was also based on an Isuzu design, although, its manufacture was undertaken in Australia.
While GM Australia's commercial vehicle range had originally been mostly based on Bedford products, these had gradually been replaced by Isuzu products. This process began in the 1970s and by 1982 Holden's commercial vehicle arm no longer offered any Bedford products.
The new Holden WB commercial vehicles and the Statesman WB limousines were introduced in 1980. However, the designs, based on the HQ and updated HJ, HX and HZ models from the 1970s were less competitive than similar models in Ford's lineup. Thus, Holden abandoned those vehicle classes altogether in 1984. Sales of the Commodore also fell, with the effects of the 1979 energy crisis lessening, and for the first time the Commodore lost ground to the Ford Falcon. Sales in other segments also suffered when competition from Ford intensified, and other Australian manufacturers: Mitsubishi, Nissan and Toyota gained market share. When released in 1982, the Camira initially generated good sales, which later declined because buyers considered the 1.6-litre engine underpowered, and the car's build and ride quality below-average. The Camira lasted just seven years, and contributed to Holden's accumulated losses of over A$500 million by the mid-1980s.
In 1984 Holden introduced the VK Commodore, with significant styling changes from the previous VH. The Commodore was next updated in 1986 as the VL, which had new front and rear styling. Controversially, the VL was powered by the 3.0-litre Nissan "RB30" six-cylinder engine and had a Nissan-built, electronically controlled four-speed automatic transmission. Holden even went to court in 1984 to stop local motoring magazine Wheels from reporting on the matter. The engine change was necessitated by the legal requirement that all new cars sold in Australia after 1986 had to consume unleaded petrol. Because it was unfeasible to convert the existing six-cylinder engine to run on unleaded fuel, the Nissan engine was chosen as the best engine available. However, changing exchange rates doubled the cost of the engine and transmission over the life of the VL. The decision to opt for a Japanese-made transmission led to the closure of the Woodville, South Australia assembly plant. Confident by the apparent sign of turnaround, GM paid off Holden's mounted losses of A$780 million on 19 December 1986. At GM headquarters’ request, Holden was then reorganised and recapitalised, separating the engine and car manufacturing divisions in the process. This involved the splitting of Holden into "Holden's Motor Company" (HMC) and "Holden's Engine Company" (HEC). For the most part, car bodies were now manufactured at Elizabeth, South Australia, with engines as before, confined to the Fishermans Bend plant in Port Melbourne, Victoria. The engine manufacturing business was successful, building four-cylinder "Family II" engines for use in cars built overseas. The final phase of the Commodore's recovery strategy involved the 1988 VN, a significantly wider model powered by the American-designed, Australian-assembled 3.8-litre Buick V6 engine.
Holden began to sell the subcompact Suzuki Swift-based Barina in 1985. The Barina was launched concurrently with the Suzuki-sourced Holden Drover, followed by the Scurry later on in 1985. In the previous year, Nissan Pulsar hatchbacks were rebadged as the Holden Astra, as a result of a deal with Nissan. This arrangement ceased in 1989 when Holden entered a new alliance with Toyota, forming a new company: United Australian Automobile Industries (UAAI). UAAI resulted in Holden selling rebadged versions of Toyota's Corolla and Camry, as the Holden Nova and Apollo respectively, with Toyota re-branding the Commodore as the Lexcen.
1990s.
The company changed throughout the 1990s, increasing its Australian market share from 21 percent in 1991 to 28.2 percent in 1999. Besides manufacturing Australia's best selling car, which was exported in significant numbers, Holden continued to export many locally produced engines to power cars made elsewhere. In this decade, Holden adopted a strategy of importing cars it needed to offer a full range of competitive vehicles. During 1998, General Motors-Holden's Ltd name was shortened to "Holden Ltd".
On 26 April 1990, GM's New Zealand subsidiary Holden New Zealand announced that production at the assembly plant based in Trentham would be phased out and vehicles would be imported duty-free—this came after the 1984 closure of the Petone assembly line due to low output volumes. During the 1990s, Holden, other Australian automakers and trade unions pressured the Australian Government to halt the lowering of car import tariffs. By 1997, the federal government had already cut tariffs to 22.5 percent, from 57.5 percent ten years earlier; by 2000, a plan was formulated to reduce the tariffs to 15 percent. Holden was critical, saying that Australia's population was not large enough, and that the changes could tarnish the local industry.
Holden re-introduced its defunct Statesman title in 1990—this time under the Holden marque, as the Statesman and Caprice. For 1991, Holden updated the Statesman and Caprice with a range of improvements, including the introduction of four-wheel anti-lock brakes (ABS); although, a rear-wheel system had been standard on the Statesman Caprice from March 1976. ABS was added to the short-wheelbase Commodore range in 1992. Another returning variant was the full-size utility, and on this occasion it was based on the Commodore. The VN Commodore received a major facelift in 1993 with the VR—compared to the VN, approximately 80 percent of the car model was new. Exterior changes resulted in a smoother overall body and a "twin-kidney" grille—a Commodore styling trait that remained until the 2002 VY model and, as of 2013, remains a permanent staple on HSV variants.
Holden introduced the all-new VT Commodore in 1997, the outcome of a A$600 million development programme that spanned more than five years. The new model featured a rounded exterior body shell, improved dynamics and many firsts for an Australian-built car. Also, a stronger body structure increased crash safety. The locally produced Buick-sourced V6 engine powered the Commodore range, as did the 5.0-litre Holden V8 engine, and was replaced in 1999 by the 5.7-litre "LS" unit.
The UAAI badge-engineered cars first introduced in 1989 sold in far fewer numbers than anticipated, but the Holden Commodore, Toyota Camry, and Corolla were all successful when sold under their original nameplates. The first generation Nova and the donor Corolla were produced at Holden's Dandenong, Victoria facility until 1994. UAAI was dissolved in 1996, and Holden returned to selling only GM products. The Holden Astra and Vectra, both designed by Opel in Germany, replaced the Toyota-sourced Holden Nova and Apollo. This came after the 1994 introduction of the Opel Corsa replacing the already available Suzuki Swift as the source for the Holden Barina. Sales of the full-size Holden Suburban SUV sourced from Chevrolet commenced in 1998—lasting until 2001. Also in 1998, local assembly of the Vectra began at Elizabeth, South Australia. These cars were exported to Japan and Southeast Asia with Opel badges. However, the Vectra did not achieve sufficient sales in Australia to justify local assembly, and reverted to being fully imported in 2000.
2000s.
Holden's market surge from the 1990s reversed in the 2000s decade. In Australia, Holden's market share dropped from 27.5 percent in 2000 to 15.2 percent in 2006. From March 2003, Holden no longer held the number one sales position in Australia, losing ground to Toyota.
This overall downturn affected Holden's profits; the company recorded a combined gain of A$842.9 million between 2002 and 2004, and a combined loss of A$290 million between 2005 and 2006. Factors contributing to the loss included the development of an all-new model, the strong Australian dollar and the cost of reducing the workforce at the Elizabeth plant, including the loss of 1,400 jobs after the closure of the third-shift assembly line in 2005, after two years in operation. Holden fared better in 2007, posting an A$6 million loss. This was followed by an A$70.2 million loss in the 2008, an A$210.6 million loss in 2009, and a profit of A$112 million in 2010. On 18 May 2005, "Holden Ltd" became "GM Holden Ltd", coinciding with the resettling to the new Holden headquarters on 191 Salmon Street, Port Melbourne, Victoria.
Holden caused controversy in 2005 with their television advertisement, which ran between October and December 2005. The campaign publicised, "for the first time ever, all Australians can enjoy the financial benefit of Holden Employee Pricing". However, this did not include a discounted dealer delivery fee and savings on factory fitted options and accessories that employees received. At the same time, employees were given a further discount between 25 and 29 percent on selected models.
Holden revived the Monaro coupe in 2001. Based on the VT Commodore architecture, the coupe attracted worldwide attention after being shown as a concept car at Australian auto shows. The VT Commodore received its first major update in 2002 with the VY series. A mildly facelifted VZ model launched in 2004, introducing the "High Feature" engine. This was built at the Fishermans Bend facility completed in 2003, with a maximum output of 900 engines per day. This has reportedly added A$5.2 billion to the Australian economy; exports account for about A$450 million alone. After the VZ, the "High Feature" engine powered the all-new VE Commodore. In contrast to previous models, the VE no longer utilises an Opel-sourced platform adapted both mechanically and in size.
Throughout the 1990s, Opel had also been the source of many Holden models. To increase profitability, Holden looked to the South Korean Daewoo brand for replacements after acquiring a 44.6 percent stake—worth US$251 million—in the company in 2002 as a representative of GM. This was increased to 50.9 percent in 2005, but when GM further increased its stake to 70.1 percent around the time of its 2009 Chapter 11 reorganisation, Holden's interest was relinquished and transferred to another (undisclosed) part of GM.
The commencement of the Holden-branded Daewoo models began with the 2005 Holden Barina, which based on the Daewoo Kalos, replaced the Opel Corsa as the source of the Barina. In the same year, the Viva, based on the Daewoo Lacetti, replaced the entry-level Holden Astra Classic, although the new-generation Astra introduced in 2004 continued on. The Captiva crossover SUV came next in 2006. After discontinuing the Frontera and Jackaroo models in 2003, Holden was only left with one all-wheel drive model: the Adventra, a Commodore-based station wagon. The fourth model to be replaced with a South Korean alternative was the Vectra by the mid-size Epica in 2007. As a result of the split between GM and Isuzu, Holden lost the rights to use the "Rodeo" nameplate. Consequently, the Holden Rodeo was facelifted and relaunched as the Colorado in 2008. Following Holden's successful application for a A$149 million government grant to build a localised version of the Chevrolet Cruze in Australia from 2011, Holden in 2009 announced that it would initially import the small car unchanged from South Korea as the Holden Cruze.
Following the government grant announcement, Kevin Rudd, Australia's Prime Minister at the time, stated that production would support 600 new jobs at the Elizabeth facility; however, this failed to take into account Holden's previous announcement, whereby 600 jobs would be shed when production of the "Family II" engine ceased in late 2009. In mid-2013, Holden sought a further A$265 million, in addition to the A$275 million that was already committed by the governments of Canberra, South Australia and Victoria, to remain viable as a car manufacturer in Australia. A source close to Holden informed the "Australian" news publication that the car company is losing money on every vehicle that it produces and consequently initiated negotiations to reduce employee wages by up to A$200 per week to cut costs, following the announcement of 400 job cuts and an assembly line reduction of 65 (400 to 335) cars per day.
2010s.
In March 2012, Holden was given a $270 million lifeline by the Gillard, Weatherill and Baillieu Ministrys. In return, Holden planned to inject over $1 billion into car manufacturing in Australia. They estimated the new investment package would return around $4 billion to the Australian economy and see GM Holden continue making cars in Australia until at least 2022.
Industry Minister Kim Carr confirmed on 10 July 2013 that talks had been scheduled between the Australian government and Holden. On 13 August 2013, 1,700 employees at the Elizabeth plant in northern Adelaide voted to accept a three-year wage freeze in order to decrease the chances of the production line's closure in 2016. Holden's ultimate survival, though, depended on continued negotiations with the Federal Government—to secure funding for the period from 2016 to 2022—and the final decision of the global headquarters in Detroit, US.
On 10 December 2013, General Motors announced that Holden will cease engine and vehicle manufacturing operations in Australia by the end of 2017. As a result, 2,900 jobs would be lost over four years. Beyond 2017 Holden's Australian presence will consist of: a national sales company, a parts distribution centre and a global design studio.
Corporate affairs and identity.
As of 08 May 2015, Jeff Rolfs, Holden's CFO, is interim chairman and managing director. Holden announced on 6 February 2015 that Mark Bernhard would return to Holden as Chairman and Managing Director, the first Australian to hold the post in 25 years. Vehicles are sold countrywide through the Holden Dealer Network (310 authorised stores and 12 service centres), which employs more than 13,500 people.
In 1987, Holden Special Vehicles (HSV) was formed in partnership with Tom Walkinshaw, who primarily manufactures modified, high-performance Commodore variants. To further reinforce the brand, HSV introduced the HSV Dealer Team into the V8 Supercar fold in 2005 under the naming rights of Toll HSV Dealer Team.
The logo, or "Holden lion and stone" as it is known, has played a vital role in establishing Holden's identity. In 1928, Holden's Motor Body Builders appointed Rayner Hoff to design the emblem. The logo refers to a prehistoric fable, in which observations of lions rolling stones led to the invention of the wheel. With the 1948 launch of the 48-215, Holden revised its logo and commissioned another redesign in 1972 to better represent the company. The emblem was reworked once more in 1994.
Exports.
Holden began to export vehicles in 1954, sending the FJ to New Zealand. Exports to New Zealand have continued ever since, but to broaden their export potential, Holden began to cater their Commodore, Monaro and Statesman/Caprice models for both right- and left-hand drive markets. The Middle East is now Holden's largest export market, with the Commodore sold as the Chevrolet Lumina since 1998, and the Statesman since 1999 as the Chevrolet Caprice. Commodores are also sold as the Chevrolet Lumina in Brunei, Fiji and South Africa, and as the Chevrolet Omega in Brazil. Pontiac in North America also imported Commodore sedans from 2008 through to 2009 as the G8. However, Pontiac went bankrupt in late 2009 and GM had to shut down Pontiac in 2010. The G8's cessation was a consequence of GM's Chapter 11 bankruptcy resulting in the demise of the Pontiac brand.
Sales of the Monaro began in 2003 to the Middle East as the Chevrolet Lumina Coupe. Later on that year, a modified version of the Monaro began selling in North America as the Pontiac GTO, and under the Monaro name through Vauxhall dealerships in the United Kingdom. This arrangement continued through to 2005 when the car was discontinued. The long-wheelbase Statesman sales in the Chinese market as the Buick Royaum began in 2005, before being replaced in 2007 by the Statesman-based Buick Park Avenue. Statesman/Caprice exports to South Korea also began in 2005. These Korean models were sold as the Daewoo Statesman, and later as the Daewoo Veritas from 2008. Holden's move into international markets has been profitable; export revenue increased from A$973 million in 1999 to just under $1.3 billion in 2006.
Since 2011 the WM Caprice has been exported to North America as the Chevrolet Caprice PPV, a version of the Caprice built exclusively for law enforcement in North America sold only to police. Since 2007, the HSV-based Commodore has been exported to the United Kingdom as the Vauxhall VXR8.
In 2013, it was announced that exports of the Commodore would resume to North America in the form of the VF Commodore as the Chevrolet SS sedan for the 2014 model year.
Motorsport.
Holden has been involved with factory backed teams in Australian touring car racing since 1968, and has won the Bathurst 1000 29 times: more than any other factory. The main factory backed teams have been the Holden Dealer Team (1969–1987) and the Holden Racing Team (1990–present). Holden also currently competes in V8 Supercars regularly with other winning teams Red Bull Racing Australia, Brad Jones Racing and Tekno Autosports making it to have the most cars of a brand racing in the championship.
Holden operate a driver training facility at Norwell on the northern Gold Coast, Queensland. In addition to driver safety programs at a number of levels of experience, the centre also offers 4WD training and performance driving courses and "hot laps" in a high-performance car.
References.
</dl>

</doc>
<doc id="13654" url="http://en.wikipedia.org/wiki?curid=13654" title="Heat engine">
Heat engine

In thermodynamics, a heat engine is a system that converts heat or thermal energy to mechanical energy, which can then be used to do mechanical work. It does this by bringing a working substance from a higher state temperature to a lower state temperature. A heat "source" generates thermal energy that brings the working substance to the high temperature state. The working substance generates work in the "working body" of the engine while transferring heat to the colder "sink" until it reaches a low temperature state. During this process some of the thermal energy is converted into work by exploiting the properties of the working substance. The working substance can be any system with a non-zero heat capacity, but it usually is a gas or liquid.
In general an engine converts energy to mechanical work. Heat engines distinguish themselves from other types of engines by the fact that their efficiency is fundamentally limited by Carnot's theorem. Although this efficiency limitation can be a drawback, an advantage of heat engines is that most forms of energy can be easily converted to heat by processes like exothermic reactions (such as combustion), absorption of light or energetic particles, friction, dissipation and resistance. Since the heat source that supplies thermal energy to the engine can thus be powered by virtually any kind of energy, heat engines are very versatile and have a wide range of applicability.
Heat engines are often confused with the cycles they attempt to mimic. Typically when describing the physical device the term 'engine' is used. When describing the model the term 'cycle' is used.
Overview.
In thermodynamics, heat engines are often modeled using a standard engineering model such as the Otto cycle. The theoretical model can be refined and augmented with actual data from an operating engine, using tools such as an indicator diagram. Since very few actual implementations of heat engines exactly match their underlying thermodynamic cycles, one could say that a thermodynamic cycle is an ideal case of a mechanical engine. In any case, fully understanding an engine and its efficiency requires gaining a good understanding of the (possibly simplified or idealized) theoretical model, the practical nuances of an actual mechanical engine, and the discrepancies between the two.
In general terms, the larger the difference in temperature between the hot source and the cold sink, the larger is the potential thermal efficiency of the cycle. On Earth, the cold side of any heat engine is limited to being close to the ambient temperature of the environment, or not much lower than 300 Kelvin, so most efforts to improve the thermodynamic efficiencies of various heat engines focus on increasing the temperature of the source, within material limits. The maximum theoretical efficiency of a heat engine (which no engine ever attains) is equal to the temperature difference between the hot and cold ends divided by the temperature at the hot end, all expressed in absolute temperature or kelvins.
The efficiency of various heat engines proposed or used today has a large range: 
All these processes gain their efficiency (or lack thereof) from the temperature drop across them. Significant energy may be used for auxiliary equipment, such as pumps, which effectively reduces efficiency.
Power.
Heat engines can be characterized by their specific power, which is typically given in kilowatts per litre of engine displacement (in the U.S. also horsepower per cubic inch). The result offers an approximation of the peak power output of an engine. This is not to be confused with fuel efficiency, since high efficiency often requires a lean fuel-air ratio, and thus lower power density. A modern high-performance car engine makes in excess of 75 kW/l (1.65 hp/in3).
Everyday examples.
Examples of everyday heat engines include the steam engine (for example in trains), the diesel engine, and the gasoline (petrol) engine in an automobile. A common toy that is also a heat engine is a drinking bird. Also the stirling engine is a heat engine. All of these familiar heat engines are powered by the expansion of heated gases. The general surroundings are the heat sink, which provides relatively cool gases that, when heated, expand rapidly to drive the mechanical motion of the engine.
Examples of heat engines.
It is important to note that although some cycles have a typical combustion location (internal or external), they often can be implemented with the other. For example, John Ericsson developed an external heated engine running on a cycle very much like the earlier Diesel cycle. In addition, externally heated engines can often be implemented in open or closed cycles.
Earth's heat engine.
Earth's atmosphere and hydrosphere—Earth’s heat engine—are coupled processes that constantly even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation, when distributing heat around the globe.
The Hadley system provides an example of a heat engine. The Hadley circulation is identified with rising of warm and moist air in the equatorial region with descent of colder air in the subtropics corresponding to a thermally driven direct circulation, with consequent net production of kinetic energy.
Phase-change cycles.
In these cycles and engines, the working fluids are gases and liquids. The engine converts the working fluid from a gas to a liquid, from liquid to gas, or both, generating work from the fluid expansion or compression.
Gas-only cycles.
In these cycles and engines the working fluid is always a gas (i.e., there is no phase change):
Liquid only cycle.
In these cycles and engines the working fluid are always like liquid:
Cycles used for refrigeration.
A domestic refrigerator is an example of a heat pump: a heat engine in reverse. Work is used to create a heat differential. Many cycles can run in reverse to move heat from the cold side to the hot side, making the cold side cooler and the hot side hotter. Internal combustion engine versions of these cycles are, by their nature, not reversible.
Refrigeration cycles include:
Evaporative heat engines.
The Barton evaporation engine is a heat engine based on a cycle producing power and cooled moist air from the evaporation of water into hot dry air.
Mesoscopic heat engines.
Mesoscopic heat engines are nanoscale devices that may serve the goal of processing heat fluxes and perform useful work at small scales. Potential applications include e.g. electric cooling devices.
In such mesoscopic heat engines, work per cycle of operation fluctuates due to thermal noise.
There is exact equality that relates average of exponents of work performed by any heat engine and the heat transfer from the hotter heat bath. This relation transforms the Carnot's inequality into exact equality.
Efficiency.
The efficiency of a heat engine relates how much useful work is output for a given amount of heat energy input.
From the laws of thermodynamics:
In other words, a heat engine absorbs heat energy from the high temperature heat source, converting part of it to useful work and delivering the rest to the cold temperature heat sink.
In general, the efficiency of a given heat transfer process (whether it be a refrigerator, a heat pump or an engine) is defined informally by the ratio of "what you get out" to "what you put in".
In the case of an engine, one desires to extract work and puts in a heat transfer.
The "theoretical" maximum efficiency of any heat engine depends only on the temperatures it operates between. This efficiency is usually derived using an ideal imaginary heat engine such as the Carnot heat engine, although other engines using different cycles can also attain maximum efficiency. Mathematically, this is because in reversible processes, the change in entropy of the cold reservoir is the negative of that of the hot reservoir (i.e., formula_7), keeping the overall change of entropy zero. Thus:
where formula_9 is the absolute temperature of the hot source and formula_10 that of the cold sink, usually measured in kelvin. Note that formula_11 is positive while formula_12 is negative; in any reversible work-extracting process, entropy is overall not increased, but rather is moved from a hot (high-entropy) system to a cold (low-entropy one), decreasing the entropy of the heat source and increasing that of the heat sink.
The reasoning behind this being the maximal efficiency goes as follows. It is first assumed that if a more efficient heat engine than a Carnot engine is possible, then it could be driven in reverse as a heat pump. Mathematical analysis can be used to show that this assumed combination would result in a net decrease in entropy. Since, by the second law of thermodynamics, this is statistically improbable to the point of exclusion, the Carnot efficiency is a theoretical upper bound on the reliable efficiency of "any" process.
Empirically, no heat engine has ever been shown to run at a greater efficiency than a Carnot cycle heat engine.
Figure 2 and Figure 3 show variations on Carnot cycle efficiency. Figure 2 indicates how efficiency changes with an increase in the heat addition temperature for a constant compressor inlet temperature. Figure 3 indicates how the efficiency changes with an increase in the heat rejection temperature for a constant turbine inlet temperature.
Endoreversible heat engines.
The most Carnot efficiency as a criterion of heat engine performance is the fact that by its nature, any maximally efficient Carnot cycle must operate at an infinitesimal temperature gradient. This is because "any" transfer of heat between two bodies at differing temperatures is irreversible, and therefore the Carnot efficiency expression only applies in the infinitesimal limit. The major problem with that is that the object of most heat engines is to output some sort of power, and infinitesimal power is usually not what is being sought.
A different measure of ideal heat engine efficiency is given by considerations of endoreversible thermodynamics, where the cycle is identical to the Carnot cycle except in that the two processes of heat transfer are "not" reversible (Callen 1985):
This model does a better job of predicting how well real-world heat engines can do (Callen 1985, see also endoreversible thermodynamics):
As shown, the endoreversible efficiency much more closely models the observed data.
History.
Heat engines have been known since antiquity but were only made into useful devices at the time of the industrial revolution in the 18th century. They continue to be developed today.
Heat engine enhancements.
Engineers have studied the various heat engine cycles extensively in effort to improve the amount of usable work they could extract from a given power source. The Carnot cycle limit cannot be reached with any gas-based cycle, but engineers have worked out at least two ways to possibly go around that limit, and one way to get better efficiency without bending any rules.
Heat engine processes.
Each process is one of the following:

</doc>
<doc id="13660" url="http://en.wikipedia.org/wiki?curid=13660" title="Homeomorphism">
Homeomorphism

In the mathematical field of topology, a homeomorphism or topological isomorphism or bi continuous function is a continuous function between topological spaces that has a continuous inverse function. Homeomorphisms are the isomorphisms in the category of topological spaces—that is, they are the mappings that preserve all the topological properties of a given space. Two spaces with a homeomorphism between them are called homeomorphic, and from a topological viewpoint they are the same. The word "homeomorphism" comes from the Greek words "ὅμοιος" ("homoios") = similar and "μορφή" ("morphē") = shape, form.
Roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. An often-repeated mathematical joke is that topologists can't tell their coffee cup from their donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.
Topology is the study of those properties of objects that do not change when homeomorphisms are applied.
Definition.
A function "f": "X" → "Y" between two topological spaces ("X", "TX") and ("Y", "TY") is called a homeomorphism if it has the following properties:
A function with these three properties is sometimes called bicontinuous. If such a function exists, we say "X" and "Y" are homeomorphic. A self-homeomorphism is a homeomorphism of a topological space and itself. The homeomorphisms form an equivalence relation on the class of all topological spaces. The resulting equivalence classes are called homeomorphism classes.
Notes.
The third requirement, that "f" −1 be continuous, is essential. Consider for instance the function "f": [0, 2π) → S1 (the unit circle in formula_6) defined by "f"(φ) = (cos(φ), sin(φ)). This function is bijective and continuous, but not a homeomorphism (S1 is compact but [0, 2π) is not). The function "f" −1 is not continuous at the point (1, 0), because although "f" −1 maps (1, 0) to 0, any neighbourhood of this point also includes points that the function maps close to 2π, but the points it maps to numbers in between lie outside the neighbourhood.
Homeomorphisms are the isomorphisms in the category of topological spaces. As such, the composition of two homeomorphisms is again a homeomorphism, and the set of all self-homeomorphisms "X" → "X" forms a group, called the homeomorphism group of "X", often denoted Homeo("X"); this group can be given a topology, such as the compact-open topology, making it a topological group.
For some purposes, the homeomorphism group happens to be too big, but by means of the isotopy relation, one can reduce this group to the mapping class group.
Similarly, as usual in category theory, given two spaces that are homeomorphic, the space of homeomorphisms between them, Homeo("X," "Y"), is a torsor for the homeomorphism groups Homeo("X") and Homeo("Y"), and given a specific homeomorphism between "X" and "Y", all three sets are identified.
Informal discussion.
The intuitive criterion of stretching, bending, cutting and gluing back together takes a certain amount of practice to apply correctly—it may not be obvious from the description above that deforming a line segment to a point is impermissible, for instance. It is thus important to realize that it is the formal definition given above that counts.
This characterization of a homeomorphism often leads to confusion with the concept of homotopy, which is actually "defined" as a continuous deformation, but from one "function" to another, rather than one space to another. In the case of a homeomorphism, envisioning a continuous deformation is a mental tool for keeping track of which points on space "X" correspond to which points on "Y"—one just follows them as "X" deforms. In the case of homotopy, the continuous deformation from one map to the other is of the essence, and it is also less restrictive, since none of the maps involved need to be one-to-one or onto. Homotopy does lead to a relation on spaces: homotopy equivalence.
There is a name for the kind of deformation involved in visualizing a homeomorphism. It is (except when cutting and regluing are required) an isotopy between the identity map on "X" and the homeomorphism from "X" to "Y".

</doc>
<doc id="13675" url="http://en.wikipedia.org/wiki?curid=13675" title="Henry Ainsworth">
Henry Ainsworth

Henry Ainsworth (1571–1622) was an English Nonconformist clergyman and scholar.
Life.
He was born of a farming family of Swanton Morley, Norfolk. He was educated at St John's College, Cambridge, later moving to Caius College, and, after associating with the Puritan party in the Church, eventually joined the Separatists.
Driven abroad about 1593, he found a home in "a blind lane at Amsterdam", acting as "porter" to a bookseller, who, on discovering his knowledge of Hebrew, introduced him to other scholars. When part of the London church, of which Francis Johnson (then in prison) was pastor, reassembled in Amsterdam, Ainsworth was chosen as their doctor or teacher. In 1596 he drew up a confession of their faith, reissued in Latin in 1598 and dedicated to the various universities of Europe (including St Andrews, Scotland). Johnson joined his flock in 1597, and in 1604 he and Ainsworth composed "An Apology or Defence of such true Christians as are commonly but unjustly called Brownists".
Organizing the church was not easy and dissension was rife. Though often involved in controversy, Ainsworth was not arrogant, but was a steadfast and cultured champion of the principles represented by the early Congregationalists. Amid all the controversy, he steadily pursued his studies. The combination was so unique that some have mistaken him for two different individuals. (Confusion has also been occasioned through his friendly controversy with one John Ainsworth, who left the Anglican for the Roman Catholic church.)
In 1610 Ainsworth was forced reluctantly to withdraw, with a large part of their church, from Johnson and those who adhered to him. A difference of principle as to the church's right to revise its officers' decisions had been growing between them; Ainsworth taking the more Congregational view. In spirit he remained a man of peace.
He died in 1622 in Amsterdam.
Works.
In 1608 Ainsworth answered Richard Bernard's "The Separatist Schisme", but his greatest minor work in this field was his reply to John Smyth (commonly called "the Se-Baptist"), entitled "Defence of Holy Scripture, Worship and Ministry used in the Christian Churches separated from Antichrist, against the Challenges, Cavils and Contradictions of Mr Smyth" (1609).
His scholarly works include his "Annotations"—on "Genesis" (1616); "Exodus" (1617); "Leviticus" (1618); "Numbers" (1619); "Deuteronomy" (1619); "Psalms" (including a metrical version, 1612); and the "Song of Solomon" (1623). These were collected in folio in 1627. From the outset the "Annotations" took a commanding place, especially among continental scholars, establishing a scholarly tradition for English nonconformity.
His publication of Psalms, "The Book of Psalmes: Englished both in Prose and Metre with Annotations" (Amsterdam, 1612), which includes thirty-nine separate monophonic psalm tunes, constituted the Ainsworth Psalter, the only book of music brought to New England in 1620 by the Pilgrim settlers. Although its content was later reworked into the Bay Psalm Book, it had an important influence on the early development of American psalmody.
Ainsworth died in 1622, or early in 1623, for in that year was published his "Seasonable Discourse, or a Censure upon a Dialogue of the Anabaptists", in which the editor speaks of him as a departed worthy.

</doc>
<doc id="13686" url="http://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum (]) is a municipality and a town in the Netherlands, in the province of North Holland. Located in the region called "Het Gooi", it is the largest town in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.
The town.
Hilversum lies 31 km south-east of Amsterdam and 19 km north of Utrecht.
The town is often called "media city" since it is the principal centre for radio and television broadcasting in The Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based here. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS (Nederlandse Omroep Stichting), as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies. As a result many old AM radio sets in Europe had a "Hilversum" dial position marked on their tuning scales (along with Athlone, Kalundborg and others).
Hilversum is also known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.
Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centers (such as Hilvertshof, De Gijsbrecht, Kerkelanden, Riebeeck Galerij and Seinhorst.) In the region, the city centre is called "het dorp", which means "the village".
History.
Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BCE material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424, on 21 March at 6:30 am (the hour at which people got up, as the farm was full of restless and loud animals), Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development. The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair. In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."
For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. The city was the headquarters of the German ground forces (Heer) in the Netherlands .
The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands, and in 1948 being taken over by Philips. By then the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. In the meantime, almost all Dutch radio broadcasting organizations (followed by television broadcasters in the 1950s) established their headquarters in Hilversum and provided a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.
In 1964, the population reached a record high – over 103,000 people called Hilversum home. The current population hovers around 85,000. Several factors figure into the decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat (""). The third reason for this decline of the population was due to the fact that the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.
Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.
Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from 'Leefbaar Hilversum' teamed up with 'Leefbaar Utrecht' leaders to found a national 'Leefbaar Nederland' party. By strange coincidence, in 2002 the most vocal 'Leefbaar Rotterdam' politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.
The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo (""), the largest man-made wildlife crossing in the world.
The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn, and of a January 29, 2015 fake gunman demanding airtime at Nederlandse Omroep Stichting's headquarters.
The population declined from 103,000 in 1964 to 84,000 in 2006.
Transport.
Hilversum is well connected to the Dutch railway network, and contains three stations: 
One can get the best connections from the station Hilversum, as this is an Intercity station.
Local government.
The municipal council of Hilversum in 2010 consists of 37 seats, which are divided as followed:
It was the first city with a "Leefbaar" party (which was intended as just a local party).
Notable residents.
Notable people born in Hilversum:

</doc>
<doc id="13722" url="http://en.wikipedia.org/wiki?curid=13722" title="Robert Koch">
Robert Koch

Robert Heinrich Herman Koch (; ]; 11 December 1843 – 27 May 1910) was a celebrated German physician and pioneering microbiologist. The founder of modern bacteriology, he is known for his role in identifying the specific causative agents of tuberculosis, cholera, and anthrax and for giving experimental support for the concept of infectious disease. In addition to his trail-blazing studies on these diseases, Koch created and improved laboratory technologies and techniques in the field of microbiology, and made key discoveries in public health. His research led to the creation of Koch’s postulates, a series of four generalized principles linking specific microorganisms to specific diseases that remain today the "gold standard" in medical microbiology. As a result of his groundbreaking research on tuberculosis, Koch received the Nobel Prize in Physiology or Medicine in 1905.
Personal life.
Robert Koch was born in Clausthal, Hanover, Germany, on 11 December 1843, to Hermann Koch and Mathilde Julie Henriette Biewand. Koch excelled in academics from an early age. Before entering school in 1848, he had taught himself how to read and write. He graduated from high school in 1862, having excelled in science and maths. At the age of 19, Koch entered the University of Göttingen, studying natural science. However, after two semesters, Koch decided to change his area of study to medicine, as he aspired to be a physician. During his fifth semester of medical school, Jacob Henle, an anatomist who had published a theory of contagion in 1840, asked him to participate in his research project on uterine nerve structure. In his sixth semester, Koch began to conduct research at the Physiological Institute, where he studied succinic acid secretion. This would eventually form the basis of his dissertation. In January 1866, Koch graduated from medical school, earning honors of the highest distinction. In July 1867, following his graduation from medical school, Koch married Emma Adolfine Josephine Fraatz, and the two had a daughter, Gertrude, in 1868. After his graduation in 1866, he worked as a surgeon in the Franco-Prussian War, and following his service, worked as a physician in Wollstein, Posen. Koch’s marriage with Emma Fraatz ended in 1893, and later that same year, he married actress Hedwig Freiberg. From 1885 to 1890, he served as an administrator and professor at Berlin University. Koch suffered a heart attack on 9 April 1910, and never made a complete recovery. On 27 May, only three days after giving a lecture on his tuberculosis research at the Prussian Academy of Sciences, Robert Koch died in Baden-Baden at the age of 66. Following his death, the Institute named its establishment after him in his honour.
Research contributions.
Anthrax.
Robert Koch is widely known for his work with anthrax, discovering the causative agent of the fatal disease to be "Bacillus anthracis". Koch discovered the formation in anthrax bacteria of spores that could remain dormant under specific conditions. However, under optimal conditions, the spores were activated and caused disease. To determine this causative agent, he dry-fixed bacterial cultures onto glass slides, used dyes to stain the cultures, and observed them through a microscope. Koch’s work with anthrax is notable in that he was the first to link a specific microorganism with a specific disease, rejecting the idea of spontaneous generation and supporting the germ theory of disease.
Koch's four postulates.
Koch accepted a position as government advisor with the Imperial Department of Health in 1880. During his time as government advisor, he published a report in which he stated the importance of pure cultures in isolating disease-causing organisms and explained the necessary steps to obtain these cultures, methods which are summarized in Koch’s four postulates. Koch’s discovery of the causative agent of anthrax led to the formation of a generic set of postulates which can be used in the determination of the cause of any infectious disease. These postulates, which not only outlined a method for linking cause and effect of an infectious disease but also established the significance of laboratory culture of infectious agents, are listed here:
1. The organism must always be present, in every case of the disease.
2. The organism must be isolated from a host containing the disease and grown in pure culture.
3. Samples of the organism taken from pure culture must cause the same disease when inoculated into a healthy, susceptible animal in the laboratory.
4. The organism must be isolated from the inoculated animal and must be identified as the same original organism first isolated from the originally diseased host.
Isolating pure culture on solid media.
Koch began conducting research on microorganisms in a laboratory connected to his patient examination room. Koch’s early research in this laboratory proved to yield one of his major contributions to the field of microbiology, as it was there that he developed the technique of growing bacteria. Koch's second postulate calls for the isolation and growth of a selected pathogen in pure laboratory culture. In an attempt to grow bacteria, Koch began to use solid nutrients such as potato slices. Through these initial experiments, Koch observed individual colonies of identical, pure cells. Coming to the conclusion that potato slices were not suitable media for all organisms, Koch later began to use nutrient solutions with gelatin. However, he soon realized that gelatin, like potato slices, was not the optimal medium for bacterial growth, as it did not remain solid at 37˚C, the ideal temperature for growth of most human pathogens. As suggested to him by Walther and Angelina Hesse, Koch began to utilize agar to grow and isolate pure cultures, as this polysaccharide remains solid at 37˚C, is not degraded by most bacteria, and results in a transparent medium.
Cholera.
Koch next turned his attention to cholera, and began to conduct research in Egypt in the hopes of isolating the causative agent of the disease. However, he was not able to complete the task before the epidemic in Egypt ended, and subsequently traveled to India to continue with the study. In India, Koch was indeed able to determine the causative agent of cholera, isolating "Vibrio cholera". The bacterium had originally been isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.
Tuberculosis.
During his time as the government advisor with the Imperial Department of Health in Berlin in the 1880s, Robert Koch became interested in tuberculosis research. At the time, it was widely believed that tuberculosis was an inherited disease. However, Koch was convinced that the disease was caused by a bacterium and was infectious, and tested his four postulates using guinea pigs. Through these experiments, he found that his experiments with tuberculosis satisfied all four of his postulates. In 1882, he published his findings on tuberculosis, in which he reported the causative agent of the disease to be the slow-growing "Mycobacterium tuberculosis". His work with this disease won Koch the Nobel Prize in Physiology and Medicine in 1905. Additionally, Koch's research on tuberculosis, along with his studies on tropical diseases, won him the Prussian Order Pour le Merite in 1906 and the Robert Koch medal, established to honour the greatest living physicians, in 1908.

</doc>
<doc id="13734" url="http://en.wikipedia.org/wiki?curid=13734" title="Heterocyclic compound">
Heterocyclic compound

A heterocyclic compound or ring structure is a cyclic compound that has atoms of at least two different elements as members of its ring(s). Heterocyclic chemistry is the branch of chemistry dealing with the synthesis, properties and applications of these heterocycles. In contrast, the rings of homocyclic compounds consist entirely of atoms of the same element.
Although heterocyclic compounds may be inorganic, most contain at least one carbon. While atoms that are neither carbon nor hydrogen are normally referred to in organic chemistry as heteroatoms, this is usually in comparison to the all-carbon backbone. But this does not prevent a compound such as borazine (which has no carbon atoms) from being labelled "heterocyclic". IUPAC recommends the Hantzsch-Widman nomenclature for naming heterocyclic compounds.
Classification based on electronic structure.
Heterocyclic compounds can be usefully classified based on their electronic structure. The saturated heterocycles behave like the acyclic derivatives. Thus, piperidine and tetrahydrofuran are conventional amines and ethers, with modified steric profiles. Therefore, the study of heterocyclic chemistry focuses especially on unsaturated derivatives, and the preponderance of work and applications involves unstrained 5- and 6-membered rings. Included are pyridine, thiophene, pyrrole, and furan. Another large class of heterocycles are fused to benzene rings, which for pyridine, thiophene, pyrrole, and furan are quinoline, benzothiophene, indole, and benzofuran, respectively. Fusion of two benzene rings gives rise to a third large family of compounds, respectively the acridine, dibenzothiophene, carbazole, and dibenzofuran. The unsaturated rings can be classified according to the participation of the heteroatom in the pi system.
3-membered rings.
Heterocycles with three atoms in the ring are more reactive because of ring strain. Those containing one heteroatom are, in general, stable. Those with two heteroatoms are more likely to occur as reactive intermediates.<br>
Common 3-membered heterocycles with "one" heteroatom are:
Those with "two" heteroatoms include:
4-membered rings.
Compounds with one heteroatom:
Compounds with two heteroatoms:
5-membered rings.
With heterocycles containing five atoms, the unsaturated compounds are frequently more stable because of aromaticity.
Five-membered rings with "one" heteroatom:
The 5-membered ring compounds containing "two" heteroatoms, at least one of which is nitrogen, are collectively called the azoles. Thiazoles and isothiazoles contain a sulfur and a nitrogen atom in the ring. Dithiolanes have two sulfur atoms.
A large group of 5-membered ring compounds with "three" heteroatoms also exists. One example is dithiazoles that contain two sulfur and a nitrogen atom.
Five-member ring compounds with "four" heteroatoms:
With 5-heteroatoms, the compound may be considered inorganic rather than heterocyclic. Pentazole is the all nitrogen heteroatom unsaturated compound.
6-membered rings.
Six-membered rings with a "single" heteroatom:
With "two" heteroatoms:
With three heteroatoms:
With four heteroatoms:
The hypothetical compound with six nitrogen heteroatoms would be hexazine.
7-membered rings.
With 7-membered rings, the heteroatom must be able to provide an empty pi orbital (e.g., boron) for "normal" aromatic stabilization to be available; otherwise, homoaromaticity may be possible. Compounds with one heteroatom include:
Those with two heteroatoms include:
Fused rings.
Heterocyclic rings systems that are formally derived by fusion with other rings, either carbocyclic or heterocyclic, have a variety of common and systematic names. For example, with the benzo-fused unsaturated nitrogen heterocycles, pyrrole provides indole or isoindole depending on the orientation. The pyridine analog is quinoline or isoquinoline. For azepine, benzazepine is the preferred name. Likewise, the compounds with two benzene rings fused to the central heterocycle are carbazole, acridine, and dibenzoazepine.
History of heterocyclic chemistry.
The history of heterocyclic chemistry began in the 1800s, in step with the development of organic chemistry. Some noteworthy developments:
1818: Brugnatelli isolates alloxan from uric acid
1832: Dobereiner produces furfural (a furan) by treating starch with sulfuric acid
1834: Runge obtains pyrrole ("fiery oil") by dry distillation of bones
1906: Friedlander synthesizes indigo dye, allowing synthetic chemistry to displace a large agricultural industry
1936: Treibs isolates chlorophyl derivatives from crude oil, explaining the biological origin of petroleum.
1951: Chargaff's rules are described, highlighting the role of heterocyclic compounds (purines and pyrimidines) in the genetic code.
Commercial exploitation.
Leading companies with a vast number of patents related to heterocyclic compounds are Bayer, Merck, Ciba-Geigy, Pfizer, Eli Lily, BASF, Hoffmann La Roche, ER Sqibb, Warner Lambert and Hoechst.

</doc>
<doc id="13744" url="http://en.wikipedia.org/wiki?curid=13744" title="List of humorists">
List of humorists

 
A humorist or humourist (see spelling differences) is a person who writes or performs humorous material.
A humorist is usually distinct from a stand-up comedian. For people who are primarily stand-ups, see list of stand-up comedians.
Notable humorists include:

</doc>
<doc id="13756" url="http://en.wikipedia.org/wiki?curid=13756" title="Hymn">
Hymn

A hymn is a type of song, usually religious, specifically written for the purpose of praise, adoration or prayer, and typically addressed to a deity or deities, or to a prominent figure or personification. Although most familiar to speakers of English in the context of Christian churches, hymns are also a fixture of other world religions, especially on the Indian subcontinent. Hymns also survive from antiquity, especially from Egyptian and Greek cultures. Some of the oldest surviving examples of notated music are hymns with Greek texts. The word "hymn" derives from Greek ὕμνος ("hymnos"), which means "a song of praise". Collections of hymns are known as hymnals or hymn books. Hymns may or may not include instrumental accompaniment.
Origins.
Ancient hymns include the Egyptian "Great Hymn to the Aten", composed by Pharaoh Akhenaten; the "Vedas", a collection of hymns in the tradition of Hinduism; and the Psalms, a collection of songs from Judaism. The Western tradition of hymnody begins with the Homeric Hymns, a collection of ancient Greek hymns, the oldest of which were written in the 7th century BC, praising deities of the ancient Greek religions. Surviving from the 3rd century BC is a collection of six literary hymns ("Ὕμνοι") by the Alexandrian poet Callimachus.
Patristic writers began applying the term ὕμνος, or "hymnus" in Latin, to Christian songs of praise, and frequently used the word as a synonym for "psalm".
The word "hymn" came from the Greek word "hymnos".<http://www.sharefaith.com/guide/Christian-Music/hymns-the-songs-and-the-stories/history-of-hymns.html>
Christian hymnody.
Originally modeled on the Psalms and other poetic passages (commonly referred to as "canticles") in the Scriptures, Christian hymns are generally directed as praise to the Christian God. Many refer to Jesus Christ either directly or indirectly.
Since the earliest times, Christians have sung "psalms and hymns and spiritual songs", both in private devotions and in corporate worship (; ; ; ; ; ; ; cf. ; ).
One definition of a hymn is "...a lyric poem, reverently and devotionally conceived, which is designed to be sung and which expresses the worshipper's attitude toward God or God's purposes in human life. It should be simple and metrical in form, genuinely emotional, poetic and literary in style, spiritual in quality, and in its ideas so direct and so immediately apparent as to unify a congregation while singing it."
Christian hymns are often written with special or seasonal themes and these are used on holy days such as Christmas, Easter and the Feast of All Saints, or during particular seasons such as Advent and Lent. Others are used to encourage reverence for the Holy Bible or to celebrate Christian practices such as the eucharist or baptism. Some hymns praise or address individual saints, particularly the Blessed Virgin Mary; such hymns are particularly prevalent in Catholicism, Eastern Orthodoxy and to some extent High Church Anglicanism.
A writer of hymns is known as a "hymnist" or "hymnodist", and the practice of singing hymns is called "hymnody"; the same word is used for the collectivity of hymns belonging to a particular denomination or period (e.g. "nineteenth century Methodist hymnody" would mean the body of hymns written and/or used by Methodists in the 19th century). A collection of hymns is called a "hymnal" or "hymnary". These may or may not include music. A student of hymnody is called a "hymnologist", and the scholarly study of hymns, hymnists and hymnody is hymnology. The music to which a hymn may be sung is a hymn tune. 
In many Evangelical churches, traditional songs are classified as hymns while more contemporary worship songs are not considered hymns. The reason for this distinction is unclear, but according to some it is due to the radical shift of style and devotional thinking that began with the Jesus movement and Jesus music.
Music and accompaniment.
In ancient and medieval times, stringed instruments such as the harp, lyre and lute were used with psalms and hymns.
Since there is a lack of musical notation in early writings, the actual musical forms in the early church can only be surmised. During the Middle Ages a rich hymnody developed in the form of Gregorian chant or plainsong. This type was sung in unison, in one of eight church modes, and most often by monastic choirs. While they were written originally in Latin, many have been translated; a familiar example is the 4th century "Of the Father's Heart Begotten" sung to the 11th century plainsong "Divinum Mysterium".
Western church.
Later hymnody in the Western church introduced four-part vocal harmony as the norm, adopting major and minor keys, and came to be led by organ and choir. It shares many elements with classical music.
Today, except for choirs, more musically inclined congregations and "a cappella" congregations, hymns are typically sung in unison. In some cases complementary full settings for organ are also published, in others organists and other accompanists are expected to transcribe the four-part vocal score for their instrument of choice.
To illustrate Protestant usage, in the traditional services and liturgies of the Methodist churches, which are based upon Anglican practice, hymns are sung (often accompanied by an organ) during the processional to the altar, during the receiving of the Eucharist, during the recessional, and sometimes at other points during the service. These hymns can be found in a common book such as the United Methodist Hymnal. The Doxology is also sung after the tithes and offerings are brought up to the altar.
Contemporary Christian worship, as often found in Evangelicalism and Pentecostalism, may include the use of contemporary worship music played with electric guitars and the drum kit, sharing many elements with rock music.
Other groups of Christians have historically excluded instrumental accompaniment, citing the absence of instruments in worship by the church in the first several centuries of its existence, and adhere to an unaccompanied "a cappella" congregational singing of hymns. These groups include the 'Brethren' (often both 'Open' and 'Exclusive'), the Churches of Christ, Mennonites, Primitive Baptists, and certain Reformed churches, although during the last century or so, several of these, such as the Free Church of Scotland have abandoned this stance.
Eastern church.
Eastern Christianity (the Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches) have a very rich and ancient hymnographical tradition. 
Eastern chant is almost always a cappella, and instrumental accompaniment is rare. The central form of chant in the Eastern Orthodoxy is Byzantine Chant, which is used to chant all forms of liturgical worship. Exceptions include the Coptic Orthodox tradition which makes use of the sistrum, the Indian Orthodox (Malankara Orthodox Syrian Church) which makes use of the organ and the Ethiopian Orthodox Tewahedo Church, which also uses drums, cymbals and other instruments on certain occasions.
The development of Christian hymnody.
Thomas Aquinas, in the introduction to his commentary on the Psalms, defined the Christian hymn thus: ""Hymnus est laus Dei cum cantico; canticum autem exultatio mentis de aeternis habita, prorumpens in vocem"." ("A hymn is the praise of God with song; a song is the exultation of the mind dwelling on eternal things, bursting forth in the voice.")
The Protestant Reformation resulted in two conflicting attitudes to hymns. One approach, the regulative principle of worship, favoured by many Zwinglians, Calvinists and some radical reformers, considered anything that was not directly authorised by the Bible to be a novel and Catholic introduction to worship, which was to be rejected. All hymns that were not direct quotations from the Bible fell into this category. Such hymns were banned, along with any form of instrumental musical accompaniment, and organs were removed from churches. Instead of hymns, biblical psalms were chanted, most often without accompaniment, to very basic melodies. This was known as exclusive psalmody. Examples of this may still be found in various places, including in some of the Presbyterian churches of western Scotland. 
The other Reformation approach, the normative principle of worship, produced a burst of hymn writing and congregational singing. Martin Luther is notable not only as a reformer, but as the author of many hymns including "Ein feste Burg ist unser Gott" ("A Mighty Fortress Is Our God"), which is sung today even by Catholics, and "Gelobet seist du, Jesu Christ" ("Praise be to You, Jesus Christ") for Christmas. Luther and his followers often used their hymns, or chorales, to teach tenets of the faith to worshipers. The first Protestant hymnal was published in Bohemia in 1532 by the Unitas Fratrum. Count Zinzendorf, the Lutheran leader of the Moravian Church in the 18th century wrote some 2,000 hymns. The earlier English writers tended to paraphrase biblical texts, particularly Psalms; Isaac Watts followed this tradition, but is also credited as having written the first English hymn which was not a direct paraphrase of Scripture.
Watts (1674–1748), whose father was an Elder of a dissenter congregation, complained at age 16, that when allowed only psalms to sing, the faithful could not even sing about their Lord, Christ Jesus. His father invited him to see what he could do about it; the result was Watts' first hymn, "Behold the glories of the Lamb".
Found in few hymnals today, the hymn has eight stanzas in common meter and is based on Revelation 5:6, 8, 9, 10, 12.
Relying heavily on Scripture, Watts wrote metered texts based on New Testament passages that brought the Christian faith into the songs of the church. Isaac Watts has been called "the father of English hymnody", but Erik Routley sees him more as "the liberator of English hymnody", because his hymns, and hymns like them, moved worshipers beyond singing only Old Testament psalms, inspiring congregations and revitalizing worship.
Later writers took even more freedom, some even including allegory and metaphor in their texts.
Charles Wesley's hymns spread Methodist theology, not only within Methodism, but in most Protestant churches. He developed a new focus: expressing one's personal feelings in the relationship with God as well as the simple worship seen in older hymns. Wesley wrote:
<poem>
Where shall my wondering soul begin?
How shall I all to heaven aspire?
A slave redeemed from death and sin,
A brand plucked from eternal fire,
How shall I equal triumphs raise,
Or sing my great deliverer's praise.
</poem>
Wesley's contribution, along with the Second Great Awakening in America led to a new style called gospel, and a new explosion of sacred music writing with Fanny Crosby, Lina Sandell, Philip Bliss, Ira D. Sankey, and others who produced testimonial music for revivals, camp meetings, and evangelistic crusades. The tune style or form is technically designated "gospel songs" as distinct from hymns. Gospel songs generally include a refrain (or chorus) and usually (though not always) a faster tempo than the hymns. As examples of the distinction, "Amazing Grace" is a hymn (no refrain), but "How Great Thou Art" is a gospel song. During the 19th century the gospel-song genre spread rapidly in Protestantism and, to a lesser but still definite extent, in Roman Catholicism; the gospel-song genre is unknown in the worship "per se" by Eastern Orthodox churches, which rely exclusively on traditional chants (a type of hymn).
The Methodist Revival of the 18th century created an explosion of hymn-writing in Welsh, which continued into the first half of the 19th century. The most prominent names among Welsh hymn-writers are William Williams Pantycelyn and Ann Griffiths. The second half of the 19th century witnessed an explosion of hymn tune composition and choir singing in Wales. 
Along with the more classical sacred music of composers ranging from Mozart to Monteverdi, the Catholic Church continued to produce many popular hymns such as Lead, Kindly Light, Silent Night, O Sacrament Divine and Faith of our Fathers. 
Many churches today use contemporary worship music which includes a range of styles often influenced by popular music. This often leads to some conflict between older and younger congregants (see contemporary worship). This is not new; the Christian pop music style began in the late 1960s and became very popular during the 1970s, as young hymnists sought ways in which to make the music of their religion relevant for their generation.
This long tradition has resulted in a wide variety of hymns. Some modern churches include within hymnody the traditional hymn (usually describing God), contemporary worship music (often directed to God) and gospel music (expressions of one's personal experience of God). This distinction is not perfectly clear; and purists remove the second two types from the classification as hymns. It is a matter of debate, even sometimes within a single congregation, often between revivalist and traditionalist movements.
American developments.
African-Americans developed a rich hymnody from spirituals during times of slavery to the modern, lively black gospel style. The first influences of African American Culture into hymns came from Slave Songs of the United States a collection of slave hymns complied by William Francis Allen who had difficulty pinning them down from the oral tradition, and though he succeeded, he points out the awe inspiring effect of the hymns when sung in by their originators.
Thomas Symmes spread throughout churches a new idea of how to sing hymns, in which anyone could sing a hymn any way they felt led to; this idea was opposed by the views of Symmes' colleagues who felt it was "like Five Hundred different Tunes roared out at the same time". William Billings, a singing school teacher, created the first tune book with only American born compositions. Within his books, Billings did not put as much emphasis on "common measure" which was the typical way hymns were sung, but he attempted "to have a Sufficiency in each measure". Boston's Handel and Haydn Society aimed at raising the level of church music in America, publishing their "Collection of Church Music". In the late 19th century Ira D. Sankey and Dwight L. Moody developed the relatively new subcategory of Gospel hymns.
Hymn meters.
The meter indicates the number of syllables for the lines in each stanza of a hymn. This provides a means of marrying the hymn's text with an appropriate hymn tune for singing. In practice many hymns conform to one of a relatively small number of meters (syllable count and stress patterns). Care must be taken, however, to ensure that not only the metre of words and tune match, but also the stresses on the words in each line. Technically speaking an iambic tune, for instance, cannot be used with words of, say, trochaic metre.
The meter is often denoted by a row of figures besides the name of the tune, such as "87.87.87", which would inform the reader that each verse has six lines, and that the first line has eight syllables, the second has seven, the third line eight, etc. The meter can also be described by initials; L.M. indicates long meter, which is 88.88 (four lines, each eight syllables long); S.M. is short meter (66.86); C.M. is common metre (86.86), while D.L.M., D.S.M. and D.C.M. (the "D" stands for double) are similar to their respective single meters except that they have eight lines in a verse instead of four.
Also, if the number of syllables in one verse differ from another verse in the same hymn (e.g., the hymn "I Sing a Song of the Saints of God"), the meter is called Irregular.
Sikh hymnody.
The Sikh holy book, the Guru Granth Sahib Ji (Punjabi: ਗੁਰੂ ਗ੍ਰੰਥ ਸਾਹਿਬ ]), is a collection of hymns (Shabad) or "Gurbani" describing the qualities of God and why one should meditate on God's name. The "Guru Granth Sahib" is divided by their musical setting in different ragas into fourteen hundred and thirty pages known as "Angs" (limbs) in Sikh tradition. Guru Gobind Singh (1666–1708), the tenth guru, after adding Guru Tegh Bahadur's bani to the Adi Granth affirmed the sacred text as his successor, elevating it to "Guru Granth Sahib". The text remains the holy scripture of the Sikhs, regarded as the teachings of the Ten Gurus. The role of Guru Granth Sahib, as a source or guide of prayer, is pivotal in Sikh worship.
External links.
The links below are restricted to either material that is historical or resources that are non-denominational or inter-denominational. Denomination-specific resources are mentioned from the relevant denomination-specific articles. 

</doc>
<doc id="13765" url="http://en.wikipedia.org/wiki?curid=13765" title="Henry Kissinger">
Henry Kissinger

Henry Alfred Kissinger (; born Heinz Alfred Kissinger ]; May 27, 1923) is an American diplomat and political scientist. He served as National Security Advisor and later concurrently as Secretary of State in the administrations of Presidents Richard Nixon and Gerald Ford. For his actions negotiating the never actualised ceasefire in Vietnam, Kissinger received the 1973 Nobel Peace Prize. Le Duc Tho, with whom Kissinger shared the prize, refused it, and two members of the Nobel judging committee resigned in protest. After his term, his opinion has still been sought by many subsequent U.S. presidents and other world leaders.
A proponent of "Realpolitik", Kissinger played a prominent role in United States foreign policy between 1969 and 1977. During this period, he pioneered the policy of "détente" with the Soviet Union, orchestrated the opening of relations with the People's Republic of China, and negotiated the Paris Peace Accords, ending American involvement in the Vietnam War. Kissinger's "Realpolitik" resulted in controversial policies such as CIA involvement in Chile and the US's support for Pakistan, despite its genocidal actions during the Bangladesh War. He is the founder and chairman of Kissinger Associates, an international consulting firm. Kissinger has been a prolific author of books in politics and international relations with over one dozen books authored.
Early life and education.
Kissinger was born Heinz Alfred Kissinger in Fürth, Bavaria, Germany, in 1923 during the Weimar Republic, to a family of German Jews. His father, Louis Kissinger (1887–1982), was a schoolteacher. His mother, Paula (Stern) Kissinger (1901–1998), was a homemaker. Kissinger has a younger brother, Walter Kissinger. The surname Kissinger was adopted in 1817 by his great-great-grandfather Meyer Löb, after the Bavarian spa town of Bad Kissingen. As a youth, Heinz enjoyed playing football, and even played for the youth side of his favorite club and one of the nation's best clubs at the time, SpVgg Fürth. In 1938, fleeing Nazi persecution, his family moved to London, England, before arriving in New York on September 5.
Kissinger spent his high school years in the Washington Heights section of upper Manhattan as part of the German Jewish immigrant community there. Although Kissinger assimilated quickly into American culture, he never lost his pronounced Frankish accent, due to childhood shyness that made him hesitant to speak. Following his first year at George Washington High School, he began attending school at night and worked in a shaving brush factory during the day.
Following high school, Kissinger enrolled in the City College of New York, studying accounting. He excelled academically as a part-time student, continuing to work while enrolled. His studies were interrupted in early 1943, when he was drafted into the U.S. Army.
Army experience.
Kissinger underwent basic training at Camp Croft in Spartanburg, South Carolina. On June 19, 1943, while stationed in South Carolina, at the age of 20 years, he became a naturalized U.S. citizen. The army sent him to study engineering at Lafayette College, Pennsylvania, but the program was cancelled, and Kissinger was reassigned to the 84th Infantry Division. There, he made the acquaintance of Fritz Kraemer, a fellow immigrant from Germany who noted Kissinger's fluency in German and his intellect, and arranged for him to be assigned to the military intelligence section of the division. Kissinger saw combat with the division, and volunteered for hazardous intelligence duties during the Battle of the Bulge.
During the American advance into Germany, Kissinger, only a private, was put in charge of the administration of the city of Krefeld, owing to a lack of German speakers on the division's intelligence staff. Within eight days he had established a civilian administration. Kissinger was then reassigned to the Counter Intelligence Corps, with the rank of sergeant. He was given charge of a team in Hanover assigned to tracking down Gestapo officers and other saboteurs, for which he was awarded the Bronze Star. In June 1945, Kissinger was made commandant of the Bensheim metro CIC detachment, Bergstrasse district of Hesse, with responsibility for de-Nazification of the district. Although he possessed absolute authority and powers of arrest, Kissinger took care to avoid abuses against the local population by his command.
In 1946, Kissinger was reassigned to teach at the European Command Intelligence School at Camp King, continuing to serve in this role as a civilian employee following his separation from the army.
Academic career.
Henry Kissinger received his AB degree "summa cum laude" in political science at Harvard College in 1950, where he lived in Adams House and studied under William Yandell Elliott. He received his MA and PhD degrees at Harvard University in 1951 and 1954, respectively. In 1952, while still at Harvard, he served as a consultant to the director of the Psychological Strategy Board. His doctoral dissertation was titled "Peace, Legitimacy, and the Equilibrium (A Study of the Statesmanship of Castlereagh and Metternich)".
Kissinger remained at Harvard as a member of the faculty in the Department of Government and, with Robert R. Bowie, co-founded the Center for International Affairs in 1958. In 1955, he was a consultant to the National Security Council's Operations Coordinating Board. During 1955 and 1956, he was also study director in nuclear weapons and foreign policy at the Council on Foreign Relations. He released his book "Nuclear Weapons and Foreign Policy" the following year. From 1956 to 1958 he worked for the Rockefeller Brothers Fund as director of its Special Studies Project. He was director of the Harvard Defense Studies Program between 1958 and 1971. He was also director of the Harvard International Seminar between 1951 and 1971. Outside of academia, he served as a consultant to several government agencies and think tanks, including the Operations Research Office, the Arms Control and Disarmament Agency, and the Department of State, and the Rand Corporation.
Keen to have a greater influence on U.S. foreign policy, Kissinger became a supporter of, and advisor to, Nelson Rockefeller, Governor of New York, who sought the Republican nomination for president in 1960, 1964 and 1968. After Richard Nixon won the presidency in 1968, he made Kissinger National Security Advisor.
Foreign policy.
Kissinger served as National Security Advisor and Secretary of State under President Richard Nixon, and continued as Secretary of State under Nixon's successor Gerald Ford.
A proponent of "Realpolitik", Kissinger played a dominant role in United States foreign policy between 1969 and 1977. In that period, he extended the policy of "détente". This policy led to a significant relaxation in U.S.-Soviet tensions and played a crucial role in 1971 talks with Chinese Premier Zhou Enlai. The talks concluded with a rapprochement between the United States and the People's Republic of China, and the formation of a new strategic anti-Soviet Sino-American alignment. He was jointly awarded the 1973 Nobel Peace Prize with Le Duc Tho for helping to establish a ceasefire and U.S. withdrawal from Vietnam. The ceasefire, however, was not durable, and Tho declined to accept the award. As National Security Advisor, in 1974 Kissinger directed the much-debated National Security Study Memorandum 200.
"Détente" and the opening to China.
As National Security Advisor under Nixon, Kissinger pioneered the policy of "détente" with the Soviet Union, seeking a relaxation in tensions between the two superpowers. As a part of this strategy, he negotiated the Strategic Arms Limitation Talks (culminating in the SALT I treaty) and the Anti-Ballistic Missile Treaty with Leonid Brezhnev, General Secretary of the Soviet Communist Party. Negotiations about strategic disarmament were originally supposed to start under the Johnson Administration but were postponed in protest to the invasion by Warsaw Pact troops of Czechoslovakia in August 1968.
Kissinger sought to place diplomatic pressure on the Soviet Union. He made two trips to the People's Republic of China in July and October 1971 (the first of which was made in secret) to confer with Premier Zhou Enlai, then in charge of Chinese foreign policy. According to Kissinger's book, "The White House Years", the first secret China trip was arranged through Pakistan's diplomatic and Presidential involvement that paved the way to initial vital contact with China since the Americans were unable to communicate directly with the Chinese leaders because of earlier cold relations.
Kissinger would show his support for the regime in Beijing by supporting their actions during the unrest which included the Tiananmen Square Massacre.
This paved the way for the groundbreaking 1972 summit between Nixon, Zhou, and Communist Party of China Chairman Mao Zedong, as well as the formalization of relations between the two countries, ending 23 years of diplomatic isolation and mutual hostility. The result was the formation of a tacit strategic anti-Soviet alliance between China and the United States.
While Kissinger's diplomacy led to economic and cultural exchanges between the two sides and the establishment of Liaison Offices in the Chinese and American capitals, with serious implications for Indochinese matters, full normalization of relations with the People's Republic of China would not occur until 1979, because the Watergate scandal overshadowed the latter years of the Nixon presidency and because the United States continued to recognize the government of Taiwan.
Vietnam War.
Kissinger's involvement in Indochina started prior to his appointment as National Security Adviser to Nixon. While still at Harvard, he had worked as a consultant on foreign policy to both the White House and State Department. Kissinger says that "In August 1965... [Henry Cabot Lodge Jr.], an old friend serving as Ambassador to Saigon, had asked me to visit Vietnam as his consultant. I toured Vietnam first for two weeks in October and November 1965, again for about ten days in July 1966, and a third time for a few days in October 1966... Lodge gave me a free hand to look into any subject of my choice". He became convinced of the meaninglessness of military victories in Vietnam, "... unless they brought about a political reality that could survive our ultimate withdrawal". In a 1967 peace initiative, he would mediate between Washington and Hanoi.
Nixon had been elected in 1968 on the promise of achieving "peace with honor" and ending the Vietnam War. In office, and assisted by Kissinger, Nixon implemented a policy of Vietnamization that aimed to gradually withdraw U.S. troops while expanding the combat role of the South Vietnamese Army so that it would be capable of independently defending its government against the National Front for the Liberation of South Vietnam, a Communist guerrilla organization, and North Vietnamese army (Vietnam People's Army or PAVN). Kissinger played a key role in secretly bombing Cambodia to disrupt PAVN and Viet Cong units launching raids into South Vietnam from within Cambodia's borders and resupplying their forces by using the Ho Chi Minh trail and other routes, as well as the 1970 Cambodian Incursion and subsequent widespread bombing of suspected Khmer Rouge targets in Cambodia. The bombing campaign contributed to the chaos of the Cambodian Civil War, which saw the forces of U.S.-backed leader Lon Nol unable to retain foreign support to combat the growing Khmer Rouge insurgency that would overthrow him in 1975. Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese invasion of Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea. The American bombing of Cambodia killed an estimated 40,000 Cambodian combatants and civilians. Pol Pot biographer David P. Chandler argues that the bombing "had the effect the Americans wanted—it broke the Communist encirclement of Phnom Penh."
Along with North Vietnamese Politburo Member Le Duc Tho, Kissinger was awarded the Nobel Peace Prize on December 10, 1973, for their work in negotiating the ceasefires contained in the Paris Peace Accords on "Ending the War and Restoring Peace in Vietnam", signed the previous January. According to Irwin Abrams, this prize was the most controversial to date. For the first time in the history of the Peace Prize two members left the Nobel Committee in protest. Tho rejected the award, telling Kissinger that peace had not been restored in South Vietnam. Kissinger wrote to the Nobel Committee that he accepted the award "with humility". The conflict continued until an invasion of the South by the North Vietnamese Army resulted in a North Vietnamese victory in 1975 and the subsequent progression of the Pathet Lao in Laos towards figurehead status.
Bangladesh War.
Under Kissinger's guidance, the United States government supported Pakistan in the Bangladesh Liberation War in 1971. Kissinger was particularly concerned about the expansion of Soviet influence in South Asia as a result of a treaty of friendship recently signed by India and the USSR, and sought to demonstrate to the People's Republic of China (Pakistan's ally and an enemy of both India and the USSR) the value of a tacit alliance with the United States.
Kissinger sneered at people who “bleed” for “the dying Bengalis” and ignored the first telegram from the United States consul general in East Pakistan, Archer K. Blood, and 20 members of his staff, which informed the US that their allies West Pakistan were undertaking, in Blood's words, "a selective genocide". In the second, more famous, Blood Telegram the word genocide was again used to describe the events, and further that with its continuing support for West Pakistan the US government had "evidenced [...] moral bankruptcy".
As a direct response to the dissent against US policy Kissinger and Nixon ended Archer Blood's tenure as United States consul general in East Pakistan and put him to work in the State Department's Personnel Office.
Henry Kissinger had also come under fire for private comments he made to Nixon during the Bangladesh–Pakistan War in which he described Indian Prime Minister Indira Gandhi as a "bitch" and a "witch". He also said "The Indians are bastards," shortly before the war. Kissinger has since expressed his regret over the comments.
Israeli policy and Soviet Jewry.
According to notes taken by H. R. Haldeman, Nixon "ordered his aides to exclude all Jewish-Americans from policy-making on Israel", including Kissinger. One note quotes Nixon as saying "get K. [Kissinger] out of the play—Haig handle it".
In 1973, Kissinger did not feel that pressing the Soviet Union concerning the plight of Jews being persecuted there was in the interest of U.S. foreign policy. In conversation with Nixon shortly after a meeting with Golda Meir on March 1, 1973, Kissinger stated, "The emigration of Jews from the Soviet Union is not an objective of American foreign policy, and if they put Jews into gas chambers in the Soviet Union, it is not an American concern. Maybe a humanitarian concern." Kissinger argued, however:
That emigration existed at all was due to the actions of "realists" in the White House. Jewish emigration rose from 700 a year in 1969 to near 40,000 in 1972. The total in Nixon's first term was more than 100,000. To maintain this flow by quiet diplomacy, we never used these figures for political purposes. ... The issue became public because of the success of our Middle East policy when Egypt evicted Soviet advisers. To restore its relations with Cairo, the Soviet Union put a tax on Jewish emigration. There was no Jackson–Vanik Amendment until there was a successful emigration effort. Sen. Henry Jackson, for whom I had, and continue to have, high regard, sought to remove the tax with his amendment. We thought the continuation of our previous approach of quiet diplomacy was the wiser course. ... Events proved our judgment correct. Jewish emigration fell to about a third of its previous high.
1973 Yom Kippur War.
Documents show that Kissinger delayed telling President Richard Nixon about the start of the Yom Kippur War in 1973 to keep him from interfering. On October 6, 1973, the Israelis informed Kissinger about the attack at 6 am; Kissinger waited nearly 3 and a half hours before he informed Nixon.
According to Kissinger, in an interview in November 2013, he was notified at 6:30 a.m. (12:30 p.m. Israel time) that war was imminent, and his urgent calls to the Soviets and Egyptians were ineffective. He says Golda Meir's decision not to preempt was wise and reasonable, balancing the risk of Israel looking like the aggressor and Israel's actual ability to strike within such a brief span of time.
The war began on October 6, 1973, when Egypt and Syria attacked Israel. Kissinger published lengthy telephone transcripts from this period in the 2002 book "Crisis". On October 12, under Nixon's direction, and against Kissinger's initial advice, while Kissinger was on his way to Moscow to discuss conditions for a cease-fire, Nixon sent a message to Brezhnev giving Kissinger full negotiating authority.
Israel regained the territory it lost in the early fighting and gained new territories from Syria and Egypt, including land in Syria east of the previously captured Golan Heights, and additionally on the western bank of the Suez Canal, although they did lose some territory on the eastern side of the Suez Canal that had been in Israeli hands since the end of the Six Day War. Kissinger pressured the Israelis to cede some of the newly captured land back to its Arab neighbors, contributing to the first phases of Israeli-Egyptian non-aggression. The move saw a warming in U.S.–Egyptian relations, bitter since the 1950s, as the country moved away from its former independent stance and into a close partnership with the United States. The peace was finalized in 1978 when U.S. President Jimmy Carter mediated the Camp David Accords, during which Israel returned the Sinai Peninsula in exchange for an Egyptian peace agreement that included the recognition of the state of Israel.
Latin American policy.
The United States continued to recognize and maintain relationships with non-left-wing governments, democratic and authoritarian alike. John F. Kennedy's Alliance for Progress was ended in 1973. In 1974, negotiations about a new settlement over the Panama Canal started. They eventually led to the Torrijos-Carter Treaties and the handing over of the Canal to Panamanian control.
Kissinger initially supported the normalization of United States-Cuba relations, broken since 1961 (all U.S.–Cuban trade was blocked in February 1962, a few weeks after the exclusion of Cuba from the Organization of American States because of U.S. pressure). However, he quickly changed his mind and followed Kennedy's policy. After the involvement of the Cuban Revolutionary Armed Forces in the independence struggles in Angola and Mozambique, Kissinger said that unless Cuba withdrew its forces relations would not be normalized. Cuba refused.
Intervention in Chile.
Chilean Socialist Party presidential candidate Salvador Allende was elected by a plurality in 1970, causing serious concern in Washington, D.C. due to his openly socialist and pro-Cuban politics. The Nixon administration, with Kissinger's input, authorized the Central Intelligence Agency (CIA) to encourage a military coup that would prevent Allende's inauguration, but the plan was not successful.:115:495:177
United States-Chile relations remained frosty during Salvador Allende's tenure, following the complete nationalization of the partially U.S.-owned copper mines and the Chilean subsidiary of the U.S.-based ITT Corporation, as well as other Chilean businesses. The U.S. claimed that the Chilean government had greatly undervalued fair compensation for the nationalization by subtracting what it deemed "excess profits". Therefore, the U.S. implemented economic sanctions against Chile. The CIA also provided funding for the mass anti-government strikes in 1972 and 1973, and extensive black propaganda in the newspaper "El Mercurio".:93
The most expeditious way to prevent Allende from assuming office was somehow to convince the Chilean congress to confirm Jorge Alessandri as the winner of the election. Once elected by the congress, Alessandri—a party to the plot through intermediaries—was prepared to resign his presidency within a matter of days so that new elections could be held. This first, nonmilitary, approach to stopping Allende was called the Track I approach. The CIA's second approach, the Track II approach, was designed to encourage a military overthrow.
On September 11, 1973, Allende died during a military coup launched by Army Commander-in-Chief Augusto Pinochet, who became President.
A document released by the CIA in 2000 titled "CIA Activities in Chile" revealed that the United States, acting through the CIA, actively supported the military junta after the overthrow of Allende and that it made many of Pinochet's officers into paid contacts of the CIA or U.S. military.
In 1976, Orlando Letelier, a Chilean opponent of the Pinochet regime, was assassinated in Washington, D.C. with a car bomb. Previously, Kissinger had helped secure his release from prison, and had chosen to cancel a letter to Chile warning them against carrying out any political assassinations. The U.S. ambassador to Chile, David H. Popper, said that Pinochet might take as an insult any inference that he was connected with assassination plots.
Argentina.
Kissinger took a similar line as he had toward Chile when the Argentinian military, led by Jorge Videla, toppled the elected government of Isabel Perón in 1976 with a process called the National Reorganization Process by the military, with which they consolidated power, launching brutal reprisals and "disappearances" against political opponents. During a meeting with Argentinian foreign minister César Augusto Guzzetti, Kissinger assured him that the United States was an ally, but urged him to "get back to normal procedures" quickly before the U.S. Congress reconvened and had a chance to consider sanctions.
Africa.
In September 1976 Kissinger was actively involved in negotiations regarding the Rhodesian Bush War. Kissinger, along with South Africa's Prime Minister John Vorster, pressured Rhodesian Prime Minister Ian Smith to hasten the transition to black majority rule in Rhodesia. With FRELIMO in control of Mozambique and even South Africa withdrawing its support, Rhodesia's isolation was nearly complete. According to Smith's autobiography, Kissinger told Smith of Mrs. Kissinger's admiration for him, but Smith stated that he thought Kissinger was asking him to sign Rhodesia's "death certificate". Kissinger, bringing the weight of the United States, and corralling other relevant parties to put pressure on Rhodesia, hastened the end of minority-rule.
East Timor.
The Portuguese decolonization process brought U.S. attention to the former Portuguese colony of East Timor, which lies within the Indonesian archipelago and declared its independence in 1975. Indonesian president Suharto was a strong U.S. ally in Southeast Asia and began to mobilize the Indonesian army, preparing to annex the nascent state, which had become increasingly dominated by the popular leftist FRETILIN party. In December 1975, Suharto discussed the invasion plans during a meeting with Kissinger and President Ford in the Indonesian capital of Jakarta. Both Ford and Kissinger made clear that U.S. relations with Indonesia would remain strong and that it would not object to the proposed annexation. They only wanted it done "fast" and proposed to delay the invasion until they had returned to Washington. Accordingly Suharto delayed the operation for one day. Finally on December 7 Indonesian forces invaded the former Portuguese colony. U.S. arms sales to Indonesia continued, and Suharto went ahead with the annexation plan.
Later roles.
Kissinger left office when a Democrat, former Governor of Georgia Jimmy Carter, defeated Republican Gerald Ford in the 1976 presidential elections. Kissinger continued to participate in policy groups, such as the Trilateral Commission, and to maintain political consulting, speaking, and writing engagements.
Shortly after Kissinger left office in 1977, he was offered an endowed chair at Columbia University. There was significant student opposition to the appointment, which eventually became a subject of wide media commentary. Columbia cancelled the appointment as a result.
Kissinger was then appointed to Georgetown University's Center for Strategic and International Studies. He taught at Georgetown's Edmund Walsh School of Foreign Service for several years in the late 1970s. In 1982, with the help of a loan from the international banking firm of E.M. Warburg, Pincus and Company, Kissinger founded a consulting firm, Kissinger Associates, and is a partner in affiliate Kissinger McLarty Associates with Mack McLarty, former chief of staff to President Bill Clinton. He also serves on board of directors of Hollinger International, a Chicago-based newspaper group, and as of March 1999, he also serves on the board of directors of Gulfstream Aerospace.
In 1978, Kissinger was named chairman of the North American Soccer League board of directors. From 1995 to 2001, he served on the board of directors for Freeport-McMoRan, a multinational copper and gold producer with significant mining and milling operations in Papua, Indonesia. In February 2000, then-president of Indonesia Abdurrahman Wahid appointed Kissinger as a political advisor. He also serves as an honorary advisor to the United States-Azerbaijan Chamber of Commerce.
From 2000–2006, Kissinger served as chairman of the board of trustees of Eisenhower Fellowships. In 2006, upon his departure from Eisenhower Fellowships, he received the Dwight D. Eisenhower Medal for Leadership and Service.
In November 2002, he was appointed by President George W. Bush to chair the newly established National Commission on Terrorist Attacks Upon the United States to investigate the September 11 attacks. Kissinger stepped down as chairman on December 13, 2002 rather than reveal his business client list, when queried about potential conflicts of interest.
Kissinger—along with William Perry, Sam Nunn, and George Shultz—has called upon governments to embrace the vision of a world free of nuclear weapons, and in three "Wall Street Journal" op-eds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. Nunn reinforced that agenda during a speech at the Harvard Kennedy School on October 21, 2008, saying, "I’m much more concerned about a terrorist without a return address that cannot be deterred than I am about deliberate war between nuclear powers. You can’t deter a group who is willing to commit suicide. We are in a different era. You have to understand the world has changed." In 2010, the four were featured in a documentary film entitled "Nuclear Tipping Point". The film is a visual and historical depiction of the ideas laid forth in the "Wall Street Journal" op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.
Role in U.S. foreign policy.
Yugoslav wars.
In several articles of his and interviews that he gave during the Yugoslav wars, he criticized the United States' policies in Southeast Europe, among other things for the recognition of Bosnia and Herzegovina as a sovereign state, which he described as a foolish act. Most importantly he dismissed the notion of Serbs, and Croats for that part, being aggressors or separatist, saying that "they can't be separating from something that has never existed".
In addition, he repeatedly warned the West of inserting itself into a conflict that has its roots at least hundreds of years back in time, and said that the West would do better if it allowed the Serbs and Croats to join their respective countries.
Kissinger shared similarly critical views on Western involvement in Kosovo. In particular, he held a disparaging view of the Rambouillet Agreement:
The Rambouillet text, which called on Serbia to admit NATO troops throughout Yugoslavia, was a provocation, an excuse to start bombing. Rambouillet is not a document that any Serb could have accepted. It was a terrible diplomatic document that should never have been presented in that form.—Henry Kissinger, Daily Telegraph, June 28, 1999
However, as the Serbs did not accept the Rambouillet text and NATO bombings started, he opted for a continuation of the bombing as NATO's credibility was now at stake, but dismissed the usage of ground forces, claiming that it was not worth it.
Iraq.
In 2006, it was reported in the book "" by Bob Woodward that Kissinger met regularly with President George W. Bush and Vice President Dick Cheney to offer advice on the Iraq War. Kissinger confirmed in recorded interviews with Woodward that the advice was the same as he had given in an August 12, 2005 column in "The Washington Post": "Victory over the insurgency is the only meaningful exit strategy."
In a November 19, 2006, interview on BBC "Sunday AM", Kissinger said, when asked whether there is any hope left for a clear military victory in Iraq, "If you mean by 'military victory' an Iraqi government that can be established and whose writ runs across the whole country, that gets the civil war under control and sectarian violence under control in a time period that the political processes of the democracies will support, I don't believe that is possible. ... I think we have to redefine the course. But I don't believe that the alternative is between military victory as it had been defined previously, or total withdrawal."
In an April 3, 2008, interview with Peter Robinson of the Hoover Institution, Kissinger reiterated that even though he supported the 2003 invasion of Iraq he thought that the George W. Bush administration rested too much of its case for war on Saddam's supposed weapons of mass destruction. Robinson noted that Kissinger had criticized the administration for invading with too few troops, for disbanding the Iraqi Army, and for mishandling relations with certain allies.
India.
Kissinger said in April 2008 that "India has parallel objectives to the United States," and he called it an ally of the U.S.
China.
Kissinger was present at the opening ceremony of the Beijing Summer Olympics.
In 2011, Kissinger published "On China", chronicling the evolution of Sino-American relations and laying out the challenges to a partnership of 'genuine strategic trust' between the U.S. and China.
Iran.
Kissinger's position on this issue of U.S.–Iran talks was reported by the "Tehran Times" to be that "Any direct talks between the U.S. and Iran on issues such as the nuclear dispute would be most likely to succeed if they first involved only diplomatic staff and progressed to the level of secretary of state before the heads of state meet."
2014 Ukrainian crisis.
On 5 March 2014, before the 16 March referendum in Crimea, "The Washington Post" published an op-ed piece by Kissinger. In it, he attempted to balance the Ukrainian, Russian and Western desires for a functional state. He made four propositions:
Kissinger also wrote: "The west speaks Ukrainian; the east speaks mostly Russian. Any attempt by one wing of Ukraine to dominate the other—as has been the pattern—would lead eventually to civil war or break up."
Following the publication of his new book titled "World Order", Kissinger participated in an interview with Charlie Rose and updated his position on Ukraine which he sees as a possible geographical mediator between Russia and the West. In a question he posed to himself for illustration regarding re-conceiving policy regarding Ukraine, Kissinger stated: "If Ukraine is considered an outpost, then the situation is that its eastern border is the NATO strategic line, and NATO will be within 200 miles of [Volgograd]. That will never be accepted by Russia. On the other hand, if the Russian western line is at the border of Poland, Europe will be permanently disquieted. The Strategic objective should have been to see whether one can build Ukraine as a bridge between East and West, and whether one can do it as a kind of a joint effort."
Public perception.
At the height of Kissinger's prominence, many commented on his wit. In one instance, at the Washington Press Club annual congressional dinner, "Kissinger mocked his reputation as a secret swinger." He was quoted as saying "Power is the ultimate aphrodisiac."
Kissinger has shied away from mainstream media and cable talk shows. He granted a rare interview to the producers of a documentary examining the underpinnings of the 1979 peace treaty between Israel and Egypt entitled "". In the film, Kissinger revealed how close he felt the world was to nuclear war during the 1973 Yom Kippur War launched by Egypt and Syria against Israel.
Since he left office, some efforts have been made to hold Kissinger responsible for perceived injustices of American foreign policy during his tenure in government. These attempts have at times followed him in his international travels. Christopher Hitchens, the British-American journalist and author, was highly critical of Kissinger, authoring "The Trial of Henry Kissinger", in which Hitchens called for the prosecution of Kissinger "for war crimes, for crimes against humanity, and for offenses against common or customary or international law, including conspiracy to commit murder, kidnap, and torture". In 2011 an interview-based documentary, titled "Kissinger", was released, in which Kissinger "reflects on some of his most important and controversial decisions" during his tenure as Secretary of State.
Family and personal life.
Kissinger married Ann Fleischer, with whom he had two children, Elizabeth and David. They divorced in 1964. Ten years later, he married Nancy Maginnes. They now live in Kent, Connecticut and New York City. His son David Kissinger was an executive with NBC Universal before becoming head of Conaco, Conan O'Brien's production company.
Since his childhood, Kissinger has been a fan of his hometown's soccer club, SpVgg Greuther Fürth. Even during his time in office he was informed about the team's results by the German Embassy every Monday morning. He is an honorary member with lifetime season tickets. In September 2012, Kissinger attended a home game in which SpVgg Greuther Fürth lost, 0–2, against Schalke after promising years ago he would attend a Greuther Fürth home game if they were promoted to the Bundesliga, the top football league in Germany, from the 2. Bundesliga.
Kissinger described "Diplomacy" as his favorite game in a 1973 interview.

</doc>
<doc id="13855" url="http://en.wikipedia.org/wiki?curid=13855" title="Halloween">
Halloween

Halloween or Hallowe'en (; a contraction of "All Hallows' Evening"), also known as Allhalloween, All Hallows' Eve, or All Saints' Eve, is a yearly celebration observed in a number of countries on 31 October, the eve of the Western Christian feast of All Hallows' Day. It initiates the triduum of Allhallowtide, the time in the liturgical year dedicated to remembering the dead, including saints (hallows), martyrs, and all the faithful departed believers. Within Allhallowtide, the traditional focus of All Hallows' Eve revolves around the theme of using "humor and ridicule to confront the power of death."
According to many scholars, All Hallows' Eve is a Christianized feast initially influenced by Celtic harvest festivals, with possible pagan roots, particularly the Gaelic Samhain. Other scholars maintain that it originated independently of Samhain and has solely Christian roots.
Typical festive Halloween activities include trick-or-treating (or the related "guising"), attending costume parties, decorating, carving pumpkins into jack-o'-lanterns, lighting bonfires, apple bobbing, visiting haunted attractions, playing pranks, telling scary stories and watching horror films. In many parts of the world, the Christian religious observances of All Hallows' Eve, including attending church services and lighting candles on the graves of the dead, remain popular, although in other locations, these solemn customs are less pronounced in favor of a more commercialized and secularized celebration. Because many Western Christian denominations encourage, although most no longer require, abstinence from meat on All Hallows' Eve, the tradition of eating certain vegetarian foods for this vigil day developed, including the consumption of apples, colcannon, cider, potato pancakes, and soul cakes.
Etymology.
The word "Halloween" or "Hallowe'en" dates to about 1745 and is of Christian origin. The word "Halloween" means "hallowed evening" or "holy evening". It comes from a Scottish term for "All Hallows' Eve" (the evening before All Hallows' Day). In Scots, the word "eve" is "even", and this is contracted to "e'en" or "een". Over time, "(All) Hallow(s) E(v)en" evolved into "Halloween". Although the phrase "All Hallows'" is found in Old English ("ealra hālgena mæssedæg", all saints mass-day), "All Hallows' Eve" is itself not seen until 1556.
History.
Gaelic and Welsh influence.
Today's Halloween customs are thought to have been influenced by folk customs and beliefs from the Celtic-speaking countries, some of which have pagan roots, and others which may be rooted in Celtic Christianity. Indeed, Jack Santino, a folklorist, writes that "the sacred and the religious are a fundamental context for understanding Halloween in Northern Ireland, but there was throughout Ireland an uneasy truce existing between customs and beliefs associated with Christianity and those associated with religions that were Irish before Christianity arrived". Historian Nicholas Rogers, exploring the origins of Halloween, notes that while "some folklorists have detected its origins in the Roman feast of Pomona, the goddess of fruits and seeds, or in the festival of the dead called Parentalia, it is more typically linked to the Celtic festival of Samhain", which comes from the Old Irish for "summer's end". Samhain (pronounced or ) was the first and most important of the four quarter days in the medieval Gaelic calendar and was celebrated in Ireland, Scotland and the Isle of Man. It was held on or about 31 October – 1 November and kindred festivals were held at the same time of year by the Brittonic Celts; for example Calan Gaeaf (in Wales), Kalan Gwav (in Cornwall) and Kalan Goañv (in Brittany). Samhain and Calan Gaeaf are mentioned in some of the earliest Irish and Welsh literature. The names have been used by historians to refer to Celtic Halloween customs up until the 19th century, and are still the Gaelic and Welsh names for Halloween.
Samhain/Calan Gaeaf marked the end of the harvest season and beginning of winter or the 'darker half' of the year. Like Beltane/Calan Mai, it was seen as a liminal time, when the spirits or fairies (the "Aos Sí") could more easily come into our world and were particularly active. Most scholars see the "Aos Sí" as "degraded versions of ancient gods [...] whose power remained active in the people's minds even after they had been officially replaced by later religious beliefs". The "Aos Sí" were both respected and feared, with individuals often invoking the protection of God when approaching their dwellings. At Samhain, it was believed that the "Aos Sí" needed to be propitiated to ensure that the people and their livestock survived the winter. Offerings of food and drink, or portions of the crops, were left for the "Aos Sí". The souls of the dead were also said to revisit their homes. Places were set at the dinner table or by the fire to welcome them. The belief that the souls of the dead return home on one night or day of the year seems to have ancient origins and is found in many cultures throughout the world. In 19th century Ireland, "candles would be lit and prayers formally offered for the souls of the dead. After this the eating, drinking, and games would begin". Throughout the Gaelic and Welsh regions, the household festivities included rituals and games intended to divine one's future, especially regarding death and marriage. Nuts and apples were often used in these divination rituals. Special bonfires were lit and there were rituals involving them. Their flames, smoke and ashes were deemed to have protective and cleansing powers, and were also used for divination. It is suggested that the fires were a kind of imitative or sympathetic magic – they mimicked the Sun, helping the "powers of growth" and holding back the decay and darkness of winter. Christian minister Eddie J. Smith suggests that the bonfires were also used to scare witches of "their awaiting punishment in hell".
During the early modern era in Ireland, Scotland, the Isle of Man and Wales, the festival included mumming and guising, the latter of which goes back at least as far as the 16th century. This involved people going house-to-house in costume (or in disguise), usually reciting verses or songs in exchange for food. It may have come from the Christian custom of souling (see below) or it may have a Gaelic folk origin, with the costumes being a means of imitating, or disguising oneself from, the "Aos Sí". In Scotland, youths went house-to-house on 31 October with masked, painted or blackened faces, often threatening to do mischief if they were not welcomed. F. Marian McNeill suggests the ancient festival included people in costume representing the spirits, and that faces were marked (or blackened) with ashes taken from the sacred bonfire. In parts of Wales, men went about dressed as fearsome beings called "gwrachod". In the late 19th and early 20th century, young people in Glamorgan and Orkney dressed as the opposite gender. In parts of southern Ireland, the guisers included a hobby horse. A man dressed as a "Láir Bhán" (white mare) led youths house-to-house reciting verses—some of which had pagan overtones—in exchange for food. If the household donated food it could expect good fortune from the 'Muck Olla'; not doing so would bring misfortune. Elsewhere in Europe, mumming and hobby horses were part of other yearly festivals. However, in the Celtic-speaking regions they were "particularly appropriate to a night upon which supernatural beings were said to be abroad and could be imitated or warded off by human wanderers". As early as the 18th century, "imitating malignant spirits" led to playing pranks in Ireland and the Scottish Highlands. Wearing costumes at Halloween spread to England in the 20th century, as did the custom of playing pranks. The "traditional illumination for guisers or pranksters abroad on the night in some places was provided by turnips or mangel wurzels, hollowed out to act as lanterns and often carved with grotesque faces to represent spirits or goblins". These were common in parts of Ireland and the Scottish Highlands in the 19th century, as well as in Somerset (see Punkie Night). In the 20th century they spread to other parts of England and became generally known as jack-o'-lanterns.
Christian influence.
Today's Halloween customs are also thought to have been influenced by Christian dogma and practices derived from it. Halloween falls on the evening before the Christian holy days of All Hallows' Day (also known as "All Saints' or Hallowmas") on 1 November and All Souls' Day on 2 November, thus giving the holiday on 31 October the full name of "All Hallows' Eve" (meaning the evening before All Hallows' Day). Since the time of the primitive Church, major feasts in the Christian Church (such as Christmas, Easter and Pentecost) had vigils which began the night before, as did the feast of All Hallows'. These three days are collectively referred to as Allhallowtide and are a time for honoring the saints and praying for the recently departed souls who have yet to reach Heaven. All Saints was introduced in the year 609, but was originally celebrated on 13 May. In 835, it was switched to 1 November (the same date as Samhain) at the behest of Pope Gregory IV. Some suggest this was due to Celtic influence, while others suggest it was a Germanic idea. It is also suggested that the change was made on the "practical grounds that Rome in summer could not accommodate the great number of pilgrims who flocked to it", and perhaps because of public health considerations regarding Roman Fever – a disease that claimed a number of lives during the sultry summers of the region.
By the end of the 12th century they had become holy days of obligation across Europe and involved such traditions as ringing church bells for the souls in purgatory. In addition, "it was customary for criers dressed in black to parade the streets, ringing a bell of mournful sound and calling on all good Christians to remember the poor souls." "Souling", the custom of baking and sharing soul cakes for all christened souls, has been suggested as the origin of trick-or-treating. The custom dates back at least as far as the 15th century and was found in parts of England, Belgium, Germany, Austria and Italy. Groups of poor people, often children, would go door-to-door during Allhallowtide, collecting soul cakes, in exchange for praying for the dead, especially the souls of the givers' friends and relatives. Shakespeare mentions the practice in his comedy "The Two Gentlemen of Verona" (1593). The custom of wearing costumes has been explicated by Prince Sorie Conteh, who wrote: "It was traditionally believed that the souls of the departed wandered the earth until All Saints' Day, and All Hallows' Eve provided one last chance for the dead to gain vengeance on their enemies before moving to the next world. In order to avoid being recognized by any soul that might be seeking such vengeance, people would don masks or costumes to disguise their identities". In the Middle Ages, churches displayed the relics of martyred saints and those parishes that were too poor to have relics let parishioners dress up as the saints instead, a practice that some Christians continue in Halloween celebrations today. Folklorist Kingsley Palmer, in addition to others, has suggested that the carved jack-o'-lantern, a popular symbol of Halloween, originally represented the souls of the dead. On Halloween, in medieval Europe, "fires [were] lit to guide these souls on their way and deflect them from haunting honest Christian folk." In addition, households in Austria, England, and Ireland often had "candles burning in every room to guide the souls back to visit their earthly homes". These were known as "soul lights". Many Christians in continental Europe, especially in France, acknowledged "a belief that once a year, on Hallowe'en, the dead of the churchyards rose for one wild, hideous carnival," known as the danse macabre, which has been commonly depicted in church decoration, especially on the walls of cathedrals, monasteries, and cemeteries. Christopher Allmand and Rosamond McKitterick write in "The New Cambridge Medieval History" that "Christians were moved by the sight of the Infant Jesus playing on his mother's knee; their hearts were touched by the Pietà; and patron saints reassured them by their presence. But, all the while, the "danse macabre" urged them not to forget the end of all earthly things." This danse macabre, which was enacted by "Christian village children [who] celebrated the vigil of All Saints" in the 16th Century, has been suggested as the predecessor of modern day costume parties on this same day.
In parts of Britain, these customs came under attack during the Reformation as some Protestants berated purgatory as a "popish" doctrine incompatible with the notion of predestination. Thus, for some Nonconformist Protestants, the theology of All Hallows’ Eve was redefined; without the doctrine of purgatory, "the returning souls cannot be journeying from Purgatory on their way to Heaven, as Catholics frequently believe and assert. Instead, the so-called ghosts are thought to be in actuality evil spirits. As such they are threatening." Other Protestants maintained belief in an intermediate state, known as Hades (Bosom of Abraham), and continued to observe the original customs, especially souling, candlelit processions and the ringing of church bells in memory of the dead. With regard to the evil spirits, on Halloween, "barns and homes were blessed to protect people and livestock from the effect of witches, who were believed to accompany the malignant spirits as they traveled the earth." In the 19th century, in some rural parts of England, families gathered on hills on the night of All Hallows' Eve. One held a bunch of burning straw on a pitchfork while the rest knelt around him in a circle, praying for the souls of relatives and friends until the flames went out. This was known as "teen'lay", derived either from the Old English "tendan" (meaning to kindle) or a word related to Old Irish "tenlach" (meaning hearth). The rising popularity of Guy Fawkes Night (5 November) from 1605 onward, saw many Halloween traditions appropriated by that holiday instead, and Halloween's popularity waned in Britain, with the noteworthy exception of Scotland. There and in Ireland, they had been celebrating Samhain and Halloween since at least the early Middle Ages, and the Scottish kirk took a more pragmatic approach to Halloween, seeing it as important to the life cycle and rites of passage of communities and thus ensuring its survival in the country.
In France, some Christian families, on the night of All Hallows' Eve, prayed beside the graves of their loved ones, setting down dishes full of milk for them. On Halloween, in Italy, some families left a large meal out for ghosts of their passed relatives, before they departed for church services. In Spain, on this night, special pastries are baked, known as "bones of the holy" (Spanish: "Huesos de Santo") and put them on the graves of the churchyard, a practice that continues to this day.
Spread to North America.
Lesley Bannatyne and Cindy Ott both write that Anglican colonists in the South and Catholic colonists in Maryland "recognized All Hallow's Eve in their church calendars", although the Puritans of New England maintained strong opposition to the holiday, along with other traditional celebrations of the established Church, including Christmas. Mass Irish and Scottish immigration during the 19th century increased the holiday’s celebration in the United States. "In Cajun areas, a nocturnal Mass was said in cemeteries on Halloween night. Candles that had been blessed were placed on graves, and families sometimes spent the entire night at the graveside." Confined to the immigrant communities during the mid-19th century, it was gradually assimilated into mainstream society and by the first decade of the 20th century it was being celebrated coast to coast by people of all social, racial and religious backgrounds.
The annual New York Halloween Parade, initiated in 1974 by puppeteer and mask maker Ralph Lee of the Lower Manhattan neighborhood of Greenwich Village in New York City, is the world's largest Halloween parade and America's only major nighttime parade, attracting more than 60,000 costumed participants, 2 million in-person spectators, and a worldwide television audience of over 100 million.
Symbols.
Development of artifacts and symbols associated with Halloween formed over time. Jack-o'-lanterns are traditionally carried by guisers on All Hallows' Eve in order to frighten evil spirits. There is a popular Irish Christian folktale associated with the jack-o'-lantern, which in lore, is said to represent a "soul who has been denied entry into both heaven and hell":
 On route home after a night's drinking, Jack encounters the Devil and tricks him into climbing a tree. A quick-thinking Jack etches the sign of the cross into the bark, thus trapping the Devil. Jack strikes a bargain that Satan can never claim his soul. After a life of sin, drink, and mendacity, Jack is refused entry to heaven when he dies. Keeping his promise, the Devil refuses to let Jack into hell and throws a live coal straight from the fires of hell at him. It was a cold night, so Jack places the coal in a hollowed out turnip to stop it from going out, since which time Jack and his lantern have been roaming looking for a place to rest.
 In Ireland and Scotland, the turnip has traditionally been carved during Halloween, but immigrants to North America used the native pumpkin, which is both much softer and much larger – making it easier to carve than a turnip. The American tradition of carving pumpkins is recorded in 1837 and was originally associated with harvest time in general, not becoming specifically associated with Halloween until the mid-to-late 19th century.
The modern imagery of Halloween comes from many sources, including Christian eschatology, national customs, works of Gothic and horror literature (such as the novels "Frankenstein" and "Dracula") and classic horror films (such as "Frankenstein" and "The Mummy"). Imagery of the skull, a reference to Golgotha, in the Christian tradition, serves as "a reminder of death and the transitory quality of human life" and is consequently found in "memento mori" and "vanitas" compositions; skulls have therefore been commonplace in Halloween, which touches on this theme. Traditionally, the back walls of churches are "decorated with a depiction of the Last Judgment, complete with graves opening and the dead rising, with a heaven filled with angels and a hell filled with devils," a motif that has permeated the observance of this triduum. One of the earliest works on the subject of Halloween is from Scottish poet John Mayne, who, in 1780, made note of pranks at Halloween; "What fearfu' pranks ensue!", as well as the supernatural associated with the night, "Bogies" (ghosts), influencing Robert Burns' "Halloween" (1785). Elements of the autumn season, such as pumpkins, corn husks and scarecrows, are also prevalent. Homes are often decorated with these types of symbols around Halloween. Halloween imagery includes themes of death, evil, and mythical monsters. Black, orange, and sometimes purple are Halloween's traditional colors.
Trick-or-treating and guising.
Trick-or-treating is a customary celebration for children on Halloween. Children go in costume from house to house, asking for treats such as candy or sometimes money, with the question, "Trick or treat?" The word "trick" refers to "threat" to perform mischief on the homeowners or their property if no treat is given. The practice is said to have roots in the medieval practice of mumming, which is closely related to souling. John Pymm writes that "many of the feast days associated with the presentation of mumming plays were celebrated by the Christian Church." These feast days included All Hallows' Eve, Christmas, Twelfth Night and Shrove Tuesday. Mumming, practiced in Germany, Scandinavia and other parts of Europe, involved masked persons in fancy dress who "paraded the streets and entered houses to dance or play dice in silence."
In England, from the medieval period, up until the 1930s, people practiced the Christian custom of souling on Halloween, which involved groups of soulers, both Protestant and Catholic, going from parish to parish, begging the rich for soul cakes, in exchange for praying for the souls of the givers and their friends. In Scotland and Ireland, guising – children disguised in costume going from door to door for food or coins  – is a traditional Halloween custom, and is recorded in Scotland at Halloween in 1895 where masqueraders in disguise carrying lanterns made out of scooped out turnips, visit homes to be rewarded with cakes, fruit and money. The practice of guising at Halloween in North America is first recorded in 1911, where a newspaper in Kingston, Ontario reported children going "guising" around the neighborhood.
American historian and author Ruth Edna Kelley of Massachusetts wrote the first book length history of Halloween in the US; "" (1919), and references souling in the chapter "Hallowe'en in America". In her book, Kelley touches on customs that arrived from across the Atlantic; "Americans have fostered them, and are making this an occasion something like what it must have been in its best days overseas. All Halloween customs in the United States are borrowed directly or adapted from those of other countries".
While the first reference to "guising" in North America occurs in 1911, another reference to ritual begging on Halloween appears, place unknown, in 1915, with a third reference in Chicago in 1920. The earliest known use in print of the term "trick or treat" appears in 1927, in the "Blackie Herald" Alberta, Canada.
The thousands of Halloween postcards produced between the turn of the 20th century and the 1920s commonly show children but not trick-or-treating. Trick-or-treating does not seem to have become a widespread practice until the 1930s, with the first U.S. appearances of the term in 1934, and the first use in a national publication occurring in 1939.
A popular variant of trick-or-treating, known as trunk-or-treating (or Halloween tailgaiting), occurs when "children are offered treats from the trunks of cars parked in a church parking lot," or sometimes, a school parking lot. In a trunk-or-treat event, the trunk (boot) of each automobile is decorated with a certain theme, such as those of children's literature, movies, scripture, and job roles. Because the traditional style of trick-or-treating was made impossible after Hurricane Katrina, trunk-or-treating provided comfort to those whose homes were devastated. Trunk-or-treating has grown in popularity due to its perception as being more safe than going door to door, a point that resonates well with parents, as well as the fact that it "solves the rural conundrum in which homes [are] built a half-mile apart".
Costumes.
Halloween costumes are traditionally modeled after supernatural figures such as vampires, monsters, ghosts, skeletons, witches, and devils. Over time, in the United States the costume selection extended to include popular characters from fiction, celebrities, and generic archetypes such as ninjas and princesses.
Dressing up in costumes and going "guising" was prevalent in Ireland and Scotland at Halloween by the late 19th century. Costuming became popular for Halloween parties in the US in the early 20th century, as often for adults as for children. The first mass-produced Halloween costumes appeared in stores in the 1930s when trick-or-treating was becoming popular in the United States.
Rev. Dr. Eddie J. Smith, in his book "Halloween, Hallowed Be Thy Name", offers a religious perspective to the wearing of costumes on All Hallows' Eve, suggesting that by dressing up as creatures "who at one time caused us to fear and tremble", people are able to poke fun at Satan "whose kingdom has been plundered by our Saviour." Images of skeletons and the dead are traditional decorations used as "memento mori".
UNICEF.
"Trick-or-Treat for UNICEF" is a fundraising program to support UNICEF, a United Nations Programme that provides humanitarian aid to children in developing countries. Started as a local event in a Northeast Philadelphia neighborhood in 1950 and expanded nationally in 1952, the program involves the distribution of small boxes by schools (or in modern times, corporate sponsors like Hallmark, at their licensed stores) to trick-or-treaters, in which they can solicit small-change donations from the houses they visit. It is estimated that children have collected more than $118 million for UNICEF since its inception. In Canada, in 2006, UNICEF decided to discontinue their Halloween collection boxes, citing safety and administrative concerns; after consultation with schools, they instead redesigned the program.
Games and other activities.
There are several games traditionally associated with Halloween parties. One common game is dunking or apple bobbing, which may be called "dooking" in Scotland in which apples float in a tub or a large basin of water and the participants must use their teeth to remove an apple from the basin. The practice is thought by some to have derived from the Roman practices in celebration of Pomona. A variant of dunking involves kneeling on a chair, holding a fork between the teeth and trying to drive the fork into an apple. Another common game involves hanging up treacle or syrup-coated scones by strings; these must be eaten without using hands while they remain attached to the string, an activity that inevitably leads to a very sticky face.
Some games traditionally played at Halloween are forms of divination. In All Hallows' Eve celebrations during the Middle Ages, these activities historically occurred only in rural areas of medieval Europe and were only done by a "rare few" as these were considered to be "deadly serious" practices. A traditional Scottish form of divining one's future spouse is to carve an apple in one long strip, then toss the peel over one's shoulder. The peel is believed to land in the shape of the first letter of the future spouse's name. Unmarried women were told that if they sat in a darkened room and gazed into a mirror on Halloween night, the face of their future husband would appear in the mirror. However, if they were destined to die before marriage, a skull would appear. The custom was widespread enough to be commemorated on greeting cards from the late 19th century and early 20th century.
Another game/superstition that was enjoyed in the early 1900s involved walnut shells. People would write fortunes in milk on white paper. After drying, the paper was folded and placed in walnut shells. When the shell was warmed, milk would turn brown therefore the writing would appear on what looked like blank paper. Folks would also play fortune teller. In order to play this game, symbols were cut out of paper and placed on a platter. Someone would enter a dark room and was ordered to put her hand on a piece of ice then lay it on a platter. Her "fortune" would stick to the hand. Paper symbols included: dollar sign-wealth, button-bachelorhood, thimble-spinsterhood, clothespin- poverty, rice-wedding, umbrella- journey, caldron-trouble, 4-leaf clover- good luck, penny-fortune, ring-early marriage, and key-fame.
The telling of ghost stories and viewing of horror films are common fixtures of Halloween parties. Episodes of television series and Hallowe'en-themed specials (with the specials usually aimed at children) are commonly aired on or before Halloween, while new horror films are often released theatrically before Halloween to take advantage of the atmosphere.
Haunted attractions.
Haunted attractions are entertainment venues designed to thrill and scare patrons. Most attractions are seasonal Halloween businesses. Origins of these paid scare venues are difficult to pinpoint, but it is generally accepted that they were first commonly used by the Junior Chamber International (Jaycees) for fundraising. They include haunted houses, corn mazes, and hayrides, and the level of sophistication of the effects has risen as the industry has grown. Haunted attractions in the United States bring in an estimated $300–500 million each year, and draw some 400,000 customers, although press sources writing in 2005 speculated that the industry had reached its peak at that time. This maturing and growth within the industry has led to technically more advanced special effects and costuming, comparable with that of Hollywood films.
Food.
On All Hallows' Eve, many Western Christian denominations encourage abstinence from meat, giving rise to a variety of vegetarian foods associated with this day.
Because in the Northern Hemisphere Halloween comes in the wake of the yearly apple harvest, candy apples (known as toffee apples outside North America), caramel or taffy apples are common Halloween treats made by rolling whole apples in a sticky sugar syrup, sometimes followed by rolling them in nuts.
At one time, candy apples were commonly given to trick-or-treating children, but the practice rapidly waned in the wake of widespread rumors that some individuals were embedding items like pins and razor blades in the apples in the United States. While there is evidence of such incidents, relative to the degree of reporting of such cases, actual cases involving malicious acts are extremely rare and have never resulted in serious injury. Nonetheless, many parents assumed that such heinous practices were rampant because of the mass media. At the peak of the hysteria, some hospitals offered free X-rays of children's Halloween hauls in order to find evidence of tampering. Virtually all of the few known candy poisoning incidents involved parents who poisoned their own children's candy.
One custom that persists in modern-day Ireland is the baking (or more often nowadays, the purchase) of a barmbrack (Irish: "báirín breac"), which is a light fruitcake, into which a plain ring, a coin and other charms are placed before baking. It is said that those who get a ring will find their true love in the ensuing year. This is similar to the tradition of king cake at the festival of Epiphany.
List of foods associated with Halloween:
Religious observances.
On Hallowe'en (All Hallows' Eve), in Poland, believers are taught to pray out loud as they walk through the forests in order that the souls of the dead might find comfort; in Spain, Christian priests toll their church bells in order to remind their congregants to remember the dead on All Hallows' Eve. In Ireland, and among immigrants in Canada, a custom includes the Christian practice of abstinence, keeping All Hallows' Eve "as a meatless day with pancakes or Callcannon" being served instead. In Mexico, on "All Hallows Eve, the children make a children's altar to invite the "angelitos" (spirits of dead children) to come back for a visit." The Christian Church traditionally observed Hallowe'en through a vigil "when worshippers would prepare themselves with prayers and fasting prior to the feast day itself." This church service is known as the "Vigil of All Hallows" or the "Vigil of All Saints"; an initiative known as "Night of Light" seeks to further spread the "Vigil of All Hallows" throughout Christendom. After the service, "suitable festivities and entertainments" often follow, as well as a visit to the graveyard or cemetery, where flowers and candles are often placed in preparation for All Hallows' Day. In Finland, because so many people visit the cemeteries on All Hallows' Eve to light votive candles there, they "are known as "valomeri", or seas of light."
Perspectives.
Christianity.
Christian attitudes towards Halloween are diverse. In the Anglican Church, some dioceses have chosen to emphasize the Christian traditions associated with All Hallow's Eve. Some of these practices include praying, fasting and attending worship services.
 Father, All-Powerful and Ever-Living God, today we rejoice in the holy men and women of every time and place. May their prayers bring us your forgiveness and love. We ask this through Christ our Lord. Amen. —All Hallow's Eve Prayer from the Liturgy of the Hours
Other Protestant Christians also celebrate All Hallows' Eve as Reformation Day, a day to remember the Protestant Reformation, alongside All Hallow's Eve or independently from it. This is because Martin Luther nailed his "Ninety-Five Theses" to All Saints' Church in Wittenberg on All Hallows' Eve, because hundreds of visitors would come to the church during the celebration of Allhallowtide. Often, "Harvest Festivals" or "Reformation Festivals" are held on All Hallows' Eve, in which children dress up as Bible characters or Reformers. In addition to distributing candy to children who are trick-or-treating on Hallowe'en, many Christians also provide gospel tracts to them. One organization, the American Tract Society, stated that around 3 million gospel tracts are ordered from them alone for Hallowe'en celebrations. Others order Halloween-themed "Scripture Candy" to pass out to children on this day.
Some Christians feel concerned about the modern celebration of Halloween because they feel it trivializes – or celebrates – paganism, the occult, or other practices and cultural phenomena deemed incompatible with their beliefs. Father Gabriele Amorth, an exorcist in Rome, has said, "if English and American children like to dress up as witches and devils on one night of the year that is not a problem. If it is just a game, there is no harm in that." In more recent years, the Roman Catholic Archdiocese of Boston has organized a "Saint Fest" on Halloween. Similarly, many contemporary Protestant churches view Halloween as a fun event for children, holding events in their churches where children and their parents can dress up, play games, and get candy for free. Many Christians ascribe no negative significance to Halloween, treating it as a fun event devoted to "imaginary spooks" and handing out candy. To these Christians, Halloween holds no threat to the spiritual lives of children: being taught about death and mortality, and the ways of the Celtic ancestors actually being a valuable life lesson and a part of many of their parishioners' heritage.
In the Roman Catholic Church, Halloween's Christian connection is cited, and Halloween celebrations are common in Catholic parochial schools throughout North America and in Ireland. Many fundamentalist and evangelical churches use "Hell houses", themed pamphlets, or comic-style tracts such as those created by Jack T. Chick in order to make use of Halloween's popularity as an opportunity for evangelism. Some consider Halloween to be completely incompatible with the Christian faith due to its putative origins in the Festival of the Dead celebration. Indeed, even though Eastern Orthodox Christians observe All Hallows' Day on the First Sunday after Pentecost, the Eastern Orthodox Church recommends the observance of Vespers and/or a Paraklesis on the Western observance of All Hallows' Eve, out of the pastoral need to provide an alternative to popular celebrations.
Other religions.
The reaction of non-Christian religions towards Halloween has often been mixed, ranging from stern disapproval to the allowance of participation in it. According to Alfred J. Kolatch in the "Second Jewish Book of Why", in Judaism, Halloween is not permitted by Jewish Halakha because it violates Leviticus 18:3 which forbids Jews from partaking in gentile customs. Many Jews observe Yizkor, which is equivalent to the observance of Allhallowtide in Christianity, as prayers are said for both "martyrs and for one's own family." Nevertheless many American Jews celebrate Halloween, disconnected from its Christian origins. Reform Rabbi Jeffrey Goldwasser has said that "There is no religious reason why contemporary Jews should not celebrate Halloween" while Orthodox Rabbi Michael Broyde has argued against Jews observing the holiday.
Sheikh Idris Palmer, author of "A Brief Illustrated Guide to Understanding Islam ", has argued that Muslims should not participate in Halloween, stating that "participation in it is similar to one commemorating Christmas or Easter, or congratulating the Christians upon their prostration to the crucifix". Javed Memon, a Muslim writer, has disagreed, saying that his "daughter dressing up like a British telephone booth will not destroy her faith".
Most Hindus do not observe All Hallows' Eve, instead remembering the dead in the festival of Pitru Paksha, during which Hindus pay homage to and perform a ceremony "to keep the souls of their ancestors at rest." The celebration of the Hindu festival Diwali sometimes conflicts with the date of Halloween; but some Hindus choose to participate in the popular customs of Halloween. Other Hindus, such as Soumya Dasgupta, have opposed the celebration on the grounds that Western holidays like Halloween have "begun to adversely affect our indigenous festivals."
Neopagans do not observe Halloween, but instead observe Samhain on 1 November, although some neopagan individuals choose to participate in cultural Halloween festivities, opining the idea that one can observe both "the solemnity of Samhain in addition to the fun of Halloween." Other neopagans are opposed to the celebration of Halloween, believing that it "trivializes Samhain", and "avoid Halloween, because of the interruptions from trick or treaters." "The Manitoban" writes that "Wiccans don’t officially celebrate Halloween, despite the fact that 31 Oct. will still have a star beside it in any good Wiccan’s day planner. Starting at sundown, Wiccans celebrate a holiday known as Samhain. Samhain actually comes from old Celtic traditions and is not exclusive to Neopagan religions like Wicca. While the traditions of this holiday originate in Celtic countries, modern day Wiccans don’t try to historically replicate Samhain celebrations. Some traditional Samhain rituals are still practised but at its core, the holiday is simply a time to celebrate darkness and the dead — a possible reason why Samhain is often confused with Halloween celebrations."
Around the world.
The traditions and importance of Halloween vary greatly among countries that observe it. In Scotland and Ireland, traditional Halloween customs include children dressing up in costume going "guising", holding parties, while other practices in Ireland include lighting bonfires, and having firework displays. In Brittany children would set candles in skulls in graveyards. Mass transatlantic immigration in the 19th century popularized Halloween in North America, and celebration in the United States and Canada has had a significant impact on how the event is observed in other nations. This larger North American influence, particularly in iconic and commercial elements, has extended to places such as South America, Australia, New Zealand, (most) continental Europe, Japan, and other parts of East Asia. In the Philippines, on the night of Halloween, Filipinos return to their hometowns and purchase candles and flowers, in preparation for the following All Saints Day and All Souls Day ("Araw ng mga Patay") on 1 November.
Further reading.
</dl>

</doc>
<doc id="13878" url="http://en.wikipedia.org/wiki?curid=13878" title="Homology">
Homology

Homology may refer to:
Homologous may refer to:
Homological may refer to:
Homologation may refer to:

</doc>
<doc id="13883" url="http://en.wikipedia.org/wiki?curid=13883" title="Huffman coding">
Huffman coding

In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Ph.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence ("weight") for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in linear time to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.
History.
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.
In doing so, the student outdid his professor, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of 
the top down, Huffman avoided the major flaw of the suboptimal Shannon-Fano coding.
Terminology.
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition.
Formalized description.
Input.<br>
Alphabet formula_1, which is the symbol alphabet of size formula_2. <br>
Set formula_3, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. formula_4. <br>
<br>
Output.<br>
Code formula_5, which is the set of (binary) codewords, where formula_6 is the codeword for formula_7.<br>
<br>
Goal.<br>
Let formula_8 be the weighted path length of code formula_9. Condition: formula_10 for any code formula_11.
Example.
We give an example of the result of Huffman coding for a code with five words and given weights. We will not verify that it minimizes "L" over all codes (it does of course), but we will compute "L" and compare it to the Shannon entropy "H" of the given set of weights; the result is nearly optimal.
For any code that is "biunique", meaning that the code is "uniquely decodeable", the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a "complete" code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it "biunique".
As defined by Shannon (1948), the information content "h" (in bits) of each symbol "a"i with non-null probability is
The entropy "H" (in bits) is the weighted sum, across all symbols "a""i" with non-zero probability "w""i", of the information content of each symbol:
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing formula_15 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique.
Compression.
The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, formula_2. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain symbol weight, links to two child nodes and the optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to formula_2 leaf nodes and formula_18 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process essentially begins with the leaf nodes containing the probabilities of the symbol they represent, then a new node whose children are the 2 nodes with smallest probability is created, such that the new node's probability is equal to the sum of the children's probability. With the previous 2 nodes merged into one node (thus not considering them anymore), and with the new node being now considered, the procedure is repeated until only one node remains, the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:
Since efficient priority queue data structures require O(log "n") time per insertion, and a tree with "n" leaves has 2"n"−1 nodes, this algorithm operates in O("n" log "n") time, where "n" is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O("n")) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:
Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O("n" log "n") time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since "n" here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when "n" grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Here's an example of optimized Huffman coding using the French subject string "j'aime aller sur le bord de l'eau les jeudis ou les jours impairs". Note that the original Huffman coding tree structure would be different from the given example:
Decompression.
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just formula_19 bits of information (where formula_20 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties.
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed.
This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality.
Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding and LZW coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. However, these methods have higher computational complexity. Also, both arithmetic coding and LZW were historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for these alternatives to Huffman coding have passed into the public domain as the early patents have expired.
However, the limitations of Huffman coding should not be overstated; it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is an the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. These situations often respond well to a form of blocking called run-length encoding; for the simple case of Bernoulli processes, Golomb coding is a provably optimal run-length code.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations.
Many variations of Huffman coding exist, some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time. An exhaustive list of papers on Huffman coding and its variations is given by "Code and Parse Trees for Lossless Source Encoding".
"n"-ary Huffman coding.
The "n"-ary Huffman algorithm uses the {0, 1, ... , "n" − 1} alphabet to encode message and build an "n"-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary ("n" equals 2) codes, except that the "n" least probable symbols are taken together, instead of just the 2 least probable. Note that for "n" greater than 2, not all sets of source words can properly form an "n"-ary tree for Huffman coding. In this case, additional 0-probability place holders must be added. This is because the tree must form an "n" to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo "n"-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding.
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, that is more flexible and has a better compression.
Huffman template algorithm.
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing formula_21, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance huffman coding.
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is formula_22, where formula_23 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs.
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is "N" digits will always have a cost of "N", no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
"Huffman coding with unequal letter costs" is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding.
Optimal alphabetic binary trees (Hu–Tucker coding).
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, formula_24 could not be assigned code formula_25, but instead should be assigned either formula_26 or formula_27. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem, which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code.
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the "canonical Huffman code" and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is formula_28, which, having the same codeword lengths as the original solution, is also optimal. But in "canonical Huffman code", the result is formula_29.
Applications.
Arithmetic coding can be viewed as a generalization of Huffman coding, in the sense that they produce the same output when every symbol has a probability of the form 1/2"k"; in particular it tends to offer significantly better compression for small alphabet sizes. Huffman coding nevertheless remains in wide use because of its simplicity and high speed. Intuitively, arithmetic coding can offer better compression than Huffman coding because its "code words" can have effectively non-integer bit lengths, whereas code words in Huffman coding can only have an integer number of bits. Therefore, there is an inefficiency in Huffman coding where a code word of length "k" only optimally matches a symbol of probability 1/2"k" and other probabilities are not represented as optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol.
Huffman coding today is often used as a "back-end" to some other compression methods.
DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by Huffman coding (or variable-length prefix-free codes with a similar structure, although perhaps not necessarily designed by using Huffman's algorithm).

</doc>
<doc id="13887" url="http://en.wikipedia.org/wiki?curid=13887" title="Honolulu">
Honolulu

Honolulu ( or ; ]) is the state capital and the most populous city in the U.S. state of Hawaii. It is the county seat of the City and County of Honolulu. Hawaii is a major tourist destination and Honolulu, situated on the island of Oahu, is the main gateway to Hawaii and a major gateway into the United States. The city is also a major hub for international business, military defense, as well as famously being host to a diverse variety of east-west and Pacific culture, cuisine, and traditions.
Honolulu is both the westernmost and the southernmost major American city. For statistical purposes, the U.S. Census Bureau recognizes the approximate area commonly referred to as "City of Honolulu" (not to be confused with the "City and County") as a census county division (CCD). Honolulu is a major financial center of the islands and of the Pacific Ocean. The population of Honolulu CCD was 390,738 at the 2010 census, while the population of the consolidated city and county was 953,207.
"Honolulu" means "sheltered harbor" or "calm port." The old name is said to be Kou, a district roughly encompassing the area from Nuuanu Avenue to Alakea Street and from Hotel Street to Queen Street which is the heart of the present downtown district. The city has been the capital of the Hawaiian Islands since 1845 and gained historical recognition following the attack on Pearl Harbor by Japan near the city on December 7, 1941.
History.
Evidence of the first settlement of Honolulu by the original Polynesian migrants to the archipelago comes from oral histories and artifacts. These indicate that there was a settlement where Honolulu now stands in the 11th century. However, after Kamehameha I conquered Oʻahu in the Battle of Nuʻuanu at Nuʻuanu Pali, he moved his royal court from the Island of Hawaiʻi to Waikīkī in 1804. His court relocated in 1809 to what is now downtown Honolulu. The capital was moved back to Kailua-Kona in 1812.
In 1794, Captain William Brown of Great Britain was the first foreigner to sail into what is now Honolulu Harbor. More foreign ships followed, making the port of Honolulu a focal point for merchant ships traveling between North America and Asia.
In 1845, Kamehameha III moved the permanent capital of the Hawaiian Kingdom from Lahaina on Maui to Honolulu. He and the kings that followed him transformed Honolulu into a modern capital, erecting buildings such as St. Andrew's Cathedral, ʻIolani Palace, and Aliʻiōlani Hale. At the same time, Honolulu became the center of commerce in the islands, with descendants of American missionaries establishing major businesses in downtown Honolulu.
Despite the turbulent history of the late 19th century and early 20th century, such as the overthrow of the Hawaiian monarchy in 1893, Hawaiʻi's subsequent annexation by the United States in 1898, followed by a large fire in 1900, and the Japanese attack on Pearl Harbor in 1941, Honolulu remained the capital, largest city, and main airport and seaport of the Hawaiian Islands.
An economic and tourism boom following statehood brought rapid economic growth to Honolulu and Hawaiʻi. Modern air travel brings, as of 2007, 7.6 million visitors annually to the islands, with 62.3% entering at Honolulu International Airport. Today, Honolulu is a modern city with numerous high-rise buildings, and Waikīkī is the center of the tourism industry in Hawaiʻi, with thousands of hotel rooms. The UK consulting firm Mercer, in a 2009 assessment "conducted to help governments and major companies place employees on international assignments", ranked Honolulu 29th worldwide in quality of living; the survey factored in political stability, personal freedom, sanitation, crime, housing, the natural environment, recreation, banking facilities, availability of consumer goods, education, and public services including transportation.
Geography.
According to the United States Census Bureau, the CDP has a total area of 177.2 km2. 156.7 km2 of it (88.44%) is land, and 20.5 km2 of it (11.56%) is water.
The closest location on the mainland to Honolulu is the Point Arena Lighthouse in California, at 2045 nmi. (Nautical vessels require some additional distance to circumnavigate Makapuʻu Point.) However, part of the Aleutian Islands of Alaska are slightly closer to Honolulu than the mainland.
Climate.
Honolulu experiences a tropical savannah climate (Köppen classification "As"), with a mostly dry summer season, due to a rain shadow effect.
Temperatures vary little throughout the months, with average high temperatures of 80 – and average lows of 65 – throughout the year. Temperatures reach or exceed 90 °F on an average 38 days annually, with lows in the upper 50s °F (14–15 °C) occurring once or twice a year. The highest recorded temperature was 95 °F during a heat wave in September 1998. The highest recorded temperature in the state was also recorded later that day in Ni'ihau. The lowest recorded temperature was 52 °F on February 16, 1902, and January 20, 1969.
Annual average rainfall is 17.05 in, which mainly occurs during the winter months of October through early April, with very little rainfall during the summer. However, both seasons experience a similar number of rainy days. Light showers occur in summer while heavier rain falls during winter. Honolulu has an average of 278 sunny days and 90 rainy days per year.
Although the city is situated in the tropics, hurricanes are quite rare. The last recorded hurricane that hit the area was Category 4 Hurricane Iniki in 1992. Tornadoes are also uncommon and usually strike once every 15 years. Waterspouts off the coast are also uncommon, hitting about once every five years.
Honolulu falls under the USDA 12a Plant Hardiness zone.
Panorama of Honolulu's waterfront in February 2007.
Demographics.
The population of Honolulu was 390,738 according to the 2010 U.S. Census. Of those, 192,781 (49.3%) were male and 197,957 (50.7%) were female. The median age for males was 40.0 and 43.0 for females; the overall median age was 41.3. Approximately 84.7% of the total population was 16 years and over; 82.6% were 18 years and over, 78.8% were 21 years and over, 21.4% were 62 years and over, and 17.8% were 65 years and over.
In terms of race and ethnicity, 54.8% were Asian, 17.9% were White, 1.5% were Black or African American, 0.2% were American Indian or Alaska Native, 8.4% were Native Hawaiian and Other Pacific Islander, 0.8% were from "some other race", and 16.3% were from two or more races. Hispanics and Latinos of any race made up 5.4% of the population. In 1970, the Census Bureau reported Honolulu's population as 33.9% white and 53.7% Asian and Pacific Islander.
Asian Americans represent the majority of Honolulu's population. The Asian ethnic groups are Japanese (19.9%), Filipinos (13.2%), Chinese (10.4%), Koreans (4.3%), Vietnamese (2.0%), Asian Indians (0.3%), Laotians (0.3%), Thais (0.2%), Cambodians (0.1%), and Indonesians (0.1%). People solely of Native Hawaiian ancestry made up 3.2% of the population. Samoan Americans made up 1.5% of the population, Marshallese people make up 0.5% of the city's population, and Tongan people comprise 0.3% of its population. People of Guamanian or Chamorro descent made up 0.2% of the population and numbered 841 residents.
Economy.
The largest city and airport in the Hawaiian Islands, Honolulu acts as a natural gateway to the islands' large tourism industry, which brings millions of visitors and contributes $10 billion annually to the local economy. Honolulu's location in the Pacific also makes it a large business and trading hub, particularly between the East and the West. Other important aspects of the city's economy include military defense, research and development, and manufacturing.
Among the companies based in Honolulu are:
Hawaiian Airlines, Island Air, and Aloha Air Cargo are headquartered in the city. Prior to its dissolution, Aloha Airlines was headquartered in the city. At one time Mid-Pacific Airlines had its headquarters on the property of Honolulu International Airport.
In 2009, Honolulu had a 4.5% increase in the average price of rent, maintaining it in the second most expensive rental market ranking among 210 U.S. metropolitan areas.
Since no national bank chains have any branches in Hawaii, many visitors and new residents use different banks. First Hawaiian Bank is the largest and oldest bank in Hawaii and their headquarters are at the First Hawaiian Center, the tallest building in the State of Hawaii.
Cultural institutions.
Natural museums.
The Bishop Museum is the largest of Honolulu's museums. It is endowed with the state's largest collection of natural history specimens and the world's largest collection of Hawaiiana and Pacific culture artifacts. The Honolulu Zoo is the main zoological institution in Hawaii while the Waikiki Aquarium is a working marine biology laboratory. The Waikiki Aquarium is partnered with the University of Hawaii and other universities worldwide. Established for appreciation and botany, Honolulu is home to several gardens: Foster Botanical Garden, Liliʻuokalani Botanical Garden, Walker Estate, among others.
Performing arts.
Established in 1900, the Honolulu Symphony is the oldest US symphony orchestra west of the Rocky Mountains. Other classical music ensembles include the Hawaii Opera Theatre. Honolulu is also a center for Hawaiian music. The main music venues include the Hawaii Theatre, the Neal Blaisdell Center Concert Hall and Arena, and the Waikiki Shell.
Honolulu also includes several venues for live theater, including the Diamond Head Theatre.
Visual arts.
Various institutions for the visual arts are located in Honolulu.
The Honolulu Museum of Art is endowed with the largest collection of Asian and Western art in Hawaii. It also has the largest collection of Islamic art, housed at the Shangri La estate. Since the merger of the Honolulu Academy of Arts and The Contemporary Museum, Honolulu (now called the Honolulu Museum of Art Spalding House) in 2011, the museum is also the only contemporary art museum in the state. The contemporary collections are housed at main campus (Spalding House) in Makiki and a multi-level gallery in downtown Honolulu at the First Hawaiian Center. The museum hosts a film and video program dedicated to arthouse and world cinema in the museum's Doris Duke Theatre, named for the museum's historic patroness Doris Duke.
The Hawaii State Art Museum (also downtown) boasts pieces by local artists as well as traditional Hawaiian art. The museum is administered by the Hawaii State Foundation on Culture and the Arts.
Honolulu also annually holds the Hawaii International Film Festival (HIFF). It showcases some of the best films from producers all across the Pacific Rim and is the largest "East meets West" style film festival of its sort in the United States.
Sports.
Honolulu's climate lends itself to year-round activities. In 2004, "Men's Fitness" magazine named Honolulu the fittest city in the United States. Honolulu has three large road races:
Ironman Hawaii was first held in Honolulu, it was the first ever Ironman and is also the World Champs.
Fans of spectator sports in Honolulu generally support the football, volleyball, basketball, rugby union, rugby league and baseball programs of the University of Hawaii at Manoa. High school sporting events, especially football, are especially popular.
Honolulu has no professional sports teams. It was the home of the Hawaii Islanders (Pacific Coast League, 1961–1987), The Hawaiians (World Football League, 1974–1975), Team Hawaii (North American Soccer League, 1977), and the Hawaiian Islanders (af2, 2002–2004).
The NCAA football Hawaii Bowl is played in Honolulu. Honolulu has also hosted the NFL's annual Pro Bowl each February since 1980, though the 2010 Pro Bowl was played in Miami. In 2011, the 2011 Pro Bowl returned once again to Honolulu. From 1993 to 2008, Honolulu hosted Hawaii Winter Baseball, featuring minor league players from Major League Baseball, Nippon Professional Baseball, Korea Baseball Organization, and independent leagues.
Venues.
Venues for spectator sports in Honolulu include:
Aloha Stadium, a venue for American football and soccer, is located in Halawa near Pearl Harbor, just outside Honolulu.
Government.
Kirk Caldwell was elected mayor of Honolulu County on November 6, 2012, and has begun serving as the county's 14th mayor on January 2, 2013. The municipal offices of the City and County of Honolulu, including Honolulu Hale, the seat of the city and county, are located in the Capitol District, as are the Hawaii state government buildings.
The Capitol District is within the Honolulu census county division (CCD), the urban area commonly regarded as the "City" of Honolulu. The Honolulu CCD is located on the southeast coast of Oahu between Makapuu and Halawa. The division boundary follows the Koolau crestline, so Makapuʻu Beach is in the Koolaupoko District. On the west, the division boundary follows Halawa Stream, then crosses Red Hill and runs just west of Aliamanu Crater, so that Aloha Stadium, Pearl Harbor (with the USS Arizona Memorial), and Hickam Air Force Base are actually all located in the island's Ewa CCD.
The Hawaii Department of Public Safety operates the Oahu Community Correctional Center, the jail for the island of Oahu, in Honolulu CCD.
The United States Postal Service operates post offices in Honolulu. The main Honolulu Post Office is located by the international airport at 3600 Aolele Street. Federal Detention Center, Honolulu, operated by the Federal Bureau of Prisons, is in the CDP.
Diplomatic missions on the island.
Several countries have diplomatic facilities in Honolulu, due to its strategically important position in the mid-Pacific. They include consulates of Japan, South Korea, Philippines, Federated States of Micronesia, Australia, and the Marshall Islands.
Education.
Colleges and universities.
Colleges and universities in Honolulu include Honolulu Community College, Kapiolani Community College, the University of Hawaii at Manoa, Chaminade University, and Hawaii Pacific University. UH Manoa houses the main offices of the University of Hawaii System.
Public primary and secondary schools.
Hawaii Department of Education operates public schools in Honolulu. Public high schools within the CDP area include Wallace Rider Farrington, Kaiser, Kaimuki, Kalani, Moanalua, William McKinley, and Theodore Roosevelt.
Private primary and secondary schools.
Private schools include Academy of the Pacific, Damien Memorial School, Hawaii Baptist Academy, Iolani School, Lutheran High School of Hawaii, Kamehameha Schools, Maryknoll School, Mid-Pacific Institute, La Pietra, Punahou School, Sacred Hearts Academy, St. Andrew's Priory School, Saint Francis School, Saint Louis School, the Education Laboratory School, Saint Patrick School, Trinity Christian School, and Varsity International School.
Public libraries.
Hawaii State Public Library System operates public libraries. The Hawaii State Library in the CDP serves as the main library of the system, while the Library for the Blind and Physically Handicapped, also in the CDP area, serves handicapped and blind people.
Branches in the CDP area include Aiea, Aina Haina, Ewa Beach, Hawaii Kai, Kahuku, Kailua, Kaimuki, Kalihi-Palama, Kaneohe, Kapolei, Liliha, Manoa, McCully-Moiliili, Mililani, Moanalua, Wahiawa, Waialua, Waianae, Waikiki-Kapahulu, Waimanalo, and Waipahu.
Weekend educational programs.
The Hawaii Japanese School - Rainbow Gakuen (ハワイレインボー学園 "Hawai Rainbō Gakuen"), a supplementary weekend Japanese school, holds its classes in Kaimuki Middle School in Honolulu and has its offices in another building in Honolulu. The school serves overseas Japanese nationals. In addition Honolulu has other weekend programs for the Japanese, Chinese, and Spanish languages.
Media.
Honolulu is served by one daily newspaper (the "Honolulu Star-Advertiser"), "Honolulu Magazine", several radio stations and television stations, among other media. Local news agency and CNN-affiliate Hawaii News Now broadcasts and is headquartered out of Honolulu.
Honolulu and the island of Oahu has also been the location for many film and television projects, including "Hawaii Five-0" and "Lost".
Transportation.
Air.
Located at the western end of the CDP, Honolulu International Airport (HNL) is the principal aviation gateway to the state of Hawaii. Kalaeloa Airport is primarily a commuter facility used by unscheduled air taxis, general aviation and transient and locally based military aircraft.
Highways.
Honolulu has been ranked as having the nation’s worst traffic congestion, beating former record holder Los Angeles. Drivers waste on average over 58 hours per year on congested roadways. The following freeways, part of the Interstate Highway System serve Honolulu:
Other major highways that link Honolulu CCD with other parts of the Island of Oahu are:
Like most major American cities, the Honolulu metropolitan area experiences heavy traffic congestion during rush hours, especially to and from the western suburbs of Kapolei, 'Ewa Beach, Aiea, Pearl City, Waipahu, and Mililani.
There is a Hawaii Electric Vehicle Demonstration Project (HEVDP).
Public transport.
Honolulu Authority for Rapid Transportation.
In November 2010, voters approved a charter amendment to create a public transit authority to oversee the planning, construction, operation and future extensions to Honolulu's future rail system. The Honolulu Authority for Rapid Transportation (HART) currently includes a 10-member board of directors; three members appointed by the mayor, three members selected by the Honolulu City Council, and the city and state transportation directors. 
The opening of the Honolulu Rail Transit is delayed until approximately 2018, as HART canceled the initial bids for the first nine stations and intends to rebid the work as three packages of three stations each, and allow more time for construction in the hope that increased competition on smaller contracts will drive down costs; initial bids ranged from $294.5 million to $320.8 million, far surpassing HART's budget of $184 million.
Bus.
Established by former Mayor Frank F. Fasi as the replacement for the Honolulu Rapid Transit Company (HRT), Honolulu's TheBus system was honored in 1994–1995 and 2000–2001 by the American Public Transportation Association as "America's Best Transit System". TheBus operates 107 routes serving Honolulu and most major cities and towns on Oahu. TheBus comprises a fleet of 531 buses, and is run by the non-profit corporation Oahu Transit Services in conjunction with the city Department of Transportation Services. Honolulu is ranked 4th for highest per-capita use of mass transit in the United States.
Rail.
Currently, there is no urban rail transit system in Honolulu, although electric street railways were operated in Honolulu by the now-defunct Honolulu Rapid Transit Company prior to World War II. Predecessors to the Honolulu Rapid Transit Company were the Honolulu Rapid Transit and Land Company (began 1903) and Hawaiian Tramways (began 1888).
The City and County of Honolulu is currently constructing a 20 mi rail transit line that will connect Honolulu with cities and suburban areas near Pearl Harbor and in the Leeward and West Oahu regions. The Honolulu High-Capacity Transit Corridor Project is aimed at alleviating traffic congestion for West Oahu commuters while being integral in the westward expansion of the metropolitan area. The project, however, has been criticized by opponents of rail for its cost, delays, and potential environmental impacts, but the line is expected to have large ridership.
Notable people.
The following are notable people who were born in Honolulu, and/or current and former residents of Honolulu:
Sister cities.
Honolulu currently has 26 sister cities:

</doc>
<doc id="13981" url="http://en.wikipedia.org/wiki?curid=13981" title="Hockey">
Hockey

Hockey is a family of sports in which two teams play against each other by trying to maneuver a ball or a puck into the opponent's goal using a hockey stick. In many areas, one sport (typically field hockey or ice hockey) is generally referred to simply as hockey.
Etymology.
The first recorded use of the word "hockey" is from the 1773 book "Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education", by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled "New Improvements on the Game of Hockey". The belief that hockey was mentioned in a 1363 proclamation by King Edward III of England is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games "Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam". The English historian and biographer John Strype did not use the word "hockey" when he translated the proclamation in 1720.
The word "hockey" itself is of unknown origin. One explanation is that it is a derivative of "hoquet", a Middle French word for a shepherd's stave. The curved, or "hooked" ends of the sticks used for hockey would indeed have resembled these staves. Another explanation is that the cork bungs that replaced wooden balls in the 18th century came from barrels containing "Hock" ale, also called "Hocky".
History.
Games played with curved sticks and a ball can be found in the histories of many cultures. In Egypt, 4000-year-old carvings feature teams with sticks and a projectile, hurling dates to before 1272 BC in Ireland, and there is a depiction from approximately 600 BC in Ancient Greece where the game may have been called "kerētízein" or "kerhtízein" (κερητίζειν) because it was played with a horn or horn-like stick("kéras", κέρας) In Inner Mongolia, the Daur people have been playing "beikou", a game similar to modern field hockey, for about 1,000 years.
Most evidence of hockey-like games during the Middle Ages is found in legislation concerning sports and games. The Galway Statute enacted in Ireland in 1527 banned certain types of ball games, including games using "hooked" (written "hockie", similar to "hooky") sticks.
 ...at no tyme to use ne occupye the horlinge of the litill balle with hockie stickes or staves, nor use no hande ball to play withoute walles, but only greate foote balle
By the 19th century, the various forms and divisions of historic games began to differentiate and coalesce into the individual sports defined today. Organizations dedicated to the codification of rules and regulations began to form, and national and international bodies sprung up to manage domestic and international competition. Ice hockey also evolved during this period as a derivative of field hockey adapted to the icy conditions of Canada and the northern United States.
Subtypes.
Field hockey.
Field hockey is played on gravel, natural grass, sand-based or water-based artificial turf, with a small, hard ball approximately 73 mm (2.9 in) in diameter. The game is popular among both males and females in many parts of the world, particularly in Europe, Asia, Australia, New Zealand, South Africa, and Argentina. In most countries, the game is played between single-sex sides, although they can be mixed-sex.
The governing body is the 126-member International Hockey Federation (FIH). Men's field hockey has been played at each summer Olympic Games since 1908 (except 1912 and 1924), while women's field hockey has been played at the Summer Olympic Games since 1980.
Modern field hockey sticks are J-shaped and constructed of a composite of wood, glass fibre or carbon fibre (sometimes both) and have a curved hook at the playing end, a flat surface on the playing side and curved surface on the rear side. All sticks are right-handed – left-handed sticks are not permitted.
While current field hockey appeared in mid-18th century England, primarily in schools, it was not until the first half of the 19th century that it became firmly established. The first club was created in 1849 at Blackheath in south-east London. Field hockey is the national sport of Pakistan. It was the national sport of India until the Ministry of Youth Affairs and Sports declared that India has no national sport in August 2012.
Ice hockey.
Ice hockey is played on a large flat area of ice, using a three-inch-diameter (76.2 mm) vulcanized rubber disc called a puck. This puck is often frozen before high-level games to decrease the amount of bouncing and friction on the ice. The game is contested between two teams of skaters. The game is played all over North America, Europe and in many other countries around the world to varying extent. It is the most popular sport in Canada, Finland, Latvia, the Czech Republic, and Slovakia.
The governing body of international play is the 72-member International Ice Hockey Federation (IIHF). Men's ice hockey has been played at the Winter Olympics since 1924, and was in the 1920 Summer Olympics. Women's ice hockey was added to the Winter Olympics in 1998. North America's National Hockey League (NHL) is the strongest professional ice hockey league, drawing top ice hockey players from around the globe. The NHL rules are slightly different from those used in Olympic ice hockey over many categories.
Ice hockey sticks are long L-shaped sticks made of wood, graphite, or composites with a blade at the bottom that can lie flat on the playing surface when the stick is held upright and can curve either way, legally, as to help a left- or right-handed player gain an advantage.
Various stick and ball games similar to field hockey, bandy and other games where two teams push a ball or object back and forth with sticks were played on ice under the name "hockey" in England throughout the 19th century, and even earlier under various other names. In Canada, there are 24 reports of hockey-like games in the 19th century before 1875 (five of them using the name "hockey"). The first organized indoor game of ice hockey was played in Montreal, Canada on March 3, 1875 and featured several McGill University students. The contemporary sport developed in Canada from these and other influences. International ice hockey rules were adopted from Canadian rules in the early 1900s.
Ice hockey is the national sport of Latvia and the national winter sport of Canada.
Ice hockey is played at a number of levels, by all ages. 
Roller hockey (quad).
Roller hockey, also known as quad hockey, international-style ball hockey, and Hoquei em Patins is an overarching name for a roller sport that has existed since long before inline skates were invented. This sport is played in over sixty countries and has a worldwide following. Roller hockey was a demonstration sport at the 1992 Barcelona Summer Olympics.
Roller hockey (inline).
Inline hockey is a variation of roller hockey very similar to ice hockey, from which it is derived. Inline hockey is played by two teams, consisting of four skaters and one goalie, on a dry rink divided into two halves by a center line, with one net at each end of the rink. The game is played in three 15-minute periods with a variation of the ice hockey off-side rule. Icings are also called, but are usually referred to as illegal clearing. For rink dimensions and an overview of the rules of the game, see IIHF Inline Rules (). Some leagues and competitions do not follow the IIHF regulations, in particular and .
Sledge hockey.
Sledge hockey is a form of ice hockey designed for players with physical disabilities affecting their lower bodies. Players sit on double-bladed sledges and use two sticks; each stick has a blade at one end and small picks at the other. Players use the sticks to pass, stickhandle and shoot the puck, and to propel their sledges. The rules are very similar to IIHF ice hockey rules.
Canada is a recognized international leader in the development of the sport, and of equipment for players. Much of the equipment for the sport was first developed in Canada, such as sledge hockey sticks laminated with fiberglass, as well as aluminum shafts with hand carved insert blades and special aluminum sledges with regulation skate blades.
Based on ice sledge hockey, inline sledge hockey is played to the same rules as inline puck hockey (essentially ice hockey played off ice using inline skates) and has been made possible by the design and manufacture of inline sledges by RGK, Europe’s premier sports wheelchair maker.
There is no classification point system dictating who can play inline sledge hockey, unlike the situation with other team sports such as wheelchair basketball and wheelchair rugby. Inline sledge hockey is being developed to allow everyone, regardless of whether they have a disability or not, to complete up to world championship level based solely on talent and ability. This makes inline sledge hockey truly inclusive.
The first game of inline sledge hockey was played at Bisley, England, on the 19th of December 2009 between the Hull Stingrays and the Grimsby Redwings. Matt Lloyd is credited with inventing inline sledge hockey, and Great Britain is seen as the international leader in the game's development.
Street hockey.
Also known as road hockey, this is a dry-land variant of ice and roller hockey played on a hard surface (usually asphalt). Most of the time, a ball is used instead of a puck, and generally no protective equipment is worn. Street hockey is played year round.
Other forms of hockey.
Other games derived from hockey or its predecessors include the following:

</doc>
<doc id="13991" url="http://en.wikipedia.org/wiki?curid=13991" title="Honeymoon">
Honeymoon

A honeymoon is the traditional holiday taken by newlyweds to celebrate their marriage in intimacy and seclusion. Today, honeymoons by Westerners are often celebrated in destinations considered exotic and/or romantic.
History of honeymoon.
This is the period when newly wed couples take a break to share some private and intimate moments that helps establish love in their relationship. This privacy in turn is believed to ease the comfort zone towards a physical relationship, which is one of the primary means of bonding during the initial days of marriage. The earliest term for this in English was "hony moone", which was recorded as early as 1546.
In Western culture, the custom of a newlywed couple going on a holiday together originated in early 19th century Great Britain. Upper-class couples would take a "bridal tour", sometimes accompanied by friends or family, to visit relatives who had not been able to attend the wedding. The practice soon spread to the European continent and was known as "voyage à la façon anglaise" (English-style voyage) in France from the 1820s on.
Honeymoons in the modern sense (i.e. a pure holiday voyage undertaken by the married couple) became widespread during the Belle Époque, as one of the first instances of modern mass tourism. This came about in spite of initial disapproval by contemporary medical opinion (which worried about women's frail health) and by "savoir vivre" guidebooks (which referred the public attention drawn to what was assumed to be the wife's sexual initiation). The most popular honeymoon destinations at the time were the French Riviera and Italy, particularly its seaside resorts and romantic cities such as Rome, Verona or Venice. Typically honeymoons would start on the night they were married, with the couple leaving midway through the reception to catch a late train or ship. However, in the 21st century, many couples will not leave until 1–3 days after the ceremony and reception in order to tie up loose ends with the reception venue and/or simply enjoy the reception to its fullest and have a relaxing night afterwards to recover, before undertaking a long journey. In Jewish traditions, honeymoons are often put off seven days to allow for the seven nights of feasting if the visits to friends and family can't be incorporated into the trip.
Etymology.
The "Oxford English Dictionary" offers no etymology, but gives examples dating back to the 16th century. The Merriam-Webster dictionary reports the etymology as from "the idea that the first month of marriage is the sweetest" (1546).
A honeymoon can also be the first moments a newly-wed couple spend together, or the first holiday they spend together to celebrate their marriage.
"The first month after marriage, when there is nothing but tenderness and pleasure" (Samuel Johnson); originally having no reference to the period of a month, but comparing the mutual affection of newly married persons to the changing moon which is no sooner full than it begins to wane; now, usually, the holiday spent together by a newly married couple, before settling down at home.
One of the more recent citations in the "Oxford English Dictionary" indicates that, while today "honeymoon" has a positive meaning, the word was originally a reference to the inevitable waning of love like a phase of the moon. This, the first known literary reference to the honeymoon, was penned in 1552, in Richard Huloet's "Abecedarium Anglico Latinum". Huloet writes:
Hony mone, a term proverbially applied to such as be newly married, which will not fall out at the first, but th'one loveth the other at the beginning exceedingly, the likelihood of their exceadinge love appearing to aswage, ye which time the vulgar people call the hony mone.—Abcedarium Anglico-Latinum pro Tyrunculis, 1552
A widely disputed explanation of the term claims that it comes from a tradition in any of a number of cultures (e.g. Welsh, German or Scandinavian or Babylonian) where Mead was drunk in great quantities at weddings, and after the ceremony nuptial couples were given a month’s supply of mead. It was believed that by faithfully drinking mead for that first month, the woman would “bear fruit” and a child would be born within the year.
There are many words of similar meaning in other languages. The Sinhalese from translates as "Madhu Samaya" ("මධු සමය"). The French form translates as "moon of honey" ("lune de miel"), as do the Spanish ("luna de miel"), Romanian ("luna de miere"), Nepali ("Madhumas") Portuguese ("lua de mel") and Italian ("luna di miele") equivalents. The Welsh word for honeymoon is "mis mêl", which means "honey month", and similarly the Ukrainian ("медовий місяць"), Polish ("miesiąc miodowy"), Russian ("медовый месяц"), Arabic ( "shahr el 'assal"), Greek ("μήνας του μέλιτος") and Hebrew (ירח דבש "yerach d'vash") versions. (Interestingly, "Yerach" is used for month, rather than the more common "Chodesh". "Yerach" is related to the word "Yare'ach" for moon and the two words are spelled alike: ירח.) The Persian word is ماه عسل "māh-e asal" which means both "honey moon" and "honey month" ("māh" in Persian means both "moon" and "month"). The same applies to the word "ay" in the Turkish equivalent, "balayı". In Hungarian language it is called "honey weeks" (mézeshetek). Likewise, the Tamil word for honeymoon is "தேனிலவு" (thaen nilavu), with "thaen" 'honey' and "nilavu" 'moon', and the Marathi word for honeymoon is "मधुचंद्र" (madhuchandra) with "Madhu" 'honey' and "chandra" 'moon', whereas in Bangla ('Bengali') language, it is referred to as মধুচন্দ্রিমা (modhuchondrima) with "modhu" 'honey" and "chondrima" 'moon'.

</doc>
<doc id="13996" url="http://en.wikipedia.org/wiki?curid=13996" title="Heap (data structure)">
Heap (data structure)

In computer science, a heap is a specialized tree-based Abstract data type that satisfies the "heap property:" If A is a parent node of B then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap. Heaps can be classified further as either a "max heap" or a "min heap". In a max heap, the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node. Heaps are crucial in several efficient graph algorithms such as Dijkstra's algorithm, and in the sorting algorithm heapsort. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree (see figure).
In a heap, the highest (or lowest) priority element is always stored at the root, hence the name heap. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the Heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes always has log N height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.
Note that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.
The heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact priority queues are often referred to as "heaps", regardless of how they may be implemented. Note that despite the similarity of the name "heap" to "stack" and "queue", the latter two are abstract data types, while a heap is a specific data structure, and "priority queue" is the proper term for the abstract data type.
A "heap" data structure should not be confused with "the heap" which is a common name for the pool of memory from which dynamically allocated memory is allocated. The term was originally used only for the data structure.
Operations.
The common operations involving heaps are:
Implementation.
Heaps are usually implemented in an array (fixed size or dynamic array), and do not require pointers between elements. After an element is inserted into or deleted from a heap, the heap property is violated and the heap must be balanced by internal operations.
Full and almost full binary heaps may be represented in a very space-efficient way (as an implicit data structure) using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position "n" would be at positions 2n and 2n + 1 in a one-based array, or 2n + 1 and 2n + 2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by shift-up or shift-down operations (swapping elements which are out of order). As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.
Different types of heaps implement the operations in different ways, but notably, insertion is often done by adding the new element at the end of the heap in the first available free space. This will generally violate the heap property, and so the elements are then sifted up until the heap property has been reestablished. Similarly, deleting the root is done by removing the root and then putting the last element in the root and sifting down to rebalance. Thus replacing is done by deleting the root and putting the "new" element in the root and sifting down, avoiding a sifting up step compared to pop (sift down of last element) followed by push (sift up of new element).
Construction of a binary (or "d"-ary) heap out of a given array of elements may be performed in linear time using the classic Floyd algorithm, with the worst-case number of comparisons equal to 2"N" − 2"s"2("N") − "e"2("N") (for a binary heap), where "s"2("N") is the sum of all digits of the binary representation of "N" and "e"2("N") is the exponent of 2 in the prime factorization of "N". This is faster than a sequence of consecutive insertions into an originally empty heap, which is log-linear (or linearithmic).
Comparison of theoretic bounds for variants.
In the following time complexities "O"("f") is an asymptotic upper bound and "Θ"("f") is an asymptotically tight bound (see Big O notation). Function names assume a min-heap.
Applications.
The heap data structure has many applications.

</doc>
<doc id="14015" url="http://en.wikipedia.org/wiki?curid=14015" title="Herstory">
Herstory

Herstory is history written from a feminist perspective, emphasizing the role of women, or told from a woman's point of view. It is a neologism coined in the late 1960s as part of a feminist critique of conventional historiography, with the word "history" reinterpreted, using a false etymology, as "his story." (The word "history"—from the Ancient Greek ἱστορία, or historia, meaning "knowledge obtained by inquiry"—is etymologically unrelated to the possessive pronoun "his".)
The herstory movement has spawned women-centered presses, such as Virago Press in 1973, which publishes fiction and non-fiction by noted women authors like Janet Frame and Sarah Dunant.
Usage.
The "Oxford English Dictionary" credits Robin Morgan with coining the term in her 1970 book "Sisterhood is Powerful." Concerning the feminist organization WITCH, Morgan writes:
In 1976, Casey Miller and Kate Swift wrote in "Words & Women,"
During the 1970s and 1980s, second-wave feminists saw the study of history as a male-dominated intellectual enterprise and presented "herstory" as a means of compensation. The term, intended to be both serious and comic, became a rallying cry used on T-shirts and buttons as well as in academia.
In feminist literature and academic discourse, the term has been used occasionally as an "economical way" to describe feminist efforts against a male-centered canon.
Criticism.
Christina Hoff Sommers has been a vocal critic of the concept of herstory, and presented her argument against the movement in her 1994 book, "Who Stole Feminism?". Sommers defined herstory as an attempt to infuse education with ideology, at the expense of knowledge. The "gender feminists", as she termed them, were the band of feminists responsible for the movement, which she felt amounted to negationism. She regarded most attempts to make historical studies more female-inclusive as being artificial in nature, and an impediment to progress.
Professor and author Devoney Looser has criticized the concept of herstory for overlooking the contributions that some women made as historians before the twentieth century.
The Global Language Monitor, a nonprofit group that analyzes and tracks trends in language, named "herstory" the third most "politically incorrect" word of 2006—rivaled only by "macaca" and ""Global Warming Denier"."
Books.
Recent books published on the topic include:

</doc>
<doc id="14020" url="http://en.wikipedia.org/wiki?curid=14020" title="History of London">
History of London

London, the capital city of England and the United Kingdom, has a history dating back over 2,000 years. During this time, it has grown to become one of the most significant financial and cultural capitals of the world. It has experienced plague, devastating fire, civil war, aerial bombardment, and terrorist attacks. The City of London is its historic core and today is its primary financial district, though it now represents a tiny part of the wider metropolis of Greater London.
Etymology.
The name of London is derived from Londinium, established in the 1st century as a commercial centre in Roman Britain. The etymology of the name is uncertain. The stems Londin- and Lundin- are the most prevalent in names used from Roman times onward.
Legendary foundations and prehistoric London.
According to the legendary "Historia Regum Britanniae", of Geoffrey of Monmouth, London was founded by Brutus of Troy about 1000–1100 B.C. after he defeated the native giant Gogmagog; the settlement was known as "Caer Troia", "Troia Nova" (Latin for New Troy), which, according to a pseudo-etymology, was corrupted to "Trinovantum". Trinovantes were the Iron Age tribe who inhabited the area prior to the Romans. Geoffrey provides prehistoric London with a rich array of legendary kings, such as King Lud (see also Lludd, from Welsh Mythology) who, he claims, renamed the town "Caer Ludein", from which London was derived, and was buried at Ludgate.
However, despite intensive excavations, archaeologists have found no evidence of a prehistoric major settlement in the area. There have been scattered prehistoric finds, evidence of farming, burial and traces of habitation, but nothing more substantial. It is now considered unlikely that a pre-Roman city existed, but as some of the Roman city remains unexcavated, it is still just possible that some major settlement may yet be discovered. London was most likely a rural area with scattered settlement. Rich finds such as the Battersea Shield, found in the Thames near Chelsea, suggest the area was important; there may have been important settlements at Egham and Brentford, and there was a hillfort at Uphall Camp, Ilford, but no city in the area of the Roman London, the present day City of London.
Some discoveries indicate probable very early settlements near the Thames in the London area. In 2010 the foundations of a large timber structure, dated to 4000BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. In 1999, the remains of a Bronze Age bridge were found, again on the foreshore south of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500BC. In 2001 a further dig found that the timbers were driven vertically into the ground on the south bank of the Thames west of Vauxhall Bridge. All these structures are on the south bank at a natural crossing point where the River Effra flows into the River Thames.
Numerous finds have been made of spear heads and weaponry from the Bronze and Iron ages near the banks of the Thames in the London area, many of which had clearly been used in battle. This suggests that the Thames was an important tribal boundary.
Early history.
Roman London (43-410 AD).
"Londinium" was established as a civilian town by the Romans about seven years after the invasion of AD 43. London, like Rome, was founded on the point of the river where it was narrow enough to bridge and the strategic location of the city provided easy access to much of Europe. Early Roman London occupied a relatively small area, roughly equivalent to the size of Hyde Park. In around AD 60, it was destroyed by the Iceni led by their queen Boudica. The city was quickly rebuilt as a planned Roman town and recovered after perhaps 10 years, the city growing rapidly over the following decades.
During the 2nd century "Londinium" was at its height and replaced Colchester as the capital of Roman Britain (Britannia). Its population was around 60,000 inhabitants. It boasted major public buildings, including the largest basilica north of the Alps, temples, bath houses, an amphitheatre and a large fort for the city garrison. Political instability and recession from the 3rd century onwards led to a slow decline.
At some time between 180 and 225 AD the Romans built the defensive London Wall around the landward side of the city. The wall was about 3 km long, 6 m high, and 2.5 m thick. The wall would survive for another 1,600 years and define the City of London's perimeters for centuries to come. The perimeters of the present City are roughly defined by the line of the ancient wall.
In the late 3rd century, Londinium was raided on several occasions by Saxon pirates. This led, from around 255 onwards, to the construction of an additional riverside wall. Six of the traditional seven city gates of London are of Roman origin, namely: Ludgate, Newgate, Aldersgate, Cripplegate, Bishopsgate and Aldgate (Moorgate is the exception, being of medieval origin).
By the 5th century the Roman Empire was in rapid decline, and in 410 AD the Roman occupation of Britain came to an end. Following this, the Roman city also went into rapid decline and by the end of the 5th century was practically abandoned.
Anglo-Saxon London (5th century – 1066 AD).
Until recently it was believed that Anglo-Saxon settlement initially avoided the area immediately around Londinium. However, the discovery in 2008 of an Anglo-Saxon cemetery at Covent Garden indicates that the incomers had begun to settle there at least as early as the 6th century and possibly in the 5th. The main focus of this settlement was outside the Roman walls, clustering a short distance to the west along what is now the Strand, between the Aldwych and Trafalgar Square. It was known as "Lundenwic", the "-wic" suffix here denoting a trading settlement. Recent excavations have also highlighted the population density and relatively sophisticated urban organisation of this earlier Anglo-Saxon London, which was laid out on a grid pattern and grew to house a likely population of 10-12,000.
Early Anglo-Saxon London belonged to a people known as the Middle Saxons, from whom the name of the county of Middlesex is derived, but who probably also occupied the approximate area of modern Hertfordshire and Surrey. However, by the early 7th century the London area had been incorporated into the kingdom of the East Saxons. In 604 King Saebert of Essex converted to Christianity and London received Mellitus, its first post-Roman bishop.
At this time Essex was under the overlordship of King Æthelberht of Kent, and it was under Æthelberht's patronage that Mellitus founded the first St. Paul's Cathedral, traditionally said to be on the site of an old Roman Temple of Diana (although Christopher Wren found no evidence of this). It would have only been a modest church at first and may well have been destroyed after he was expelled from the city by Saeberht's pagan successors.
The permanent establishment of Christianity in the East Saxon kingdom took place in the reign of King Sigeberht II in the 650s. During the 8th century the kingdom of Mercia extended its dominance over south-eastern England, initially through overlordship which at times developed into outright annexation. London seems to have come under direct Mercian control in the 730s.
Viking attacks dominated most of the 9th century, becoming increasingly common from around 830 onwards. London was sacked in 842 and again in 851. The Danish "Great Heathen Army", which had rampaged across England since 865, wintered in London in 871. The city remained in Danish hands until 886, when it was captured by the forces of King Alfred the Great of Wessex and reincorporated into Mercia, then governed under Alfred's sovereignty by his son-in-law Ealdorman Æthelred.
Around this time the focus of settlement moved within the old Roman walls for the sake of defence, and the city became known as "Lundenburgh". The Roman walls were repaired and the defensive ditch re-cut, while the bridge was probably rebuilt at this time. A second fortified Borough was established on the south bank at Southwark, the "Suthringa Geworc" (defensive work of the men of Surrey). The old settlement of "Lundenwic" became known as the "ealdwic" or "old settlement", a name which survives today as Aldwych.
From this point, the City of London began to develop its own unique local government. Following Æthelred's death in 911 it was transferred to Wessex, preceding the absorption of the rest of Mercia in 918. Although it faced competition for political preeminence in the united Kingdom of England from the traditional West Saxon centre of Winchester, London's size and commercial wealth brought it a steadily increasing importance as a focus of governmental activity. King Aethelstan held many meetings of the "witan" in London and issued laws from there, while King Æthelred the Unready issued the Laws of London there in 978.
Following the resumption of Viking attacks in the reign of Æthelred, London was unsuccessfully attacked in 994 by an army under King Sweyn Forkbeard of Denmark. As English resistance to the sustained and escalating Danish onslaught finally collapsed in 1013, London repulsed an attack by the Danes and was the last place to hold out while the rest of the country submitted to Sweyn, but by the end of the year it too capitulated and Æthelred fled abroad. Sweyn died just five weeks after having been proclaimed king and Æthelred was restored to the throne, but Sweyn's son Cnut returned to the attack in 1015.
After Æthelred's death at London in 1016 his son Edmund Ironside was proclaimed king there by the "witangemot" and left to gather forces in Wessex. London was then subjected to a systematic siege by Cnut but was relieved by King Edmund's army; when Edmund again left to recruit reinforcements in Wessex the Danes resumed the siege but were again unsuccessful. However, following his defeat at the Battle of Assandun Edmund ceded to Cnut all of England north of the Thames, including London, and his death a few weeks later left Cnut in control of the whole country.
A Norse saga tells of a battle when King Æthelred returned to attack Danish-occupied London. According to the saga, the Danes lined London Bridge and showered the attackers with spears. Undaunted, the attackers pulled the roofs off nearby houses and held them over their heads in the boats. Thus protected, they were able to get close enough to the bridge to attach ropes to the piers and pull the bridge down, thus ending the Viking occupation of London. This story presumably relates to Æthelred's return to power after Sweyn's death in 1014, but there is no strong evidence of any such struggle for control of London on that occasion.
Following the extinction of Cnut's dynasty in 1042 English rule was restored under Edward the Confessor. He was responsible for the foundation of Westminster Abbey and spent much of his time at Westminster, which from this time steadily supplanted the City itself as the centre of government. Edward's death at Westminster in 1066 without a clear heir led to a succession dispute and the Norman conquest of England. Earl Harold Godwinson was elected king by the "witangemot" and crowned in Westminster Abbey but was defeated and killed by William the Bastard, Duke of Normandy at the Battle of Hastings. The surviving members of the "witan" met in London and elected King Edward's young nephew Edgar the Ætheling as king.
The Normans advanced to the south bank of the Thames opposite London, where they defeated an English attack and burned Southwark but were unable to storm the bridge. They moved upstream and crossed the river at Wallingford before advancing on London from the north-west. The resolve of the English leadership to resist collapsed and the chief citizens of London went out together with the leading members of the Church and aristocracy to submit to William at Berkhamstead, although according to some accounts there was a subsequent violent clash when the Normans reached the city. Having occupied London, William was crowned king in Westminster Abbey.
Norman and Medieval London (1066 – late 15th century).
The new Norman regime established new fortresses within the city to dominate the native population. By far the most important of these was the Tower of London at the eastern end of the city, where the initial wooden fortification was rapidly replaced by the construction of the first stone castle in England. The smaller forts of Baynard's Castle and Montfichet's Castle were also established along the waterfront. King William also granted a charter in 1067 confirming the city's existing rights, privileges and laws. Its growing self-government was consolidated by the election rights granted by King John in 1199 and 1215.
In 1097 William Rufus, the son of William the Conqueror began the construction of 'Westminster Hall', which became the focus of the Palace of Westminster.
In 1176 construction began of the most famous incarnation of London Bridge (completed in 1209) which was built on the site of several earlier wooden bridges. This bridge would last for 600 years, and remained the only bridge across the River Thames until 1739.
In 1216 during the First Barons' War London was occupied by Prince Louis of France, who had been called in by the baronial rebels against King John and was acclaimed as King of England in St Paul's Cathedral. However, following John's death in 1217 Louis's supporters reverted to their Plantagenet allegiance, rallying round John's son Henry III, and Louis was forced to withdraw from England.
Over the following centuries, London would shake off the heavy French cultural and linguistic influence which had been there since the times of the Norman conquest. The city would figure heavily in the development of Early Modern English.
During the Peasants' Revolt of 1381 London was invaded by rebels led by Wat Tyler. A group of peasants stormed the Tower of London and executed the Lord Chancellor, Archbishop Simon Sudbury, and the Lord Treasurer. The peasants looted the city and set fire to numerous buildings. Tyler was stabbed to death by the Lord Mayor William Walworth in a confrontation at Smithfield and the revolt collapsed.
Trade increased steadily during the Middle Ages, and London grew rapidly as a result. In 1100 London's population was somewhat more than 15,000. By 1300 it had grown to roughly 80,000. London lost at least half of its population during the Black Death in the mid-14th century, but its economic and political importance stimulated a rapid recovery despite further epidemics. Trade in London was organised into various guilds, which effectively controlled the city, and elected the Lord Mayor of the City of London.
Medieval London was made up of narrow and twisting streets, and most of the buildings were made from combustible materials such as wood and straw, which made fire a constant threat, while sanitation in cities was poor.
Modern history.
Tudor London (1485–1603).
During the Reformation, London was the principal early centre of Protestantism in England. Its close commercial connections with the Protestant heartlands in northern continental Europe, large foreign mercantile communities, disproportionately large number of literate inhabitants and role as the centre of the English print trade all contributed to the spread of the new ideas of religious reform. Before the Reformation, more than half of the area of London was the property of monasteries, nunneries and other religious houses.
Henry VIII's "Dissolution of the Monasteries" had a profound effect on the city as nearly all of this property changed hands. The process started in the mid 1530s, and by 1538 most of the larger monastic houses had been abolished. Holy Trinity Aldgate went to Lord Audley, and the Marquess of Winchester built himself a house in part of its precincts. The Charterhouse went to Lord North, Blackfriars to , the leper hospital of St Giles to Lord Dudley, while the king took for himself the leper hospital of St James, which was rebuilt as St James's Palace.
The period saw London rapidly rising in importance amongst Europe's commercial centres. Trade expanded beyond Western Europe to Russia, the Levant, and the Americas. This was the period of mercantilism and monopoly trading companies such as the Muscovy Company (1555) and the British East India Company (1600) were established in London by Royal Charter. The latter, which ultimately came to rule India, was one of the key institutions in London, and in Britain as a whole, for two and a half centuries. Immigrants arrived in London not just from all over England and Wales, but from abroad as well, for example Huguenots from France; the population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. The growth of the population and wealth of London was fuelled by a vast expansion in the use of coastal shipping.
The late 16th and early 17th century saw the great flourishing of drama in London whose preeminent figure was William Shakespeare. During the mostly calm later years of Elizabeth's reign, some of her courtiers and some of the wealthier citizens of London built themselves country residences in Middlesex, Essex and Surrey. This was an early stirring of the villa movement, the taste for residences which were neither of the city nor on an agricultural estate, but at the time of Elizabeth's death in 1603, London was still very compact.
Xenophobia was rampant in London, and increased after the 1580s. Many immigrants became disillusioned by routine threats of violence and molestation, attempts at expulsion of foreigners, and the great difficulty in acquiring English citizenship. Dutch cities proved more hospitable, and many left London permanently.
Stuart London (1603–1714).
A panorama of London by Claes Jansz. Visscher, 1616. Old St Paul's had lost its spire by this time. The two theatres on the foreground (Southwark) side of the Thames are The Bear Garden and The Globe. The large church in the foreground is St Mary Overie, now Southwark Cathedral.
London's expansion beyond the boundaries of the City was decisively established in the 17th century. In the opening years of that century the immediate environs of the City, with the principal exception of the aristocratic residences in the direction of Westminster, were still considered not conducive to health. Immediately to the north was Moorfields, which had recently been drained and laid out in walks, but it was frequented by beggars and travellers, who crossed it in order to get into London. Adjoining Moorfields were Finsbury Fields, a favourite practising ground for the archers, Mile End, then a common on the Great Eastern Road and famous as a rendezvous for the troops.
The preparations for King James I becoming king were interrupted by a severe plague epidemic, which may have killed over thirty thousand people. The Lord Mayor's Show, which had been discontinued for some years, was revived by order of the king in 1609. The dissolved monastery of the Charterhouse, which had been bought and sold by the courtiers several times, was purchased by Thomas Sutton for £13,000. The new hospital, chapel, and schoolhouse were begun in 1611. Charterhouse School was to be one of the principal public schools in London until it moved to Surrey in Victorian times, and the site is still used as a medical school.
The general meeting-place of Londoners in the day-time was the nave of Old St. Paul's Cathedral. Merchants conducted business in the aisles, and used the font as a counter upon which to make their payments; lawyers received clients at their particular pillars; and the unemployed looked for work. St Paul's Churchyard was the centre of the book trade and Fleet Street was a centre of public entertainment. Under James I the theatre, which established itself so firmly in the latter years of Elizabeth, grew further in popularity. The performances at the public theatres were complemented by elaborate masques at the royal court and at the inns of court.
Charles I acceded to the throne in 1625. During his reign, aristocrats began to inhabit the West End in large numbers. In addition to those who had specific business at court, increasing numbers of country landowners and their families lived in London for part of the year simply for the social life. This was the beginning of the "London season". Lincoln's Inn Fields was built about 1629. The piazza of Covent Garden, designed by England's first classically trained architect Inigo Jones followed in about 1632. The neighbouring streets were built shortly afterwards, and the names of Henrietta, Charles, James, King and York Streets were given after members of the royal family.
In January 1642 five members of parliament whom the King wished to arrest were granted refuge in the City. In August of the same year the King raised his banner at Nottingham, and during the English Civil War London took the side of the parliament. Initially the king had the upper hand in military terms and in November he won the Battle of Brentford a few miles to the west of London. The City organised a new makeshift army and Charles hesitated and retreated. Subsequently an extensive system of fortifications was built to protect London from a renewed attack by the Royalists. This comprised a strong earthen rampart, enhanced with bastions and redoubts. It was well beyond the City walls and encompassed the whole urban area, including Westminster and Southwark. London was not seriously threatened by the royalists again, and the financial resources of the City made an important contribution to the parliamentarians victory in the war.
The unsanitary and overcrowded City of London has suffered from the numerous outbreaks of the plague many times over the centuries, but in Britain it is the last major outbreak which is remembered as the "Great Plague" It occurred in 1665 and 1666 and killed around 60,000 people, which was one fifth of the population. Samuel Pepys chronicled the epidemic in his diary. On 4 September 1665 he wrote "I have stayed in the city till above 7400 died in one week, and of them about 6000 of the plague, and little noise heard day or night but tolling of bells."
Great Fire of London (1666).
The Great Plague was immediately followed by another catastrophe, albeit one which helped to put an end to the plague. On the Sunday, 2 September 1666 the Great Fire of London broke out at one o'clock in the morning at a bakery in Pudding Lane in the southern part of the City. Fanned by an eastern wind the fire spread, and efforts to arrest it by pulling down houses to make firebreaks were disorganised to begin with. On Tuesday night the wind fell somewhat, and on Wednesday the fire slackened. On Thursday it was extinguished, but on the evening of that day the flames again burst forth at the Temple. Some houses were at once blown up by gunpowder, and thus the fire was finally mastered. The Monument was built to commemorate the fire: for over a century and a half it bore an inscription attributing the conflagration to a "popish frenzy".
The fire destroyed about 60% of the City, including Old St Paul's Cathedral, 87 parish churches, 44 livery company halls and the Royal Exchange. However, the number of lives lost was surprisingly small; it is believed to have been 16 at most. Within a few days of the fire, three plans were presented to the king for the rebuilding of the city, by Christopher Wren, John Evelyn and Robert Hooke.
Wren proposed to build main thoroughfares north and south, and east and west, to insulate all the churches in conspicuous positions, to form the most public places into large piazzas, to unite the halls of the 12 chief livery companies into one regular square annexed to the Guildhall, and to make a fine quay on the bank of the river from Blackfriars to the Tower of London. Wren wished to build the new streets straight and in three standard widths of thirty, sixty and ninety feet. Evelyn's plan differed from Wren's chiefly in proposing a street from the church of St Dunstan's in the East to the St Paul's, and in having no quay or terrace along the river. These plans were not implemented, and the rebuilt city generally followed the streetplan of the old one, and most of it has survived into the 21st century.
Nonetheless, the new City was different from the old one. Many aristocratic residents never returned, preferring to take new houses in the West End, where fashionable new districts such as St. James's were built close to the main royal residence, which was Whitehall Palace until it was destroyed by fire in the 1690s, and thereafter St. James's Palace. The rural lane of Piccadilly sprouted courtiers mansions such as Burlington House. Thus the separation between the middle class mercantile City of London, and the aristocratic world of the court in Westminster became complete.
In the City itself there was a move from wooden buildings to stone and brick construction to reduce the risk of fire. Parliament's Rebuilding of London Act 1666 stated "building with brick [is] not only more comely and durable, but also more safe against future perils of fire". From then on only doorcases, window-frames and shop fronts were allowed to be made of wood.
Christopher Wren's plan for a new model London came to nothing, but he was appointed to rebuild the ruined parish churches and to replace St Paul's Cathedral. His domed baroque cathedral was the primary symbol of London for at least a century and a half. As city surveyor, Robert Hooke oversaw the reconstruction of the City's houses. The East End, that is the area immediately to the east of the city walls, also became heavily populated in the decades after the Great Fire. London's docks began to extend downstream, attracting many working people who worked on the docks themselves and in the processing and distributive trades. These people lived in Whitechapel, Wapping, Stepney and Limehouse, generally in slum conditions.
In the winter of 1683–4 a frost fair was held on the Thames. The frost, which began about seven weeks before Christmas and continued for six weeks after, was the greatest on record. The Revocation of the Edict of Nantes in 1685 led to a large migration on Huguenots to London. They established a silk industry at Spitalfields.
At this time the Bank of England was founded, and the British East India Company was expanding its influence. Lloyd's of London also began to operate in the late 17th century. In 1700 London handled 80% of England's imports, 69% of its exports and 86% of its re-exports. Many of the goods were luxuries from the Americas and Asia such as silk, sugar, tea and tobacco. The last figure emphasises London's role as an entrepot: while it had many craftsmen in the 17th century, and would later acquire some large factories, its economic prominence was never based primarily on industry. Instead it was a great trading and redistribution centre. Goods were brought to London by England's increasingly dominant merchant navy, not only to satisfy domestic demand, but also for re-export throughout Europe and beyond.
William III, a Dutchman, cared little for London, the smoke of which gave him asthma, and after the first fire at Whitehall Palace (1691) he purchased Nottingham House and transformed it into Kensington Palace. Kensington was then an insignificant village, but the arrival of the court soon caused it to grow in importance. The palace was rarely favoured by future monarchs, but its construction was another step in the expansion of the bounds of London. During the same reign Greenwich Hospital, then well outside the boundary of London, but now comfortably inside it, was begun; it was the naval complement to the Chelsea Hospital for former soldiers, which had been founded in 1681. During the reign of Queen Anne an act was passed authorising the building of 50 new churches to serve the greatly increased population living outside the boundaries of the City of London.
18th century.
The 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire.
In 1707 an Act of Union was passed merging the Scottish and the English Parliaments, thus establishing the Kingdom of Great Britain. A year later, in 1708 Christopher Wren's masterpiece, St Paul's Cathedral was completed on his birthday. However, the first service had been held on 2 December 1697; more than 10 years earlier. This Cathedral replaced the original St. Paul's which had been completely destroyed in the Great Fire of London. This building is considered one of the finest in Britain and a fine example of Baroque architecture.
Many tradesmen from different countries came to London to trade goods and merchandise. Also, more immigrants moved to London making the population greater. More people also moved to London for work and for business making London an altogether bigger and busier city. Britain's victory in the Seven Years' War increased the country's international standing and opened large new markets to British trade, further boosting London's prosperity.
During the Georgian period London spread beyond its traditional limits at an accelerating pace. This is shown in a series of detailed maps, particularly John Rocque's 1741–45 map "(see below)" and his 1746 Map of London. New districts such as Mayfair were built for the rich in the West End, new bridges over the Thames encouraged an acceleration of development in South London and in the East End, the Port of London expanded downstream from the City. During this period was also the uprising of the American colonies. In 1780, the Tower of London held its only American prisoner, former President of the Continental Congress, Henry Laurens. In 1779 he was the Congress's representative of Holland, and got the country's support for the Revolution. On his return voyage back to America, the Royal Navy captured him and charged him with treason after finding evidence of a reason of war between Great Britain and the Netherlands. He was released from the Tower on 21 December 1781 in exchange for General Lord Cornwallis.
In 1762 George III acquired Buckingham Palace (then called Buckingham House) from the Duke of Buckingham. It was enlarged over the next 75 years by architects such as John Nash.
A phenomenon of 18th-century London was the coffeehouse, which became a popular place to debate ideas. Growing literacy and the development of the printing press meant that news became widely available. Fleet Street became the centre of the embryonic British press during the century.
18th-century London was dogged by crime, the Bow Street Runners were established in 1750 as a professional police force. Penalties for crime were harsh, with the death penalty being applied for fairly minor crimes. Public hangings were common in London, and were popular public events.
In 1780 London was rocked by the Gordon Riots, an uprising by Protestants against Roman Catholic emancipation led by Lord George Gordon. Severe damage was caused to Catholic churches and homes, and 285 rioters were killed.
In the year 1787, freed slaves from London, America, and many of Britain's colonies founded Freetown in modern-day Sierra Leone.
Up until 1750, London Bridge was the only crossing over the Thames, but in that year Westminster Bridge was opened and, for the first time in history, London Bridge, in a sense, had a rival. In 1798, Frankfurt banker Nathan Mayer Rothschild arrived in London and set up a banking house in the city, with a large sum of money given to him by his father, Amschel Mayer Rothschild. The Rothschilds also had banks in Paris and Vienna. The bank financed numerous large-scale projects, especially regarding railways around the world and the Suez Canal.
The 18th century saw the breakaway of the American colonies and many other unfortunate events in London, but also great change and Enlightenment. This all led into the beginning of modern times, the 19th century.
19th century.
During the 19th century, London was transformed into the world's largest city and capital of the British Empire. Its population expanded from 1 million in 1800 to 6.7 million a century later. During this period, London became a global political, financial, and trading capital. In this position, it was largely unrivalled until the latter part of the century, when Paris and New York began to threaten its dominance.
While the city grew wealthy as Britain's holdings expanded, 19th-century London was also a city of poverty, where millions lived in overcrowded and unsanitary slums. Life for the poor was immortalised by Charles Dickens in such novels as Oliver Twist In 1810, after the death of Sir Francis Baring and Abraham Goldsmid, Rothschild emerges as the major banker in London.
In 1829 the then Home Secretary (and future prime minister) Robert Peel established the Metropolitan Police as a police force covering the entire urban area. The force gained the nickname of "bobbies" or "peelers" named after Robert Peel.
19th-century London was transformed by the coming of the railways. A new network of metropolitan railways allowed for the development of suburbs in neighboring counties from which middle-class and wealthy people could commute to the centre. While this spurred the massive outward growth of the city, the growth of greater London also exacerbated the class divide, as the wealthier classes emigrated to the suburbs, leaving the poor to inhabit the inner city areas.
The first railway to be built in London was a line from London Bridge to Greenwich, which opened in 1836. This was soon followed by the opening of great rail termini which linked London to every corner of Britain. These included Euston station (1837), Paddington station (1838), Fenchurch Street station (1841), Waterloo station (1848), King's Cross station (1850), and St Pancras station (1863). From 1863, the first lines of the London Underground were constructed.
The urbanised area continued to grow rapidly, spreading into Islington, Paddington, Belgravia, Holborn, Finsbury, Shoreditch, Southwark and Lambeth. Towards the middle of the century, London's antiquated local government system, consisting of ancient parishes and vestries, struggled to cope with the rapid growth in population. In 1855 the Metropolitan Board of Works (MBW) was created to provide London with adequate infrastructure to cope with its growth. One of its first tasks was addressing London's sanitation problems. At the time, raw sewage was pumped straight into the River Thames. This culminated in The Great Stink of 1858. Parliament finally gave consent for the MBW to construct a large system of sewers. The engineer put in charge of building the new system was Joseph Bazalgette. In what was one of the largest civil engineering projects of the 19th century, he oversaw construction of over 2100 km of tunnels and pipes under London to take away sewage and provide clean drinking water. When the London sewerage system was completed, the death toll in London dropped dramatically, and epidemics of cholera and other diseases were curtailed. Bazalgette's system is still in use today.
One of the most famous events of 19th-century London was the Great Exhibition of 1851. Held at The Crystal Palace, the fair attracted 6 million visitors from across the world and displayed Britain at the height of its Imperial dominance.
As the capital of a massive empire, London became a magnet for immigrants from the colonies and poorer parts of Europe. A large Irish population settled in the city during the Victorian period, with many of the newcomers refugees from the Great Famine (1845–1849). At one point, Catholic Irish made up about 20% of London's population; they typically lived in overcrowded slums. London also became home to a sizable Jewish community, which was notable for its entrepreneurship in the clothing trade and merchandising.
In 1888, the new County of London was established, administered by the London County Council. This was the first elected London-wide administrative body, replacing the earlier Metropolitan Board of Works, which had been made up of appointees. The County of London covered broadly what was then the full extent of the London conurbation, although the conurbation later outgrew the boundaries of the county. In 1900, the county was sub-divided into 28 metropolitan boroughs, which formed a more local tier of administration than the county council.
Many famous buildings and landmarks of London were constructed during the 19th century including:
20th century.
1900 to World War II.
London entered the 20th century at the height of its influence as the capital of one of the largest empires in history, but the new century was to bring many challenges.
London's population continued to grow rapidly in the early decades of the century, and public transport was greatly expanded. A large tram network was constructed by the London County Council, through the LCC Tramways; the first motorbus service began in the 1900s. Improvements to London's overground and underground rail network, including large scale electrification were progressively carried out.
During World War I, London experienced its first bombing raids carried out by German zeppelin airships; these killed around 700 people and caused great terror, but were merely a foretaste of what was to come. The city of London would experience many more terrors as a result of both World Wars. The largest explosion in London occurred during World War I: the Silvertown explosion, when a munitions factory containing 50 tons of TNT exploded, killing 73 and injuring 400.
The period between the two World Wars saw London's geographical extent growing more quickly than ever before or since. A preference for lower density suburban housing, typically semi-detached, by Londoners seeking a more "rural" lifestyle, superseded Londoners' old predilection for terraced houses. This was facilitated not only by a continuing expansion of the rail network, including trams and the Underground, but also by slowly widening car ownership. London's suburbs expanded outside the boundaries of the County of London, into the neighbouring counties of Essex, Hertfordshire, Kent, Middlesex and Surrey.
Like the rest of the country, London suffered severe unemployment during the Great Depression of the 1930s. In the East End during the 1930s, politically extreme parties of both right and left flourished. The Communist Party of Great Britain and the British Union of Fascists both gained serious support. Clashes between right and left culminated in the Battle of Cable Street in 1936. The population of London reached an all-time peak of 8.6 million in 1939.
Large numbers of Jewish immigrants fleeing from Nazi Germany, settled in London during the 1930s, mostly in the East End.
In World War II.
During World War II, London, as many other British cities, suffered severe damage, being bombed extensively by the "Luftwaffe" as a part of The Blitz. Prior to the bombing, hundreds of thousands of children in London were evacuated to the countryside to avoid the bombing. Civilians took shelter from the air raids in underground stations.
The heaviest bombing took place during The Blitz between 7 September 1940 and 10 May 1941. During this period, London was subjected to 71 separate raids receiving over 18,000 tonnes of high explosive. One raid in December 1940, which became known as the Second Great Fire of London saw a firestorm engulf much of the City of London and destroy many historic buildings. St Paul's Cathedral however remained unscathed; A photograph showing the Cathedral shrouded in smoke became a famous image of the war.
Having failed to defeat Britain, Hitler turned his attention to the Eastern front and regular bombing raids ceased. They began again, but on a smaller scale with the "Little Blitz" in early 1944. Towards the end of the war, during 1944/45 London again came under heavy attack by pilotless V-1 flying bombs and V-2 rockets, which were fired from Nazi occupied Europe. These attacks only came to an end when their launch sites were captured by advancing Allied forces.
London suffered severe damage and heavy casualties, the worst hit part being the Docklands area. By the war's end, just under 30,000 Londoners had been killed by the bombing, and over 50,000 seriously injured, tens of thousands of buildings were destroyed, and hundreds of thousands of people were made homeless.
1945–2000.
Three years after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when the city had barely recovered from the war. London's rebuilding was slow to begin. However, in 1951 the Festival of Britain was held, which marked an increasing mood of optimism and forward looking.
In the immediate postwar years housing was a major issue in London, due to the large amount of housing which had been destroyed in the war. The authorities decided upon high-rise blocks of flats as the answer to housing shortages. During the 1950s and 1960s the skyline of London altered dramatically as tower blocks were erected, although these later proved unpopular. In a bid to reduce the number of people living in overcrowded housing, a policy was introduced of encouraging people to move into newly built new towns surrounding London.
Through the 19th and in the early half of the 20th century, Londoners used coal for heating their homes, which produced large amounts of smoke. In combination with climatic conditions this often caused a characteristic smog, and London became known for its typical "London Fog", also known as "Pea Soupers". London was sometimes referred to as "The Smoke" because of this. In 1952 this culminated in the disastrous Great Smog of 1952 which lasted for five days and killed over 4,000 people. In response to this, the Clean Air Act 1956 was passed, mandating the creating of "smokeless zones" where the use of "smokeless" fuels was required (this was at a time when most households still used open fires); the Act was effective.
Starting in the mid-1960s, and partly as a result of the success of such UK musicians as the Beatles and the Rolling Stones, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture which made Carnaby Street a household name of youth fashion around the world. London's role as a trendsetter for youth fashion was revived strongly in the 1980s during the new wave and punk eras. In the mid-1990s this was revived to some extent with the emergence of the Britpop era.
From the 1950s onwards London became home to a large number of immigrants, largely from Commonwealth countries such as Jamaica, India, Bangladesh, Pakistan, which dramatically changed the face of London, turning it into one of the most diverse cities in Europe. However, the integration of the new immigrants was not always easy. Racial tensions emerged in events such as the Brixton Riots in the early 1980s.
From the beginning of "The Troubles" in Northern Ireland in the early 1970s until the mid-1990s, London was subjected to repeated terrorist attacks by the Provisional IRA.
The outward expansion of London was slowed by the war, and the introduction of the Metropolitan Green Belt. Due to this outward expansion, in 1965 the old County of London (which by now only covered part of the London conurbation) and the London County Council were abolished, and the much larger area of Greater London was established with a new Greater London Council (GLC) to administer it, along with 32 new London boroughs.
Greater London's population declined steadily in the decades after World War II, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. However, it then began to increase again in the late 1980s, encouraged by strong economic performance and an increasingly positive image.
London's traditional status as a major port declined dramatically in the post-war decades as the old Docklands could not accommodate large modern container ships. The principal ports for London moved downstream to the ports of Felixstowe and Tilbury. The docklands area had become largely derelict by the 1980s, but was redeveloped into flats and offices from the mid-1980s onwards. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea.
In the early 1980s political disputes between the GLC run by Ken Livingstone and the Conservative government of Margaret Thatcher led to the GLC's abolition in 1986, with most of its powers relegated to the London boroughs. This left London as the only large metropolis in the world without a central administration.
In 2000, London-wide government was restored, with the creation of the Greater London Authority (GLA) by Tony Blair's government, covering the same area of Greater London. The new authority had similar powers to the old GLC, but was made up of a directly elected Mayor and a London Assembly. The first election took place on 4 May, with Ken Livingstone comfortably regaining his previous post. London was recognised as one of the nine regions of England. In global perspective, it was emerging as a World city widely compared to New York and Tokyo.
21st century.
Around the start of the 21st century, London hosted the much derided Millennium Dome at Greenwich, to mark the new century. Other Millennium projects were more successful. One was the largest observation wheel in the world, the "Millennium Wheel", or the London Eye, which was erected as a temporary structure, but soon became a fixture, and draws four million visitors a year. The National Lottery also released a flood of funds for major enhancements to existing attractions, for example the roofing of the Great Court at the British Museum.
The London Plan, published by the Mayor of London in 2004, estimated that the population would reach 8.1 million by 2016, and continue to rise thereafter. This was reflected in a move towards denser, more urban styles of building, including a greatly increased number of tall buildings, and proposals for major enhancements to the public transport network. However, funding for projects such as Crossrail remained a struggle.
On 6 July 2005 London won the right to host the 2012 Olympics and Paralympics making it the first city to host the modern games three times. However, celebrations were cut short the following day when the city was rocked by a series of terrorist attacks. More than 50 were killed and 750 injured in three bombings on London Underground trains and a fourth on a double decker bus near King's Cross.
In the public there was ambivalence leading-up to the Olympics, though public sentiment changed strongly in their favour following a successful opening ceremony and when the anticipated organisational and transport problems never occurred.

</doc>
<doc id="14063" url="http://en.wikipedia.org/wiki?curid=14063" title="Hugh Binning">
Hugh Binning

Hugh Binning (1627–1653) was a Scottish philosopher and theologian. Binning was born in Scotland during the reign of Charles I, ordained in the (Presbyterian) Church of Scotland and died during time of Cromwell and the Commonwealth of England.
A precocious child, Binning at age 13 was admitted to the study of philosophy at the University of Glasgow. By the age of 19, he was appointed regent and professor of philosophy at the University of Glasgow. Three years later, he was called to be minister and presided at a church in Govan, adjacent to the city of Glasgow; a post he held until his untimely death of consumption at the age of 26. He was a follower of James Dalrymple. In later life he was well known as an evangelical Christian.
Impact of the Commonwealth.
Hugh Binning was born two years after Charles I ascended to the thrones of England, Ireland, and Scotland. At the time, each was an independent country sharing the same monarch. The Acts of Union 1707 integrated Scotland and England to form the Kingdom of Great Britain; the Acts of Union 1800 integrated Ireland to form United Kingdom of Great Britain and Ireland.
The period was dominated by both political and religious strife between the three independent countries. The religious dispute centered on whether religion was to be dictated by the monarch or was to be the choice of the people; whether people have a direct relationship with God or they needed to use an intermediary. The civil disputes centered on the extent of the king's power, a question of the Divine right of kings; specifically whether the King has right to raise taxes and armed forces without the Consent of the governed. These wars ultimately changed the relationship between king and subjects.
In 1638 the General Assembly of the Church of Scotland voted to remove bishops and the Book of Common Prayer that had been introduced by Charles I to impose the Anglican model on the Presbyterian Church of Scotland. Public riots occurred. The result was the Wars of the Three Kingdoms, an interrelated series of conflicts that took place in the three countries sharing the same monarch. The first of the conflicts was in 1639, the First of the Bishops' Wars, a single border skirmish between England and Scotland; also known as "the war the armys did not wanted to fight."
To maintain his English power base Charles I made secret alliances with Catholic Ireland and Presbyterian Scotland to invade Anglican England, promising that each country could establish their own separate state religion. Once these secret entreaties became known to the English Long Parliament, the Congregationalist faction (of which Oliver Cromwell was a primary spokesman) took matters into their own hands and Parliament established an army separate from the King. Then, Charles I was executed in January 1649, which led to the rule of Cromwell and the establishment of the Commonwealth. The conflicts concluded with the The English Restoration of the monarchy with the return of Charles II, in 1660.
The Act of Classes was passed by the Parliament of Scotland in 23 January 1649; the act banned Royalists (people supporting the monarchy) from holding political or military office. In exile, Charles II signed the Treaty of Breda (1650) with the Scottish Parliament; among other things, the treaty established Presbyterianism as the national religion. Charles was crowned King of Scots at Scone in January 1651. By September 1651 Scotland was annexed by England, its legislative institutions abolished, Presbyterianism dis-established, and Charles was forced into exile in France.
The Scottish Parliament rescinded the Act of Classes in 1651, which produced a split within Scottish Society. The sides of the conflict were called the Resolutioners (who supported the rescission of the act – supported the monarchy and the Scottish House of Stewart) and the Protesters (who supported Cromwell and the Commonwealth); Binning sided with the Resolutioners.
When Cromwell sent troops to Scotland, he was also attempting to dis-establish Presbyterianism and the Church of Scotland, Binning spoke against Cromwell's act. On Saturday 19 April 1651, Cromwell entered Glasgow and the next day he heard a sermon by three ministers who condemned Cromwell for invading Scotland. That evening, Cromwell summoned those ministers and others, to a debate on the issue. At the debate, Rev Hugh Binning is said to have out-debated Cromwell’s ministers so completely that he silenced Cromwell’s ministers.
Politics.
Hugh Binning political views were based on his theology. Binning was a Covenanter, a movement that began in Scotland at Greyfriars Kirkyard in 1638 with the National Covenant and continued with the 1643 Solemn League and Covenant – in effect a treaty between the English Long Parliament and Scotland for the preservation of the reformed religion in exchange for troops to confront the threat of Irish Catholic troops joining the Royalist army. Binning could also be described as a Resolutioners; both political positions were taken because of their religious implications. However, he saw the evils of the politics of his day was not a “fomenter of factions” writing “A Treatise of Christian Love” as a response.
Theology.
Because of the turmoil time in which Hugh Binning lived, politics and religion were inexorably intertwined. Binning was a Calvinist and follower of John Knox. As a profession, Binning was trained as a Philosopher, and he believed that philosophy was the servant of theology. He thought that both Philosophy and Theology should be taught in parallel. Binning’s writing, which are primarily a collection of his sermons, “forms an important bridge between the 17th century, when philosophy in Scotland was heavily dominated by Calvinism, and the 18th century when figures such as Francis Hutcheson re-asserted a greater degree of independence between the two and allied philosophy with the developing human sciences.”
Religiously, Hugh Binning was, what we would call today, an Evangelical Calvinist. He spoke on the primacy of God’s love as the ground of salvation: 
With regards to the extent of the ‘atonement’, Hugh Binning, like many Scottish theologians of his day, was not a ‘hyper-Calvinist,’ since it was not until the Synod of Dort in 1619 that the Reformed tradition come to accept limited atonement, one of the primary tenants of the Five Points of Calvinism. Binning did not hold that the offer of redemption applied only to the few that are elect but said that “the ultimate ground of faith is in the electing will of God.” In Scotland during the 1600s the questions concerning atonement revolved around the terms in which the offer was experssed.
Binning believed that "forgiveness is based on Christ's death, understood as a satisfaction and as a sacrifice: 'If he had pardoned sin without any satisfaction what rich grace it had
been! But truly, to provide the Lamb and sacrifice himself, to find out
the ransom, and to exact it of his own Son, in our name, is a testimony
of mercy and grace far beyond that. But then, his justice is very conspicuous
in this work.'":65
Works.
All of the works of Hugh Binning were published posthumously and were primairly collections of his sermons. Of his speaking style, it was said: "There is originality without any affectation, a rich imagination, without anything fanciful or extravert, the utmost simplicity, without an thing mean or trifling." 
Personal life.
Hugh Binning was the son of John Binning and Margaret M'Kell. Margaret was the daughter of Rev. Matthew M'Kell,
who was a minister in the parish of Bothwell, Scotland, and sister of Hugh M'Kell, a minister in Edinburgh.
 
Hugh Binning was born on the estate of his father in Dalvennan, Straiton, in the shire of Ayr. The family owned other land in the parishes of Straiton and Colmonell as well as Maybole in Carrick. 
In 1645, James Dalrymple, 1st Viscount of Stair, who was Hugh’s master (primary professor) in the study of philosophy, announced he was retiring from the University of Glasgow. After a national search for a replacement on the faculty, three men were selected to compete for the position. Hugh was one of those selected, but was at a disadvantage because of his extreme youth and because he was not of noble birth. However, he had strong support from the existing faculty, who suggested that the candidates speak extemporaneously on any topic of the candidate’s choice. After hearing Hugh speak, the other candidates withdrew, making Hugh a regent and professor of philosophy, while he was still 18 years old.:203
On 7 Feb 1648 (at the age of 21) Hugh was appointed an Advocate before the Court of Sessions (an attorney). In the same year he married Barbara Simpson (sometimes called Mary), daughter of Rev. James Simpson a minister in Ireland. Their son, John, was born in 1650.
Hugh died around September 1653 and was buried in the churchyard of Govan, where Patrick Gillespie, then principal of the University of Glasgow, ordered a monument inscribed in Latin, roughly translated: 
Hugh’s widow, Barbara (sometimes called Mary), then remarried James Gordon, an Anglican priest at Cumber in Ireland. Together they had a daughter, Jean who married Daniel MacKenzie, who was on the winning side of the Battle of Bothwell Bridge serving as an ensign under Lieutenant-Colonel William Ramsay (who became the third Earl of Dalhousie), in the Earl of Mar’s Regiment of Foot.
Hugh’s son, John Binning, married Hanna Keir, who was born in Ireland. The Binning’s were Covenanters, a resistance movement that objected to the return of Charles II (who was received into the Catholic Church on his deathbed). They were on the losing side in the 1679 Battle of Bothwell Bridge. Most of the rebels who were not executed were exiled to the Americas; about 30 Covenanters were exiled to the Carolinas on the Carolina Merchant in 1684. After the battle, John and Hanna were separated. 
In the aftermath of the battle at Bothwell Bridge, Hugh’s widow (now Barbara Gordon) tried to reclaim the family estate at Dalvennan by saying that John and his wife owed his step father a considerable some of money. The legal action was successful and Dalvennan became the possession of John’s half sister Jean, and her husband Daniel MacKenzie. In addition, Jean came into possession Hanna Keir's property in Ireland.
By 1683, Jean was widowed. John Binning was branded a traitor, was sentenced to death and forfeited his property to the Crown. John’s wife (Hanna Keir) was branded as a traitor and forfeited her property in Ireland. In 1685 Jean "donated" the Binning family's home at Dalvennan and other properties, along with the Keir properties to Roderick MacKenzie, who was a Scottish advocate of James II (James VII of Scotland), and the baillie of Carrick. According to an act of the Scottish Parliament, Roderick MacKenzie was also very effective in “suppressing the rebellious, fanatical party in the western and other shires of this realm, and putting the laws to vigorous execution against them”
Since Bothwell Bridge, Hanna had been hiding from the authorities. In 1685 Hanna was in Edinburgh where she was found during a sweep for subversives and imprisoned in the Tolbooth of Edinburgh, a combination city hall and prison. Those arrested with Hanna were exiled to North America, however she developed Dysentery and remained behind. By 1687, near death, Hanna petitioned the Privy Council of Scotland for her release; she was exiled to her family in Ireland, where she died around 1692. 
In 1690 the Scottish Parliament rescinded John's fines and forfeiture, but he was not able to recover his family’s estates, the courts suggesting that John had relinquished his claim to Dalvennan in exchange for forgiveness of debt, rather than forfeiture.
There is little documentation about John after his wife's death. John received a small income from royalties on his father Hugh’s works after parliament extended copyrights on Hugh’s writings to him. However, the income was not significant and John made several petitions to the Scottish parliament for money, the last occurring in 1717. It is thought that John died in Somerset county, in southwestern England. 

</doc>
<doc id="14067" url="http://en.wikipedia.org/wiki?curid=14067" title="Hendrick Avercamp">
Hendrick Avercamp

Hendrick Avercamp (January 27, 1585 (bapt.) - May 15, 1634 (buried)) was a Dutch painter. Avercamp was born in Amsterdam, where he studied with the Danish-born portrait painter Pieter Isaacks (1569–1625), and perhaps also with David Vinckboons. In 1608 he moved from Amsterdam to Kampen in the province of Overijssel. Avercamp was mute and was known as "de Stomme van Kampen" (the mute of Kampen).
As one of the first landscape painters of the 17th-century Dutch school, he specialized in painting the Netherlands in winter. Avercamp's paintings are colorful and lively, with carefully crafted images of the people in the landscape. Many of Avercamp's paintings feature people ice skating on frozen lakes.
Avercamp's work enjoyed great popularity and he sold his drawings, many of which were tinted with water-color, as finished pictures to be pasted into the albums of collectors. The Royal Collection has an outstanding collection of his works.
Avercamp died in Kampen and was interred there in the Sint Nicolaaskerk.
Artwork.
Avercamp probably painted in his studio on the basis of sketches he had made in the winter.
Avercamp is famous even abroad for his winter landscapes. The passion for painting skating characters probably came from his childhood: he was practicing this hobby with his parents. The last quarter of the 16th century, during which Avercamp was born, was one of the coldest periods of the Little Ice Age.
The Flemish painting tradition is mainly expressed in Avercamp's early work. This is consistent with the landscapes of Pieter Bruegel the Elder. Avercamp painted landscapes with a high horizon and many figures who are working on something. The paintings are narrative, with many anecdotes. For instance, naughty details are included in the painting "Winter landscape with skaters": a couple making love, buttocks and a peeing male. 
Later in his life drawing the atmosphere was also important in his work. The horizon also gradually dropped down under more and more air.
Avercamp used the painting technique of aerial perspective. The depth is suggested by change of color in the distance. To the front objects are painted, such as trees or a boat. This technique strengthens the impression of depth in the painting.
Avercamp has also painted cattle and seascapes.
Sometimes Avercamp used paper frames, which were a cheap alternative to oil paintings. He first drew with pen and ink. This work was then covered with finishing paint. The contours of the drawing remained. Even with this technique Avercamp could show the pale wintry colors and nuances of the ice .
Avercamp produced about a hundred paintings. His main artwork can be seen in the Rijksmuseum in Amsterdam, the Mauritshuis in The Hague and abroad. The Rijksmuseum presented from November 20, 2009 to February 15, 2010 an exhibition of his work entitled "Little Ice Age".

</doc>
<doc id="14068" url="http://en.wikipedia.org/wiki?curid=14068" title="Hans Baldung">
Hans Baldung

Hans Baldung Grien or Grün (c. 1484 – September 1545) was a German artist in painting and printmaking who was considered the most gifted student of Albrecht Dürer. Throughout his lifetime, Baldung developed a distinctive style, full of color, expression and imagination. His talents were varied, and he produced a great and extensive variety of work including portraits, woodcuts, altarpieces, drawings, tapestries, allegories and mythological motifs.
Early life.
Hans Baldung was born in Swabia, Germany in the year 1484 to a family of intellectuals, academics and professionals. His father was a lawyer and his uncle was a doctor, and many other of his family members maintained professional degrees. In fact, Baldung was the first male in his family not to attend university but was one of the first German artists to come from an academic family. His earliest training as an artist began around 1500 in the Upper Rhineland by an artist from Strasbourg.
Life as a student of Dürer.
Beginning in 1503, Baldung was an apprentice for the most well renowned German artist of the day: Albrecht Dürer. Here, he was given his nickname “Grien.” This name foremost comes from his preference to the color green, because he usually wore green clothes. He got this nickname also to distinguish him from the two other Hans’ in the apprenticeship, Hans Schäufelein and Hans Suess von Kulmbach. He later included it in his monogram, and it has also been suggested that it came from "grienhals", a German word for witch. Hans quickly picked up Dürer's influence and style, and they became good friends. In his later trip to the Netherlands in 1521 Dürer's diary shows that he took with him and sold prints by Baldung. On Dürer's death Baldung was sent a lock of his hair, which suggests a close friendship. Near the end of his apprenticeship, Grien oversaw the production of stained glass, woodcuts and engravings, and therefore developed an affinity for them.
Strasbourg.
In 1509, when Baldung’s apprenticeship was complete, he moved back to Strasbourg and became a citizen there. He became the celebrity of the town, and was commissioned to make lots of art. The following year he married Margarethe Herlin, joined the guild "zur Steltz", opened a workshop, and began signing his works with the HGB monogram that he used for the rest of his career. His style also became much more mannerist. He loved it there.
Witchcraft and Religious Imagery.
In addition to traditional, religious subjects, Baldung was concerned during these years with the profane theme of the imminence of death and with scenes of sorcery and witchcraft. He was responsible for introducing supernatural and erotic themes into German art. He often depicted witches, also a local interest: Strasbourg's humanists studied witchcraft and its bishop was charged with ferreting out witches. His most characteristic paintings are fairly small in scale; a series of puzzling, often erotic allegories and mythological works. The number of Hans Baldung's religious works diminished with the Protestant Reformation's discouragement of idolatry. But earlier, around the same time that he produced Adam and Eve, the artist became interested in themes related to death, the supernatural, witchcraft and sorcery. That Mankind's mortality became a subject for Baldung was not unusual. Baldung’s fascination with witchcraft lasted well into the end of his career.
Work.
Painting.
Throughout his life, Baldung painted numerous portraits, known for their sharp characterizations. While Dürer rigorously details his models, Baldung's style differs by focusing more on the personality of the represented character, an abstract conception of the model's state of mind. Baldung settled eventually in Strasbourg and then to Freiburg im Breisgau, where he executed what is known as his masterpiece. Here in painted an eleven-panel altarpiece for the Freiburg Cathedral, still intact today, depicting scenes from the life of the Virgin, including, The Annunciation, The Visitation, The Nativity, The Flight into Egypt, The Crucifixion, Four Saints and The Donators. These depictions were a large part of the artist’s greater body of work containing several renowned pieces of the Virgin.
The earliest pictures assigned to him by some are altar-pieces with the monogram H. B. interlaced, and the date of 1496, in the monastery chapel of Lichtenthal near Baden-Baden. Another early work is a portrait of the emperor Maximilian, drawn in 1501 on a leaf of a sketch-book now in the print-room at Karlsruhe. "The Martyrdom of St Sebastian and the Epiphany" (now Berlin, 1507), were painted for the market-church of Halle in Saxony.
Baldung's prints, though Düreresque, are very individual in style, and often in subject. They show little direct Italian influence. His paintings are less important than his prints. He worked mainly in woodcut, although he made six engravings, one very fine. He joined in the fashion for chiaroscuro woodcuts, adding a tone block to a woodcut of 1510. Most of his hundreds of woodcuts were commissioned for books, as was usual at the time; his "single-leaf" woodcuts (i.e. prints not for book illustration) are fewer than 100, though no two catalogues agree as to the exact number.
Unconventional as a draughtsman, his treatment of human form is often exaggerated and eccentric (hence his linkage, in the art historical literature, with European Mannerism), whilst his ornamental style—profuse, eclectic, and akin to the self-consciously "German" strain of contemporary limewood sculptors—is equally distinctive. Though Baldung has been commonly called the Correggio of the north, his compositions are a curious medley of glaring and heterogeneous colours, in which pure black is contrasted with pale yellow, dirty grey, impure red and glowing green. Flesh is a mere glaze under which the features are indicated by lines. (1911)
His works are notable for their individualistic departure from the Renaissance composure of his model, Dürer, for the wild and fantastic strength that some of them display, and for their remarkable themes. In the field of painting, his "Eve, the Serpent and Death" (National Gallery of Canada) shows his strengths well. There is special force in the "Death and the Maiden" panel of 1517 (Basel), in the "Weather Witches" (Frankfurt), in the monumental panels of "Adam" and "Eve" (Madrid), and in his many powerful portraits. Baldung's most sustained effort is the altarpiece of Freiburg, where the Coronation of the Virgin, and the Twelve Apostles, the Annunciation, Visitation, Nativity and Flight into Egypt, and the Crucifixion, with portraits of donors, are executed with some of that fanciful power that Martin Schongauer bequeathed to the Swabian school.
As a portrait painter he is well known. He drew Charles V, as well as Maximilian; and his bust of Margrave Philip in the Munich Gallery tells us that he was connected with the reigning family of Baden as early as 1514. At a later period he had sittings with Margrave Christopher of Baden, Ottilia his wife, and all their children, and the picture containing these portraits is still in the gallery at Karlsruhe. Like Dürer and Cranach, Baldung supported the Protestant Reformation. He was present at the diet of Augsburg in 1518, and one of his woodcuts represents Luther in quasi-saintly guise, under the protection of (or being inspired by) the Holy Spirit, which hovers over him in the shape of a dove.

</doc>
<doc id="14084" url="http://en.wikipedia.org/wiki?curid=14084" title="Heterosexuality">
Heterosexuality

Heterosexuality is romantic attraction, sexual attraction or sexual behavior between persons of opposite sex or gender. As a sexual orientation, heterosexuality is "an enduring pattern of emotional, romantic, and/or sexual attractions" to persons of the opposite sex; it "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions."
Heterosexuality is one of the three main classifications of sexual orientation along with bisexuality and homosexuality, which are each parts of the heterosexual–homosexual continuum.
The term "heterosexual" or "heterosexuality" is usually applied to humans, but heterosexual behavior is observed in all mammals and in other non-human animals. The word "heterosexual" is etymologically formed by adding the combining form of Greek έτερος "heteros" (meaning "different" or "other") as a prefix to "sexuality".
History and demographics.
The demographics of sexual orientation are difficult to establish due to a lack of reliable data. However, the history of human sexuality shows that attitudes and behaviour have varied across societies.
Academic study.
Biological.
Prenatal hormonal theory.
The neurobiology of the masculinization of the brain is fairly well understood. Estradiol and testosterone, which is catalyzed by the enzyme 5α-reductase into dihydrotestosterone, act upon androgen receptors in the brain to masculinize it. If there are few androgen receptors (people with androgen insensitivity syndrome) or too much androgen (females with congenital adrenal hyperplasia), there can be physical and psychological effects. It has been suggested that both male and female heterosexuality are results of variation in this process. In these studies heterosexuality in females is linked to a lower amount of masculinization than is found in lesbian females, though when dealing with male heterosexuality there are results supporting both higher and lower degrees of masculinization than homosexual males.
Heterosexual behaviors in animals.
Most of the reproduction in the animal world is facilitated through heterosexual sex, although there are also animals that reproduce asexually, including protozoa and lower invertebrates.
Reproductive sex does not necessarily require a heterosexual orientation, since orientation refers to a long-term enduring pattern of sexual and emotional attraction leading often to long-term social bonding, while reproductive sex requires only the basic act of intercourse, often done one time only.
Psychological.
Behavioral studies.
At the beginning of the 20th century, early theoretical discussions in the field of psychoanalysis posited original bisexuality in human psychological development. Quantitative studies by Alfred Kinsey in the 1940s and Dr. Fritz Klein's sexual orientation grid in the 1980s find distributions similar to those postulated by their predecessors.
According to "Sexual Behavior in the Human Male" by Alfred Kinsey and several other modern studies, the majority of humans have had both heterosexual and homosexual experiences or sensations and are bisexual. Kinsey himself, along with current sex therapists, focused on the historicity and fluidity of sexual orientation. Kinsey's studies consistently found sexual orientation to be something that evolves in many directions over a person's lifetime; rarely, but not necessarily, including forming attractions to a new sex. Rarely do individuals radically reorient their sexualities rapidly—and still less do they do so volitionally—but often sexualities expand, shift, and absorb new elements over decades. For example, socially normative "age-appropriate" sexuality requires a shifting object of attraction (especially in the passage through adolescence). Contemporary queer theory, incorporating many ideas from social constructionism, tends to look at sexuality as something that has meaning only within a given historical framework. Sexuality, then, is seen as a participation in a larger social discourse and, though in some sense fluid, not as something strictly determinable by the individual.
Other studies have disputed Kinsey's methodology. "His figures were undermined when it was revealed that he had disproportionately interviewed homosexuals and prisoners (many sex offenders)."
Sexologists have attributed discrepancies in some findings to negative societal attitudes towards a particular sexual orientation. For example, people may state different sexual orientations depending on whether their immediate social environment is public or private. Reluctance to disclose one's actual sexual orientation is often referred to as "being in the closet." Individuals capable of enjoyable sexual relations with both sexes or one sex may feel inclined to restrict themselves to heterosexual or homosexual relations in societies that stigmatize same-sex or opposite-sex relations.
Nature and nurture.
The considerable "nature and nurture" debate exists over whether predominantly biological or psychological factors produce sexual orientation in humans. Candidate factors include genes, the exposure of fetuses to certain hormones (or lack thereof) and environmental factors.
The APA currently officially states that "some people believe that sexual orientation is innate and fixed; however, sexual orientation develops across a person’s lifetime", a reversal from the recent past, when non-normative sexuality was considered a deviancy or mental ailment treatable through institutionalization or other means.
Critique of studies.
The studies performed in order to find the origin of sexual orientation have been criticized for being too limited in scope, mostly for focusing only on heterosexuality and homosexuality as two diametrically opposite poles with no orientation in between. It is also asserted that scientific studies focus too much on the search for a biological explanation for sexual orientation, and not enough on the combined effects of both biology and psychology.
In a brief put forth by the Council for Responsible Genetics, it was stated that sexual orientation is not fixed either way, and on the discourse over sexual orientation: "Noticeably missing from this debate is the notion, championed by Kinsey, that human sexual expression is as variable among people as many other complex traits. Yet just like intelligence, sexuality is a complex human feature that modern science is attempting to explain with genetics... Rather than determining that this results from purely biological processes, a trait evolves from developmental processes that include both biological and social elements. According to the American Psychological Association (APA), there are numerous theories about the origins of a person's sexual orientation, but some believe that "sexual orientation is most likely the result of a complex interaction of environmental, cognitive and biological factors," and that genetic factors play a "significant role" in determining a person's sexuality.
Social and historical.
Since the 1960s and 1970s, a large body of scholarship has provided evidence and analysis of the extent to which heterosexuality and homosexuality are socially organized and historically changing. This work challenges the assumption that heterosexuality, homosexuality, and sexualities of all varieties, can be understood as primarily biological and psychological phenomena.
A heterosexual couple, a man and woman in an intimate relationship, form the core of a nuclear family.
Many societies throughout history have insisted that a marriage take place before the couple settle down, but enforcement of this rule or compliance with it has varied considerably. In some jurisdictions, when an unmarried man and woman live together long enough, they are deemed to have established a common-law marriage.
Heterosexism and heteronormativity.
Heterosexism is a form of bias or discrimination in favor of opposite-sex sexuality and relationships. It may include an assumption that everyone is heterosexual and may involve a varied level of discrimination against gays, lesbians, bisexuals, heteroflexibles, or transgender individuals.
Heteronormativity denotes or relates to a world view that promotes heterosexuality as the normal or preferred sexual orientation for people to have. It can assign strict gender roles to males and females. The term was popularized by Michael Warner in 1991. Many gender and sexuality scholars argue that compulsory heterosexuality, a continual and repeating reassertion of hetersoexual norms, is facet of heterosexism. 
A heterosexual ally is a heterosexual person who supports equal civil rights for persons with non-heterosexual orientations. Heterosexual allies may also support LGBT social movements.
Religious aspects.
The Judeo-Christian tradition has several scriptures related to heterosexuality. In Genesis 2:24, there is a commandment stating "Therefore shall a man leave his father and his mother, and shall cleave unto his wife: and they shall be one flesh" () In 1 Corinthians, Christians are advised:
Now for the matters you wrote about: It is good for a man not to marry. But since there is so much immorality, each man should have his own wife, and each woman her own husband. The husband should fulfill his marital duty to his wife, and likewise the wife to her husband. The wife's body does not belong to her alone but also to her husband. In the same way, the husband's body does not belong to him alone but also to his wife. Do not deprive each other except by mutual consent and for a time, so that you may devote yourselves to prayer. Then come together again so that Satan will not tempt you because of your lack of self-control. I say this as a concession, not as a command. (NIV)
For the most part, religious traditions in the world reserve marriage to heterosexual unions, but there are exceptions including certain Buddhist and Hindu traditions, Unitarian Universalist, Metropolitan Community Church and some Anglican dioceses and some Quaker, United Church of Canada and Reform and Conservative Jewish congregations.
Almost all religions believe sex between a man and a woman is allowed, but there are a few that believe that it is a sin, such as The Shakers, The Harmony Society, and The Ephrata Cloister. These religions tend to view all sexual relations as sinful, and promote celibacy. Other religions view heterosexual relationships as being inferior to celibacy. Some religions require celibacy for certain roles, such as Catholic priests; however, the Catholic Church also views heterosexual marriage as sacred and necessary.
Language.
Etymology.
"Hetero-" comes from the Greek word "έτερος" [héteros], meaning "other party" or "another", used in science as a prefix meaning "different"; and the Latin word for sex (that is, characteristic sex or sexual differentiation). The term "heterosexual" was first published in 1892 in C.G. Chaddock's translation of Krafft-Ebing's "Psychopathia Sexualis". The noun came into use from the early 1920s, but did not enter common use until the 1960s. The colloquial shortening "hetero" is attested from 1933. The abstract noun "heterosexuality" is first recorded in 1900. The word "heterosexual" was first listed in Merriam-Webster's "New International Dictionary" as a medical term for "morbid sexual passion for one of the opposite sex"; however, in 1934 in their "Second Edition Unabridged" it is defined as a "manifestation of sexual passion for one of the opposite sex; normal sexuality". (p. 92, Katz)
The adjective "heterosexual" is used for intimate relationships or sexual relations between male and female.
Terminology.
The current use of the term "heterosexual" has its roots in the broader 19th century tradition of personality taxonomy. It continues to influence the development of the modern concept of sexual orientation, and can be used to describe individuals' sexual orientation, sexual history, or self-identification. Some reject the term "heterosexual" as the word only refers to one's sexual behavior and does not refer to non-sexual romantic feelings. The term "heterosexual" is suggested to have come into use as a neologism after, and opposite to the word "homosexual" by Karl Maria Kertbeny in 1868. In LGBT slang, the term "breeder" has been used as a denigrating phrase to deride heterosexuals. Hyponyms of heterosexual include "heteroflexible".
The word can be informally shortened to "hetero". The term ""straight" originated as a mid-20th century gay slang term for heterosexuals, ultimately coming from the phrase "to go straight"" (as in "straight and narrow"), or stop engaging in homosexual sex. One of the first uses of the word in this way was in 1941 by author G. W. Henry. Henry's book concerned conversations with homosexual males and used this term in connection with the reference to "ex-gays". It currently simply is a colloquial term for "heterosexual" having, like many words, changed in primary meaning over time. Some object to usage of the term "straight" because it implies that non-heteros are crooked.
Symbolism.
Heterosexual symbolism dates back to the earliest artifacts of humanity, with ritual fertility carvings and primitive art. This was later expressed in the symbolism of fertility rites and polytheistic worship, which often included images of human reproductive organs. Modern symbols of heterosexuality in societies derived from European traditions still reference symbols used in these ancient beliefs. One such image is a combination of the symbol for Mars, the Roman god of war, as the definitive male symbol of masculinity, and Venus, the Roman goddess of love and beauty, as the definitive female symbol of femininity. The unicode character for this combined symbol is ⚤ (U+26A4).

</doc>
<doc id="14110" url="http://en.wikipedia.org/wiki?curid=14110" title="Holomorphic function">
Holomorphic function

In mathematics, holomorphic functions are the central objects of study in complex analysis. A holomorphic function is a complex-valued function of one or more complex variables that is complex differentiable in a neighborhood of every point in its domain. The existence of a complex derivative in a neighborhood is a very strong condition, for it implies that any holomorphic function is actually infinitely differentiable and equal to its own Taylor series.
The term "analytic function" is often used interchangeably with "holomorphic function", although the word “analytic” is also used in a broader sense to describe any function (real, complex, or of more general type) that can be written as a convergent power series in a neighborhood of each point in its domain. The fact that all holomorphic functions are complex analytical functions, and vice versa, is a major theorem in complex analysis.
Holomorphic functions are also sometimes referred to as "regular functions" or as "conformal maps". A holomorphic function whose domain is the whole complex plane is called an entire function. The phrase "holomorphic at a point "z"0" means not just differentiable at "z"0, but differentiable everywhere within some neighborhood of "z"0 in the complex plane.
Definition.
Given a complex-valued function "f" of a single complex variable, the derivative of "f" at a point "z"0 in its domain is defined by the limit
This is the same as the definition of the derivative for real functions, except that all of the quantities are complex. In particular, the limit is taken as the complex number "z" approaches "z"0, and must have the same value for any sequence of complex values for "z" that approach "z"0 on the complex plane. If the limit exists, we say that "f" is complex-differentiable at the point "z"0. This concept of complex differentiability shares several properties with real differentiability: it is linear and obeys the product rule, quotient rule, and chain rule.
If "f" is "complex differentiable" at "every" point "z"0 in an open set "U", we say that "f" is holomorphic on U. We say that "f" is holomorphic at the point "z"0 if it is holomorphic on some neighborhood of "z"0. We say that "f" is holomorphic on some non-open set "A" if it is holomorphic in an open set containing "A".
The relationship between real differentiability and complex differentiability is the following. If a complex function "f"("x" + i "y") = "u"("x", "y") + i "v"("x", "y") is holomorphic, then "u" and "v" have first partial derivatives with respect to "x" and "y", and satisfy the Cauchy–Riemann equations:
or, equivalently, the Wirtinger derivative of "f" with respect to the complex conjugate of "z" is zero:
which is to say that, roughly, "f" is functionally independent from the complex conjugate of "z".
If continuity is not a given, the converse is not necessarily true. A simple converse is that if "u" and "v" have "continuous" first partial derivatives and satisfy the Cauchy–Riemann equations, then "f" is holomorphic. A more satisfying converse, which is much harder to prove, is the Looman–Menchoff theorem: if "f" is continuous, "u" and "v" have first partial derivatives (but not necessarily continuous), and they satisfy the Cauchy–Riemann equations, then "f" is holomorphic.
Terminology.
The word "holomorphic" was introduced by two of Cauchy's students, Briot (1817–1882) and Bouquet (1819–1895), and derives from the Greek ὅλος ("holos") meaning "entire", and μορφή ("morphē") meaning "form" or "appearance".
Today, the term "holomorphic function" is sometimes preferred to "analytic function", as the latter is a more general concept. This is also because an important result in complex analysis is that every holomorphic function is complex analytic, a fact that does not follow directly from the definitions. The term "analytic" is however also in wide use.
Properties.
Because complex differentiation is linear and obeys the product, quotient, and chain rules; the sums, products and compositions of holomorphic functions are holomorphic, and the quotient of two holomorphic functions is holomorphic wherever the denominator is not zero.
If one identifies C with R2, then the holomorphic functions coincide with those functions of two real variables with continuous first derivatives which solve the Cauchy–Riemann equations, a set of two partial differential equations.
Every holomorphic function can be separated into its real and imaginary parts, and each of these is a solution of Laplace's equation on R2. In other words, if we express a holomorphic function "f"("z") as "u"("x", "y") + "i v"("x", "y") both "u" and "v" are harmonic functions, where v is the harmonic conjugate of u and vice versa.
Cauchy's integral theorem implies that the line integral of every holomorphic function along a loop vanishes:
Here "γ" is a rectifiable path in a simply connected open subset "U" of the complex plane C whose start point is equal to its end point, and "f" : "U" → C is a holomorphic function.
Cauchy's integral formula states that every function holomorphic inside a disk is completely determined by its values on the disk's boundary. Furthermore: Suppose "U" is an open subset of C, "f" : "U" → C is a holomorphic function and the closed disk "D" = {"z" : |"z" − "z"0| ≤ "r"} is completely contained in "U". Let γ be the circle forming the boundary of "D". Then for every "a" in the interior of "D":
where the contour integral is taken counter-clockwise.
The derivative "f"′("a") can be written as a contour integral using Cauchy's differentiation formula:
for any simple loop positively winding once around "a", and
for infinitesimal positive loops γ around "a".
In regions where the first derivative is not zero, holomorphic functions are conformal in the sense that they preserve angles and the shape (but not size) of small figures.
Every holomorphic function is analytic. That is, a holomorphic function "f" has derivatives of every order at each point "a" in its domain, and it coincides with its own Taylor series at "a" in a neighborhood of "a". In fact, "f" coincides with its Taylor series at "a" in any disk centered at that point and lying within the domain of the function.
From an algebraic point of view, the set of holomorphic functions on an open set is a commutative ring and a complex vector space. In fact, it is a locally convex topological vector space, with the seminorms being the suprema on compact subsets.
From a geometric perspective, a function "f" is holomorphic at "z"0 if and only if its exterior derivative "df" in a neighborhood "U" of "z"0 is equal to "f"′("z") "dz" for some continuous function "f"′. It follows from
that "df"′ is also proportional to "dz", implying that the derivative "f"′ is itself holomorphic and thus that "f" is infinitely differentiable. Similarly, the fact that "d"("f dz") = "f"′ "dz" ∧ "dz" = 0 implies that any function "f" that is holomorphic on the simply connected region "U" is also integrable on "U". (For a path γ from "z"0 to "z" lying entirely in "U", define
in light of the Jordan curve theorem and the generalized Stokes' theorem, "F"γ("z") is independent of the particular choice of path γ, and thus "F"("z") is a well-defined function on "U" having "F"("z"0) = "F"0 and "dF" = "f dz".)
Examples.
All polynomial functions in "z" with complex coefficients are holomorphic on C, and so are sine, cosine and the exponential function. (The trigonometric functions are in fact closely related to and can be defined via the exponential function using Euler's formula). The principal branch of the complex logarithm function is holomorphic on the set C ∖ {"z" ∈ R : z ≤ 0}. The square root function can be defined as
and is therefore holomorphic wherever the logarithm log("z") is. The function 1/"z" is holomorphic on {"z" : "z" ≠ 0}.
As a consequence of the Cauchy–Riemann equations, a real-valued holomorphic function must be constant. Therefore, the absolute value of "z", the argument of "z", the real part of "z" and the imaginary part of "z" are not holomorphic. Another typical example of a continuous function which is not holomorphic is the complex conjugate "z" formed by complex conjugation.
Several variables.
The definition of a holomorphic function generalizes to several complex variables in a straightforward way. Let "D" denote an open subset of C"n", and let "f" : "D" → C. The function "f" is analytic at a point "p" in "D" if there exists an open neighborhood of "p" in which "f" is equal to a convergent power series in "n" complex variables. Define "f" to be holomorphic if it is analytic at each point in its domain. Osgood's lemma shows (using the multivariate Cauchy integral formula) that, for a continuous function "f", this is equivalent to "f" being holomorphic in each variable separately (meaning that if any "n" − 1 coordinates are fixed, then the restriction of "f" is a holomorphic function of the remaining coordinate). The much deeper Hartogs' theorem proves that the continuity hypothesis is unnecessary: "f" is holomorphic if and only if it is holomorphic in each variable separately.
More generally, a function of several complex variables that is square integrable over every compact subset of its domain is analytic if and only if it satisfies the Cauchy–Riemann equations in the sense of distributions.
Functions of several complex variables are in some basic ways more complicated than functions of a single complex variable. For example, the region of convergence of a power series is not necessarily an open ball; these regions are Reinhardt domains, the simplest example of which is a polydisk. However, they also come with some fundamental restrictions. Unlike functions of a single complex variable, the possible domains on which there are holomorphic functions that cannot be extended to larger domains are highly limited. Such a set is called a domain of holomorphy.
Extension to functional analysis.
The concept of a holomorphic function can be extended to the infinite-dimensional spaces of functional analysis. For instance, the Fréchet or Gâteaux derivative can be used to define a notion of a holomorphic function on a Banach space over the field of complex numbers.

</doc>
<doc id="14115" url="http://en.wikipedia.org/wiki?curid=14115" title="History of Russia">
History of Russia

The history of Russia begins with that of the Eastern Slavs and the Finno-Ugric peoples. The state of Garðaríki ("the realm of towns"), which was centered in Novgorod and included the entire areas inhabited by Ilmen Slavs, Veps, and Votes, was established by the Varangian chieftain Rurik in 862 (the traditional beginning of Russian history). Kievan Rus', the first united East Slavic state, was founded by Rurik's successor Oleg of Novgorod in 882. The state adopted Christianity from the Byzantine Empire in 988, beginning the synthesis of Byzantine and Slavic cultures that defined Russian culture for the next millennium. Kievan Rus' ultimately disintegrated as a state because of the Mongol invasion of Rus' in 1237–1240 and the death of about half the population of Rus'. During that time, a number of regional magnates, in particular Novgorod and Pskov, fought to inherit the cultural and political legacy of Kievan Rus'.
After the 13th century, Moscow became a cultural center. By the 18th century, the Tsardom of Russia had become the huge Russian Empire, stretching from the Polish–Lithuanian Commonwealth eastward to the Pacific Ocean. Expansion in the western direction sharpened Russia's awareness of its separation from much of the rest of Europe and shattered the isolation in which the initial stages of expansion had occurred. Successive regimes of the 19th century responded to such pressures with a combination of halfhearted reform and repression. Russian serfdom was abolished in 1861, but its abolition was achieved on terms unfavorable to the peasants and served to increase revolutionary pressures. Between the abolition of serfdom and the beginning of World War I in 1914, the Stolypin reforms, the constitution of 1906, and State Duma attempted to open and liberalize the economy and politics of Russia but the tsars were still not willing to relinquish autocratic rule or share their power.
The Russian Revolution in 1917 was triggered by a combination of economic breakdown, war-weariness, and discontent with the autocratic system of government, and it first brought a coalition of liberals and moderate socialists to power, but their failed policies led to seizure of power by the Communist Bolsheviks on 25 October. Between 1922 and 1991, the history of Russia is essentially the history of the Soviet Union, effectively an ideologically based state which was roughly conterminous with the Russian Empire before the Treaty of Brest-Litovsk. The approach to the building of socialism, however, varied over different periods in Soviet history, from the mixed economy and diverse society and culture of the 1920s to the command economy and repressions of the Joseph Stalin era to the "era of stagnation" in the 1980s. From its first years, government in the Soviet Union was based on the one-party rule of the Communists, as the Bolsheviks called themselves, beginning in March 1918. However, by the mid 1980s, with the weaknesses of its economic and political structures becoming acute, Mikhail Gorbachev embarked on major reforms, which led to the fall of the Soviet Union. 
The history of the Russian Federation officially starts in January 1992. The Russian Federation was the legal successor to the Soviet Union on the international stage. However, Russia has lost its superpower status after facing serious challenges in its efforts to forge a new post-Soviet political and economic system. Scrapping the socialist central planning and state ownership of property of the Soviet era, Russia attempted to build an economy based on market capitalism, often with painful results. Since the new millennium, Vladimir Putin has been its dominant leader. Even today Russia shares many continuities of political culture and social structure with its tsarist and Soviet past.
Prehistory.
In 2006, 1.5-million-year-old Oldowan flint tools were discovered in the Dagestan Akusha region of the north Caucasus, demonstrating the presence of early humans in Russia from a very early time. 
The discovery of some of the earliest evidence for the presence of anatomically modern humans found anywhere in Europe was reported in 2007 from the deepest levels of the Kostenki archaeological site near the Don River in Russia, which has been dated to at least 40,000 years ago. Arctic Russia was reached by 40,000 years ago.
That Russia was also home to some of the last surviving Neanderthals was revealed by the discovery of the partial skeleton of a Neanderthal infant in Mezmaiskaya cave in Adygea, which was carbon dated to only 29,000 years ago. In 2008, Russian archaeologists from the Institute of Archaeology and Ethnology of Novosibirsk, working at the site of Denisova Cave in the Altai Mountains of Siberia, uncovered a 40,000 year old small bone fragment from the fifth finger of a juvenile hominin, which DNA analysis revealed to be a previously unknown species of human, which was named the Denisova hominin.
During the prehistoric eras the vast steppes of Southern Russia were home to tribes of nomadic pastoralists. In classical antiquity, the Pontic Steppe was known as Scythia. Remnants of these long gone steppe cultures were discovered in the course of the 20th century in such places as Ipatovo, Sintashta, Arkaim, and Pazyryk.
Early history.
Antiquity.
In the latter part of the 8th century BC, Greek merchants brought classical civilization to the trade emporiums in Tanais and Phanagoria. Gelonus was described by Herodotos as a huge (Europe's biggest) earth- and wood-fortified grad inhabited around 500 BC by Heloni and Budini. At about the 2nd century AD Goths migrated to the Black Sea, and in the 3rd and 4th centuries AD, a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by successive waves of nomadic invasions, led by warlike tribes which would often move on to Europe, as was the case with the Huns and Turkish Avars.
A Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas through to the 8th century. Noted for their laws, tolerance, and cosmopolitanism, the Khazars were the main commercial link between the Baltic and the Muslim Abbasid empire centered in Baghdad. They were important allies of the Byzantine Empire, and waged a series of successful wars against the Arab Caliphates. In the 8th century, the Khazars embraced Judaism.
Early East Slavs.
Some of the ancestors of the modern Russians were the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pripet Marshes. The Early East Slavs gradually settled Western Russia in two waves: one moving from Kiev towards present-day Suzdal and Murom and another from Polotsk towards Novgorod and Rostov.
From the 7th century onwards, East Slavs constituted the bulk of the population in Western Russia and slowly but peacefully assimilated the native Finno-Ugric tribes, such as the Merya, the Muromians, and the Meshchera.
Kievan Rus' (882–1283).
Scandinavian Norsemen, called "Vikings" in Western Europe and "Varangians" in the East, combined piracy and trade in their roamings over much of Northern Europe. In the mid-9th century, they began to venture along the waterways from the eastern Baltic to the Black and Caspian Seas. According to the earliest Russian chronicle, a Varangian named Rurik was elected ruler ("knyaz") of Novgorod in about 860, before his successors moved south and extended their authority to Kiev, which had been previously dominated by the Khazars. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.
Thus, the first East Slavic state, Rus", emerged in the 9th century along the Dnieper River valley. A coordinated group of princely states with a common interest in maintaining trade along the river routes, Kievan Rus' controlled the trade route for furs, wax, and slaves between Scandinavia and the Byzantine Empire along the Volkhov and Dnieper Rivers. 
By the end of the 10th century, the Norse minority had merged with the Slavic population, which also absorbed Greek Christian influences in the course of the multiple campaigns to loot Tsargrad, or Constantinople. One such campaign claimed the life of the foremost Slavic druzhina leader, Svyatoslav I, who was renowned for having crushed the power of the Khazars on the Volga. At the time, the Byzantine Empire was experiencing a major military and cultural revival; despite its later decline, its culture would have a continuous influence on the development of Russia in its formative centuries.
Kievan Rus' is important for its introduction of a Slavic variant of the Eastern Orthodox religion, dramatically deepening a synthesis of Byzantine and Slavic cultures that defined Russian culture for the next thousand years. The region adopted Christianity in 988 by the official act of public baptism of Kiev inhabitants by Prince Vladimir I. Some years later the first code of laws, Russkaya Pravda, was introduced. From the onset the Kievan princes followed the Byzantine example and kept the Church dependent on them, even for its revenues, so that the Russian Church and state were always closely linked.
By the 11th century, particularly during the reign of Yaroslav the Wise, Kievan Rus' displayed an economy and achievements in architecture and literature superior to those that then existed in the western part of the continent. Compared with the languages of European Christendom, the Russian language was little influenced by the Greek and Latin of early Christian writings. This was because Church Slavonic was used directly in liturgy instead.
A nomadic Turkic people, the Kipchaks (also known as the Cumans), replaced the earlier Pechenegs as the dominant force in the south steppe regions neighbouring to Rus' at the end of the 11th century and founded a nomadic state in the steppes along the Black Sea (Desht-e-Kipchak). Repelling their regular attacks, especially on Kiev, which was just one day's ride from the steppe, was a heavy burden for the southern areas of Rus'. The nomadic incursions caused a massive influx of Slavs to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.
Kievan Rus' ultimately disintegrated as a state because of in-fighting between members of the princely family that ruled it collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod in the north, and Halych-Volhynia in the south-west. Conquest by the Mongol Golden Horde in the 13th century was the final blow. Kiev was destroyed. Halych-Volhynia would eventually be absorbed into the Polish–Lithuanian Commonwealth, while the Mongol-dominated Vladimir-Suzdal and independent Novgorod Republic, two regions on the periphery of Kiev, would establish the basis for the modern Russian nation.
Mongol invasion (1223–1240).
The invading Mongols accelerated the fragmentation of the Rus'. In 1223, the disunited southern princes faced a Mongol raiding party at the Kalka River and were soundly defeated. In 1237–1238 the Mongols burnt down the city of Vladimir (4 February 1238) and other major cities of northeast Russia, routed the Russians at the Sit' River, and then moved west into Poland and Hungary. By then they had conquered most of the Russian principalities. Only the Novgorod Republic escaped occupation and continued to flourish in the orbit of the Hanseatic League.
The impact of the Mongol invasion on the territories of Kievan Rus' was uneven. The advanced city culture was almost completely destroyed. As older centers such as Kiev and Vladimir never recovered from the devastation of the initial attack, the new cities of Moscow, Tver and Nizhny Novgorod began to compete for hegemony in the Mongol-dominated Russia. Although a Russian army defeated the Golden Horde at Kulikovo in 1380, Mongol domination of the Russian-inhabited territories, along with demands of tribute from Russian princes, continued until about 1480.
Russo-Tatar relations.
After the fall of the Khazars in the 10th century, the middle Volga came to be dominated by the mercantile state of Volga Bulgaria, the last vestige of Greater Bulgaria centered at Phanagoria. In the 10th century the Turkic population of Volga Bulgaria converted to Islam, which facilitated its trade with the Middle East and Central Asia. In the wake of the Mongol invasions of the 1230s, Volga Bulgaria was absorbed by the Golden Horde and its population evolved into the modern Chuvashes and Kazan Tatars.
The Mongols held Russia and Volga Bulgaria in sway from their western capital at Sarai, one of the largest cities of the medieval world. The princes of southern and eastern Russia had to pay tribute to the Mongols of the Golden Horde, commonly called Tatars; but in return they received charters authorizing them to act as deputies to the khans. In general, the princes were allowed considerable freedom to rule as they wished, while the Russian Orthodox Church even experienced a spiritual revival under the guidance of Metropolitan Alexis and Sergius of Radonezh.
To the Orthodox Church and most princes, the fanatical Northern Crusaders seemed a greater threat to the Russian way of life than the Mongols. In the mid-13th century, Alexander Nevsky, elected prince of Novgorod, acquired heroic status as the result of major victories over the Teutonic Knights and the Swedes. Alexander obtained Mongol protection and assistance in fighting invaders from the west who, hoping to profit from the Russian collapse since the Mongol invasions, tried to grab territory and convert the Russians to Roman Catholicism. 
The Mongols left their impact on the Russians in such areas as military tactics and transportation. Under Mongol occupation, Russia also developed its postal road network, census, fiscal system, and military organization. Eastern influence remained strong well until the 17th century, when Russian rulers made a conscious effort to modernize their country.
Grand Duchy of Moscow (1283–1547).
Rise of Moscow.
Daniil Aleksandrovich, the youngest son of Alexander Nevsky, founded the principality of Moscow (known as Muscovy in English), which eventually expelled the Tatars from Russia. Well-situated in the central river system of Russia and surrounded by protective forests and marshes, Moscow was at first only a vassal of Vladimir, but soon it absorbed its parent state.
A major factor in the ascendancy of Moscow was the cooperation of its rulers with the Mongol overlords, who granted them the title of Grand Prince of Moscow and made them agents for collecting the Tatar tribute from the Russian principalities. The principality's prestige was further enhanced when it became the center of the Russian Orthodox Church. Its head, the Metropolitan, fled from Kiev to Vladimir in 1299 and a few years later established the permanent headquarters of the Church in Moscow under the original title of Kiev Metropolitan.
By the middle of the 14th century, the power of the Mongols was declining, and the Grand Princes felt able to openly oppose the Mongol yoke. In 1380, at Kulikovo on the Don River, the Mongols were defeated, and although this hard-fought victory did not end Tatar rule of Russia, it did bring great fame to the Grand Prince Dmitry Donskoy. Moscow's leadership in Russia was now firmly based and by the middle of the 14th century its territory had greatly expanded through purchase, war, and marriage.
Ivan III, the Great.
In the 15th century, the grand princes of Moscow went on gathering Russian lands to increase the population and wealth under their rule. The most successful practitioner of this process was Ivan III who laid the foundations for a Russian national state. Ivan competed with his powerful northwestern rival, the Grand Duchy of Lithuania, for control over some of the semi-independent Upper Principalities in the upper Dnieper and Oka River basins.
Through the defections of some princes, border skirmishes, and a long war with the Novgorod Republic, Ivan III was able to annex Novgorod and Tver. As a result, the Grand Duchy of Moscow tripled in size under his rule. During his conflict with Pskov, a monk named Filofei (Philotheus of Pskov) composed a letter to Ivan III, with the prophecy that the latter's kingdom would be the Third Rome. The Fall of Constantinople and the death of the last Greek Orthodox Christian emperor contributed to this new idea of Moscow as 'New Rome' and the seat of Orthodox Christianity.
A contemporary of the Tudors and other "new monarchs" in Western Europe, Ivan proclaimed his absolute sovereignty over all Russian princes and nobles. Refusing further tribute to the Tatars, Ivan initiated a series of attacks that opened the way for the complete defeat of the declining Golden Horde, now divided into several Khanates and hordes. Ivan and his successors sought to protect the southern boundaries of their domain against attacks of the Crimean Tatars and other hordes. To achieve this aim, they sponsored the construction of the Great Abatis Belt and granted manors to nobles, who were obliged to serve in the military. The manor system provided a basis for an emerging cavalry based army.
In this way, internal consolidation accompanied outward expansion of the state. By the 16th century, the rulers of Moscow considered the entire Russian territory their collective property. Various semi-independent princes still claimed specific territories, but Ivan III forced the lesser princes to acknowledge the grand prince of Moscow and his descendants as unquestioned rulers with control over military, judicial, and foreign affairs. Gradually, the Russian ruler emerged as a powerful, autocratic ruler, a tsar. The first Russian ruler to officially crown himself "Tsar" was Ivan IV.
Ivan III tripled the territory of his state, ended the dominance of the Golden Horde over the Rus, renovated the Moscow Kremlin, and laid the foundations of the Russian state. Biographer Fennell concludes that his reign was "militarily glorious and economically sound," and especially points to his territorial annexations and his centralized control over local rulers. However Fennell, the leading British specialist on Ivan III, argues that his reign was also "a period of cultural depression and spiritual barrenness. Freedom was stamped out within the Russian lands. By his bigoted anti-Catholicism Ivan brought down the curtain between Russia and the west. For the sake of territorial aggrandizement he deprived his country of the fruits of Western learning and civilization."
Tsardom of Russia (1547–1721).
Ivan IV, the Terrible.
The development of the Tsar's autocratic powers reached a peak during the reign (1547–1584) of Ivan IV ("Ivan the Terrible"). He strengthened the position of the monarch to an unprecedented degree, as he ruthlessly subordinated the nobles to his will, exiling or executing many on the slightest provocation. Nevertheless, Ivan is often seen as a farsighted statesman who reformed Russia as he promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor), curbed the influence of clergy, and introduced the local self-management in rural regions.
Although his long Livonian War for the control of the Baltic coast and the access to sea trade ultimately proved a costly failure, Ivan managed to annex the Khanates of Kazan, Astrakhan, and Siberia. These conquests complicated the migration of the aggressive nomadic hordes from Asia to Europe through Volga and Ural.
Through these conquests, Russia acquired a significant Muslim Tatar population and emerged as a multiethnic and multiconfessional state. Also around this period, the mercantile Stroganov family established a firm foothold at the Urals and recruited Russian Cossacks to colonise Siberia.
In the later part of his reign, Ivan divided his realm in two. In the zone known as the "oprichnina", Ivan's followers carried out a series of bloody purges of the feudal aristocracy (which he suspected of treachery after the betrayal of prince Kurbsky), culminating in the Massacre of Novgorod (1570). This combined with the military losses, epidemics, poor harvests so weakened Russia that the Crimean Tatars were able to sack central Russian regions and burn down Moscow (1571). In 1572 Ivan abandoned the "oprichnina".
At the end of Ivan IV's reign the Polish–Lithuanian and Swedish armies carried out a powerful intervention in Russia, devastating its northern and northwest regions.
Time of Troubles.
The death of Ivan's childless son Feodor was followed by a period of civil wars and foreign intervention known as the "Time of Troubles" (1606–13). Extremely cold summers (1601–1603) wrecked crops, which led to the Russian famine of 1601–1603 and increased the social disorganization. Boris Godunov's (Борис Годунов) reign ended in chaos, civil war combined with foreign intrusion, devastation of many cities and depopulation of the rural regions. The country rocked by internal chaos also attracted several waves of interventions by the Polish–Lithuanian Commonwealth.
During the Polish–Muscovite War (1605–1618), Polish–Lithuanian forces reached Moscow and installed the impostor False Dmitriy I in 1605, then supported False Dmitry II in 1607. The decisive moment came when a combined Russian-Swedish army was routed by the Polish forces under hetman Stanisław Żółkiewski at the Battle of Klushino on 4 July [O.S. 24 June] 1610. As the result of the battle, the Seven Boyars, a group of Russian nobles, deposed the tsar Vasily Shuysky on 27 July [O.S. 17 July] 1610, and recognized the Polish prince Władysław IV Vasa as the Tsar of Russia on 6 September [O.S. 27 August] 1610. The Poles entered Moscow on 21 September [O.S. 11 September] 1610. Moscow revolted but riots there were brutally suppressed and the city was set on fire.
The crisis provoked a patriotic national uprising against the invasion, both in 1611 and 1612. Finally, a volunteer army, led by the merchant Kuzma Minin and prince Dmitry Pozharsky, expelled the foreign forces from the capital on 4 November [O.S. 22 October] 1612.
The Russian statehood survived the "Time of Troubles" and the rule of weak or corrupt Tsars because of the strength of the government's central bureaucracy. Government functionaries continued to serve, regardless of the ruler's legitimacy or the faction controlling the throne. However, the "Time of Troubles" provoked by the dynastic crisis resulted in the loss of much territory to the Polish–Lithuanian Commonwealth in the Russo-Polish war, as well as to the Swedish Empire in the Ingrian War.
Accession of the Romanovs and early rule.
In February 1613, with the chaos ended and the Poles expelled from Moscow, a national assembly, composed of representatives from fifty cities and even some peasants, elected Michael Romanov, the young son of Patriarch Filaret, to the throne. The Romanov dynasty ruled Russia until 1917.
The immediate task of the new dynasty was to restore peace. Fortunately for Moscow, its major enemies, the Polish–Lithuanian Commonwealth and Sweden, were engaged in a bitter conflict with each other, which provided Russia the opportunity to make peace with Sweden in 1617 and to sign a truce with the Polish–Lithuanian Commonwealth in 1619. Recovery of lost territories started in the mid-17th century, when the Khmelnitsky Uprising in Ukraine against Polish rule brought about the Treaty of Pereyaslav concluded between Russia and the Ukrainian Cossacks.
According to the treaty, Russia granted protection to the Cossacks state in the Left-bank Ukraine, formerly under Polish control. This triggered a prolonged Russo-Polish War which ended with the Treaty of Andrusovo (1667), where Poland accepted the loss of Left-bank Ukraine, Kiev and Smolensk.
Rather than risk their estates in more civil war, the great nobles or "boyars" cooperated with the first Romanovs, enabling them to finish the work of bureaucratic centralization. Thus, the state required service from both the old and the new nobility, primarily in the military. In return the tsars allowed the "boyars" to complete the process of enserfing the peasants.
In the preceding century, the state had gradually curtailed peasants' rights to move from one landlord to another. With the state now fully sanctioning serfdom, runaway peasants became state fugitives, and the power of the landlords over the peasants "attached" to their land had become almost complete. Together the state and the nobles placed the overwhelming burden of taxation on the peasants, whose rate was 100 times greater in the mid-17th century than it had been a century earlier. In addition, middle-class urban tradesmen and craftsmen were assessed taxes, and, like the serfs, they were forbidden to change residence. All segments of the population were subject to military levy and to special taxes.
Under such circumstances, peasant disorders were endemic; even the citizens of Moscow revolted against the Romanovs during the Salt Riot (1648), Copper Riot (1662), and the Moscow Uprising (1682). By far the greatest peasant uprising in 17th-century Europe erupted in 1667. As the free settlers of South Russia, the Cossacks, reacted against the growing centralization of the state, serfs escaped from their landlords and joined the rebels. The Cossack leader Stenka Razin led his followers up the Volga River, inciting peasant uprisings and replacing local governments with Cossack rule. The tsar's army finally crushed his forces in 1670; a year later Stenka was captured and beheaded. Yet, less than half a century later, the strains of military expeditions produced another revolt in Astrakhan, ultimately subdued.
Imperial Russia (1721–1917).
Peter the Great.
Peter the Great (1672–1725) brought autocracy into Russia and played a major role in bringing his country into the European state system. Russia had now become the largest country in the world, stretching from the Baltic Sea to the Pacific Ocean. The vast majority of the land was unoccupied, and travel was slow. Much of its expansion had taken place in the 17th century, culminating in the first Russian settlement of the Pacific in the mid-17th century, the reconquest of Kiev, and the pacification of the Siberian tribes. However, this vast land had a population of only 14 million. With a short growing season grain yields trailed behind those in the West; potatoes were not yet widespread. The great majority were tied to agriculture. Russia remained isolated from the sea trade, its internal trade communications and many manufactures were dependent on the seasonal changes.
Peter's first military efforts were directed against the Ottoman Turks. His aim was to establish a Russian foothold on the Black Sea by taking the town of Azov. His attention then turned to the north. Peter still lacked a secure northern seaport except at Archangel on the White Sea, whose harbor was frozen nine months a year. Access to the Baltic was blocked by Sweden, whose territory enclosed it on three sides. Peter's ambitions for a "window to the sea" led him in 1699 to make a secret alliance with the Polish–Lithuanian Commonwealth and Denmark against Sweden resulting in the Great Northern War.
The war ended in 1721 when an exhausted Sweden sued for peace with Russia. Peter acquired four provinces situated south and east of the Gulf of Finland, thus securing his coveted access to the sea. There, in 1703, he had already founded the city that was to become Russia's new capital, Saint Petersburg, as a "window opened upon Europe" to replace Moscow, long Russia's cultural center. Russian intervention in the Commonwealth marked, with the Silent Sejm, the beginning of a 200-year domination of that region by the Russian Empire.
In celebration of his conquests, Peter assumed the title of emperor as well as tsar, and Russian Tsardom officially became the Russian Empire in 1721.
Peter reorganized his government on the latest Western models, molding Russia into an absolutist state. He replaced the old "boyar" Duma (council of nobles) with a nine-member senate, in effect a supreme council of state. The countryside was also divided into new provinces and districts. Peter told the senate that its mission was to collect tax revenues. In turn tax revenues tripled over the course of his reign.
Administrative Collegia were established in St. Petersburg, to replace the old governmental departments. In 1722 Peter promulgated his famous Table of ranks. As part of the government reform, the Orthodox Church was partially incorporated into the country's administrative structure, in effect making it a tool of the state. Peter abolished the patriarchate and replaced it with a collective body, the Holy Synod, led by a lay government official. Peter continued and intensified his predecessors' requirement of state service for all nobles.
By this same time, the once powerful Persian Safavid Empire to its south was heavily declining. Making advantage of the profitable situation, Peter launched the Russo-Persian War (1722-1723) otherwise known as "The Persian Expedition of Peter the Great" by the Russian histographers, in order to be the first Russian emperor to increase Russian influence in the Caucasus and Caspian Sea. After considerable successes and the capture of many provinces and cities in the Caucasus and northern Persia, the Safavids were forced to hand over the territories to Russia. However, 9 years later all territories would be ceded back to Persia, now led by the charismatic and military genius Nader Shah, as part of the Treaty of Resht and the Russo-Persian alliance against the Ottoman Empire.
Russia, by the end of Peter's reign, had become a great power. Peter the Great died in 1725, leaving an unsettled succession.
Ruling the Empire (1725–1825).
Catherine the Great.
Peter I was succeeded by his second wife (Catherine I, 1725–1727) who was merely a figurehead for a powerful group of high officials, then by his minor grandson (Peter II, 1727–1730), then by his niece, Anna, daughter of Tsar Ivan V. In 1741 Elizabeth, daughter of Peter, seized the throne, assisted by the Preobrazhensky Regiment. She reigned for twenty years, a period marked by the establishment of Moscow University and the abolition of capital punishment, except in cases of high treason.
Nearly forty years were to pass before a comparably ambitious ruler appeared on the Russian throne. Catherine II, the Great, was a German princess who married the German heir to the Russian crown. Finding him incompetent, Catherine tacitly consented to his murder and in 1762 she became ruler.
Catherine patronized arts, science and learning. She contributed to the resurgence of the Russian nobility that began after the death of Peter the Great. Catherine promulgated Charter to the Gentry reaffirming rights and freedoms of the Russian nobility, and abolishing mandatory state service.
Catherine the Great extended Russian political control over the Polish–Lithuanian Commonwealth with actions including the support of the Targowica Confederation, although the cost of her campaigns, on top of the oppressive social system that required lords' serfs to spend almost all of their time laboring on the lords' land, provoked a major peasant uprising in 1773, after Catherine legalized the selling of serfs separate from land. Inspired by another Cossack named Pugachev, with the emphatic cry of "Hang all the landlords!" the rebels threatened to take Moscow before they were ruthlessly suppressed. Catherine had Pugachev drawn and quartered in Red Square, but the specter of revolution continued to haunt her and her successors.
Catherine successfully waged war against the decaying Ottoman Empire and advanced Russia's southern boundary to the Black Sea. Then, by allying with the rulers of Austria and Prussia, she incorporated the territories of the Polish–Lithuanian Commonwealth, where after a century of Russian rule non-Catholic mainly Orthodox population prevailed during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In accordance to the treaty Russia had signed with the Georgians to protect them against any new invasion of their Persian suzerains and further political aspirations, Catherine waged a new war against Persia in 1796 after they had again invaded Georgia and established rule over it about a year prior, and had expelled the newly established Russian garrisons in the Caucasus.
Alexander I.
By the time of her death in 1796, Catherine's expansionist policy had made Russia into a major European power. This continued with Alexander I's wresting of Finland from the weakened kingdom of Sweden in 1809 and of Bessarabia from the Ottomans in 1812.
After the Russian armies liberated allied Georgia from Persian occupation in 1802, they clashed with Persia over control and consolidation over Georgia, Azerbaijan, and Dagestan, and also got involved in the Caucasian War against the Caucasian Imamate. To the south west, Russia attempted to expand at the expense of the Ottoman Empire, using Georgia at its base for the Caucasus and Anatolian front. Late 1820's were successful military years. In the 1828-29 Russo-Turkish War Russia invaded northeastern Anatolia and occupied the strategic Ottoman towns of Erzurum and Gumushane and, posing as protector and saviour of the Greek Orthodox population, received extensive support from the region's Pontic Greeks. Following a brief occupation, the Russian imperial army withdrew back into Georgia.
In European policy, Alexander I switched Russia back and forth four times in 1804-1812 from neutral peacemaker to anti-Napoleon to an ally of Napoleon, winding up in 1812 as Napoleon’s enemy. In 1805, he joined Britain in the War of the Third Coalition against Napoleon, but after the massive defeat at the Battle of Austerlitz he switched and formed an alliance with Napoleon by the Treaty of Tilsit (1807) and joined Napoleon's Continental System. He fought a small-scale naval war against Britain, 1807-12.
He and Napoleon could never agree, especially about Poland, and the alliance collapsed by 1810.
Furthermore Russia's economy had been hurt by Napoleon's Continental System, which cut off trade with Britain. As Esdaile notes, "Implicit in the idea of a Russian Poland was, of course, a war against Napoleon." Schroeder says Poland was the root cause of the conflict but Russia's refusal to support the Continental System was also a factor.
The invasion of Russia was a catastrophe for Napoleon and his 450,000 invasion troops. One major battle was fought at Borodino; casualties were very high but it was indecisive and Napoleon was unable to engage and defeat the Russian armies. He attempted to force the Tsar to terms by capturing Moscow at the onset of winter, even though the French Army had already lost most of its men. The expectation proved futile. The Russians retreated, burning crops and food supplies in a scorched earth policy that multiplied Napoleon's logistic problems. Unprepared for winter warfare, 85%-90% of Napoleon's soldiers died from disease, cold, starvation or by ambush by peasant guerrilla fighters. As Napoleon's forces retreated, Russian troops pursued them into Central and Western Europe and finally captured Paris. Out of a total population of around 43 million people, Russia lost about 1.5 million in the year 1812; of these about 250,000 to 300,000 were soldiers and the rest peasants and serfs.
After the final defeat of Napoleon in 1815, Alexander became known as the 'savior of Europe.' He presided over the redrawing of the map of Europe at the Congress of Vienna (1814–15), which made him the king of Congress Poland. He formed the Holy Alliance with Austria and Prussia, to suppress revolutionary movements in Europe that he saw as immoral threats to legitimate Christian monarchs. He helped Austria's Klemens von Metternich in suppressing all national and liberal movements.
Although the Russian Empire would play a leading political role as late as 1848, its retention of serfdom precluded economic progress of any significant degree. As West European economic growth accelerated during the Industrial Revolution, sea trade and colonialism which had begun in the second half of the 18th century, Russia began to lag ever farther behind, undermining its ability to field strong armies.
Nicholas I and the Decembrist Revolt.
Russia's great power status obscured the inefficiency of its government, the isolation of its people, and its economic backwardness. Following the defeat of Napoleon, Alexander I was willing to discuss constitutional reforms, and though a few were introduced, no thoroughgoing changes were attempted.
The tsar was succeeded by his younger brother, Nicholas I (1825–1855), who at the onset of his reign was confronted with an uprising. The background of this revolt lay in the Napoleonic Wars, when a number of well-educated Russian officers traveled in Europe in the course of the military campaigns, where their exposure to the liberalism of Western Europe encouraged them to seek change on their return to autocratic Russia. The result was the Decembrist Revolt (December 1825), the work of a small circle of liberal nobles and army officers who wanted to install Nicholas' brother as a constitutional monarch. But the revolt was easily crushed, leading Nicholas to turn away from the Westernization program begun by Peter the Great and champion the doctrine "Orthodoxy, Autocracy, and Nationality".
In 1826 another war was fought against Persia, and despite losing almost all recently consolidated territories in the first year of the battle in a successful Persian offensive, Russia managed to bring an end to the war on highly favourable terms, including the official gains of Armenia, Nakhchivan, Nagorno-Karabakh, Azerbaijan, and Iğdır.
By the 1830s, Russia had conquered all Persian territories and major Ottoman territories in the Caucasus. In 1831 Nicholas crushed a major uprising in Congress Poland; it would be followed by another large-scale Polish and Lithuanian revolt in 1863.
The Russian Army.
Tsar Nicholas I (reigned 1825–1855) lavished attention on his very large army; with a population of 60-70 million people, the army included a million men. They had outdated equipment and tactics, but the tsar, who dressed like a soldier and surrounded himself with officers, gloried in the victory over Napoleon in 1812 and took enormous pride in its smartness on parade. The cavalry horses, for example, were only trained in parade formations, and did poorly in battle. The glitter and braid masked profound weaknesses that he did not see. He put generals in charge of most of his civilian agencies regardless of their qualifications. An agnostic who won fame in cavalry charges was made supervisor of Church affairs. The Army became the vehicle of upward social mobility for noble youths from non-Russian areas, such as Poland, the Baltic, Finland and Georgia. On the other hand, many miscreants, petty criminals and undesirables were punished by local officials by enlisting them for life in the Army. The conscription system was highly unpopular with people, as was the practice of forcing peasants to house the soldiers for six months of the year. Curtiss finds that "The pedantry of Nicholas' military system, which stressed unthinking obedience and parade ground evolutions rather than combat training, produced ineffective commanders in time of war." His commanders in the Crimean War were old and incompetent, and indeed so were his muskets as the colonels sold the best equipment and the best food.
Finally the Crimean war at the end of his reign demonstrated to the world what no one had previously realized: Russia was militarily weak, technologically backward, and administratively incompetent. Despite his grand ambitions toward the south and Turkey, Russia had not built its railroad network in that direction, and communications were bad. The bureaucracy was riddled with graft, corruption and inefficiency and was unprepared for war. The Navy was weak and technologically backward; the Army, although very large, was good only for parades, suffered from colonels who pocketed their men's pay, poor morale, and was even more out of touch with the latest technology as developed by Britain and France. As Fuller notes, "Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness."
Radicals and reactionaries.
As Western Europe modernized after 1840 the issue became one of Russian direction. Some favored imitating Europe while others renounced the West and called for a return of the traditions of the past. The latter path was championed by Slavophiles, who heaped scorn on the "decadent" West. The Slavophiles were opponents of bureaucracy, preferred the collectivism of the medieval Russian "mir", or village community, to the individualism of the West.
Since the war against Napoleon, Russia had become deeply involved in the affairs of Europe, as part of the "Holy Alliance." The Holy Alliance was formed to serve as the "policeman of Europe." However, to be the policeman of Europe and maintain the Holy alliance needed large armies. Prussia, Austria, Britain and France, (the other members of the "Holy Alliance") lacked the large armies required to do so. They needed Russia to supply the required armies. Their need for large armies fit the philosophy of Tsar Nicholas I. When the Revolutions of 1848 swept Europe, Russia was quiet. The Tsar sent his army into Hungary in 1849 at the request of the Austrian Empire and broke the revolt, while preventing its spread to Russian Poland. Indeed the Tsar cracked down on any signs of unrest.
Russia expected that in exchange for supplying the troops to be the policeman of Europe, it should have a free hand in dealing with the decaying Ottoman Empire—the "sick man of Europe." The upshot was that Russia invaded the Crimea peninsula and other regions, leading to the Crimean War of 1853-56 when Britain and France came to the rescue of the Ottomans. As Fuller notes, "Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness."
In this setting Michael Bakunin would emerge as the father of anarchism. He left Russia in 1842 to Western Europe, where he became active in the socialist movement. After participating in the May Uprising in Dresden of 1849, he was handed over to Russia and sent to Siberia, He escaped in 1861, then began to organize. He argued with Karl Marx over socialism. Marx won and had Bakunin and the anarchists Bakunin expelled from the First International in 1872. He died in obscurity but other anarchists took up the torch, especially Russian radicals as Alexander Herzen and Peter Kropotkin.
Alexander II and the abolition of serfdom.
Tsar Nicholas died with his philosophy in dispute. One year earlier, Russia had become involved in the Crimean War, a conflict fought primarily in the Crimean peninsula. Since playing a major role in the defeat of Napoleon, Russia had been regarded as militarily invincible, but, once pitted against a coalition of the great powers of Europe, the reverses it suffered on land and sea exposed the weakness of Tsar Nicholas' regime.
When Alexander II came to the throne in 1855, desire for reform was widespread. The most pressing problem which confronted the Government was that of serfdom. In 1859, there were 23 million serfs (total population of Russia 67.1 Million). Alexander II made up his own mind to abolish serfdom from above rather than wait for it to be abolished from below through revolution. The emancipation of the serfs in 1861 was the single most important event in 19th-century Russian history. It was the beginning of the end for the landed aristocracy's monopoly of power. Emancipation brought a supply of free labor to the cities, industry was stimulated, and the middle class grew in number and influence. The freed peasants had to buy land, allotted to them, from the landowners with the state assistance. The Government issued special bonds to the landowners for the land that they had lost, and collected a special tax from the peasants, called redemption payments, at a rate of 5% of the total cost of allotted land yearly. All the land turned over to the peasants was owned collectively by the "mir", the village community, which divided the land among the peasants and supervised the various holdings.
Alexander was the most successful Russian reformer since Peter the Great, and was responsible for numerous reforms besides abolishing serfdom. He reorganized the judicial system, setting up elected local judges, abolishing capital punishment, promoting local self-government through the zemstvo system, imposing universal military service, ending some of the privileges of the nobility, and promoting the universities. In foreign policy, he sold Alaska to the United States in 1867, fearing the remote colony would fall into British hands if there was another war. He modernized the military command system. He sought peace, and moved away from bellicose France when Napoleon III fell. He joined with Germany and Austria in the League of the Three Emperors that stabilized the European situation. The Russian Empire expanded in Siberia and in the Caucasus and made gains at the expense of China. Faced with an uprising in Poland in 1863, he stripped that land of its separate Constitution and incorporated it directly into Russia. To counter the rise of a revolutionary and anarchistic movements, he sent thousands of dissidents into exile in Siberia and was proposing additional parliamentary reforms when he was assassinated in 1881.
In the late 1870s Russia and the Ottoman Empire again clashed in the Balkans. The Russo-Turkish War was popular among Russian people, who supported the independence of their fellow Orthodox Slavs, the Serbs and the Bulgarians. However, the war increased tension with Austria-Hungary, which also had ambitions in the region. The tsar was disappointed by the results of the Congress of Berlin in 1878, but abided by that agreement. During this period Russia expanded its empire into Central Asia, which was rich in raw materials, conquering the khanates of Kokand, Bokhara and Khiva, as well as the Trans-Caspian region.
Nihilism.
In the 1860s a movement known as Nihilism developed in Russia. A term originally coined by Ivan Turgenev in his 1862 novel "Fathers and Sons", Nihilists favoured the destruction of human institutions and laws, based on the assumption that such institutions and laws are artificial and corrupt. At its core, Russian nihilism was characterized by the belief that the world lacks comprehensible meaning, objective truth, or value. For some time many Russian liberals had been dissatisfied by what they regarded as the empty discussions of the intelligentsia. The Nihilists questioned all old values and shocked the Russian establishment. They moved beyond being purely philosophical to becoming major political forces after becoming involved in the cause of reform. Their path was facilitated by the previous actions of the Decembrists, who revolted in 1825, and the financial and political hardship caused by the Crimean War, which caused large numbers of Russian people to lose faith in political institutions.
The Nihilists first attempted to convert the aristocracy to the cause of reform. Failing there, they turned to the peasants. Their campaign, which targeted the people instead of the aristocracy or the landed gentry, became known as the Populist movement. It was based upon the belief that the common people possessed the wisdom and peaceful ability to lead the nation.
While the Narodnik movement was gaining momentum, the government quickly moved to extirpate it. In response to the growing reaction of the government, a radical branch of the Narodniks advocated and practiced terrorism. One after another, prominent officials were shot or killed by bombs. This represented the ascendancy of anarchism in Russia as a powerful revolutionary force. Finally, after several attempts, Alexander II was assassinated by anarchists in 1881, on the very day he had approved a proposal to call a representative assembly to consider new reforms in addition to the abolition of serfdom designed to ameliorate revolutionary demands.
Autocracy and reaction under Alexander III.
Unlike his father, the new tsar Alexander III (1881–1894) was throughout his reign a staunch reactionary who revived the maxim of "Orthodoxy, Autocracy, and National Character". A committed Slavophile, Alexander III believed that Russia could be saved from chaos only by shutting itself off from the subversive influences of Western Europe. In his reign Russia concluded the union with republican France to contain the growing power of Germany, completed the conquest of Central Asia, and exacted important territorial and commercial concessions from China.
The tsar's most influential adviser was Konstantin Pobedonostsev, tutor to Alexander III and his son Nicholas, and procurator of the Holy Synod from 1880 to 1895. He taught his royal pupils to fear freedom of speech and press and to hate democracy, constitutions, and the parliamentary system. Under Pobedonostsev, revolutionaries were hunted down and a policy of Russification was carried out throughout the empire.
Nicholas II and new revolutionary movement.
Alexander was succeeded by his son Nicholas II (1894–1917). The Industrial Revolution, which began to exert a significant influence in Russia, was meanwhile creating forces that would finally overthrow the tsar. Politically, these opposition forces organized into three competing parties: The liberal elements among the industrial capitalists and nobility, who believed in peaceful social reform and a constitutional monarchy, founded the Constitutional Democratic party or "Kadets" in 1905. Followers of the Narodnik tradition established the Socialist-Revolutionary Party or "Esers" in 1901, advocating the distribution of land among those who actually worked it—the peasants. A third and more radical group founded the Russian Social Democratic Labour Party or "RSDLP" in 1898; this party was the primary exponent of Marxism in Russia. Gathering their support from the radical intellectuals and the urban working class, they advocated complete social, economic and political revolution. 
In 1903 the RSDLP split into two wings: the radical Bolsheviks, led by Vladimir Lenin, and the relatively moderate Mensheviks, led by Yuli Martov. The Mensheviks believed that Russian socialism would grow gradually and peacefully and that the tsar’s regime should be succeeded by a democratic republic in which the socialists would cooperate with the liberal bourgeois parties. The Bolsheviks advocated the formation of a small elite of professional revolutionists, subject to strong party discipline, to act as the vanguard of the proletariat in order to seize power by force.
Revolution of 1905.
The disastrous performance of the Russian armed forces in the Russo-Japanese War was a major blow to the Russian State and increased the potential for unrest.
In January 1905, an incident known as "Bloody Sunday" occurred when Father Gapon led an enormous crowd to the Winter Palace in Saint Petersburg to present a petition to the tsar. When the procession reached the palace, Cossacks opened fire on the crowd, killing hundreds. The Russian masses were so aroused over the massacre that a general strike was declared demanding a democratic republic. This marked the beginning of the Russian Revolution of 1905. Soviets (councils of workers) appeared in most cities to direct revolutionary activity.
In October 1905, Nicholas reluctantly issued the famous October Manifesto, which conceded the creation of a national Duma (legislature) to be called without delay. The right to vote was extended, and no law was to go into force without confirmation by the Duma. The moderate groups were satisfied; but the socialists rejected the concessions as insufficient and tried to organize new strikes. By the end of 1905, there was disunity among the reformers, and the tsar's position was strengthened for the time being.
World War I.
Bound by treaty, Tsar Nicholas II entered World War I to defend Serbia from Austria. At the opening of hostilities in August 1914, the Russians took the offensive against both Germany and Austria-Hungary.
The very large but poorly equipped Russian army fought tenaciously and desperately at times despite its lack of organization and very weak logistics. Casualties were enormous. By 1915, many soldiers were sent to the front unarmed, and told to pick up whatever weapons they could from the battlefield. Nevertheless the Russian army fought on, and tied down large numbers of Germans and Austrians. When civilians showed a surge of patriotism, the tsar and his entourage failed to exploit it for military benefit. Instead, they relied on slow-moving bureaucracies. In areas where they did advance against the Austrians, they failed to rally the ethnic and religious minorities that were hostile to Austria, such as Poles. The tsar refused to cooperate with the national legislature, the Duma, and listened less to experts than to his wife, who was in thrall to her chief advisor, the ignorant hypnotic peasant Grigori Rasputin.
Repeated military failures and bureaucratic ineptitude soon turned large segments of the population against the government. The German and Ottoman fleets prevented Russia from importing supplies and exporting goods through the Baltic and Black seas.
By the middle of 1915 the impact of the war was demoralizing. Food and fuel were in short supply, casualties kept occurring, and inflation was mounting. Strikes increased among low-paid factory workers, and the peasants, who wanted land reforms, were restless. Meanwhile, elite distrust of the regime was deepened by reports that Rasputin was gaining influence; his assassination in late 1916 ended the scandal but did not restore the autocracy's lost prestige.
Russian Revolution.
The Tsarist system was completely overthrown in February 1917.
Rabinowitch argues:
In late February (3 March 1917), a strike occurred in a factory in the capital Petrograd (the new name for Saint Petersburg). On 23 February (8 March) 1917, thousands of women textile workers walked out of their factories protesting the lack of food and calling on other workers to join them. Within days, nearly all the workers in the city were idle, and street fighting broke out. The tsar ordered the Duma to disband, ordered strikers to return to work, and ordered troops to shoot at demonstrators in the streets. His orders triggered the February Revolution, especially when soldiers openly sided with the strikers. The tsar and the aristocracy fell on 2 March, as Nicholas II abdicated.
To fill the vacuum of authority, the Duma declared a Provisional Government, headed by Prince Lvov which was collectively known as the Russian Republic. Meanwhile, the socialists in Petrograd organized elections among workers and soldiers to form a soviet (council) of workers' and soldiers' deputies, as an organ of popular power that could pressure the "bourgeois" Provisional Government.
In July, following a series of crises that undermined their authority with the public, the head of the Provisional Government resigned and was succeeded by Alexander Kerensky, who was more progressive than his predecessor but not radical enough for the Bolsheviks or many Russians discontented with the deepening economic crisis and the continuation of the war. While Kerensky's government marked time, the socialist-led soviet in Petrograd joined with soviets that formed throughout the country to create a national movement.
Vladimir Lenin returned to Russia from exile in Switzerland with the help of Germany, which hoped that widespread strife would cause Russia to withdraw from the war. After many behind-the-scenes maneuvers, the soviets seized control of the government in November 1917 and drove Kerensky and his moderate provisional government into exile, in the events that would become known as the October Revolution.
When the national Constituent Assembly (elected in December 1917) refused to become a rubber stamp of the Bolsheviks, it was dissolved by Lenin's troops and all vestiges of democracy were removed. With the handicap of the moderate opposition removed, Lenin was able to free his regime from the war problem by the harsh Treaty of Brest-Litovsk (1918) with Germany Russia lost much of her western borderlands. However when Germany was defeated the Soviet government repudiated the Treaty.
Russian Civil War.
The Bolshevik grip on power was by no means secure, and a lengthy struggle broke out between the new regime and its opponents, which included the Socialist Revolutionaries, right-wing "Whites", and large numbers of peasants. At the same time the Allied powers sent several expeditionary armies to support the anti-Communist forces in an attempt to force Russia to rejoin the world war. The Bolsheviks fought against both these forces and national independence movements in the former Russian Empire. By 1921, they had defeated their internal enemies and brought most of the newly independent states under their control, with the exception of Finland, the Baltic States, the Moldavian Democratic Republic
(which joined Romania), and Poland (with whom they had fought the Polish-Soviet War). Finland also annexed the region Pechenga of the Russian Kola peninsula; Soviet Russia and allied Soviet republics conceded the parts of its territory to Estonia (Petseri County and Estonian Ingria), Latvia (Pytalovo), and Turkey (Kars). Poland incorporated the contested territories of Western Belarus and Western Ukraine, the former parts of the Russian Empire (except Galicia) east to Curzon Line.
Soviet Union (1917–91).
Creation of the Soviet Union.
The history of Russia between 1922 and 1991 is essentially the history of the Union of Soviet Socialist Republics, or Soviet Union. This ideologically based union, established in December 1922 by the leaders of the Russian Communist Party, was roughly coterminous with Russia before the Treaty of Brest-Litovsk. At that time, the new nation included four constituent republics: the Russian SFSR, the Ukrainian SSR, the Belarusian SSR, and the Transcaucasian SFSR.
The constitution, adopted in 1924, established a federal system of government based on a succession of soviets set up in villages, factories, and cities in larger regions. This pyramid of soviets in each constituent republic culminated in the All-Union Congress of Soviets. However, while it appeared that the congress exercised sovereign power, this body was actually governed by the Communist Party, which in turn was controlled by the Politburo from Moscow, the capital of the Soviet Union, just as it had been under the tsars before Peter the Great.
War Communism and the New Economic Policy.
The period from the consolidation of the Bolshevik Revolution in 1917 until 1921 is known as the period of war communism.
Land, all industry, and small businesses were nationalized, and the money economy was restricted. Strong opposition soon developed. The peasants wanted cash payments for their products and resented having to surrender their surplus grain to the government as a part of its civil war policies. Confronted with peasant opposition, Lenin began a strategic retreat from war communism known as the New Economic Policy (NEP). The peasants were freed from wholesale levies of grain and allowed to sell their surplus produce in the open market. Commerce was stimulated by permitting private retail trading. The state continued to be responsible for banking, transportation, heavy industry, and public utilities.
Although the left opposition among the Communists criticized the rich peasants, or kulaks, who benefited from the NEP, the program proved highly beneficial and the economy revived. The NEP would later come under increasing opposition from within the party following Lenin's death in early 1924.
Changes to Russian society.
While the Russian economy was being transformed, the social life of the people underwent equally drastic changes. From the beginning of the revolution, the government attempted to weaken patriarchal domination of the family. Divorce no longer required court procedure,
and to make women completely free of the responsibilities of childbearing, abortion was made legal as early as 1920. As a side effect, the emancipation of women increased the labor market. Girls were encouraged to secure an education and pursue a career in the factory or the office. Communal nurseries were set up for the care of small children, and efforts were made to shift the center of people's social life from the home to educational and recreational groups, the soviet clubs.
The regime abandoned the tsarist policy of discriminating against national minorities in favor of a policy of incorporating the more than two hundred minority groups into Soviet life. Another feature of the regime was the extension of medical services. Campaigns were carried out against typhus, cholera, and malaria; the number of doctors was increased as rapidly as facilities and training would permit; and infant mortality rates rapidly decreased while life expectancy rapidly increased.
In accordance with Marxist theory, the government also promoted atheism and materialism. It opposed organized religion, especially to break the power of the Russian Orthodox Church, a former pillar of the old tsarist regime and a major barrier to social change. Many religious leaders were sent to internal exile camps. Members of the party were forbidden to attend religious services, and the education system was separated from the Church. Religious teaching was prohibited except in the home, and atheist instruction was stressed in the schools.
Industrialization and Collectivization.
The years from 1929 to 1939 comprised a tumultuous decade in Soviet history—a period of massive industrialization and internal struggles as Joseph Stalin established near total control over Soviet society, wielding virtually unrestrained power. Following Lenin's death Stalin wrestled to gain control of the Soviet Union with rival factions in the Politburo, especially Leon Trotsky's. By 1928, with the Trotskyists either exiled or rendered powerless, Stalin was ready to put a radical programme of industrialisation into action.
In 1929 Stalin proposed the First Five-Year Plan. Abolishing the NEP, it was the first of a number of plans aimed at swift accumulation of capital resources through the buildup of heavy industry, the collectivization of agriculture, and the restricted manufacture of consumer goods. For the first time in history a government controlled all economic activity.
As a part of the plan, the government took control of agriculture through the state and collective farms ("kolkhozes"). By a decree of February 1930, about one million individual peasants ("kulaks") were forced off their land. Many peasants strongly opposed regimentation by the state, often slaughtering their herds when faced with the loss of their land. In some sections they revolted, and countless peasants deemed "kulaks" by the authorities were executed. The combination of bad weather, deficiencies of the hastily established collective farms, and massive confiscation of grain precipitated a serious famine, and several million peasants died of starvation, mostly in Ukraine, Kazakhstan and parts of southwestern Russia. The deteriorating conditions in the countryside drove millions of desperate peasants to the rapidly growing cities, fueling industrialization, and vastly increasing Russia's urban population in the space of just a few years.
The plans received remarkable results in areas aside from agriculture. Russia, in many measures the poorest nation in Europe at the time of the Bolshevik Revolution, now industrialized at a phenomenal rate, far surpassing Germany's pace of industrialization in the 19th century and Japan's earlier in the 20th century. 
While the Five-Year Plans were forging ahead, Stalin was establishing his personal power. The NKVD gathered in tens of thousands of Soviet citizens to face arrest, deportation, or execution. Of the six original members of the 1920 Politburo who survived Lenin, all were purged by Stalin. Old Bolsheviks who had been loyal comrades of Lenin, high officers in the Red Army, and directors of industry were liquidated in the Great Purges. Purges in other Soviet republics also helped centralize control in the USSR.
Stalin's repressions led to the creation of a vast system of internal exile, of considerably greater dimensions than those set up in the past by the tsars. Draconian penalties were introduced and many citizens were prosecuted for fictitious crimes of sabotage and espionage. The labor provided by convicts working in the labor camps of the Gulag system became an important component of the industrialization effort, especially in Siberia. An estimated 18 million people passed through the Gulag system, and perhaps another 15 million had experience of some other form of forced labor.
Soviet Union on the international stage.
The Soviet Union viewed the 1933 accession of fervently anti-Communist Hitler's government to power in Germany with great alarm from the onset, especially since Hitler proclaimed the Drang nach Osten as one of the major objectives in his vision of the German strategy of Lebensraum. The Soviets supported the republicans of Spain who struggled against fascist German and Italian troops in the Spanish Civil War In 1938–1939, immediately prior to WWII, the Soviet Union successfully fought against Imperial Japan in the Soviet-Japanese Border Wars in the Russian Far East, which led to Soviet-Japanese neutrality and the tense border peace that lasted until August 1945.
In 1938 Germany annexed Austria and, together with major Western European powers, signed the Munich Agreement following which Germany, Hungary and Poland divided parts of Czechoslovakia between themselves. German plans for further eastward expansion, as well as the lack of resolve from Western powers to oppose it, became more apparent. Despite the Soviet Union strongly opposing the Munich deal and repeatedly reaffirming its readiness to militarily back commitments given earlier to Czechoslovakia, the Western Betrayal led to the end of Czechoslovakia and further increased fears in the Soviet Union of a coming German attack. This led the Soviet Union to rush the modernization of its military industry and to carry out its own diplomatic maneuvers. In 1939 the Soviet Union signed the Molotov-Ribbentrop Pact: a non-aggression pact with Nazi Germany dividing Eastern Europe into two separate spheres of influence. Following the pact, the USSR normalized relations with Nazi Germany and resumed Soviet-German trade.
World War II.
On 17 September 1939, seventeen days after the start of World War II and victorious German advance deep into the Polish territory, the Red Army invaded eastern portions of Poland stating the "need to protect Ukrainians and Belarusians" there, after the "cessation of existence" of the Polish state as the justification of the action. As a result, the Belarusian and Ukrainian Soviet republics' western borders were moved westward and the new Soviet western border was drawn close to the original Curzon line. In the meantime the negotiations with Finland about the Soviet-proposed land swap that would redraw the Soviet-Finnish border further away from Leningrad failed; and in December 1939 the USSR started a campaign against Finland, known as the Winter War (1939–40). The war took a heavy death toll on the Red Army but forced Finland to sign a Moscow Peace Treaty and cede the Karelian Isthmus and Ladoga Karelia. In summer 1940 the USSR issued an ultimatum to Romania forcing it to cede the territories of Bessarabia and Northern Bukovina. At the same time, the Soviet Union also occupied the three formerly independent Baltic states (Estonia, Latvia and Lithuania).
The peace with Germany was tense, as both sides were preparing for the military conflict, and abruptly ended when the Axis forces led by Germany swept across the Soviet border on 22 June 1941. By the autumn the German army had seized Ukraine, laid a siege of Leningrad, and threatened to capture the capital, Moscow, itself. Despite the fact that in December 1941 the Red Army threw off the German forces from Moscow in a successful counterattack, the Germans retained the strategic initiative for approximately another year and held a deep offensive in the south-eastern direction, reaching the Volga and the Caucasus. However, two major German defeats in Stalingrad and Kursk proved decisive and reversed the course of the entire World War as Germans never regained the strength to sustain their offensive operations and the Soviet Union recaptured the initiative for the rest of the conflict. By the end of 1943, the Red Army had broken through the German siege of Leningrad and liberated much of Ukraine, much of Western Russia and moved into Belarus. By the end of 1944, the front had moved beyond the 1939 Soviet frontiers into eastern Europe. Soviet forces drove into eastern Germany, capturing Berlin in May 1945. The war with Germany thus ended triumphantly for the Soviet Union.
As agreed at the Yalta Conference, three months after the Victory Day in Europe the USSR launched the Soviet invasion of Manchuria, defeating the Japanese troops in neighboring Manchuria, the last Soviet battle of World War II.
Although the Soviet Union was victorious in World War II, the war resulted in around 26–27 million Soviet deaths (estimates vary) and had devastated the Soviet economy in the struggle. Some 1,710 towns and 70 thousand settlements were destroyed. The occupied territories suffered from the ravages of German occupation and deportations of slave labor in Germany. Thirteen million Soviet citizens became victims of a repressive policy of Germans and their allies on an occupied territory, where people died because of mass murders, famine, absence of elementary medical aid and slave labor. The Nazi Genocide of the Jews carried by German "Einsatzgruppen", along the local collaborators resulted in almost complete annihilation of the Jewish population over the entire territory temporary occupied by Germany and its allies., , . During occupation, Russia's Leningrad, now Saint Petersburg, region lost around a quarter of its population. Soviet Belarus lost from a quarter to a third of its population. 3.6 million Soviet prisoners of war (of 5.5 million) died in German camps.
Cold War.
Collaboration among the major Allies had won the war and was supposed to serve as the basis for postwar reconstruction and security. However, the conflict between Soviet and U.S. national interests, known as the Cold War, came to dominate the international stage in the postwar period.
The Cold War emerged from a conflict between Stalin and U.S. President Harry Truman over the future of Eastern Europe during the Potsdam Conference in the summer of 1945. Russia had suffered three devastating Western onslaughts in the previous 150 years during the Napoleonic Wars, the First World War, and the Second World War, and Stalin's goal was to establish a buffer zone of states between Germany and the Soviet Union. Truman charged that Stalin had betrayed the Yalta agreement. With Eastern Europe under Red Army occupation, Stalin was also biding his time, as his own atomic bomb project was steadily and secretly progressing.
In April 1949 the United States sponsored the North Atlantic Treaty Organization (NATO), a mutual defense pact in which most Western nations pledged to treat an armed attack against one nation as an assault on all. The Soviet Union established an Eastern counterpart to NATO in 1955, dubbed the Warsaw Pact. The division of Europe into Western and Soviet blocks later took on a more global character, especially after 1949, when the U.S. nuclear monopoly ended with the testing of a Soviet bomb and the Communist takeover in China.
The foremost objectives of Soviet foreign policy were the maintenance and enhancement of national security and the maintenance of hegemony over Eastern Europe. The Soviet Union maintained its dominance over the Warsaw Pact through crushing the Hungarian Revolution of 1956, suppressing the Prague Spring in Czechoslovakia in 1968, and supporting the suppression of the Solidarity movement in Poland in the early 1980s. The Soviet Union opposed the United States in a number of proxy conflicts all over the world, including Korean War and Vietnam War.
As the Soviet Union continued to maintain tight control over its sphere of influence in Eastern Europe, the Cold War gave way to "Détente" and a more complicated pattern of international relations in the 1970s in which the world was no longer clearly split into two clearly opposed blocs. Less powerful countries had more room to assert their independence, and the two superpowers were partially able to recognize their common interest in trying to check the further spread and proliferation of nuclear weapons in treaties such as SALT I, SALT II, and the Anti-Ballistic Missile Treaty.
U.S.-Soviet relations deteriorated following the beginning of the nine-year Soviet War in Afghanistan in 1979 and the 1980 election of Ronald Reagan, a staunch anti-communist, but improved as the communist bloc started to unravel in the late 1980s. With the collapse of the Soviet Union in 1991, Russia lost the superpower status that it had won in the Second World War.
De-Stalinization and the era of stagnation.
In the power struggle that erupted after Stalin's death in 1953, his closest followers lost out. Nikita Khrushchev solidified his position in a speech before the Twentieth Congress of the Communist Party in 1956 detailing Stalin's atrocities.
In 1964 Khrushchev was impeached by the Communist Party's Central Committee, charging him with a host of errors that included Soviet setbacks such as the Cuban Missile Crisis. After a period of collective leadership led by Leonid Brezhnev, Alexei Kosygin and Nikolai Podgorny, a veteran bureaucrat, Brezhnev, took Khrushchev's place as Soviet leader. Brezhnev followed emphasis on heavy industry, instituted the Soviet economic reform of 1965, and also attempted to ease relationships with the United States. In the 1960s the USSR became a leading producer and exporter of petroleum and natural gas.
Khrushchev and Brezhnev years were time when Soviet science and industry peaked. The world's first nuclear power plant was established in 1954 in Obninsk. Baikal Amur Mainline was built.
The Soviet space program, founded by Sergey Korolev, was especially successful. On 4 October 1957 Soviet Union launched the first space satellite Sputnik. On 12 April 1961 Yuri Gagarin became the first human to travel into space in the Soviet spaceship Vostok 1. Other achievements of Russian space program include: the first photo of the far side of the Moon; exploration of Venus; the first spacewalk by Alexey Leonov; first female spaceflight by Valentina Tereshkova. More recently, the Soviet Union produced the world's first space station, Salyut which in 1986 was replaced by Mir, the first consistently inhabited long-term space station, that served from 1986 to 2001.
While all modernized economies were rapidly moving to computerization after 1965, the USSR fell further and further behind. Moscow's decision to copy the IBM 360 of 1965 proved a decisive mistake for it locked scientists into an antiquated system they were unable to improve. They had enormous difficulties in manufacturing the necessary chips reliably and in quantity, in programming workable and efficient programs, in coordinating entirely separate operations, and in providing support to computer users. 
One of the greatest strengths of Soviet economy was its vast supplies of oil and gas; world oil prices quadrupled in the 1973-74, and rose again in 1979-1981, making the energy sector the chief driver of the Soviet economy, and was used to cover multiple weaknesses. At one point, Soviet Premier Alexei Kosygin told the head of oil and gas production, "things are bad with bread. Give me 3 million tons [of oil] over the plan." Former prime minister Yegor Gaidar, an economist looking back three decades, in 2007 wrote:
Breakup of the Union.
Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. After the rapid succession of former KGB Chief Yuri Andropov and Konstantin Chernenko, transitional figures with deep roots in Brezhnevite tradition, Mikhail Gorbachev announced perestroika in an attempt to modernize Soviet communism, and made significant changes in the party leadership. However, Gorbachev's social reforms led to unintended consequences. Because of his policy of "glasnost", which facilitated public access to information after decades of government repression, social problems received wider public attention, undermining the Communist Party's authority. In the revolutions of 1989 the USSR lost its allies in Eastern Europe. "Glasnost" allowed ethnic and nationalist disaffection to reach the surface. Many constituent republics, especially the Baltic republics, Georgian SSR and Moldavian SSR, sought greater autonomy, which Moscow was unwilling to provide. Gorbachev's attempts at economic reform were not sufficient, and the Soviet government left intact most of the fundamental elements of communist economy. Suffering from low pricing of petroleum and natural gas, ongoing war in Afghanistan, outdated industry and pervasive corruption, the Soviet planned economy proved to be ineffective, and by 1990 the Soviet government had lost control over economic conditions. Due to price control, there were shortages of almost all products, reaching their peak in the end of 1991, when people had to stand in long lines and to be lucky enough to buy even the essentials. Control over the constituent republics was also relaxed, and they began to assert their national sovereignty over Moscow.
The tension between Soviet Union and Russian SFSR authorities came to be personified in the bitter power struggle between Gorbachev and Boris Yeltsin. Squeezed out of Union politics by Gorbachev in 1987, Yeltsin, who represented himself as a committed democrat, presented a significant opposition to Gorbachev authority. In a remarkable reversal of fortunes, he gained election as chairman of the Russian republic's new Supreme Soviet in May 1990. The following month, he secured legislation giving Russian laws priority over Soviet laws and withholding two-thirds of the budget. In the first Russian presidential election in 1991 Yeltsin became president of the Russian SFSR.
At last Gorbachev attempted to restructure the Soviet Union into a less centralized state. However, on 19 August 1991, a coup against Gorbachev, conspired by senior Soviet officials, was attempted. The coup faced wide popular opposition and collapsed in three days, but disintegration of the Union became imminent. The Russian government took over most of the Soviet Union government institutions on its territory. Because of the dominant position of Russians in the Soviet Union, most gave little thought to any distinction between Russia and the Soviet Union before the late 1980s. In the Soviet Union, only Russian SFSR lacked even the paltry instruments of statehood that the other republics possessed, such as its own republic-level Communist Party branch, trade union councils, Academy of Sciences, and the like. The Communist Party of the Soviet Union was banned in Russia in 1991–1992, although no lustration has ever taken place, and many of its members became top Russian officials. However, as the Soviet government was still opposed to market reforms, the economic situation continued to deteriorate. By December 1991, the shortages had resulted in the introduction of food rationing in Moscow and Saint Petersburg for the first time since World War II. Russia received humanitarian food aid from abroad. After the Belavezha Accords, the Supreme Soviet of Russia withdrew Russia from the Soviet Union on 12 December. The Soviet Union officially ended on 25 December 1991, and the Russian Federation (formerly the Russian Soviet Federative Socialist Republic) took power on 26 December. The Russian government lifted price control on January 1992. Prices rose dramatically, but shortages disappeared.
Russian Federation.
Although Yeltsin came to power on a wave of optimism, he never recovered his popularity after endorsing Yegor Gaidar's "shock therapy" of ending Soviet-era price controls, drastic cuts in state spending, and an open foreign trade regime in early 1992 ("see" Russian economic reform in the 1990s). The reforms immediately devastated the living standards of much of the population. In the 1990s Russia suffered an economic downturn that was, in some ways, more severe than the United States or Germany had undergone six decades earlier in the Great Depression. Hyperinflation hit the ruble, due to monetary overhang from the days of the planned economy.
Meanwhile, the profusion of small parties and their aversion to coherent alliances left the legislature chaotic. During 1993, Yeltsin's rift with the parliamentary leadership led to the September–October 1993 constitutional crisis. The crisis climaxed on 3 October, when Yeltsin chose a radical solution to settle his dispute with parliament: he called up tanks to shell the Russian White House, blasting out his opponents. As Yeltsin was taking the unconstitutional step of dissolving the legislature, Russia came close to a serious civil conflict. Yeltsin was then free to impose the current Russian constitution with strong presidential powers, which was approved by referendum in December 1993. The cohesion of the Russian Federation was also threatened when the republic of Chechnya attempted to break away, leading to the First and Second Chechen Wars.
Economic reforms also consolidated a semi-criminal oligarchy with roots in the old Soviet system. Advised by Western governments, the World Bank, and the International Monetary Fund, Russia embarked on the largest and fastest privatization that the world had ever seen in order to reform the fully nationalized Soviet economy. By mid-decade, retail, trade, services, and small industry was in private hands. Most big enterprises were acquired by their old managers, engendering a new rich (Russian tycoons) in league with criminal mafias or Western investors. That being said, there were corporate raiders such as Andrei Volgin engaged in hostile takeovers of corrupt corporations by the mid-1990s.
By the mid-1990s Russia had a system of multiparty electoral politics. But it was harder to establish a representative government because of two structural problems—the struggle between president and parliament and the anarchic party system.
Meanwhile, the central government had lost control of the localities, bureaucracy, and economic fiefdoms; tax revenues had collapsed. Still in deep depression by the mid-1990s, Russia's economy was hit further by the financial crash of 1998. After the 1998 financial crisis, Yeltsin was at the end of his political career. Just hours before the first day of 2000, Yeltsin made a surprise announcement of his resignation, leaving the government in the hands of the little-known Prime Minister Vladimir Putin, a former KGB official and head of the FSB, the KGB's post-Soviet successor agency. In 2000, the new acting president defeated his opponents in the presidential election on 26 March, and won a landslide 4 years later. International observers were alarmed by late 2004 moves to further tighten the presidency's control over parliament, civil society, and regional officeholders. In 2008 Dmitri Medvedev, a former Gazprom chairman and Putin's head of staff, was elected new President of Russia. In 2012, Putin was once again elected as President.
Russia has had difficulty attracting foreign direct investment and has experienced large capital outflows in the past several years. Russia's long-term problems also include a shrinking workforce, rampant corruption, and underinvestment in infrastructure.
Nevertheless, reversion to a socialist command economy seemed almost impossible. Russia ended 2006 with its eighth straight year of growth, averaging 6.7% annually since the financial crisis of 1998. Although high oil prices and a relatively cheap ruble initially drove this growth, since 2003 consumer demand and, more recently, investment have played a significant role. Russia is well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry.
Scholars have examined the prevailing political philosophy in recent years under President Putin. Barbashin and Hannah Thoburn argue regarding the philosophy behind Putin's dealing with Ukraine in 2014:
Further reading.
Post-Soviet era.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "History of Russia" article dated 2012-03-18, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="14312" url="http://en.wikipedia.org/wiki?curid=14312" title="Harpsichord">
Harpsichord

A harpsichord is a musical instrument played by means of a keyboard. It produces sound by plucking a string when a key is pressed.
"Harpsichord" designates the whole family of similar plucked keyboard instruments, including the smaller virginals, muselar, and spinet.
The harpsichord was widely used in Renaissance and Baroque music. During the late 18th century, it gradually disappeared from the musical scene with the rise of the piano. But in the 20th century, it made a resurgence, being used in historically informed performances of older music, in new (contemporary) compositions, and in popular culture.
Mechanism.
Harpsichords vary in size and shape, but all have the same basic functional arrangement. The player depresses a key that rocks over a pivot in the middle of its length. The other end of the key lifts a jack (a long strip of wood) that holds a small plectrum (a wedge-shaped piece of quill, nowadays often plastic), which plucks the string. When the player releases the key, the far end returns to its rest position, and the jack falls back. The plectrum, mounted on a tongue that can swivel backwards away from the string, passes the string without plucking it again. As the key reaches its rest position, a felt damper atop the jack stops the string's vibrations. These basic principles are explained in detail below.
Strings, tuning, and soundboard.
Each string is wound around a "tuning pin", normally at the end of the string closer to the player. When rotated with a wrench or tuning hammer, the tuning pin adjusts the tension so that the string sounds the correct pitch. Tuning pins are held tightly in holes drilled in the "pinblock" or "wrestplank", an oblong hardwood plank.
Proceeding from the tuning pin, a string next passes over the "nut", a sharp edge that is made of hardwood and is normally attached to the wrestplank. The section of the string beyond the nut forms its "vibrating length", which is plucked and creates sound.
At the other end of its vibrating length, the string passes over the bridge, another sharp edge made of hardwood. As with the nut, the horizontal position of the string along the bridge is determined by a vertical metal pin inserted into the bridge, against which the string rests.
The bridge itself rests on a "soundboard", a thin panel of wood usually made of spruce, fir or—in some Italian harpsichords—cypress. The soundboard efficiently transduces the vibrations of the strings into vibrations in the air; without a soundboard, the strings would produce only a very feeble sound.
A string is attached at its far end by a loop to a "hitchpin" that secures it to the case.
Multiple choirs of strings.
While many harpsichords have exactly one string per note, more elaborate harpsichords can have more. This provides two advantages: ability to vary volume and ability to vary tonal quality. Volume is increased when the mechanism of the instrument is set up by the player (see below) so that the press of a single key plucks more than one string. Tonal quality can be varied in two ways. First, different choirs of strings can be designed to have distinct tonal qualities, usually by having one set of strings plucked closer to the nut, which emphasizes the higher harmonics, and produces a "nasal" sound quality; the mechanism of the instrument permits the player to select one choir or the other. Second, having one key pluck two strings at once changes not just volume but also tonal quality; for instance, when two strings tuned to the same pitch are plucked simultaneously, the note is not just louder but also richer and more complex. A particularly vivid effect is obtained when the strings plucked simultaneously are an octave apart. This is normally heard by the ear not as two pitches but as one: the sound of the higher string is blended with that of the lower one, and the ear hears the lower pitch, enriched in tonal quality by the additional strength in the upper harmonics of the note sounded by the higher string.
When describing a harpsichord it is customary to specify its choirs of strings, often called its disposition. Strings at eight foot pitch sound at the normal expected pitch, strings at four foot pitch sound an octave higher. Harpsichords occasionally include a sixteen-foot choir (one octave lower than eight-foot) or a two-foot choir (two octaves higher; quite rare).
When there are multiple choirs of strings, the player is often able to control which choirs sound. This is usually done by having a set of jacks for each choir, and a mechanism for "turning off" each set, often by moving the upper register (through which the jacks slide) sideways a short distance, so that their plectra miss the strings. In simpler instruments this is done by manually moving the registers, but as the harpsichord evolved, builders invented levers, knee levers and pedal mechanisms to make it easier to change registration.
Harpsichords with more than one keyboard provide flexibility in selecting which strings play, since each manual can control the plucking of a different set of strings. In addition, such harpsichords often have a mechanism that couples manuals together, so that a single manual plays both sets of strings. The most flexible system is the French shove coupler, in which the lower manual slides forward and backward. In the backward position, "dogs" attached to the upper surface of the lower manual engage the lower surface of the upper manual's keys. Depending on choice of keyboard and coupler position, the player can select any of the sets of jacks labeled in figure 4 as A, or B and C, or all three.
The English dogleg jack system (also used in Baroque Flanders) does not require a coupler. The jacks labeled A in Figure 5 have a "dogleg" shape that permits either keyboard to play A. If the player wishes to play the upper 8' from the upper manual only and not from the lower manual, a stop handle disengages the jacks labeled A and engages instead an alternative row of jacks called "lute stop" (not shown in the Figure).
The use of multiple manuals in a harpsichord was not originally provided for the flexibility in choosing which strings would sound, but rather for transposition. For discussion, see "History of the harpsichord".
The case.
The case holds in position all of the important structural members: pinblock, soundboard, hitchpins, keyboard, and the jack action. It usually includes a solid bottom, and also internal bracing to maintain its form without warping under the tension of the strings. Cases vary greatly in weight and sturdiness: Italian harpsichords are often of light construction; heavier construction is found in the later Flemish instruments and those derived from them (see "History of the harpsichord").
The case also gives the harpsichord its external appearance and protects the instrument. A large harpsichord is, in a sense, a piece of furniture, as it stands alone on legs and may be styled in the manner of other furniture of its place and period. Early Italian instruments, on the other hand, were so light in construction that they were treated rather like a violin: kept for storage in a protective outer case, and played after taking it out of its case and placing it on a table. Such tables were often quite high – until the late 18th century people usually played standing up. Eventually, harpsichords came to be built with just a single case, though an intermediate stage also existed: the "false inner–outer", which for purely aesthetic reasons was built to look as if the outer case contained an inner one, in the old style. Even after harpsichords became self-encased objects, they often were supported by separate stands, and some modern harpsichords have separate legs for improved portability.
Many harpsichords have a lid that can be raised, a cover for the keyboard, and a stand for music.
Harpsichords have been decorated in a great many different ways: with plain buff paint (e.g. some Flemish instruments), with paper printed with patterns, with leather or velvet coverings, with chinoiserie, or occasionally with highly elaborate painted artwork.
Variants.
Harpsichord.
In modern usage, "harpsichord" can mean any member of the family of instruments. More often, though, it specifically denotes a grand-piano-shaped instrument with a roughly triangular case accommodating long bass strings at the left and short treble strings at the right. The characteristic profile of such a harpsichord is more elongated than a modern piano, with a sharper curve to the bentside.
Virginals.
The virginal is a smaller and simpler rectangular form of the harpsichord having only one string per note; the strings run parallel to the keyboard, which is on the long side of the case.
Spinet.
A spinet is a harpsichord with the strings set at an angle (usually about 30 degrees) to the keyboard. The strings are too close together for the jacks to fit between them. Instead, the strings are arranged in pairs, and the jacks are in the larger gaps between the pairs. The two jacks in each gap face in opposite directions, and each plucks a string adjacent to a gap.
Clavicytherium.
A clavicytherium is a harpsichord with the soundboard and strings mounted vertically facing the player, the same space-saving principle as an upright piano. In a clavicytherium, the jacks move horizontally without the assistance of gravity, so that clavicytherium actions are more complex than those of other harpsichords.
Clavicymbalum.
An early relative of the harpsichord, first attested in 1323, with an unusual jack system, and lacking any method to damped a string once sounded.
Ottavino.
Ottavini are small spinets or virginals at four foot pitch. Harpsichords at octave pitch were more common in the early Renaissance, but lessened in popularity later on. However, the ottavino remained very popular as a domestic instrument in Italy until the 19th century. The English diarist Samuel Pepys mentions his "tryangle" several times. This was not the percussion instrument that we call triangle today; rather, it was a name for octave-pitched spinets, which were triangular in shape. In the Low Countries, an ottavino was commonly paired with an 8' virginals, encased in a small cubby under the soundboard of the larger instrument. The ottavino could be removed and placed on top of the virginal, making in effect a double manual instrument. These are sometimes called 'mother-and-child' or 'double' virginals.
Other.
The archicembalo, built in the 16th century, had an unusual keyboard layout, designed to accommodate variant tuning systems demanded by compositional practice and theoretical experimentation. More common were instruments with split sharps, also designed to accommodate the tuning systems of the time.
The folding harpsichord was an instrument that could be folded up for travel.
Pedal Harpsichord: Occasionally, harpsichords were built which included another set or sets of strings underneath and operated by pedals which pluck the lowest keys of the harpsichord. Although there are no known extant pedal harpsichords from the 18th century or before, from Adlung (1758): the lower set of usually 8' strings "...is built like an ordinary harpsichord, but with an extent of two octaves only. The jacks are similar, but they will benefit from being arranged back to back, since the two [bass] octaves take as much space as four in an ordinary harpsichord Prior to 1980 when Keith Hill introduced his design for a pedal harpsichord, most pedal harpsichords were built based on the designs of extant pedal pianos from the 19th century, in which the instrument is as wide as the pedalboard. While these were mostly intended as practice instruments for organists, a few pieces are believed to have been written specifically for the pedal harpsichord. However, the set of pedals can augment the sound from any piece performed on the instrument, as demonstrated on several albums by E. Power Biggs.
Compass and pitch range.
On the whole, earlier harpsichords have smaller ranges than later ones, although there are many exceptions. The largest harpsichords have a range of just over five octaves, and the smallest have under four. Usually, the shortest keyboards were given extended range in the bass with a "short octave". The traditional pitch range for a 5-octave instrument is F1 - F6 (FF - f3).
Tuning pitch is often taken to be a=415 Hz, roughly a semitone lower than the modern standard concert pitch of a=440 Hz. An accepted exception is for French baroque repertoire, which is often performed with a=392 Hz, approximately a semitone lower again. See Jean-Philippe Rameau's "Treatise on Harmony" (1722) [Dover Publications], Book One, chapter five, for insight into French baroque tuning; "Since most of these semitones are absolutely necessary in the tuning of organs and other similar instruments, the following chromatic system has been drawn up." Tuning an instrument nowadays usually starts with setting an A; historically it would commence from a C or an F.
Some modern instruments are built with keyboards that can shift sideways, allowing the player to align the mechanism with strings at either a=415 Hz or a=440 Hz. If a tuning other than equal temperament is used, the instrument requires retuning once the keyboard is shifted.
History.
The harpsichord was most probably invented in the late Middle Ages. By the 16th century, harpsichord makers in Italy were making lightweight instruments with low string tension. A different approach was taken in the Southern Netherlands starting in the late 16th century, notably by the Ruckers family. Their harpsichords used a heavier construction and produced a more powerful and distinctive tone. They included the first harpsichords with two keyboards, used for transposition.
The Flemish instruments served as the model for 18th century harpsichord construction in other nations. In France, the double keyboards were adapted to control different choirs of strings, making a more musically flexible instrument. Instruments from the peak of the French tradition, by makers such as the Blanchet family and Pascal Taskin, are among the most widely admired of all harpsichords, and are frequently used as models for the construction of modern instruments. In England, the Kirkman and Shudi firms produced sophisticated harpsichords of great power and sonority. German builders extended the sound repertoire of the instrument by adding sixteen foot and two foot choirs; these instruments have recently served as models for modern builders.
In the late 18th century the harpsichord was supplanted by the piano and almost disappeared from view for most of the 19th century: an exception was its continued use in opera for accompanying recitative, but the piano sometimes displaced it even there. 20th century efforts to revive the harpsichord began with instruments that used piano technology, with heavy strings and metal frames. Starting in the middle of the 20th century, ideas about harpsichord making underwent a major change, when builders such as Frank Hubbard, William Dowd, and Martin Skowroneck sought to re-establish the building traditions of the Baroque period. Harpsichords of this type of historically informed building practice dominate the current scene.
Music for the harpsichord.
Historical period.
The great bulk of the standard repertoire for the harpsichord was written during its first historical flowering, the Renaissance and Baroque eras.
The first music written specifically for solo harpsichord was published around the early 16th century. Composers who wrote solo harpsichord music were numerous during the whole Baroque era in European countries including Italy, Germany, England and France. Solo harpsichord compositions included dance suites, fantasias, and fugues. Among the most famous composers who wrote for the harpsichord were the members of English virginal school of the late Renaissance, notably William Byrd (ca. 1540 – 1623). In France, a great number of highly characteristic solo works were created and compiled into four books of "ordres" by François Couperin (1668–1733). Domenico Scarlatti (1685–1757) began his career in Italy but wrote most of his solo harpsichord works in Spain; his most famous work is his series of 555 harpsichord sonatas. Perhaps the most celebrated composer who wrote for the harpsichord was J. S. Bach (1685–1750), whose solo works (for instance, the Well-Tempered Clavier and the Goldberg Variations), continue to be performed very widely, often on the piano. Bach was also a pioneer of the harpsichord concerto, both in works designated as such, and in the harpsichord part of his Fifth Brandenburg Concerto.
Two of the most prominent composers of the Classical era, Joseph Haydn (1732–1809) and Wolfgang Amadeus Mozart (1756–1791), wrote harpsichord music. For both, the instrument featured in the earlier period of their careers and was abandoned once they had shifted their efforts to the piano.
Besides solo works, the historical harpsichord was widely used for accompaniment in the basso continuo style (a function it maintained in operatic recitative even into the 19th century).
Music written for the revived harpsichord.
Through the 19th century, the harpsichord was almost completely supplanted by the piano. In the 20th century, composers returned to the instrument, as they sought out variation in the sounds available to them. Under the influence of Arnold Dolmetsch, the harpsichordists Violet Gordon-Woodhouse (1872–1951) and in France, Wanda Landowska (1879–1959), were at the forefront of the instrument's renaissance.
Concertos for the instrument were written by Francis Poulenc (the "Concert champêtre", 1927–28), Manuel de Falla, Walter Leigh, Bertold Hummel, Henryk Mikołaj Górecki, Michael Nyman, Philip Glass, and Roberto Carnevale. Bohuslav Martinů wrote both a concerto and a sonata for the instrument, and Elliott Carter's "Double Concerto" is scored for harpsichord, piano and two chamber orchestras.
In chamber music, György Ligeti wrote a small number of solo works for the instrument (including "Continuum"), and Henri Dutilleux's "Les Citations" (1991) is scored for harpsichord, oboe, double bass and percussions. Elliott Carter's "Sonata for Flute, Oboe, Cello and Harpsichord" (1952) explores the timbre possibilities of the modern harpsichord.
Josef Tal wrote "Concerto for harpsichord & electronic music" (1964) and "Chamber Music" (1982) for s-recorder, marimba & harpsichord. Both Dmitri Shostakovich ("Hamlet", 1964) and Alfred Schnittke ("Symphony No.8", 1994) wrote works that use the harpsichord as part of the orchestral texture. John Cage and Lejaren Hiller wrote "HPSCHD" (1969) for harpsichord and computer-generated tape. John Zorn has also used harpsichord in works like "Rituals" (1998), "Contes de Fées" (1999), and "La Machine de l'Etre" (2000).
In the Preface to his piano collection "Mikrokosmos", Béla Bartók suggests some ten pieces as being suitable for the harpsichord.
Seán Ó Riada used the harpischord both for the soundtracks of film music, and for interpretations of the Irish harper Turlough O'Carolan during Riada's work with the band Ceoltori Cualann (later to become The Chieftans). Riada used harpsichord for the latter with the justification that the harpichord best replicated the sound of the metal strings of the early Irish harp.
Benjamin Britten included harpsichord parts in his opera "A Midsummer Night's Dream" and his cantata "Phaedra".
Harpsichordist Hendrik Bouman has composed pieces in the 17th and 18th century style, including works for solo harpsichord, harpsichord concerti, and other works that call for harpsichord continuo. Other contemporary composers writing new harpsichord music in period styles include Grant Colburn, and Fernando De Luca. Notable performers include Oscar Milani and Mario Raskin.
During the last half of the 20th century, the sound of the harpsichord (or perhaps rather more often, its electronically created equivalent) became very familiar in popular culture, appearing frequently in popular music, television, films, computer games, and so on.

</doc>
<doc id="14314" url="http://en.wikipedia.org/wiki?curid=14314" title="Hawker Siddeley Harrier">
Hawker Siddeley Harrier

The Hawker Siddeley Harrier, known colloquially as the "Harrier Jump Jet", was developed in the 1960s and formed the first generation of the Harrier series of aircraft. It was the first operational close-support and reconnaissance fighter aircraft with vertical/short takeoff and landing (V/STOL) capabilities and the only truly successful V/STOL design of the many that arose in that era. The Harrier was produced directly from the Hawker Siddeley Kestrel prototypes following the cancellation of a more advanced supersonic aircraft, the Hawker Siddeley P.1154. The British Royal Air Force (RAF) ordered the Harrier GR.1 and GR.3 variants in the late 1960s. It was exported to the United States as the AV-8A, for use by the US Marine Corps (USMC), in the 1970s.
The RAF positioned the bulk of their Harriers in West Germany to defend against a potential invasion of Western Europe by the Soviet Union; the unique abilities of the Harrier allowed the RAF to disperse their forces away from vulnerable airbases. The USMC used their Harriers primarily for close air support, operating from amphibious assault ships, and, if needed, forward operating bases. Harrier squadrons saw several deployments overseas. The Harrier's ability to operate with minimal ground facilities and very short runways allowed it to be used at locations unavailable to other fixed-wing aircraft. The Harrier received criticism for having a high accident rate and for a time-consuming maintenance process.
In the 1970s the British Aerospace Sea Harrier was developed from the Harrier for use by the Royal Navy (RN) on "Invincible"-class aircraft carriers. The Sea Harrier and the Harrier fought in the 1982 Falklands War, in which the aircraft proved to be crucial and versatile. The RN Sea Harriers provided fixed-wing air defence while the RAF Harriers focused on ground-attack missions in support of the advancing British land force. The Harrier was also extensively redesigned as the AV-8B Harrier II and British Aerospace Harrier II by the team of McDonnell Douglas and British Aerospace. The innovative Harrier family and its Rolls-Royce Pegasus engines with thrust vectoring nozzles have generated long-term interest in V/STOL aircraft. Similar V/STOL operational aircraft include the contemporary Soviet Yakovlev Yak-38. A V/STOL variant of the Lockheed Martin F-35 Lightning II is currently under development.
Development.
Origins.
The Harrier's design was derived from the Hawker P.1127. Prior to developing the P.1127 Hawker Aircraft had been working on a replacement for the Hawker Hunter, the Hawker P.1121. The P.1121 was cancelled after the release of the British Government's 1957 Defence White Paper, which advocated a policy shift away from manned aircraft and towards missiles. This policy resulted in the termination of the majority of aircraft development projects then underway for the British military. Hawker sought to quickly move on to a new project and became interested in Vertical Take Off/Landing (VTOL) aircraft, which did not need runways. According to Air Chief Marshal Sir Patrick Hine this interest may have been stimulated by the presence of Air Staff Requirement 345, which sought a V/STOL ground attack fighter for the Royal Air Force.
Design work on the P.1127 was formally started in 1957 by Sir Sydney Camm, Ralph Hooper of Hawker Aircraft and Stanley Hooker (later Sir Stanley Hooker) of the Bristol Engine Company. The close cooperation between Hawker, the airframe company, and Bristol, the engine company, was viewed by project engineer Gordon Lewis as one of the key factors that allowed the development of the Harrier to continue in spite of technical obstacles and political setbacks. Rather than using rotors or a direct jet thrust, the P.1127 had an innovative vectored thrust turbofan engine, the Pegasus. The Pegasus I was rated at 9000 lb(f) of thrust and first ran in September 1959. A contract for two development prototypes was signed in June 1960 and the first flight followed in October 1960. Of the six prototypes built three crashed—including one during an air display at the 1963 Paris Air Show.
Tripartite evaluation.
In 1961 the United Kingdom, United States and West Germany jointly agreed to purchase nine aircraft developed from the P.1127, for the evaluation of the performance and potential of V/STOL aircraft. These aircraft were built by Hawker Siddeley and were designated "Kestrel FGA.1" by the UK. The Kestrel was strictly an evaluation aircraft and to save money the Pegasus 5 engine was not fully developed as intended, only having 15000 lb(f) of thrust instead of the projected 18200 lb(f). The Tripartite Evaluation Squadron numbered ten pilots; four each from the UK and US and two from West Germany. The Kestrel's first flight took place on 7 March 1964.
A total of 960 sorties had been made during the trials, including 1,366 takeoffs and landings, by the end of evaluations in November 1965. One aircraft was destroyed in an accident and six others were transferred to the United States, assigned the US designation "XV-6A Kestrel", and underwent further testing. The two remaining British-based Kestrels were assigned for further trials and experimentation at RAE Bedford with one being modified to use the uprated Pegasus 6 engine.
P.1154.
At the time of the development of the P.1127 Hawker and Bristol had also undertaken considerable development work on a supersonic version, the Hawker Siddeley P.1154, to meet a North Atlantic Treaty Organisation (NATO) requirement issued for such an aircraft. The design used a single Bristol Siddeley BS100 engine with four swivelling nozzles, in a fashion similar to the P.1127, and required the use of plenum chamber burning (PCB) to achieve supersonic speeds. The P.1154 won the competition to meet the requirement against stern competition from other aircraft manufacturers such as Dassault Aviation's Mirage IIIV. The French government did not accept the decision and withdrew; the NATO requirement was cancelled shortly after in 1965.
The Royal Air Force and the Royal Navy planned to develop and introduce the supersonic P.1154 independently of the cancelled NATO requirement. This ambition was complicated by the conflicting requirements between the two services—while the RAF wanted a low-level supersonic strike aircraft, the Navy sought a twin-engined air defence fighter. Following the election of the Labour Government of 1964 the P.1154 was cancelled, as the Royal Navy had already begun procurement of the McDonnell Douglas Phantom II and the RAF placed a greater importance on the BAC TSR-2's ongoing development. Work continued on elements of the project, such as a supersonic PCB-equipped Pegasus engine, with the intention of developing a future Harrier variant for the decades following cancellation.
Production.
Following the collapse of the P.1154's development the RAF began considering a simple upgrade of the existing subsonic Kestrel and issued Requirement ASR 384 for a V/STOL ground attack jet. Hawker Siddeley received an order for six pre-production aircraft in 1965, designated "P.1127 (RAF)", of which the first made its maiden flight on 31 August 1966. An order for 60 production aircraft, designated as Harrier GR.1, was received in early 1967. The aircraft was named for the Harrier, a small bird of prey capable of hovering.
The Harrier GR.1 made its first flight on 28 December 1967. It officially entered service with the RAF on 18 April 1969 when the Harrier Conversion Unit at RAF Wittering received its first aircraft. The aircraft were built in two factories—one in Kingston upon Thames, southwest London, and the other at Dunsfold Aerodrome, Surrey—and underwent initial testing at Dunsfold. The ski-jump technique for launching Harriers from Royal Navy aircraft carriers was extensively trialled at RNAS Yeovilton from 1977. Following these tests ski-jumps were added to the flight decks of all RN carriers from 1979 onwards, in preparation for the new variant for the navy, the Sea Harrier.
In the late 1960s the British and American governments held talks on producing Harriers in the United States. Hawker Siddeley and McDonnell Douglas formed a partnership in 1969 in preparation for American production, but Congressman Mendel Rivers and the House Appropriations Committee held that it would be cheaper to produce the AV-8A on the pre-existing production lines in the United Kingdom—hence all AV-8A Harriers were purchased from Hawker Siddeley. Improved Harrier versions with better sensors and more powerful engines were developed in later years. The USMC received 102 AV-8A and 8 TAV-8A Harriers between 1971 and 1976.
Design.
Overview.
The Harrier is typically used as a ground attack aircraft, though its manoeuvrability also allows it to effectively engage other aircraft at short ranges. The Harrier is powered by a single Pegasus turbofan engine mounted in the fuselage. The engine is fitted with two air intakes and four vectoring nozzles for directing the thrust generated: two for the bypass flow and two for the jet exhaust. Several smaller reaction nozzles are also fitted, in the nose, tail and wingtips, for the purpose of balancing during vertical flight. It has two landing gear units on the fuselage and two outrigger landing gear units, one on each wing tip. The Harrier is equipped with four wing and three fuselage pylons for carrying a variety of weapons and external fuel tanks.
The Kestrel and the Harrier were similar in appearance, though approximately 90 per cent of the Kestrel's airframe was redesigned for the Harrier. The Harrier was powered by the more powerful Pegasus 6 engine; new air intakes with auxiliary blow-in doors were added to produce the required airflow at low speed. Its wing was modified to increase area and the landing gear was strengthened. Several hardpoints were installed, two under each wing and one underneath the fuselage; two 30 mm ADEN cannon gun pods could also be fitted to the aircraft's underside. The Harrier was outfitted with updated avionics to replace the basic systems used in the Kestrel; a navigational-attack system incorporating an inertial navigation system, originally for the P.1154, was installed and information was presented to the pilot by a head-up display and a moving map display.
The Harrier's VTOL abilities allowed it to be deployed from very small prepared clearings or helipads as well as normal airfields. It was believed that, in a high-intensity conflict, air bases would be vulnerable and likely to be quickly knocked out. The capability to scatter Harrier squadrons to dozens of small "alert pads" on the front lines was highly prized by military strategists and the USMC procured the aircraft because of this ability. Hawker Siddeley noted that STOL operation provided additional benefits over VTOL operation, saving fuel and allowing the aircraft to carry more ordnance.
"I still don't believe the Harrier. Think of the millions that have been spent on VTO in America and Russia, and quite a bit in Europe, and yet the only vertical take-off aircraft which you can call a success is the Harrier. When I saw the Harrier hovering and flying backwards under control, I reckoned I'd seen everything. And it's not difficult to fly." -Thomas Sopwith
The Harrier, while serving for many decades in various forms, has been criticised on multiple issues; in particular a high accident rate, though Nordeen notes that several conventional single-engine strike aircraft like the Douglas A-4 Skyhawk and LTV A-7 Corsair II had worse accident rates. The "Los Angeles Times" reported in 2003 that the Harrier "...has amassed the highest major accident rate of any military plane now in service. Forty-five Marines have died in 148 noncombat accidents". Colonel Lee Buland of the USMC declared the maintenance of a Harrier to be a "challenge"; the need to remove the wings before performing most work upon the engine, including engine replacements, meant the Harrier required considerable man-hours in maintenance, more than most aircraft. Buland noted however that the maintenance difficulties were unavoidable in order to create a V/STOL aircraft.
Engine.
The Pegasus turbofan jet engine, developed in tandem with the P.1127 then the Harrier, was designed specifically for V/STOL manoeuvring. Bristol Siddeley developed it from their earlier conventional Orpheus turbofan engine, the main difference being the thrust generated is directed through four rotatable nozzles. The engine is equipped for water injection to increase thrust and takeoff performance in hot and high altitude conditions; in normal V/STOL operations the system would be used in landing vertically with a heavy weapons load. The water injection function had originally been added following the input of US Air Force Colonel Bill Chapman, who worked for the Mutual Weapons Development Team. Water injection was necessary in order to generate maximum thrust, if only for a limited time, and was typically used during landing, especially in high ambient temperatures.
The aircraft was initially powered by the Pegasus 6 engine which was replaced by the more powerful Pegasus 11 during the Harrier GR.1 to GR.3 upgrade process. The primary focus throughout the engine's development was on achieving high performance with as little weight as possible, tempered by the amount of funding that was available. Following the Harrier's entry to service the focus switched to improving reliability and extending engine life; a formal joint US–UK Pegasus Support Program operated for many years and spent a £3-million annual budget to develop engine improvements. Several variants have been released; the latest is the Pegasus 11–61 (Mk 107), which provides 23,800 lbf (106 kN) thrust, more than any previous engine.
Controls and handling.
The Harrier has been described by pilots as "unforgiving". The aircraft is capable of both forward flight (where it behaves in the manner of a typical fixed-wing aircraft above its stall speed), as well as VTOL and STOL manoeuvres (where the traditional lift and control surfaces are useless) requiring skills and technical knowledge usually associated with helicopters. Most services demand great aptitude and extensive training for Harrier pilots, as well as experience in piloting both types of aircraft. Trainee pilots are often drawn from highly experienced and skilled helicopter pilots.
In addition to normal flight controls, the Harrier has a lever for controlling the direction of the four vectoring nozzles. It is viewed by senior RAF officers as a significant design success, that to enable and control the aircraft's vertical flight required only a single lever added in the cockpit. For horizontal flight, the nozzles are directed rearwards by shifting the lever to the forward position; for short or vertical takeoffs and landings, the lever is pulled back to point the nozzles downwards.
The Harrier has two control elements not found in conventional fixed-wing aircraft: the thrust vector and the reaction control system. The thrust vector refers to the slant of the four engine nozzles and can be set between 0° (horizontal, pointing directly backwards) and 98° (pointing down and slightly forwards). The 90° vector is normally deployed for VTOL manoeuvring. The reaction control is achieved by manipulating the control stick and is similar in action to the cyclic control of a helicopter. While irrelevant during forward flight mode, these controls are essential during VTOL and STOL manoeuvres.
The wind direction is a critical factor in VTOL manoeuvres. The procedure for vertical takeoff involves facing the aircraft into the wind. The thrust vector is set to 90° and the throttle is brought up to maximum, at which point the aircraft leaves the ground. The throttle is trimmed until a hover state is achieved at the desired altitude. The short-takeoff procedure involves proceeding with normal takeoff and then applying a thrust vector (less than 90°) at a runway speed below normal takeoff speed; usually the point of application is around 65 kn. For lower takeoff speeds the thrust vector is greater. The reaction control system involves a series of thrusters at key points in the aircraft's fuselage and nose, also the wingtips. Thrust from the engine can be temporarily syphoned to control and correct the aircraft's pitch and roll during vertical flight.
Rotating the vectored thrust nozzles into a forward-facing position during normal flight is called vectoring in forward flight, or "VIFFing". This is a dog-fighting tactic, allowing for more sudden braking and higher turn rates. Braking could cause a chasing aircraft to overshoot and present itself as a target for the Harrier it was chasing, a combat technique formally developed by the USMC for the Harrier in the early 1970s.
Differences between versions.
The two largest users of the Harrier were the Royal Air Force and the United States Marine Corps (USMC). The exported model of the aircraft operated by the USMC was designated the AV-8A Harrier, which was broadly similar to the RAF's Harrier GR.1. Changes included the removal of all magnesium components, which corroded quickly at sea, and the integration of American radios and Identification Friend or Foe (IFF) systems; furthermore the outer pylons, unlike the RAF aircraft, were designed from delivery to be equipped with self-defence AIM-9 Sidewinder heat-seeking air-to-air missiles. Most of the AV-8As had been delivered with the more powerful Pegasus engine used in the GR.3 instead of the one used in the earlier GR.1. Two-seat Harriers were operated for training purposes; the body was stretched and a taller tail fin added. The RAF trained in the T.2 and T.4 versions, while T.4N and T.8 were training versions the Navy's Sea Harrier, with appropriate fittings. The US and Spain flew the TAV-8A and TAV-8S, respectively.
All RAF GR.1s and the initial AV-8As were fitted with the Ferranti FE541 inertial navigation/attack suite, but these were replaced in the USMC Harriers by a simpler Interface/Weapon Aiming Computer to aid quick turnaround between missions. The Martin-Baker ejection seats were also replaced by the Stencel SEU-3A in the American aircraft. The RAF had their GR.1 aircraft upgraded to the GR.3 standard, which featured improved sensors, a nose-mounted laser tracker, the integration of electronic countermeasure (ECM) systems and a further upgraded Pegasus Mk 103. The USMC upgraded their AV-8As to the AV-8C configuration; this programme involved the installation of ECM equipment and adding a new inertial navigation system to the aircraft's avionics. Substantial changes were the Lift Improvement Devices, to increase VTOL performance; at the same time several airframe components were restored or replaced to extend the life of the aircraft. Spain's Harriers, designated AV-8S or VA.1 Matador for the single-seater and TAV-8S or VAE.1 for the two-seater, were almost identical to USMC Harriers differing only in the radios fitted.
The Royal Navy's Fleet Air Arm (FAA) operated a substantially modified variant of the Harrier, the British Aerospace Sea Harrier. The Sea Harrier was not intended for ground-attack duties and, unlike the standard Harrier, was equipped with radar and Sidewinder missiles for air combat duties and fleet air defence. The Sea Harrier was also fitted with navigational aids for carrier landings, modifications to reduce corrosion and a raised bubble-canopy for greater visibility. The aircraft were later equipped to use AIM-120 AMRAAM beyond-visual-range anti-aircraft missiles and the more advanced Blue Vixen radar for longer range air-to-air combat, as well as Sea Eagle missiles for conducting anti-ship missions.
The McDonnell Douglas AV-8B Harrier II is the latest Harrier variant, a second-generation series to replace the first generation of Harrier jets already in service; all the above variants of the Harrier have mainly been retired with the Harrier II taking their place in the RAF, USMC and FAA. In the 1970s the United Kingdom considered two options for replacing their existing Harriers: joining McDonnell Douglas (MDD) in developing the BAE Harrier II, or the independent development of a "Big Wing" Harrier. This proposal would have increased the wing area from 200 to, allowing for significant increases in weapons load and internal fuel reserves. The option of cooperation with MDD was chosen in 1982 over the more risky isolated approach.
Operational history.
Royal Air Force.
The first RAF squadron to be equipped with the Harrier GR.1, No. 1 Squadron, started to convert to the aircraft at RAF Wittering in April 1969. An early demonstration of the Harrier's capabilities was the participation of two aircraft in the "Daily Mail" Transatlantic Air Race in May 1969, flying between St Pancras railway station, London and downtown Manhattan with the use of aerial refuelling. The Harrier completed the journey in 6 hours 11 minutes. Two Harrier squadrons were established in 1970 at the RAF's air base in Wildenrath to be part of its air force in Germany; another squadron was formed there two years later. In 1977, these three squadrons were moved forward to the air base at Gütersloh, closer to the prospective front line in the event of an outbreak of a European war. One of the squadrons was disbanded and its aircraft distributed between the other two.
In RAF service, the Harrier was used in close air support (CAS), reconnaissance, and other ground-attack roles. The flexibility of the Harrier led to a long-term heavy deployment in West Germany as a conventional deterrent and potential strike weapon against Soviet aggression; from camouflaged rough bases the Harrier was expected to launch attacks on advancing armour columns from East Germany. Harriers were also deployed to bases in Norway and Belize, a former British colony. No. 1 Squadron was specifically earmarked for Norwegian operations in the event of war, operating as part of Allied Forces Northern Europe. The Harrier's capabilities were necessary in the Belize deployment, as it was the only RAF combat aircraft capable of safely operating from the airport's short runway; British forces had been stationed in Belize for several years due to tensions over a Guatemalan claim to Belizean territory; the forces were withdrawn in 1993, two years after Guatemala recognized the independence of Belize.
In the Falklands War in 1982, 10 Harrier GR.3s of No. 1 Squadron operated from the aircraft carrier HMS "Hermes". As the RAF Harrier GR.3 had not been designed for naval service, the 10 aircraft had to be rapidly modified prior to the departure of the task force. Special sealants against corrosion were applied and a new deck-based inertial guidance aid was devised to allow the RAF Harrier to land on a carrier as easily as the Sea Harrier. Transponders to guide aircraft back to the carriers during night-time operations were also installed, along with flares and chaff dispensers.
As there was little space on the carriers, two requisitioned merchant container ships, "Atlantic Conveyor" and "Atlantic Causeway", were modified with temporary flight decks and used to carry Harriers and helicopters to the South Atlantic. The Harrier GR.3s focused on providing close air support to the ground forces on the Falklands and attacking Argentine positions; suppressing enemy artillery was often a high priority. Sea Harriers were also used in the war, primarily conducting fleet air defence and combat air patrols against the threat of attacking Argentine fighters. However, both Sea Harriers and Harrier GR.3s were used in ground-attack missions against the main airfield and runway at Stanley.
If most of the Sea Harriers had been lost, the GR.3s would have replaced them in air patrol duties, even though the Harrier GR.3 was not designed for air defence operations; as such the GR.3s quickly had their outboard weapons pylons modified to take air-to-air Sidewinder missiles. From 10 to 24 May 1982, prior to British forces landing in the Falklands, a detachment of three GR.3s provided air defence for Ascension Island until three F-4 Phantom IIs arrived to take on this responsibility. During the Falklands War, the greatest threats to the Harriers were deemed to be surface-to-air missiles (SAMs) and small arms fire from the ground. In total, four Harrier GR.3s and six Sea Harriers were lost to ground fire, accidents, or mechanical failure. More than 2,000 Harrier sorties were conducted during the conflict—equivalent to six sorties per day per aircraft.
Following the Falklands war, British Aerospace explored the Skyhook, a new technique to operate Harriers from smaller ships. Skyhook would have allowed the launching and landing of Harriers from smaller ships by holding the aircraft in midair by a crane; secondary cranes were to hold weapons for rapid re-arming. This would potentially have saved fuel and allowed for operations in rougher seas. The system was marketed to foreign customers, and it was speculated that Skyhook could be applied to large submarines such as the Russian Typhoon class, but the system attracted no interest.
The first generation of Harriers did not see further combat with the RAF after the Falklands War, although they continued to serve for years afterwards. As a deterrent against further Argentine invasion attempts, No. 1453 Flight RAF was deployed to the Falkland Islands from August 1983 to June 1985. However the second generation Harrier IIs saw action in Bosnia, Iraq, and Afghanistan. The first generation Hawker Siddeley airframes were replaced by the improved Harrier II, which had been developed jointly between McDonnell Douglas and British Aerospace.
United States Marine Corps.
"In my mind the AV-8A Harrier was like the helicopter in Korea. [It] had limited capability, but that's how the first-generation automobile, boat, or other major systems evolved... it brought us into the world of flexible basing and the Marine Corps into the concept of vertical development"
Major General Joe Anderson.
The United States Marine Corps began showing a significant interest in the aircraft around the time the first RAF Harrier squadron was established in 1969, and this motivated Hawker Siddeley to further develop the aircraft to encourage a purchase. Although there were concerns in Congress about multiple coinciding projects in the close air support role, the Marine Corps were enthusiastic about the Harrier and managed to overcome efforts to obstruct its procurement.
The AV-8A entered service with the Marine Corps in 1971, replacing other aircraft in the Marines' attack squadrons. The service became interested in performing ship-borne operations with the Harrier. Admiral Elmo Zumwalt promoted the concept of a Sea Control Ship, a 15,000-ton light carrier equipped with Harriers and helicopters, to supplement the larger aircraft carriers of the US Navy. An amphibious assault ship, the USS "Guam", was converted into the "Interim Sea Control Ship" and operated as such between 1971 and 1973 with the purpose of studying the limits and possible obstacles for operating such a vessel. Since then the Sea Control Ship concept has been subject to periodic re-examinations and studies, often in the light of budget cuts and questions over the use of supercarriers.
Other exercises were performed to demonstrate the AV-8A's suitability for operating from various amphibious assault ships and aircraft carriers, including a deployment of 14 Harriers aboard USS "Franklin D. Roosevelt" for six months in 1976. The tests showed, amongst other things, that the Harrier was capable of performing in weather where conventional carrier aircraft could not. In support of naval operations, the USMC devised and studied several methods to further integrate the Harrier. One result was "Arapaho", a stand-by system to rapidly convert civilian cargo ships into seagoing platforms for operating and maintaining a handful of Harriers, to be used to augment the number of available ships to deploy upon.
When the reactivation of the "Iowa"-class battleships was under consideration, a radical design for a battleship-carrier hybrid emerged that would have replaced the ship's rear turret with a flight deck, complete with a hangar and two ski jumps, for operating several Harriers. However, the USMC considered the need for naval gunfire support to be a greater priority than additional platforms for carrier operations, while the cost and delay associated with such elaborate conversions was significant, and the concept was dropped.
The Marines Corps' concept for deploying the Harriers in a land-based expeditionary role focused on aggressive speed. Harrier forward bases and light maintenance facilities were to be set up in under 24 hours on any prospective battle area. The forward bases, containing one to four aircraft, were to be located 20 mi from the forward edge of battle (FEBA), while a more established permanent airbase would be located around 50 mi from the FEBA. The close proximity of forward bases allowed for a far greater sortie rate and reduced fuel consumption.
The AV-8A's abilities in air-to-air combat were tested by the Marine Corps by conducting mock dogfights with McDonnell Douglas F-4 Phantom IIs; these exercises trained pilots to use the vectoring-in-forward-flight (VIFF) capability to outmanoeuvre their opponents and showed that the Harriers could act as effective air-to-air fighters at close range. The success of Harrier operations countered scepticism of V/STOL aircraft, which had been judged to be expensive failures in the past. Marine Corps officers became convinced of the military advantages of the Harrier and pursued extensive development of the aircraft.
Starting in 1979 the USMC began upgrading their AV-8As to the AV-8C configuration—the work focused mainly on extending useful service lives and improving VTOL performance. The AV-8C and the remaining AV-8A Harriers were retired by 1987. These were replaced by the Harrier II, designated as the AV-8B, which was introduced into service in 1985. The performance of the Harrier in USMC service led to calls for the United States Air Force to procure Harrier IIs in addition to the USMC's own plans, but these never resulted in Air Force orders. Since the late 1990s, the AV-8B has been slated to be replaced by the F-35B variant of the Lockheed Martin F-35 Lightning II, a more modern V/STOL jet aircraft.
Other operators.
Due to the Harrier's unique characteristics it attracted a large amount of interest from other nations, often as attempts to make their own V/STOL jets were unsuccessful, such as in the cases of the American XV-4 Hummingbird and the German VFW VAK 191B. Operations by the USMC aboard the USS "Nassau" in 1981 and by British Harriers and Sea Harriers in the Falklands War proved that the aircraft were highly effective in combat. These operations also demonstrated that "Harrier Carriers" provided a powerful presence at sea without the expense of big deck carriers.
Following the display of Harrier operations from small carriers, the navies of Spain and later Thailand bought the Harrier for use as their main carrier-based fixed-wing aircraft. Spain's purchase of Harriers was complicated by long-standing political friction between the British and Spanish governments of the era; even though the Harriers were manufactured in the UK they were sold to Spain with the US acting as an intermediary. Since 1976, the Spanish Navy operated the AV-8S Matador from their aircraft carrier "Dédalo" (formerly the USS "Cabot"); the aircraft provided both air defence and strike capabilities for the Spanish fleet. Spain later purchased five Harriers directly from the British government to replace losses.
Hawker Siddeley aggressively marketed the Harrier for export. At one point the company was holding talks with Australia, Brazil, Switzerland, India and Japan. Of these only India became a customer, purchasing the Sea Harrier. At one point China came very close to becoming an operator of the first generation Harrier. Following an overture by the UK in the early 1970s, when relations with the West were warming, China became interested in the aircraft as it sought to modernise its armed forces; British Prime Minister James Callaghan noted significant hostility from the USSR over the sales bid. The deal was later cancelled by the UK as part of a diplomatic backlash after China invaded Vietnam in 1979.
The Spanish Navy, Thai Navy, Royal Air Force, and United States Marine Corps have all retired their first-generation Harriers. Spain sold seven single-seat and two twin-seat Harriers to Thailand in 1998. The Royal Thai Navy's AV-8S Matadors were delivered as part of the air wing deployed on the new light aircraft carrier HTMS "Chakri Naruebet". The Thai Navy had from the start significant logistical problems keeping the Harriers operational due to a shortage of funds for spare parts and equipment, leaving only a few Harriers serviceable at a time. In 1999, two years after being delivered, only one airframe was in airworthy condition. Around 2003, Thailand considered acquiring former Royal Navy Sea Harriers, which were more suitable for maritime operations and better equipped for air defence, to replace their AV-8S Harriers; this investigation did not progress to a purchase. The last first-generation Harriers were retired by Thailand in 2006.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="14323" url="http://en.wikipedia.org/wiki?curid=14323" title="Hunt the Wumpus">
Hunt the Wumpus

Hunt the Wumpus is an early computer game, based on a simple hide and seek format featuring a mysterious monster (the Wumpus) that lurks deep inside a network of rooms. It was originally a text-based game written in BASIC. It has since been ported to various programming languages and platforms including graphical versions.
Development.
"Hunt the Wumpus" was originally written by Gregory Yob in BASIC while attending the Dartmouth campus of the University of Massachusetts in 1972 or 1973. Out of frustration with all the grid-based hunting games he had seen, such as "Snark," "Mugwump," and "Hurkle," Yob decided to create a map-based game. "Hunt the Wumpus" was first published in the "People's Computer Company" journal Vol. 2 No. 1 in mid-1973, and again in "Creative Computing" in its October, 1975, issue. This article was later reprinted in the book "The Best of Creative Computing, Volume 1". Yob later developed "Wumpus 2" and "Wumpus 3", which offered more hazards and other cave layouts.
By the release of Version 6 Unix (1975), the game had been ported to Unix C. An implementation of "Hunt the Wumpus" was typically included with MBASIC, Microsoft's BASIC interpreter for CP/M and one of the company's first products. "Hunt the Wumpus" was adapted as an early game for the Commodore PET entitled "Twonky," which was distributed in the late 1970s with "Cursor Magazine." A version of the game can still be found as part of the bsdgames package on modern BSD and Linux operating systems, where it is known as "wump."
Among the many computers it was ported to is the HP-41C calculator. The 1980 port of the game for the TI-99/4A differs quite a bit from the original while retaining the same concept. It is a graphical rather than text-based game, and uses a regular grid equivalent to a torus rather than an icosahedron. In this version, the Wumpus is depicted as a large red head with a pair of legs growing out of its sides.
Gameplay.
The original text-based version of "Hunt the Wumpus" uses a command line text interface. A player of the game enters commands to move through the rooms or to shoot "crooked arrows" along a tunnel into one of the adjoining rooms. There are twenty rooms, each connecting to three others, arranged like the vertices of a dodecahedron or the faces of an icosahedron (which are identical in layout). Hazards include bottomless pits, super bats (which drop the player in a random location, a feature duplicated in later, commercially published adventure games, such as "Zork I", "Valley of the Minotaur", and "Adventure"), and the Wumpus itself. The Wumpus is described as having sucker feet (to escape the bottomless pits) and being too heavy for a super bat to lift. When the player has deduced from hints which chamber the Wumpus is in without entering the chamber, he fires an arrow into the Wumpus's chamber to kill it. The player wins the game if he kills the Wumpus. However, firing the arrow into the wrong chamber startles the Wumpus, which may cause it to move to an adjacent room. The player loses if he or she is in the same room as the Wumpus (which then eats him or her) or a bottomless pit.

</doc>
<doc id="14340" url="http://en.wikipedia.org/wiki?curid=14340" title="Hydraulic ram">
Hydraulic ram

A hydraulic ram, or hydram, is a cyclic water pump powered by hydropower. It takes in water at one "hydraulic head" (pressure) and flow rate, and outputs water at a higher hydraulic head and lower flow rate. The device uses the water hammer effect to develop pressure that allows a portion of the input water that powers the pump to be lifted to a point higher than where the water originally started. The hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.
History.
In 1772, John Whitehurst of Cheshire, United Kingdom, invented a manually controlled precursor of the hydraulic ram called the "pulsation engine" and installed the first one at Oulton, Cheshire to raise water to a height of 4.9 m. In 1783, he installed another in Ireland. He did not patent it, and details are obscure, but it is known to have had an air vessel.
The first self-acting ram pump was invented by the Frenchman Joseph Michel Montgolfier (best known as a co-inventor of the hot air balloon) in 1796 for raising water in his paper mill at Voiron. His friend Matthew Boulton took out a British patent on his behalf in 1797. The sons of Montgolfier obtained a British patent for an improved version in 1816, and this was acquired, together with Whitehurst's design, in 1820 by Josiah Easton, a Somerset-born engineer who had just moved to London.
Easton's firm, inherited by his son James (1796–1871), grew during the nineteenth century to become one of the more important engineering manufacturers in the United Kingdom, with a large works at Erith, Kent. They specialised in water supply and sewerage systems world-wide, as well as land drainage projects. Eastons had a good business supplying rams for water supply purposes to large country houses, farms, and village communities. Some of their installations still survived as of 2004.
The firm closed in 1909, but the ram business was continued by James R Easton. In 1929, it was acquired by Green & Carter of Winchester, Hampshire, who were engaged in the manufacturing and installation of the well-known Vulcan and Vacher Rams.
The first US patent was issued to Joseph Cerneau (or Curneau) and Stephen (Étienne) S. Hallet (1755-1825) in 1809. US interest in hydraulic rams picked up around 1840, as further patents were issued and domestic companies started offering rams for sale. Toward the end of the 19th century, interest waned as electricity and electric pumps became widely available.
By the end of the twentieth century interest in hydraulic rams has revived, due to the needs of sustainable technology in developing countries, and energy conservation in developed ones. A good example is in the Philippines, who won an Ashden Award for their work developing ram pumps that could be easily maintained for use in remote villages. The hydraulic ram principle has been used in some proposals for exploiting wave power, one of which was discussed as long ago as 1931 by Hanns Günther in his book "In hundert Jahren".
Some later ram designs in the UK called compound rams were designed to pump treated water using an untreated drive water source, which overcomes some of the problems of having drinking water sourced from an open stream.
Construction and principle of operation.
A hydraulic ram has only two moving parts, a spring or weight loaded "waste" valve sometimes known as the "clack" valve and a "delivery" check valve, making it cheap to build, easy to maintain, and very reliable. In addition, there is a drive pipe supplying water from an elevated source, and a delivery pipe, taking a portion of the water that comes through the drive pipe to an elevation higher than the source.
Sequence of operation.
A simplified hydraulic ram is shown in Figure 2. Initially, the waste valve [4] is open, and the delivery valve [5] is closed. The water in the drive pipe [1] starts to flow under the force of gravity and picks up speed and kinetic energy until the increasing drag force closes the waste valve. The momentum of the water flow in the supply pipe against the now closed waste valve causes a water hammer that raises the pressure in the pump, opens the delivery valve [5], and forces some water to flow into the delivery pipe [3]. Because this water is being forced uphill through the delivery pipe farther than it is falling downhill from the source, the flow slows; when the flow reverses, the delivery check valve closes. Meanwhile, the water hammer from the closing of the waste valve also produces a pressure pulse which propagates back up the supply pipe to the source where it converts to a suction pulse that propagates back down the pipe. This suction pulse, with the weight or spring on the valve, pulls the waste valve back open and allows the process to begin again.
A pressure vessel [6] containing air cushions the hydraulic pressure shock when the waste valve closes, and it also improves the pumping efficiency by allowing a more constant flow through the delivery pipe. Although, in theory, the pump could work without it, the efficiency would drop drastically and the pump would be subject to extraordinary stresses that could shorten its life considerably. One problem is that the pressurized air will gradually dissolve into the water until none remains. One solution to this problem is to have the air separated from the water by an elastic diaphragm (similar to an expansion tank); however, this solution can be problematic in developing countries where replacements are difficult to procure. Another solution is to have a mechanism such as a snifting valve that automatically inserts a small bubble of air when the suction pulse mentioned above reaches the pump. Another solution is to insert an inner tube of a car or bicycle tire into the pressure vessel with some air in it and the valve closed. This tube is in effect the same as the diaphragm, but it is implemented with more widely available materials. The air in the tube cushions the shock of the water the same as the air in other configurations does.
Efficiency.
A typical energy efficiency is 60%, but up to 80% is possible. This should not be confused with the volumetric efficiency, which relates the volume of water delivered to total water taken from the source. The portion of water available at the delivery pipe will be reduced by the ratio of the delivery head to the supply head. Thus if the source is 2 meters above the ram and the water is lifted to 10 meters above the ram, only 20% of the supplied water can be available, the other 80% being spilled via the waste valve. These ratios assume 100% energy efficiency. Actual water delivered will be further reduced by the energy efficiency factor. In the above example, if the energy efficiency is 70%, the water delivered will be 70% of 20%, i.e. 14%. Assuming a 2-to-1 supply head to delivery head ratio and 70% efficiency, the delivered water would be 70% of 50%, i.e. 35%. Very high ratios of delivery to supply head usually result in lowered energy efficiency. Suppliers of rams often provide tables giving expected volume ratios based on actual tests.
Drive and delivery pipe design.
Since both efficiency and reliable cycling depend on water hammer effects, the drive pipe design is important. It should be between 3 and 7 times longer than the vertical distance between the source and the ram. Commercial rams may have an input fitting designed to accommodate this optimum slope. The diameter of the supply pipe would normally match the diameter of the input fitting on the ram, which in turn is based on its pumping capacity. The drive pipe should be of constant diameter and material, and should be as straight as possible. Where bends are necessary, they should be smooth, large diameter curves. Even a large spiral is allowed, but elbows are to be avoided. PVC will work in some installations, but steel pipe is preferred, although much more expensive. If valves are used they should be a free flow type such as a ball valve or gate valve.
The delivery pipe is much less critical since the pressure vessel prevents water hammer effects from traveling up it. Its overall design would be determined by the allowable pressure drop based on the expected flow. Typically the pipe size will be about half that of the supply pipe, but for very long runs a larger size may be indicated. PVC pipe and any necessary valves are not a problem.
Starting operation.
A ram newly placed into operation or which has stopped cycling must be started as follows. If the waste valve is in the raised (closed) position, which is most common, it must be pushed down manually into the open position and released. If the flow is sufficient, it will then cycle at least once. If it does not continue to cycle, it must be pushed down repeatedly until it cycles continuously on its own, usually after three or four manual cycles. If the ram stops with the waste valve in the down position it must be lifted manually and kept up for as long as necessary for the supply pipe to fill with water and for any air bubbles to travel up the pipe to the source. This may take a minute or more. Then it can be started manually by pushing it down a few times as described above. Having a valve on the delivery pipe at the ram makes starting easier. Close the valve until the ram starts cycling, then gradually open it to fill the delivery pipe. If opened too quickly it will stop the cycling. Once the delivery pipe is full the valve can be left open.
Common operational problems.
Failure to deliver sufficient water may be due to improper adjustment of the waste valve, having too little air in the pressure vessel, or simply attempting to raise the water higher than the level of which the ram is capable.
The ram may be damaged by freezing in winter, or loss of air in the pressure vessel leading to excess stress on the ram parts. These failures will require welding or other repair methods and perhaps parts replacement.
It is not uncommon for an operating ram to require occasional restarts. The cycling may stop due to poor adjustment of the waste valve, or insufficient water flow at the source. Air can enter if the supply water level is not at least a few inches above the input end of the supply pipe. Other problems are blockage of the valves with debris, or improper installation, such as using a supply pipe of non uniform diameter or material, having sharp bends or a rough interior, or one that is too long or short for the drop, or is made of an insufficiently rigid material. A PVC supply pipe will work in some installations but is not as optimal as steel.
Water-powered pump.
An alternative to the hydraulic ram is the water-powered pump. It can be used if a high flow rate at high head ratio is required. A water-powered pump unit is a hydraulic turbine coupled to a water pump. The motive power needed by the pump is generated by the hydraulic turbine from the available low head water energy.
Further reading.
</dl>

</doc>
<doc id="14348" url="http://en.wikipedia.org/wiki?curid=14348" title="Homo habilis">
Homo habilis

Homo habilis is a species of the "Hominini" tribe, during the Gelasian and early Calabrian stages of the Pleistocene period, between roughly 2.8 and 1.5 million years ago.
Classification as "Homo".
There has been scholarly debate regarding its placement in the genus "Homo" rather than the genus "Australopithecus". The small size and rather primitive attributes have led some experts (Richard Leakey among them) to propose excluding "H. habilis" from the genus "Homo" and placing them instead in "Australopithecus" as "Australopithecus habilis". 
In its appearance and morphology, "H. habilis" is the least similar to modern humans of all species in the genus "Homo" (except the equally controversial "H. rudolfensis"), and its classification as "Homo" has been the subject of controversial debate since its first proposal in the 1960s. "H. habilis" was short and had disproportionately long arms compared to modern humans; however, it had a less protruding face than the australopithecines from which it is thought to have descended. "H. habilis" had a cranial capacity slightly less than half of the size of modern humans. Despite the ape-like morphology of the bodies, "H. habilis" remains are often accompanied by primitive stone tools (e.g. Olduvai Gorge, Tanzania and Lake Turkana, Kenya). 
"Homo habilis" has often been thought to be the ancestor of the more gracile and sophisticated "Homo ergaster", which in turn gave rise to the more human-appearing species, "Homo erectus". Debates continue over whether all of the known fossils are properly attributed to the species, and some paleoanthropologists regard the taxon as invalid, made up of fossil specimens of "Australopithecus" and "Homo". New findings in 2007 seemed to confirm the view that "H. habilis" and "H. erectus" coexisted, representing separate lineages from a common ancestor instead of "H. erectus" being descended from "H. habilis". An alternative explanation would be that any ancestral relationship from "H. habilis" to "H. erectus" would have to have been cladogenetic rather than anagenetic (meaning that if an isolated subgroup population of "H. habilis" became the ancestor of "H. erectus", other subgroups remained as unchanged "H. habilis" until their much later extinction).
Its brain size has been shown to range from 550 cm3 to 687 cm3, rather than from 363 cm3 to 600 cm3 as formerly thought. 
Newly published (as of 2015) virtual reconstruction estimated the endocranial volume at between 729 and 824 ml it is larger than any previously published value.
Compared to australopithecines, "H. habilis"' brain capacity of around 640 cm³ was on average 50% larger than australopithecines, but considerably smaller than the 1350 to 1450 cm³ range of modern "Homo sapiens". These hominins were smaller than modern humans, on average standing no more than 1.3 m (4 ft 3 in) tall.
A fragment of fossilized jawbone, dated to around 2.8 million years ago, was discovered in the Ledi-Geraru research area in Afar Regional State in 2013. 
The fossil is considered the earliest evidence of the "Homo" genus known to date, and seems to be intermediate between "Australopithecus" and "H. habilis". The individual in question lived just after a major climate shift in the region, when forests and waterways were rapidly replaced by arid savannah.
Fossils.
OH 62.
One set of fossil remains (OH 62), discovered by Donald Johanson and Tim White in Olduvai Gorge in 1986, included the important upper and lower limbs. Their finding stimulated some debate at the time. 
 
KNM ER 1813.
KNM ER 1813 is a relatively complete cranium which dates to 1.9 million years old, discovered at Koobi Fora, Kenya by Kamoya Kimeu in 1973. The brain capacity is 510 cm³, not as impressive as other early specimen and forms of "H. habilis" discovered.
OH 24.
OH 24 ("Twiggy") is a roughly deformed cranium about 1.8 million years old discovered in October 1968 at Olduvai Gorge, Tanzania. The brain volume is just under 600 cm³; also a reduction in a protruding face is present compared to members of more primitive australopithecines.
OH 7.
OH 7 dates to 1.75 million years old, and was discovered by Mary and Louis Leakey on November 4, 1960 at Olduvai Gorge, Tanzania. It is a lower jaw complete with teeth; due to the size of the small teeth, researchers estimate this juvenile individual had a brain volume of 363 cm³. Also found were more than 20 fragments of the left hand. Tobias and Napier assisted in classifying OH 7 as the type fossil.
KNM ER 1805.
KNM ER 1805 is a specimen of an adult "H. habilis" made of three pieces of cranium dating to 1.74 million years old from Koobi Fora, Kenya. Previous assumptions were that this specimen belongs to "H. erectus" based on the degree of prognathism and overall cranial shape.
Interpretations.
"Homo habilis" is thought to have mastered the Lower Paleolithic Olduwan tool set which used stone flakes. These stone flakes were more advanced than any tools previously used, and gave "H. habilis" the edge it needed to prosper in hostile environments previously too formidable for primates. Whether "H. habilis" was the first hominid to master stone tool technology remains controversial, as "Australopithecus garhi", dated to 2.6 million years ago, has been found along with stone tool implements.
Most experts assume the intelligence and social organization of "H. habilis" were more sophisticated than typical australopithecines or chimpanzees. "H. habilis" used tools primarily for scavenging, such as cleaving meat off carrion, rather than defense or hunting. Yet despite tool usage, "H. habilis" was not the master hunter its sister species (or descendants) proved to be, as ample fossil evidence indicates "H. habilis" was a staple in the diet of large predatory animals, such as "Dinofelis", a large scimitar-toothed predatory cat the size of a jaguar.
"Homo habilis" coexisted with other "Homo"-like bipedal primates, such as "Paranthropus boisei", some of which prospered for many millennia. However, "H. habilis", possibly because of its early tool innovation and a less specialized diet, became the precursor of an entire line of new species, whereas "Paranthropus boisei" and its robust relatives disappeared from the fossil record. "H. habilis" may also have coexisted with "H. erectus" in Africa for a period of 500,000 years.
Evolutionary biologist Jeremy Griffith has drawn parallels between "H. habilis" and the psychological developmental evolution of modern humans as a manifestation of Ernst Haeckel's theory of ontogeny being a summarised recapitulation of phylogeny, suggesting elements of the phenotype of "H. habilis" relate to early adolescence (12-13 years of age) in modern humans.

</doc>
<doc id="14403" url="http://en.wikipedia.org/wiki?curid=14403" title="Hydrogen peroxide">
Hydrogen peroxide

Hydrogen peroxide is a chemical compound with the formula H2O2. In its pure form it is a colorless liquid, slightly more viscous than water; however, for safety reasons it is normally used as an aqueous solution. Hydrogen peroxide is the simplest peroxide (a compound with an oxygen-oxygen single bond) and finds use as a strong oxidizer, bleaching agent and disinfectant. Concentrated hydrogen peroxide, or 'high-test peroxide,' is a reactive oxygen species and has been used as a propellant in rocketry.
Hydrogen peroxide is often described as being “water but with one more oxygen atom”, a description which can give the incorrect impression that there is a great deal of similarity between the two compounds. Pure hydrogen peroxide will explode if heated to boiling, will cause serious contact burns to the skin and can set materials alight on contact. For these reasons it is usually handled as a dilute solution (household grades are typically 3-6%). Its chemistry is dominated by the nature of its unstable peroxide bond.
Structure and properties.
Properties.
The boiling point of H2O2 has been extrapolated as being 150.2 °C, approximately 50 degrees higher than water; in practice hydrogen peroxide will undergo potentially explosive thermal decomposition if heated to this temperature. It may be safely distilled under reduced pressure.
In aqueous solutions.
In aqueous solutions hydrogen peroxide differs from the pure material due to the effects of hydrogen bonding between water and hydrogen peroxide molecules. Hydrogen peroxide and water form a eutectic mixture, exhibiting freezing-point depression; pure water has a melting point of 0 °C and pure hydrogen peroxide of −0.43 °C, but a 50% (by volume) solution of the two freezes at -51 °C. The boiling point of the same mixtures is also depressed in relation with the median of both boiling points (125.1 °C). It occurs at 114 °C. This boiling point is 14° greater than that of pure water and 36.2° less than that of pure hydrogen peroxide.
Structure.
Hydrogen peroxide (H2O2) is a nonplanar molecule with (twisted) C2 symmetry. Although the O−O bond is a single bond, the molecule has a relatively high barrier to rotation of 2460 cm−1 (29.45 kJ/mol); for comparison, the rotational barrier for ethane is 12.5 kJ/mol. The increased barrier is ascribed to repulsion between the lone pairs of the adjacent oxygen atoms and results in hydrogen peroxide displaying atropisomerism.
The molecular structures of gaseous and crystalline H2O2 are significantly different. This difference is attributed to the effects of hydrogen bonding, which is absent in the gaseous state. Crystals of H2O2 are tetragonal with the space group formula_1.
Comparison with analogues.
Hydrogen peroxide has several structural analogues with Hm-E-E-Hn bonding arrangements (Water also shown for comparison). It has the highest (theoretical) boiling point of this series (X = O, N, S). Its melting point is also fairly high, being comparable to that of hydrazine and water, with only hydroxylamine crystallising significantly more readily, indicative of particularly strong hydrogen bonding. Diphosphane and hydrogen disulfide exhibit only weak hydrogen bonding and have little chemical similarity to hydrogen peroxide. All of these analogues are thermodynamically unstable. Structurally, the analogues all adopt similar skewed structures, due to repulsion between adjacent lone pairs.
Discovery.
Hydrogen peroxide was first described in 1818 by Louis Jacques Thénard, who produced it by treating barium peroxide with nitric acid. An improved version of this process used hydrochloric acid, followed by addition of sulfuric acid to precipitate the barium sulfate byproduct. Thénard's process was used from the end of the 19th century until the middle of the 20th century.
Pure hydrogen peroxide was long believed to be unstable as early attempts to separate it from the water, which is present during synthesis, all failed. This instability was due to traces of impurities (transition metals salts) which catalyze the decomposition of the hydrogen peroxide. Pure hydrogen peroxide was first obtained in 1894 — almost 80 years after its discovery — by Richard Wolffenstein, who produced it via vacuum distillation.
Determination of the molecular structure of hydrogen peroxide proved to be very difficult. In 1892 the Italian physical chemist Giacomo Carrara (1864–1925) determined its molecular weight by freezing point depression, which confirmed that its molecular formula is H2O2. At least half a dozen hypothetical molecular structures seemed to be consistent with the available evidence. In 1934, the English mathematical physicist William Penney and the Scottish physicist Gordon Sutherland proposed a molecular structure for hydrogen peroxide which was very similar to the presently accepted one.
Manufacture.
Previously, hydrogen peroxide was prepared industrially by hydrolysis of the ammonium peroxydisulfate, which was itself obtained via the electrolysis of a solution of ammonium bisulfate (NH4HSO4) in sulfuric acid.
Today, hydrogen peroxide is manufactured almost exclusively by the anthraquinone process, which was formalized in 1936 and patented in 1939. It begins with the reduction of an anthraquinone (such as 2-ethylanthraquinone or the 2-amyl derivative) to the corresponding anthrahydroquinone, typically via hydrogenation on a palladium catalyst; the anthrahydroquinone then undergoes to autoxidation to regenerate the starting anthraquinone, with hydrogen peroxide being produced as a by-product. Most commercial processes achieve oxidation by bubbling compressed air through a solution of the derivatized anthracene, whereby the oxygen present in the air reacts with the labile hydrogen atoms (of the hydroxy group), giving hydrogen peroxide and regenerating the anthraquinone. Hydrogen peroxide is then extracted and the anthraquinone derivative is reduced back to the dihydroxy (anthracene) compound using hydrogen gas in the presence of a metal catalyst. The cycle then repeats itself.
The simplified overall equation for the process is deceptively simple:
The economics of the process depend heavily on effective recycling of the quinone (which is expensive) and extraction solvents, and of the hydrogenation catalyst.
A process to produce hydrogen peroxide directly from the elements has been of interest for many years. Direct synthesis is difficult to achieve as, in terms of thermodynamics, the reaction of hydrogen with oxygen favours production of water. Systems for direct synthesis have been developed, most of which are based around finely dispersed metal catalysts. None of these has yet reached a point where they can be used for industrial-scale synthesis.
Availability.
Hydrogen peroxide is most commonly available as a solution in water. For consumers, it is usually available from pharmacies at 3 and 6 wt% concentrations. The concentrations are sometimes described in terms of the volume of oxygen gas generated; one milliliter of a 20-volume solution generates twenty milliliters of oxygen gas when completely decomposed. For laboratory use, 30 wt% solutions are most common. Commercial grades from 70% to 98% are also available, but due to the potential of solutions of more than 68% hydrogen peroxide to be converted entirely to steam and oxygen (with the temperature of the steam increasing as the concentration increases above 68%) these grades are potentially far more hazardous, and require special care in dedicated storage areas. Buyers must typically allow inspection by commercial manufacturers.
In 1994, world production of H2O2 was around 1.9 million tonnes and grew to 2.2 million in 2006, most of which was at a concentration of 70% or less. In that year bulk 30% H2O2 sold for around US $0.54 per kg, equivalent to US $1.50 per kg (US $0.68 per lb) on a "100% basis".
Reactions.
Decomposition.
Hydrogen peroxide is thermodynamically unstable and decomposes to form water and oxygen with a Δ"H"o of −98.2 kJ·mol−1 and a ΔS of 70.5 J·mol−1·K−1.
The rate of decomposition increases with rising temperature, concentration and pH, with cool, dilute, acidic solutions showing the best stability. Decomposition is catalysed by various compounds, including most transition metals and their compounds (e.g. manganese dioxide, silver, and platinum). Certain metal ions, such as Fe2+ or Ti3+, can cause the decomposition to take a different path, with free radicals such as (HO·) and (HOO·) being formed.
Non-metallic catalysts include potassium iodide, which reacts particularly rapidly and forms the basis of the elephant toothpaste experiment. Hydrogen peroxide can also be decomposed biologically by enzyme catalase.
The decomposition of hydrogen peroxide liberates oxygen and heat; this can be dangerous as spilling high concentrations of hydrogen peroxide on a flammable substance can cause an immediate fire.
Redox reactions.
Hydrogen peroxide exhibits oxidizing and reducing properties, depending on pH.
In acidic solutions, H2O2 is one of the most powerful oxidizers known—stronger than chlorine, chlorine dioxide, and potassium permanganate. Also, through catalysis, H2O2 can be converted into hydroxyl radicals (•OH), which are highly reactive.
In acidic solutions Fe2+ is oxidized to Fe3+ (hydrogen peroxide acting as an oxidizing agent),
and sulfite (SO32−) is oxidized to sulfate (SO42−). However, potassium permanganate is reduced to Mn2+ by acidic H2O2. Under alkaline conditions, however, some of these reactions reverse; for example, Mn2+ is oxidized to Mn4+ (as MnO2).
In basic solution, hydrogen peroxide can reduce a variety of inorganic ions. When it acts as a reducing agent, oxygen gas is also produced. For example hydrogen peroxide will reduce sodium hypochlorite and potassium permanganate, which is a convenient method for preparing oxygen in the laboratory.
Organic reactions.
Hydrogen peroxide is frequently used as an oxidizing agent. Illustrative is oxidation of thioethers to sulfoxides.
Alkaline hydrogen peroxide is used for epoxidation of electron-deficient alkenes such as acrylic acid derivatives, and for the oxidation of alkylboranes to alcohols, the second step of hydroboration-oxidation. It is also the principal reagent in the Dakin oxidation process.
Precursor to other peroxide compounds.
Hydrogen peroxide is a weak acid, forming hydroperoxide or peroxide salts with many metals.
It also converts metal oxides into the corresponding peroxides. For example, upon treatment with hydrogen peroxide, chromic acid (CrO3) form an unstable blue peroxide CrO(O2)2.
This kind of reaction is used industrially to produce peroxoanions. For example, reaction with borax leads to sodium perborate, a bleach used in laundry detergents:
H2O2 converts carboxylic acids (RCO2H) into peroxy acids (RC(O)O2H), which are themselves used as oxidizing agents. Hydrogen peroxide reacts with acetone to form acetone peroxide, and it interacts with ozone to form hydrogen trioxide, also known as trioxidane. Reaction with urea produces the adduct hydrogen peroxide – urea, used for whitening teeth. An acid-base adduct with triphenylphosphine oxide is a useful "carrier" for H2O2 in some reactions.
Biological function.
Hydrogen peroxide is also one of the two chief chemicals in the defense system of the bombardier beetle, reacting with hydroquinone to discourage predators.
A study published in "Nature" found that hydrogen peroxide plays a role in the immune system. Scientists found that hydrogen peroxide presence inside cells increased after tissues are damaged in zebra fish, which is thought to act as a signal to white blood cells to converge on the site and initiate the healing process. When the genes required to produce hydrogen peroxide were disabled, white blood cells did not accumulate at the site of damage. The experiments were conducted on fish; however, because fish are genetically similar to humans, the same process is speculated to occur in humans. The study in "Nature" suggested asthma sufferers have higher levels of hydrogen peroxide in their lungs than healthy people, which could explain why asthma sufferers have inappropriate levels of white blood cells in their lungs.
Hydrogen peroxide has important roles as a signaling molecule in the regulation of a wide variety of biological processes. The compound is a major factor implicated in the free-radical theory of aging, based on how readily hydrogen peroxide can decompose into a hydroxyl radical and how superoxide radical byproducts of cellular metabolism can react with ambient water to form hydrogen peroxide. These hydroxyl radicals in turn readily react with and damage vital cellular components, especially those of the mitochondria. At least one study has also tried to link hydrogen peroxide production to cancer. These studies have frequently been quoted in fraudulent treatment claims.
The amount of hydrogen peroxide in biological systems can be assayed using a fluorimetric assay.
Applications.
Industrial.
About 60% of the world's production of hydrogen peroxide is used for pulp- and paper-bleaching.
The second major industrial application is the manufacture of sodium percarbonate and sodium perborate which are used as mild bleaches in laundry detergents.
It is used in the production of various organic peroxides with dibenzoyl peroxide being a high volume example. This is used in polymerisations, as a flour bleaching agent and as a treatment for acne. Peroxy acids, such as peracetic acid and meta-chloroperoxybenzoic acid are also typically produced using hydrogen peroxide.
Hydrogen peroxide is used in certain waste-water treatment processes to remove organic impurities. This is achieved by advanced oxidation processes, such as the Fenton reaction, which use it to generate highly reactive hydroxyl radicals (·OH). These are able to destroy organic contaminates which are ordinarily difficult to remove, such as aromatic or halogenated compounds. It can also oxidize sulphur based compounds present in the waste; which is beneficial as it generally reduces their odour.
Medical.
Disinfectant.
Hydrogen peroxide can be used for the sterilization of various surfaces, including surgical tools and may be deployed as a vapor (VHP) for room sterilization. H2O2 demonstrates broad-spectrum efficacy against viruses, bacteria, yeasts, and bacterial spores. In general, greater activity is seen against gram-positive than gram-negative bacteria; however, the presence of catalase or other peroxidases in these organisms can increase tolerance in the presence of lower concentrations. Higher concentrations of H2O2 (10 to 30%) and longer contact times are required for sporicidal activity.
Hydrogen peroxide is seen as an environmentally safe alternative to chlorine-based bleaches, as it degrades to form oxygen and water and it is generally recognized as safe as an antimicrobial agent by the U.S. Food and Drug Administration (FDA).
Historically hydrogen peroxide was used for disinfecting wounds, partly because of its low cost and prompt availability compared to other antiseptics. It is now thought to slow healing and lead to scarring because it destroys newly formed skin cells. Only a very low concentration of H2O2 can induce healing, and only if not repeatedly applied. Surgical use can lead to gas embolism formation. Despite this it is still used for wound treatment in many developing countries.
It is absorbed by skin upon contact and creates a local capillary embolism that appears as a temporary whitening of the skin.
Cosmetic applications.
Diluted H2O2 (between 1.9% and 12%) mixed with ammonium hydroxide is used to bleach human hair. The chemical's bleaching property lends its name to the phrase "peroxide blonde".
Hydrogen peroxide is also used for tooth whitening and can be mixed with baking soda and salt to make a home-made toothpaste.
Hydrogen peroxide may be used to treat acne, although benzoyl peroxide is a more common treatment.
Use in alternative medicine.
Practitioners of alternative medicine have advocated the use of hydrogen peroxide for the treatment of various conditions, including emphysema, influenza, AIDS and in particular cancer. The practise calls for the daily consumption of hydrogen peroxide, either orally or by injection and is, in general, based around two precepts. Firstly that hydrogen peroxide is naturally produced by the body to combat infection. Secondly, that human pathogens (including cancer: See Warburg hypothesis) are anaerobic and cannot survive in oxygen-rich environments. The ingestion or injection of hydrogen peroxide is therefore believed to kill disease by mimicking the immune response in addition to increasing levels of oxygen within the body. This makes it similar to other oxygen-based therapies, such as ozone therapy and hyperbaric oxygen therapy.
Both the effectiveness and safety of hydrogen peroxide therapy is disputed by mainstream scientists. Hydrogen peroxide is produced by the immune system but in a carefully controlled manner. Cells called by phagocytes engulf pathogens and then use hydrogen peroxide to destroy them. The peroxide is toxic to both the cell and the pathogen and so is kept within a special compartment, called a phagosome. Free hydrogen peroxide will damage any tissue it encounters via oxidative stress; a process which also has been proposed as a cause of cancer.
Claims that hydrogen peroxide therapy increase cellular levels of oxygen have not been supported. The quantities administered would be expected to provide very little additional oxygen compared to that available from normal respiration. It should also be noted that it is difficult to raise the level of oxygen around cancer cells within a tumour, as the blood supply tends to be poor, a situation known as tumour hypoxia.
Large oral doses of hydrogen peroxide at a 3% concentration may cause irritation and blistering to the mouth, throat, and abdomen as well as abdominal pain, vomiting, and diarrhea.
Intravenous injection of hydrogen peroxide has been linked to several deaths.
The American Cancer Society states that "there is no scientific evidence that hydrogen peroxide is a safe, effective or useful cancer treatment" The therapy is not approved by the U.S. FDA.
Propellant.
High concentration H2O2 is referred to as High Test Peroxide (HTP). It can be used either as a monopropellant (not mixed with fuel) or as the oxidizer component of a bipropellant rocket. Use as a monopropellant takes advantage of the decomposition of 70–98+% concentration hydrogen peroxide into steam and oxygen. The propellant is pumped into a reaction chamber where a catalyst, usually a silver or platinum screen, triggers decomposition, producing steam at over 600 °C (1,112 °F), which is expelled through a nozzle, generating thrust. H2O2 monopropellant produces a maximum specific impulse ("I"sp) of 161 s (1.6 kN·s/kg). Peroxide was the first major monopropellant adopted for use in rocket applications. Hydrazine eventually replaced hydrogen peroxide monopropellant thruster applications primarily because of a 25% increase in the vacuum specific impulse. Hydrazine (toxic) and hydrogen peroxide (non-toxic) are the only two monopropellants (other than cold gases) to have been widely adopted and utilized for propulsion and power applications. The Bell Rocket Belt, reaction control systems for X-1, X-15, Centaur, Mercury, Little Joe as well as the turbo-pump gas generators for X-1, X-15, Jupiter, Redstone and Viking used hydrogen peroxide as a monopropellant.
As a bipropellant H2O2 is decomposed to burn a fuel as an oxidizer. Specific impulses as high as 350 s (3.5 kN·s/kg) can be achieved, depending on the fuel. Peroxide used as an oxidizer gives a somewhat lower "I"sp than liquid oxygen, but is dense, storable, noncryogenic and can be more easily used to drive gas turbines to give high pressures using an efficient "closed cycle". It can also be used for regenerative cooling of rocket engines. Peroxide was used very successfully as an oxidizer in World War II German rocket motors (e.g. T-Stoff, containing oxyquinoline stabilizer, for the Me 163B), most often used with C-Stoff in a self-igniting hypergolic combination, and for the low-cost British Black Knight and Black Arrow launchers. 
In the 1940s and 1950s, the Walter turbine used hydrogen peroxide for use in submarines while submerged; it was found to be too noisy and require too much maintenance compared to diesel-electric power systems. Some torpedoes used hydrogen peroxide as oxidizer or propellant. Operator error in the use of hydrogen peroxide torpedoes were named as possible causes for the sinkings of HMS "Sidon" and the Russian submarine "Kursk". SAAB Underwater Systems is manufacturing the Torpedo 2000. This torpedo, used by the Swedish navy, is powered by a piston engine propelled by HTP as an oxidizer and kerosene as a fuel in a bipropellant system.
Explosives.
Hydrogen peroxide has been used for creating organic peroxide based explosives, such as acetone peroxide, for improvised explosive devices, including the 7 July 2005 London bombings. These explosives tend to degrade quickly and hence are not used as commercial or military explosives.
Other uses.
Hydrogen peroxide has various domestic uses, primarily as a cleaning and disinfecting agent.
Hydrogen peroxide reacts with esters, such as and phenyl oxalate ester (cyalume), to produce chemiluminescence; this application is most commonly encountered in the form of glow sticks.
Some horticulturalists and users of hydroponics advocate the use of weak hydrogen peroxide solution in watering solutions. Its spontaneous decomposition releases oxygen that enhances a plant's root development and helps to treat root rot (cellular root death due to lack of oxygen) and a variety of other pests.
Laboratory tests conducted by fish culturists in recent years have demonstrated that common household hydrogen peroxide can be used safely to provide oxygen for small fish. The hydrogen peroxide releases oxygen by decomposition when it is exposed to catalysts such as manganese dioxide.
Safety.
Regulations vary, but low concentrations, such as 6%, are widely available and legal to buy for medical use. Most over-the-counter peroxide solutions are not suitable for ingestion. Higher concentrations may be considered hazardous and are typically accompanied by a Material Safety Data Sheet (MSDS). In high concentrations, hydrogen peroxide is an aggressive oxidizer and will corrode many materials, including human skin. In the presence of a reducing agent, high concentrations of H2O2 will react violently.
High-concentration hydrogen peroxide streams, typically above 40%, should be considered hazardous due to concentrated hydrogen peroxide's meeting the definition of a DOT oxidizer according to U.S. regulations, if released into the environment. The EPA Reportable Quantity (RQ) for D001 hazardous wastes is 100 lb, or approximately 10 USgal, of concentrated hydrogen peroxide.
Hydrogen peroxide should be stored in a cool, dry, well-ventilated area and away from any flammable or combustible substances. It should be stored in a container composed of non-reactive materials such as stainless steel or glass (other materials including some plastics and aluminium alloys may also be suitable). Because it breaks down quickly when exposed to light, it should be stored in an opaque container, and pharmaceutical formulations typically come in brown bottles that filter out light.
Hydrogen peroxide, either in pure or diluted form, can pose several risks, the main one being that it forms explosive mixtures upon contact with organic compounds. Highly concentrated hydrogen peroxide itself is unstable, and can then cause a boiling liquid expanding vapor explosion (BLEVE) of the remaining liquid. Distillation of hydrogen peroxide at normal pressures is thus highly dangerous. It is also corrosive especially when concentrated but even domestic-strength solutions can cause irritation to the eyes, mucous membranes and skin. Swallowing hydrogen peroxide solutions is particularly dangerous, as decomposition in the stomach releases large quantities of gas (10 times the volume of a 3% solution) leading to internal bleeding. Inhaling over 10% can cause severe pulmonary irritation.
With a significant vapor pressure (1.2 kPa at 50 °C[CRC Handbook of Chemistry and Physics, 76th Ed, 1995–1996]), hydrogen peroxide vapor is potentially hazardous. According to U.S. NIOSH, the Immediately Dangerous to Life and Health (IDLH) limit is only 75 ppm. The U.S. Occupational Safety and Health Administration (OSHA) has established a permissible exposure limit of 1.0 ppm calculated as an eight-hour time weighted average (29 CFR 1910.1000, Table Z-1) and hydrogen peroxide has also been classified by the American Conference of Governmental Industrial Hygienists (ACGIH) as a "known animal carcinogen, with unknown relevance on humans." For workplaces where there is a risk of exposure to the hazardous concentrations of the vapors, continuous monitors for hydrogen peroxide should be used. Information on the hazards of hydrogen peroxide is available from OSHA and from the ATSDR.
References.
Notes
Bibliography
</dl>

</doc>
<doc id="14646" url="http://en.wikipedia.org/wiki?curid=14646" title="Politics of Indonesia">
Politics of Indonesia

Politics of Indonesia takes place in a framework of a presidential representative democratic republic, whereby the President of Indonesia is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the two People's Representative Councils. The judiciary is independent of the executive and the legislature.
The 1945 constitution provided for a limited separation of executive, legislative and judicial power. The governmental system has been described as "presidential with parliamentary characteristics." Following the Indonesian riots of May 1998 and the resignation of President Suharto, several political reforms were set in motion via amendments to the Constitution of Indonesia, which resulted in changes to all branches of government.
History.
The Reformation.
A constitutional reform process lasted from 1999 to 2002, with four constitutional amendments producing important changes.
Among these are term limits of up to 2 five-year terms for the President and Vice President, and measures to institute checks and balances. The highest state institution is the People's Consultative Assembly (MPR), whose functions previously included electing the president and vice president (since 2004 the president has been elected directly by the people), establishing broad guidelines of state policy, and amending the constitution. The 695-member MPR includes all 550 members of the People's Representative Council (DPR) (the House of Representatives) plus 130 "regional representatives" elected by the twenty-six provincial parliaments and sixty-five appointed members from societal groups
The DPR, which is the premier legislative institution, originally included 462 members elected through a mixed proportional/district representational system and thirty-eight appointed members of the armed forces (TNI) and police (POLRI). TNI/POLRI representation in the DPR and MPR ended in 2004. Societal group representation in the MPR was eliminated in 2004 through further constitutional change.
Having served as rubberstamp bodies in the past, the DPR and MPR have gained considerable power and are increasingly assertive in oversight of the executive branch. Under constitutional changes in 2004, the MPR became a bicameral legislature, with the creation of the Dewan Perwakilan Daerah (DPD), in which each province is represented by four members, although its legislative powers are more limited than those of the DPR. Through his appointed cabinet, the president retains the authority to conduct the administration of the government.
A general election in June 1999 produced the first freely elected national, provincial and regional parliaments in over forty years. In October 1999 the MPR elected a compromise candidate, Abdurrahman Wahid, as the country's fourth president, and Megawati Sukarnoputri — a daughter of Sukarno, the country's first president — as the vice president. Megawati's PDI-P party had won the largest share of the vote (34%) in the general election, while Golkar, the dominant party during the Soeharto era, came in second (22%). Several other, mostly Islamic parties won shares large enough to be seated in the DPR. Further democratic elections took place in 2004 and 2009.
Executive branch.
The president and vice president are selected by vote of the citizens for five-year terms. Prior to 2004, they were chosen by People's Consultative Assembly. The last election was held July 9, 2014. The president heads the Kabinet Kerja.
The President of Indonesia is directly elected for a maximum of two five-year terms, and is the head of state, commander-in-chief of Indonesian armed forces and responsible for domestic governance and policy-making and foreign affairs. The president appoints a cabinet, who do not have to be elected members of the legislature.
Legislative branch.
The People's Consultative Assembly (Indonesian: "Majelis Permusyawaratan Rakyat", "MPR") is the legislative branch in Indonesia's political system. Following elections in 2004, the MPR became a bicameral parliament, with the creation of the DPD as its second chamber in an effort to increase regional representation. The Regional Representatives Council (Indonesian: "Dewan Perwakilan Daerah", "DPD") is the upper house of The People's Consultative Assembly. The lower house is The People's Representative Council (Indonesian: "Dewan Perwakilan Rakyat", "DPR"), sometimes referred to as the House of Representatives, which has 550 members, elected for a five-year term by proportional representation in multi-member constituencies.
Political parties and elections.
The General Elections Commission (Indonesian: "Komisi Pemilihan Umum", "KPU ") is the body responsible for running both parliamentary and presidential elections in Indonesia. Article 22E(5) of the Constitution rules that the Commission is national, permanent, and independent. Prior to the General Election of 2004, KPU was made up of members who were also members of political parties. However, members of KPU must now be non-partisan.
Judicial branch.
The Indonesian Supreme Court (Indonesian: "Mahkamah Agung") is the highest level of the judicial branch. Its judges are appointed by the president. The Constitutional Court rules on constitutional and political matters (Indonesian: "Mahkamah Konstitusi"), while a Judicial Commission (Indonesian: "Komisi Yudisial") oversees the judges.
Foreign relations.
During the regime of president Suharto, Indonesia built strong relations with the United States and had difficult relations with the People's Republic of China owing to Indonesia's anti-communist policies and domestic tensions with the Chinese community. It received international denunciation for its annexation of East Timor in 1978. Indonesia is a founding member of the Association of South East Asian Nations, and thereby a member of both ASEAN+3 and the East Asia Summit. Since the 1980s, Indonesia has worked to develop close political and economic ties between South East Asian nations, and is also influential in the Organisation of Islamic Cooperation. Indonesia was heavily criticized between 1975 and 1999 for allegedly suppressing human rights in East Timor, and for supporting violence against the East Timorese following the latter's secession and independence in 1999. Since 2001, the government of Indonesia has co-operated with the U.S. in cracking down on Islamic fundamentalism and terrorist groups.

</doc>
<doc id="14687" url="http://en.wikipedia.org/wiki?curid=14687" title="Geography of Israel">
Geography of Israel

The geography of Israel is very diverse, with desert conditions in the south, and snow-capped mountains in the north. Israel is located at at the eastern end of the Mediterranean Sea in western Asia. It is bounded to the north by Lebanon, the northeast by Syria, the east by Jordan and the West Bank, and to the southwest by Egypt. To the west of Israel is the Mediterranean Sea, which makes up the majority of Israel's 273 km coastline and the Gaza Strip. Israel has a small coastline on the Red Sea in the south.
Israel's area is approximately 20770 km2, which includes 445 km2 of inland water. Israel stretches 424 km from north to south, and its width ranges from 114 km to, at its narrowest point, 15 km. The Israeli-occupied territories include the West Bank, 5879 km2, East Jerusalem, 70 km2 and the Golan Heights, 1150 km2. Geographical features in these territories will be noted as such.
Southern Israel is dominated by the Negev desert, covering some 16000 km2, more than half of the country's total land area. The north of the Negev contains the Judean Desert, which, at its border with Jordan, contains the Dead Sea which, at -417 m is the lowest point on Earth. The inland area of central Israel is dominated by the Judean Hills of the West Bank, whilst the central and northern coastline consists of the flat and fertile Israeli coastal plain. Inland, the northern region contains the Mount Carmel mountain range, which is followed inland by the fertile Jezreel Valley, and then the hilly Galilee region. The Sea of Galilee is located beyond this, and is bordered to the east by the Golan Heights, which contains the highest point under Israel's control, a peak in the Israeli-occupied Mount Hermon massif, at 2224 m. The highest point in territory internationally recognized as Israeli is Mount Meron at 1208 m.
Location and boundaries.
Israel lies to the north of the equator around 31°30' north latitude and 34°45' east longitude. It measures 424 km from north to south and, at its widest point 114 km, from east to west. At its narrowest point, however, this is reduced to just 15 km. It has a land frontier of 1017 km and a coastline of 273 km. It is ranked 153 on the List of countries and outlying territories by total area.
Prior to the establishment of the British Mandate for Palestine, there was no clear-cut definition of the geographical and territorial limits of the area known as "Palestine." On the eve of World War I it was described by Encyclopedia Britannica as a "nebulous geographical concept." The Sykes-Picot Treaty in 1916 divided the region that later became Palestine into four political units. Under the British Mandate for Palestine, the first geo-political framework was created that distinguished Palestine from the larger countries that surrounded it. The boundary demarcation at this time did not introduce geographical changes near the frontiers and both sides of the border were controlled by the British administration.
Modern Israel is bounded to the north by Lebanon, the northeast by Syria, the east by Jordan and the West Bank, and to the southwest by Egypt. To the west of Israel is the Mediterranean Sea, which makes up the majority of Israel's 273 km (170 mi) coastline and the Gaza Strip. Israel has a small coastline on the Red Sea in the south. The southernmost settlement in Israel is the city of Eilat whilst the northernmost is the town of Metula. The territorial waters of Israel extend into the sea to a distance of twelve nautical miles measured from the appropriate baseline.
The statistics provided by the Israel Central Bureau of Statistics include the annexed East Jerusalem and Golan Heights, but exclude the West Bank and Gaza Strip. The population of Israel includes Israeli settlers in the West Bank. The route of the Israeli West Bank barrier incorporates some parts of the West Bank.
Physiographic regions.
Israel is divided into four physiographic regions: the Mediterranean coastal plain, the Central Hills, the Jordan Rift Valley and the Negev Desert.
Coastal plain.
The Israeli Coastal Plain stretches from the Lebanese border in the north to Gaza in the south, interrupted only by Cape Carmel at Haifa Bay. It is about 40 km wide at Gaza and narrows toward the north to about 5 km at the Lebanese border. The region is fertile and humid (historically malarial) and is known for its citrus orchards and viticulture. The plain is traversed by several short streams. From north to south these are: the Kishon, the Hadera, the Alexander, the Poleg, and the Yarkon. All of these streams were badly polluted, but in the last ten years much work has been done to clean them up. Today the Kishon, Alexander and Yarkon again flow year round, and also have parks along their banks.
The region is divided into five sub-regions. The Western Galilee stretches from Rosh HaNikra in the far north, down to Israel's third-largest city, Haifa. It is a fertile region containing with a coastline with many small islands off of it. South of Haifa is the Hof HaCarmel region which runs to the town of Zikhron Ya'akov. The Sharon plain is the next stage down the Coastal Plain, running from Zikhron Ya'akov to Tel Aviv's Yarkon River. This area is Israel's most densely populated. South of this, running to Nahal Shikma, is the Central Coastal Plain. The southern region of the Coastal Plain is the Southern Coastal Plain (also known as the Shephelah, Plain of Judea, and Western Negev) and extends south to the Gaza Strip. It is divided into two. The Besor region, a savanna-type area with a relatively large number of communities, in the north, and the Agur-Halutsa region in the south which is very sparsely populated.
Central hills.
Inland (east) of the coastal plain lies the central highland region. In the north of this region lie the mountains and hills of Upper Galilee and Lower Galilee, which are generally 500 m to 700 m in height, although they reach a maximum height of 1208 m at Mount Meron. South of the Galilee, in the West Bank, are the Samarian Hills with numerous small, fertile valleys rarely reaching the height of 800 m. South of Jerusalem, also mainly within the West Bank, are the Judean Hills, including Mount Hebron. The central highlands average 610 m in height and reach their highest elevation at Har Meron, at 1208 m, in Galilee near Safed. Several valleys cut across the highlands roughly from east to west; the largest is the Jezreel Valley (also known as the Plain of Esdraelon), which stretches 48 km from Haifa southeast to the valley of the Jordan River, and is 19 km across at its widest point.
Jordan Rift Valley.
East of the central highlands lies the Jordan Rift Valley, which is a small part of the 6500 km-long Syrian-East African Rift. In Israel the Rift Valley is dominated by the Jordan River, the Sea of Galilee (an important freshwater source also known as Lake Tiberias and Lake Kinneret), and the Dead Sea. The Jordan, Israel's largest river (322 km), originates in the Dan, Baniyas, and Hasbani rivers near Mount Hermon in the Anti-Lebanon Mountains and flows south through the drained Hula Basin into the freshwater Lake Tiberias. Lake Tiberias is 165 km2 in size and, depending on the season and rainfall, is at about 213 m below sea level. With a water capacity estimated at 3 km3, it serves as the principal reservoir of the National Water Carrier (also known as the Kinneret-Negev Conduit). The Jordan River continues its course from the southern end of Lake Tiberias (forming the boundary between the West Bank and Jordan) to its terminus in the highly saline Dead Sea. The Dead Sea is 1020 km2 in size and, at 420 m below sea level, is the lowest surface point on the earth. South of the Dead Sea, the Rift Valley continues in the Nahal HaArava (Wadi al Arabah), which has no permanent water flow, for 170 km to the Gulf of Eilat.
Negev Desert.
The Negev Desert comprises approximately 12000 km2, more than half of Israel's total land area. Geographically it is an extension of the Sinai Desert, forming a rough triangle with its base in the north near Beersheba, the Dead Sea, and the southern Judean Mountains, and it has its apex in the southern tip of the country at Eilat. Topographically, it parallels the other regions of the country, with lowlands in the west, hills in the central portion, and the Arabah valley as its eastern border.
Unique to the Negev region are the craterlike makhteshim cirques; Makhtesh Ramon, Makhtesh Gadol and Makhtesh Katan. The Negev is also sub-divided into five different ecological regions: northern, western and central Negev, the high plateau and the Arabah Valley. The northern Negev receives 300 mm of rain annually and has fairly fertile soils. The western Negev receives 250 mm of rain per year, with light and partially sandy soils. The central Negev has an annual precipitation of 200 mm and is characterized by impervious soil, allowing minimum penetration of water with greater soil erosion and water runoff. This can result in rare flash floods during heavy rains as water runs across the surface of the impervious desert soil. The high plateau area of Ramat HaNegev stands between 370 m and 520 m above sea level with extreme temperatures in summer and winter. The area gets 100 mm of rain each year, with inferior and partially salty soils. The Arabah Valley along the Jordanian border stretches 180 km from Eilat in the south to the tip of the Dead Sea in the north and is very arid with barely 50 mm of rain annually.
Geology.
Israel is divided east-west by a mountain range running north to south along the coast. Jerusalem sits on the top of this ridge, east of which lies the Dead Sea graben which is a pull apart basin on the Dead Sea Transform fault.
The numerous limestone and sandstone layers of the Israeli mountains serve as aquifers through which water flows from the west flank to the east. Several springs have formed along the Dead Sea, each an oasis, most notably the oases at Ein Gedi and Ein Bokek (Neve Zohar) where settlements have developed. Israel also has a number of areas of karst topography. Caves in the region have been used for thousands of years as shelter, storage rooms, barns and as places of public gatherings.
The far northern coastline of the country has some chalk landscapes best seen at Rosh HaNikra, a chalk cliff into which a series of grottoes have been eroded.
Rivers and lakes.
Israel's longest and most famous river is the 320 km long River Jordan, which rises on the southern slopes of Mount Hermon in the Anti-Lebanon mountains. The river flows south through the freshwater Sea of Galilee, and from there forms the boundary with the Kingdom of Jordan for much of its route, eventually emptying into the Dead Sea. The northern tributaries to the Jordan are the Dan, Banias, and Hasbani. Only the Dan is within undisputed Israel; the Hasbani flows from Lebanon and the Banias from territory captured from Syria in the Six-Day War.
The Sea of Galilee (also called the Kinneret) is Israel's largest and most important freshwater lake, located in the northeast of the country. The pear-shaped lake is 23 km long from north to south, with a maximum width of 13 km in the north, covering 166 km2. The Kinneret lies 207 m below sea level and reaches depths of 46 m. In a previous geological epoch the lake was part of a large inland sea which extended from the Hula marshes in northern Israel to 64 km south of the Dead Sea. The bed of the lake forms part of the Jordan Rift Valley.
South of the Kinneret lies the saltwater Dead Sea which forms the border between Israel and Jordan and is 418 m below sea level, making it the lowest water surface on Earth. The Dead Sea is 67 km long with a maximum width of 16 km and also makes up part of the Rift Valley. A peninsula juts out into the lake from the eastern shore, south of which the lake is shallow, less than 6 m deep. To the north is the lake's greatest depth.
There are no navigable, artificial waterways in Israel, although the National Water Carrier, a conduit for drinking water, might be classified as such. The idea of a channel connecting the Mediterranean and Dead Seas or the Red and Dead Seas has been discussed.
Selected elevations.
The following are selected elevations of notable locations, from highest to lowest:
Climate.
Israel has a Mediterranean climate with long, hot, rainless summers and relatively short, cool, rainy winters (Köppen climate classification "Csa"). The climate is as such due to Israel's location between the subtropical aridity of the Sahara and the Arabian deserts, and the subtropical humidity of the Levant and Eastern Mediterranean. The climate conditions are highly variable within the state and modified locally by altitude, latitude, and the proximity to the Mediterranean.
On average, January is the coldest month with average temperatures ranging from 6 to, and July and August are the hottest months at 22 to, on average across the country. Summers are very humid along the Mediterranean coast but dry in the central highlands, the Rift Valley, and the Negev Desert. In Eilat, a desert city, summer daytime-temperatures are often the highest in the state, at times reaching 44 to. More than 70% of the average rainfall in Israel falls between November and March; June through September are usually rainless. Rainfall is unevenly distributed, significantly lower in the south of the country. In the extreme south, rainfall averages near 30 mm annually; in the north, average annual rainfall exceeds 900 mm. Rainfall varies from season to season and from year to year, particularly in the Negev Desert. Precipitation is often concentrated in violent storms, causing erosion and flash floods. In winter, precipitation often takes the form of snow at the higher elevations of the central highlands, including Jerusalem. Mount Hermon has seasonal snow which covers all three of its peaks for most of the year in winter and spring. In rare occasions, snow gets to the northern mountain peaks and only in extremely rare occasions even to the coast. The areas of the country most cultivated are those receiving more than 300 mm of rainfall annually, making approximately one-third of the country cultivable.
Thunderstorms and hail are common throughout the rainy season and waterspouts occasionally hit the Mediterranean coast, capable of causing only minor damage. However, supercell thunderstorms and a true F2 tornado hit the Western Galilee in April 2006, causing significant damage and 75 injuries.
Heat waves are frequent. 2010 was the hottest year in the history of Israel with absolute record high in several places in August. The heat became stronger from August when temperatures were considerably above the average. October and November were also dry, and November was almost rainless when it was supposed to be rainy.
Natural resources.
Unlike much of the Middle East which is rich in lucrative crude oil, Israel has limited natural resources. These include copper, phosphates, bromide, potash, clay, sand, sulfur, asphalt, and manganese. Small amounts of natural gas and crude oil are present, often too little to merit commercial extraction. In 2009, significant reserves of natural gas were discovered at the Tamar 1 offshore drilling site, 90 kilometers west of Haifa. It is the largest natural gas reserve ever discovered in Israel.
Environmental concerns.
Israel has a large number of environmental concerns ranging from natural hazards to man-made issues both resulting from ancient times to modern development. Natural hazards facing the country include sandstorms which sometimes occur during spring in the desert south, droughts which are usually concentrated in summer months, flash floods which create great danger in the deserts due to their lack of notice, and regular earthquakes,most of which are small, although there is a constant risk due to Israel's location along the Jordan Rift Valley. Current environmental concerns include the lack of arable land and natural fresh water resources. Whilst measures have been taken to irrigate and grow in the desert, the amount of water needed here poses issues. Desertification is also a risk possible on the desert fringe, whilst air pollution from industrial and vehicle emissions and groundwater pollution from industrial and domestic waste are also issues facing the country. Furthermore, the effects of the use of chemical fertilizers, and pesticides are issues facing the country.
Israel has signed many international environmental agreements and is party to:
Signed but not ratified:
Rural settlements.
Israel's rural space includes several unique kinds of settlements, notably the moshav and the kibbutz. Originally these were collective and cooperative settlements respectively. Over time, the degree of cooperation in these settlements has decreased and in several of them the cooperative structure has been dismantled altogether. All rural settlements and many small towns (some of which are dubbed "rurban settlements") are incorporated in regional councils. Land use in Israel is 17% arable land, 4% permanent crops, and 79% other uses. As of 2003 1940 km2 were irrigated.
There are 242 Israeli settlements and civilian land use sites in the West Bank, 42 in the Golan Heights, and 29 in East Jerusalem.
Islands.
Israel currently has no offshore islands within its territorial waters. However, the Israeli government plans to build artificial islands off the coast to house an airport, a seaport, a desalination plant, a power plant, and a military testing base, as an answer to Israel's lack of space.
Human geography.
As of 2013, the population of Israel is 8 million, 6,015,000 of them Jewish.
For statistical purposes, the country has three metropolitan areas; Gush Dan-Tel Aviv (population 3,150,000), Haifa (population 996,000), and Beersheba (population 531,600). Some argue that Jerusalem, Israel's largest city with a population of 763,600, and Nazareth, should also be classified as metropolitan areas. In total, Israel has 74 cities, 14 of which have populations of over 100,000. Other forms of local government in Israel are local councils of which there are 144 governing small municipalities generally over 2,000 in population, and regional councils of which there are 53, governing a group of small communities over a relatively large geographical area.
Israel's population is diverse demographically; 76% Jewish, 20% Arab, and 4% unaffiliated. In terms of religion, 76% are Jewish, 16% Muslim, 2% Christian, 2% Druze, and 4% are unclassified by choice. 8% of Israeli Jews are haredi; 9% are "religious", 12% "religious-traditionalists", 27% are "non-religious traditionalists", and 43% are "secular". Other small, but notable groups in Israel, include Circassians of whom there are approximately 3,000 living mostly in two northern villages, 2,500 Lebanese, and 5,000 Armenians predominantly in Jerusalem.
Overshoot.
Israel is ranked 34th in the world in terms of population density with, as noted, a climate of long, hot, rainless summers and relatively short, cool, rainy winters. The Population Matters 2011 overshoot index ranked Israel as the third most dependent region in the World after Singapore and Kuwait.

</doc>
<doc id="14767" url="http://en.wikipedia.org/wiki?curid=14767" title="Economy of the Isle of Man">
Economy of the Isle of Man

Offshore banking, manufacturing, and tourism form key sectors of the economy of the Isle of Man, a British Crown dependency in the Irish Sea.
The government's policy of offering incentives to high-technology companies and financial institutions to locate on the island has expanded employment opportunities in high-income industries. As a result, agriculture and fishing, once the mainstays of the economy, now make declining contributions to the Island's Gross Domestic Product (GDP). Banking and other services now contribute the great bulk of GDP. The stability of the Government and openness for business make the Isle of Man an attractive alternative jurisdiction (DAW Index ranked 3).
Trade is mostly with the United Kingdom. The Isle of Man has free access to European Union markets for goods, but only has restricted access for services, people, or financial products.
The Isle of Man is a low tax economy with no capital gains tax, wealth tax, stamp duty, death duty or inheritance tax and income tax rates of 10% and 20%; corporation tax is at 0%.
Gambling.
The Isle of Man has also recently entered the online gambling industry. In 2005 PokerStars, one of the world's largest online poker sites, relocated its headquarters to the Isle of Man from Costa Rica. In 2006, RNG Gaming a large gaming software developer of P2P tournaments and Get21, a multiplayer online blackjack site, based their corporate offices on the island. 
The Isle of Man Government Lottery operated from 1986 to 1997. Since 2 December 1999 the island has participated in the United Kingdom National Lottery. The island is the only jurisdiction outside the United Kingdom where it is possible to play the UK National Lottery. Since 2010 it has also been possible for projects in the Isle of Man to receive national lottery Good Causes Funding. The good causes funding is distributed by the Manx Lottery Trust. Tynwald receives the 12p lottery duty for tickets sold in the Island.
Filmmaking.
The Manx government also promotes island locations for making films by contributing to the production costs. Among the most successful productions funded in part by the Isle of Man film industry were "Waking Ned", where the Manx countryside stood in for rural Ireland, and films like "Stormbreaker", "Shergar", "Tom Brown's Schooldays", "I Capture the Castle", "The Libertine", "Island at War" (TV series), "Five Children and It", "Colour Me Kubrick", "Sparkle", and others. Other films that have been filmed on the Isle of Man include "Thomas and the Magic Railroad", "Harry Potter and the Chamber of Secrets", and "Keeping Mum".
Radio and television.
The British television show "Top Gear" frequently tests high-powered cars on the island because many of the rural roads do not have speed limits.
Electricity.
Since 1999, the Isle of Man has received electricity through the world's longest submarine AC cable, the 90 kV Isle of Man to England Interconnector, as well as from a natural gas power station in Douglas, an oil power station in Peel and a small hydro-electric power station in Sulby Glen.
Tourism.
Tourism in the Isle of Man developed from the advancement of transportation to the Isle. In 1819 the first steamship "Robert Bruce" came to the Isle, only seven years after the first steam-vessel in the UK. In the 1820s the tourist scene was growing due to betterment of transportation capabilities.
Statistics.
GDP:
purchasing power parity: $2.113 billion (2003 est.) 
GDP—real growth rate:
NA%
GDP—per capita:
purchasing power parity: $28,500 (2003 est.) 
GDP—composition by sector:
<br>"agriculture:"
1%
<br>"industry:"
13%
<br>"services:"
86% (2000 est.)
Population below poverty line:
NA%
Household income or consumption by percentage share:
<br>"lowest 10%:"
NA%
<br>"highest 10%:"
NA%
Inflation rate (consumer prices):
3.6% (2003 est.)
Labor force:
39,690(2001)
Labour force—by occupation:
agriculture, forestry and fishing 3%, manufacturing 11%, construction 10%, transport and communication 8%, wholesale and retail distribution 11%, professional and scientific services 18%, public administration 6%, banking and finance 18%, tourism 2%, entertainment and catering 3%, miscellaneous services 10%
Unemployment rate:
0.6% (2004 est.)
Budget:
<br>"revenues:"
$485 million
<br>"expenditures:"
$463 million, including capital expenditures of $NA (FY00/01 est.)
Industries:
financial services, light manufacturing, tourism
Industrial production growth rate:
3.2% (1996/97)
Electricity—production:
329 GWh (1999)
Electricity—production by source:
<br>"fossil fuel:"
100%
<br>"hydro:"
0%
<br>"nuclear:"
0%
<br>"other:"
0% (1999)
Electricity—consumption:
287 GWh (1999)
Electricity—exports:
NA kWh
Electricity—imports:
NA kWh
Agriculture—products:
cereals, vegetables, cattle, sheep, pigs, poultry
Exports:
$NA
Exports—commodities:
tweeds, herring, processed shellfish, beef, lamb
Exports—partners:
UK
Imports:
$NA
Imports—commodities:
timber, fertilizers, fish
Imports—partners:
UK
Debt—external:
$NA
Economic aid—recipient:
$NA
Currency:
1 Isle of Man pound = 100 pence
Exchange rates:
Manx pounds per US$1: 0.6092 (January 2000), 0.6180 (1999), 0.6037 (1998), 0.6106 (1997), 0.6403 (1996), 0.6335 (1995); the Manx pound is at par with the British pound
Fiscal year:
1 April – 31 March

</doc>
<doc id="14768" url="http://en.wikipedia.org/wiki?curid=14768" title="Communications in the Isle of Man">
Communications in the Isle of Man

The Isle of Man has an extensive communications infrastructure. Consisting of telephone wires, submarine cables, and an array of television and mobile phone transmitters and towers.
Telecommunications.
Telegraph.
The history of Manx telecommunications starts in 1859, when the Isle of Man Electric Telegraph Company was formed on the island with the intention of connecting across the island by telegraph, and allowing messages to be sent onwards to the UK. In August 1859, a 36 nmi long cable was commissioned from Glass, Elliot and Company of Greenwich and laid from Cranstal (north of Ramsey) to St Bees in Cumbria using the chartered cable ship "Resolute". The cable was single-core, with gutta-percha insulation.
Twenty miles of overhead cable were also erected from Cranstal south to Ramsey, and on to Douglas. In England, the telegraph was connected to Whitehaven and the circuits of the Electric Telegraph Company.
The telegraph offices were located at 64 Athol Street, Douglas (also the company's head office) and at East Quay, Ramsey (now Marina House).
On 10 August 1860 the company was statutorily incorporated by an Act of Tynwald with a capital of £5,500.
The currents at Cranstal proved too strong, and in 1864 the cable was taken up and relaid further south, at Port-e-Vullen in Ramsey Bay. It was later relaid to land even further south at Port Cornaa.
Following the 1869 finalisation of UK telegraph nationalisation into a General Post Office monopoly, the Isle of Man Telegraph Company was nationalised in 1870 under the Telegraph Act 1870 (an Act of Parliament) at a cost to the British Government of £16,106 (paid in 1872 following arbitration proceedings over the value). Prior to nationalisation, the island's telegraph operations had been performing poorly and the company's share price valued it at around £100.
Subsequent to nationalisation, operations were taken over by the GPO. The internal telegraph system was extended within a year to Castletown and Peel, however by then the previous lack of modern communications in Castletown had already started the Isle of Man Government on its move to Douglas.
Due to increasing usage in the years following nationalisation, further cables between Port Cornaa and St Bees were laid in 1875 and 1885.
By 1883 Smith's Directory listed several telegraph offices operated by the Post Office, in addition to those at Douglas, Ramsey, Castletown and Peel the telegraph was also available at Laxey, Ballaugh, and Port St. Mary.
Throughout the First World War, the cable landing station at Port Cornaa was guarded by the Isle of Man Volunteer Corps.
The undersea telegraph cables have been disused since the 1950s, but remain in place.
Telephones.
The main telephone provider on the Isle of Man today is Manx Telecom.
In 1889 George Gillmore, formerly an electrician for the GPO's Manx telegraph operations, was granted a licence by the Postmaster General to operate the Isle of Man's first telephone service. Based in an exchange in Athol Street, early customers of Gilbert's telephone service included the Isle of Man Steam Packet Company and the Isle of Man Railway. Not having the resources to fund expansion or a link to England, Gillmore sold his licence to the National Telephone Company and stayed on as their manager on the island.
By 1901 there were 600 subscribers, and the telephone system had been extended to Ramsey, Castletown, Peel, Port Erin, Port St. Mary and Onchan.
On 1 January 1912 the National Telephone Company was nationalised and merged into the General Post Office by the Telephone Transfer Act 1911. Only Guernsey, Portsmouth and Hull remained outside of the GPO.
In 1922, the General Post Office offered to sell the island's telephone service to the Manx government, but the offer was not taken up. A similar arrangement in Jersey for that island's telephone service was concluded in 1923.
The first off-island telephone link was established in 1929, with the laying of a cable by the "CS Faraday" between Port Erin and Ballyhornan in Northern Ireland, a distance of 57 km, and then between Port Grenaugh and Blackpool, primarily to provide a link to Northern Ireland. The cable was completed on 6 June 1929 and the first call between the Isle of Man and the outside world was made on 28 June 1929 by Lieutenant Governor Sir Claude Hill in Douglas to the Postmaster General in Liverpool. The cable initially carried only two trunk circuits.
In 1942, a pioneering VHF frequency-modulated radio-link was established between Creg-na-Baa and the UK to provide an alternative to the sub-sea cable. This has since been discontinued.
This was augmented on 24 June 1943 by a 74 km long cable between Cemaes Bay in Anglesea and Port Erin, which had the world's first submerged repeater, laid by "HMCS Iris". The repeater doubled the possible number of circuits on the cable, and although it failed after only five months, its replacement worked for seven years.
In 1962 a further undersea cable was laid by "HMTS Ariel" between Colwyn Bay and the Island.
Historically, the telephone system on the Isle of Man had been run as a monopoly by the British General Post Office, and later British Telecommunications, and operated as part of the Liverpool telephone district.
By 1985 the privatised British Telecom had inherited the telephone operations of the GPO, including those on the Isle of Man. At this time the Manx Government announced that it would award a 20-year licence to operate the telephone system in a tender process. As part of this process, in 1986 British Telecom created a Manx-registered subsidiary company, Manx Telecom, to bid for the tender. It was believed that a local identity and management would be more politically acceptable in the tendering process as they competed with Cable & Wireless to win the licence. Manx Telecom won the tender, and commenced operations under the new identity from 1 January 1987.
On 28 March 1988 an 8,000 telephone circuit fibre optic cable, the longest unregenerated system in Europe, was inaugurated. In links Port Grenaugh to Silecroft in Cumbria, and was laid in September 1987. The cable was buried in the seabed along its entire length.
A further fibre optic cable, known as BT-MT1 was laid in October 1990 between Millom in Cumbria and Douglas, a distance of 43 nmi. Jointly operated by BT and Manx Telecom, it provides six channels each with a bandwidth of 140 Mbit/s. This cable remains in use today.
In July 1992, Mercury Communications laid the LANIS fibre-optic cables. LANIS-1 runs for 61 nmi between Port Grenaugh and Blackpool, and LANIS-2 runs for 36 nmi between the Isle of Man and Northern Ireland. They have six channels each with a bandwidth of 565 Mbit/s. The LANIS cables are now operated by Cable & Wireless. The LANIS-1 cable was damaged 600 m off Port Grenaugh on 27 November 2006, causing loss of the link and resulting in temporary Internet access issues for some Manx customers whilst it was awaiting repair.
On 17 November 2001 Manx Telecom became part of mmO2 following the demerger of BT Wireless's operations from BT Group, and the company was owned by Telefónica. On 4 June 2010 Manx Telecom was sold by Telefónica to UK private equity investor HgCapital (who were buying the majority stake), alongside telecoms management company CPS Partners
In December 2007, the Manx Electricity Authority and its telecoms subsidiary, e-llan Communications, commissioned the lighting of a new undersea fibre-optic link. It was laid in 1999 between Blackpool and Douglas as part of the Isle of Man to England Interconnector which connects the Manx electricity system to the UK's National Grid.
In March 2009, BlueWave Communications installed microwave links to Ireland and the UK. These were the first off-island microwave links.
According to the CIA World Factbook, in 1999 there were 51,000 fixed telephone lines in use in the Isle of Man.
The Isle of Man is included within the UK telephone numbering system, and is accessed externally via UK area codes, rather than by its own country calling code. The area codes currently in use are: +44 1624 (landlines) and +44 7425 / +44 7624 / +44 7924 (mobiles).
Submarine communications cables in service.
Submarine cables in Manx waters are governed by the Submarine Cables Act 2003 (an Act of Tynwald).
Telecoms service providers.
It is also rumoured that various online gaming companies operate their own networks outside of these providers, although they do not resell that service.
Mobile telephones.
The mobile phone network operated by Manx Telecom has been used by O2 as an environment for developing and testing new products and services prior to wider rollout. In December 2001, the company became the first telecommunications operator in Europe to launch a live 3G network. In November 2005, the company became the first in Europe to offer its customers an HSDPA (3.5G) service.
Internet.
In 1996 the Isle of Man Government obtained permission to use the .im National Top Level Domain (TLD) and has ultimate responsibility for its use. The domain is managed on a daily basis by Domicilium (IOM) Limited, an island based internet service provider. Broadband internet services are available through five local providers which are Manx Telecom, Sure, Wi-Manx, Domicilium, and BlueWave Communications.
Broadcasting.
Radio.
The public-service commercial radio station for the island is Manx Radio. Manx Radio is part funded by government grant, and partly by advertising.
There are two other Manx-based FM radio stations, Energy FM and 3 FM.
BBC national radio stations are also relayed locally via a transmitter located to the south of Douglas, relayed from Sandale transmitting station in Cumbria, as well as a signal feed from the Holme Moss transmitting station in West Yorkshire. The Douglas transmitter also broadcasts the BBC's DAB digital radio services and Classic FM.
Manx Radio is the only local service to broadcast on AM medium wave. No UK services are relayed via local AM transmitters. No longwave stations operate from the Island, although one (Musicmann279) was proposed.
A Channel 4 operated DAB multiplex is proposed, but there are currently no proposals to broadcast any of the three insular FM stations on DAB.
Television.
There is no Island-specific television service. Local transmitters retransmit UK Freeview broadcasts. The BBC region is BBC North West and the ITV region is Granada.
Many TV services are available by satellite, such as Sky, and Freesat from the Astra 2/Eurobird 1 group, as well as services from a range of other satellites around Europe such as Astra 1 and Hot Bird.
Manx ViaSat-IOM, ManSat, Telesat-IOM companies uses the first communications satellite ViaSat-1 that launched in 2011 and positioned at the Isle of Man registered 115.1 degrees West longitude geostationary orbit point.
In some areas, terrestrial television directly from the United Kingdom or Republic of Ireland can also be received.
Analogue television transmission ceased between 2008 and 2009, when limited local transmission of digital terrestrial television commenced. The UK's television licence regime extends to the Island.
There is no Island-specific opt-out of the BBC regional news programme "North West Tonight", in the way that the Channel Islands get their own version of "Spotlight".
ITV television has been available on parts of the east of the Isle of Man on 3 May 1956 when Granada Television transmissions started from the Winter Hill transmitting station, and to parts of the west of the island on 1 October 1959 from the Black Mountain transmitting station in Northern Ireland which broadcast Ulster Television. Parts of the north of the island received Border Television since 1 September 1961, initially directly from the Caldbeck transmitting station in Cumberland. On 26 March 1965 Border Television commenced relay of their signal through a local transmitter on Richmond Hill, 542 ft above sea level and three miles (5 km) from the centre of Douglas. The site allowed reliable reception of the Caldbeck signal, which is rebroadcast on a different frequency. The 200 ft high transmission tower was re-sited from London, where it had been used for early ITV transmissions. Richmond Hill was decommissioned after the close of 405 line broadcasts, although the 200 ft tower remained in use for radio with Manx Radio transmitting on 96.9 MHz and then 97.3 MHz until 1989. Manx Radio moved their FM service to the Carnane site and the frequecny changed to the current 97.2 MHz.
The television broadcasts are now transmitted from a 195 ft high transmitter on a hill to the south of Douglas. The transmitter is operated by Arqiva and is directly fed using a Fibre Optic cable. There are further sub-relay transmitters across the island. Following a realignment of ITV regional services and the digital switchover, the Douglas relay switched ITV broadcasts to Granada Television on Thursday 17 July 2009.
The Broadcasting Act 1993 (An Act of Tynwald) allows for the establishment of local television services. Only one application for a licence to run such a service was received by the Communications Commission. That application was rejected.
According to the CIA World Factbook, in 1999 there were 27,490 televisions in use in the Isle of Man.
Post.
Isle of Man Post issues its own stamps for use within the island and for sending post off-island. Only Manx stamps are valid for sending mail using the postal system. The Isle of Man adopted postcodes in 1993 using the prefix IM to fit in with the already established UK postcode system.

</doc>
<doc id="14960" url="http://en.wikipedia.org/wiki?curid=14960" title="IPA (disambiguation)">
IPA (disambiguation)

IPA commonly refers to:
IPA may also refer to:

</doc>
<doc id="14971" url="http://en.wikipedia.org/wiki?curid=14971" title="Industrial archaeology of Dartmoor">
Industrial archaeology of Dartmoor

The industrial archaeology of Dartmoor covers a number of the industries which have, over the ages, taken place on Dartmoor, and the remaining evidence surrounding them. Currently only three industries are economically significant, yet all three will inevitably leave their own traces on the moor: china clay mining, farming and tourism.
A good general guide to the commercial activities on Dartmoor at the end of the 19th century is William Crossing's "The Dartmoor Worker".
Mining.
In former times, lead, silver, tin and copper were mined extensively on Dartmoor. The most obvious evidence of mining to the casual visitor to Dartmoor are the remains of the old engine-house at Wheal Betsy which is alongside the A386 road between Tavistock and Okehampton. The word "Wheal" has a particular meaning in Devon and Cornwall being either a tin or a copper mine, however in the case of Wheal Betsy it was principally lead and silver which were mined. 
Once widely practised by many miners across the moor, by the early 1900s only a few tinners remained, and mining had almost completely ceased twenty years later. Some of the more significant mines were Eylesbarrow, Knock Mine, Vitifer Mine and Hexworthy Mine. The last active mine in the Dartmoor area was Great Rock Mine, which shut down in 1969.
Quarrying.
Dartmoor granite has been used in many Devon and Cornish buildings. The prison at Princetown was built from granite taken from Walkhampton Common. When the horse tramroad from Plymouth to Princetown was completed in 1823, large quantities of granite were more easily transported.
There were three major granite quarries on the moor: Haytor, Foggintor and Merrivale. The granite quarries around Haytor were the source of the stone used in several famous structures, including the New London Bridge, completed in 1831. This granite was transported from the moor via the Haytor Granite Tramway, stretches of which are still visible.
The extensive quarries at Foggintor provided granite for the construction of London's Nelson's Column in the early 1840s, and New Scotland Yard was faced with granite from the quarry at Merrivale. Merrivale Quarry continued excavating and working its own granite until the 1970s, producing gravestones and agricultural rollers. Work at Merrivale continued until the 1990s, for the last 20 years imported stone such as gabbro from Norway and Italian marble was dressed and polished. The unusual pink granite at Great Trowlesworthy Tor was also quarried, and there were many other small granite quarries dotted around the moor. Various metamorphic rocks were also quarried in the metamorphic aureole around the edge of the moor, most notably at Meldon.
Gunpowder factory.
In 1844 a factory for making gunpowder was built on the open moor, not far from Postbridge. Gunpowder was needed for the tin mines and granite quarries then in operation on the moor. The buildings were widely spaced from one another for safety and the mechanical power for grinding ("incorporating") the powder was derived from waterwheels driven by a leat.
Now known as "Powdermills" or "Powder Mills", there are extensive remains of this factory still visible. Two chimneys still stand and the walls of the two sturdily-built incorporating mills with central waterwheels survive well: they were built with substantial walls but flimsy roofs so that in the event of an explosion, the force of the blast would be directed safely upwards. The ruins of a number of ancillary buildings also survive. A proving mortar—a type of small cannon used to gauge the strength of the gunpowder—used by the factory still lies by the side of the road to the nearby pottery.
Peat-cutting.
Peat-cutting for fuel occurred at some locations on Dartmoor until certainly the 1970s, usually for personal use. The right of Dartmoor commoners to cut peat for fuel is known as "turbary". These rights were conferred a long time ago, pre-dating most written records. The area once known as the "Turbary of Alberysheved" between the River Teign and the headwaters of the River Bovey is mentioned in the Perambulation of the Forest of Dartmoor of 1240 (by 1609 the name of the area had changed to Turf Hill).
An attempt was made to commercialise the cutting of peat in 1901 at Rattle Brook Head, however this quickly failed.
Warrens.
From at least the 13th century until early in the 20th, rabbits were kept on a commercial scale, both for their flesh and their fur. Documentary evidence for this exists in place names such as Trowlesworthy Warren (mentioned in a document dated 1272) and Warren House Inn. The physical evidence, in the form of pillow mounds is also plentiful, for example there are 50 pillow mounds at Legis Tor Warren. The sophistication of the warreners is shown by the existence of vermin traps that were placed near the warrens to capture weasels and stoats attempting to get at the rabbits.
The significance of the term "warren" nowadays is not what it once was. In the Middle Ages it was a privileged place, and the creatures of the warren were protected by the king 'for his princely delight and pleasure'.
The subject of warrening on Dartmoor was addressed in Eden Phillpotts' story "The River".
Farming.
Farming has been practised on Dartmoor since time immemorial. The dry-stone walls which separate fields and mark boundaries give an idea of the extent to which the landscape has been shaped by farming. There is little or no arable farming within the moor, mostly being given over to livestock farming on account of the thin and rocky soil. Some Dartmoor farms are remote in the extreme.

</doc>
<doc id="14975" url="http://en.wikipedia.org/wiki?curid=14975" title="Ivy League">
Ivy League

The Ivy League is a collegiate athletic conference comprising sports teams from eight private institutions of higher education in the Northeastern United States. The conference name is also commonly used to refer to those eight schools as a group. The eight institutions are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, the University of Pennsylvania, Princeton University, and Yale University. The term "Ivy League" has connotations of academic excellence, selectivity in admissions, and social elitism.
The term became official after the formation of the NCAA Division I athletic conference in 1954. Seven of the eight schools were founded during the United States colonial period; the exception is Cornell, which was founded in 1865. Ivy League institutions account for seven of the nine Colonial Colleges chartered before the American Revolution, the other two being Rutgers University and College of William & Mary.
Ivy League schools are generally viewed as some of the most prestigious, and are ranked among the best universities worldwide. All eight universities place in the top sixteen of the "U.S. News & World Report" 2015 university rankings, including the top four schools and six of the top eleven. U.S. News has named a member of the Ivy League as the best national university in each of the past fifteen years ending with the 2015 rankings: Princeton eight times, Harvard twice and the two schools tied for first five times.
Undergraduate enrollments range from about 4,000 to 14,000, making them larger than those of a typical private liberal arts college and smaller than a typical public state university. Total enrollments, including graduate students, range from approximately 6,100 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $3.2 billion to Harvard's $36.4 billion, the largest financial endowment of any academic institution in the world.
Members.
Ivy League universities have some of the largest university financial endowments in the world, which allows the universities to provide many resources for their academic programs and research endeavors. As of 2014, Harvard University has an endowment of $36.4 billion. Additionally, each university receives millions of dollars in research grants and other subsidies from federal and state government.
History.
Origin of the name.
Students have long revered the ivied walls of older colleges. "Planting the ivy" was a customary class day ceremony at many colleges in the 1800s. In 1893 an alumnus told "The Harvard Crimson", "In 1850, class day was placed upon the University Calendar... the custom of planting the ivy, while the ivy oration was delivered, arose about this time." At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as "Ivy Day" in 1874. Ivy planting ceremonies are reported for Yale, Simmons, Bryn Mawr and many others. Princeton's "Ivy Club" was founded in 1879.
The first usage of "Ivy" in reference to a group of colleges is from sportswriter Stanley Woodward (1895–1965).
A proportion of our eastern ivy colleges are meeting little fellows another Saturday before plunging into the strife and the turmoil.—Stanley Woodward, "New York Tribune", October 14, 1933, describing the football season
The first known instance of the term "Ivy League" being used appeared in "The Christian Science Monitor" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. However, at this time, none of these institutions made efforts to form an athletic league.
A common folk etymology attributes the name to the Roman numeral for four (IV), asserting that there was such a sports league originally with four members. The "Morris Dictionary of Word and Phrase Origins" helped to perpetuate this belief. The supposed "IV League" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a fourth school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Yale and Columbia met on November 23, 1876 at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.
Pre–Ivy League.
Seven of the Ivy League schools were founded before the American Revolution; Cornell was founded just after the American Civil War. These seven were the primary colleges in the Northern and Middle Colonies, and their early faculties and founding boards were largely, therefore, drawn from other Ivy League institutions. There were also some British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, the University of Edinburgh, and elsewhere on their boards. Similarly, the founder of The College of William & Mary, in 1693, was a British graduate of the University of Edinburgh. Cornell provided Stanford University with its first president.
The influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities for each of these states. In 1801 a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California came from Yale, hence the school colors of University of California are Yale Blue and California Gold.
Some of the Ivy League schools have identifiable Protestant roots, while others were founded as nonsectarian schools. Church of England "King's College" broke up during the Revolution and was reformed as public nonsectarian Columbia College. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and such relics as compulsory chapel often lasted well into the twentieth century. Penn and Brown were officially founded as nonsectarian schools. Brown's charter promised no religious tests and "full liberty of conscience", but placed control in the hands of a board of twenty-two Baptists, five Quakers, four Congregationalists, and five Episcopalians. Cornell has been strongly nonsectarian from its founding.
"Ivy League" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This sense dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.
After the Second World War, the present Ivy League institutions slowly widened their selection of students. They had always had distinguished faculties; some of the first Americans with doctorates had taught for them; but they now decided that they could not both be world-class research institutions and be competitive in the highest ranks of American college sport; in addition, the schools experienced the scandals of any other big-time football programs, although more quietly.
History of the athletic league.
19th and early 20th centuries.
The first formal athletic league involving eventual Ivy League schools (or any US colleges, for that matter) was created in 1870 with the formation of the Rowing Association of American Colleges. The RAAC hosted a de facto national championship in rowing during the period 1870–1894. In 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete
A basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.
In 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies.
In February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie. Two years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.
Before the formal establishment of the Ivy League, there was an "unwritten and unspoken agreement among certain Eastern colleges on athletic relations". In 1935, the Associated Press reported on an example of collaboration between the schools:The athletic authorities of the so-called "Ivy League" are considering drastic measures to curb the increasing tendency toward riotous attacks on goal posts and other encroachments by spectators on playing fields.—The Associated Press, "The New York Times"
Despite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:
I can say with certainty that in the last five years—and markedly in the last three months—there has been a strong drift among the eight or ten universities of the East which see a good deal of one another in sport toward a closer bond of confidence and cooperation and toward the formation of a common front against the threat of a breakdown in the ideals of amateur sport in the interests of supposed expediency.
Please do not regard that statement as implying the organization of an Eastern conference or even a poetic "Ivy League". That sort of thing does not seem to be in the cards at the moment.
Within a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of "the formation of an Ivy League" gained enough traction among the undergraduate bodies of the universities that the "Columbia Daily Spectator", "The Cornell Daily Sun", "The Dartmouth", "The Harvard Crimson", "The Daily Pennsylvanian", "The Daily Princetonian" and the "Yale Daily News" would simultaneously run an editorial entitled "Now Is the Time", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:The Ivy League exists already in the minds of a good many of those connected with football, and we fail to see why the seven schools concerned should be satisfied to let it exist as a purely nebulous entity where there are so many practical benefits which would be possible under definite organized association. The seven colleges involved fall naturally together by reason of their common interests and similar general standards and by dint of their established national reputation they are in a particularly advantageous position to assume leadership for the preservation of the ideals of intercollegiate athletics.
The Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, "The Oneida", won the race and was presented with trophy black walnut oars from then presidential nominee General Franklin Pierce.
The proposal did not succeed—on January 11, 1937, the athletic authorities at the schools rejected the "possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track." However, they noted that the league "has such promising possibilities that it may not be dismissed and must be the subject of further consideration."
Post-World War II.
In 1945 the presidents of the eight schools signed the first "Ivy Group Agreement", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton Presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:The members of the Group reaffirm their prohibition of athletic scholarships. Athletes shall be admitted as students and awarded financial aid only on the basis of the same academic standards and economic need as are applied to all other students.
In 1954, the date generally accepted as the birth of the Ivy League, the presidents extended the Ivy Group Agreement to all intercollegiate sports effective with the 1955-56 basketball season. As part of the transition, Brown, the only Ivy that hadn't joined the EIBL, did so for the 1954-55 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.
As late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie "Animal House" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, "The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men’s colleges."
In 1982 the Ivy League considered adding two members, with the United States Military Academy, the United States Naval Academy, and Northwestern University as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.
Academics.
Admissions.
The Ivy League schools are highly selective, with acceptance rates since 2000 ranging from 6 to 16 percent at each of the universities. Admitted students come from around the world, although students from New England and the Northeastern United States make up a significant proportion of students.
Prestige.
Members of the League have been highly ranked by various university rankings.
Further, Ivy League members have produced many Nobel laureates, winners of the Nobel Prize and the Nobel Memorial Prize in Economic Sciences. Listed from in order from greatest number of Nobel laureates are: Harvard with 153 Nobel winners, the most out of any university in the world. This is followed by Columbia with 101 winners, Yale with 52, Cornell with 45, Princeton with 37, and Penn with 29 Nobel laureates. These figures are self-reported by the universities themselves, who use widely varying definitions for which Nobel winners are claimed as affiliates, for example, only degree-holding alumni or active faculty or former faculty, visiting faculty, adjunct faculty, etc. Many universities are notorious for claiming laureates with only tenuous informal connections in order to inflate their count of winners.
Collaboration.
Collaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group Presidents, composed of each university president. During meetings, the presidents often discuss common procedures and initiatives for the universities.
Libraries.
Up until recently, seven of the eight schools (Harvard excluded) participated in the Borrow Direct interlibrary loan program, making a total of 88 million items available to participants with a waiting period of four working days. This ILL program is not affiliated with the formal Ivy arrangement. Harvard and MIT joined the Direct Borrow partnership in January 2011, together contributing over 70 million books to the existing collection.
Culture.
Fashion and lifestyle.
Different fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and Preppy are styles often associated with the Ivy League and its culture.
Ivy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.
Preppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Princeton, and Yale.
Some typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colours, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colour combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney & Bourke became associated with preppy style.'
Today, these styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as "Classic American style" or "Traditional American style".
Social elitism.
The Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old Money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper middle and upper class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.
Phrases such as "Ivy League snobbery" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads "the aridity of snobbery which he knew infected the Ivy League colleges". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):
"We Ivy Leaguers [read: mostly white and Anglo] know that an Ivy League degree is a mark of the kind of person who is likely to succeed in this organization.
The phrase "Ivy League" historically has been perceived as connected not only with academic excellence, but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Princeton, Cornell, Columbia, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider "Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt" as candidates for membership, he exhorted:
"It would be well for the proponents of the Ivy League to make it clear (to themselves especially) that the proposed group would be inclusive but not "exclusive" as this term is used with a slight up-tilting of the tip of the nose.
Aspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having "foreign-policy views born in Harvard Yard's boutique." "New York Times" columnist Maureen Dowd asked "Wasn't this a case of the pot calling the kettle elite?" Bush explained however that, unlike Harvard, Yale's reputation was "so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it... Harvard boutique to me has the connotation of liberalism and elitism" and said Harvard in his remark was intended to represent "a philosophical enclave" and not a statement about class. Columnist Russell Baker opined that "Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets." Still, the last four presidents have all attended Ivy League schools for at least part of their education— George H.W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), and Barack Obama (Columbia undergrad, Harvard Law School).
U.S. presidents in the Ivy League.
Of the forty-three men who have served as President of the United States, fourteen have graduated from an Ivy League university. Of them, eight have degrees from Harvard, five from Yale, three from Columbia, two from Princeton, and one from Pennsylvania. John Adams was the first president to graduate from university, graduating from Harvard University in 1755.
Student demographics.
Geographic distribution.
Students of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, it is no surprise that most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.
Socioeconomics and social class.
Students of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and middle class American families.
In 2013, 46% of Harvard College students came from families in the top 3.8% of all American households (over $200,000 per annum). In 2012, the bottom 25% of the American income distribution accounted for only 3-4% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper middle and/or upper class. (The median household income in the U.S. in 2013 was $52,700.)
In the 2011-2012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) comprised 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.
Competition and athletics.
Ivy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. Unlike all other Division I basketball conferences, the Ivy League has no tournament for the league title; the school with the best conference record represents the conference in the Division I NCAA Men's and Women's Basketball Tournament (with a playoff, or playoffs, in the case of a tie). Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA Basketball Tournament.
On average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools.
Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies.
In the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 13, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the "Father of American Football," held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 17. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and 1999 "Mr. Irrelevant" Jim Finn (Penn).
Beginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS in 2006). The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, the Ivy League has not played any postseason games at all since 1956 due to the league's concerns about the extended December schedule's effects on academics. For this reason, any Ivy League team invited to the FCS playoffs turns down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which was most recently expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football is the only sport in which the Ivy League declines to compete for a national title.
In addition to varsity football, Penn, Princeton and Cornell also field teams in the eight-team Collegiate Sprint Football League, in which all players must weight 172 pounds or less. Penn and Princeton are the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.
The Ivy League is home to some of the oldest college rugby teams. Although these teams are not "varsity" sports, they compete annually in the Ivy Rugby Conference.
Historical results.
The table above includes the number of team championships won from the beginning of official Ivy League competition (1956–57 academic year) through 2011–12. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished six times by Harvard and 21 times by Princeton, including a conference-record 15 championships in 2010-11. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005-06). In the 33 academic years beginning 1979-80, Princeton has averaged 11 championships per year, one-third of the conference total of 33 sponsored sports.
In the seven academic years beginning 2005-06, Harvard has won Ivy titles in 22 different sports, two-thirds of the league total, and Princeton has won championships in 31 different sports, all except wrestling and men's tennis.
Rivalries.
Rivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; "Puck Frinceton", and "Pennetrate the Puss" t-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion). Harvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014.
Rivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have each had two unbeaten seasons since 2001). In men's lacrosse, Cornell and Princeton are perennial rivals, and they are the only two Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002-03, with one exception (Columbia women won indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 11 football games and all but one of the last 13 crew races.
Intra-Conference Football Rivalries.
The Yale-Princeton series is the nation's second longest, exceeded only by "The Rivalry" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional sports were fledgling baseball leagues, these high profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.
National team championships.
Through July 2, 2014
Other Ivies.
Marketing groups, journalists, and some educators sometimes promote other colleges as "Ivies," as in Little Ivies (colloquialism referring to a group of small, selective American liberal arts colleges), Public Ivies, or Southern Ivies. These uses of "Ivy" are intended to promote the other schools by comparing them to the Ivy League. For example, in the 2007 edition of Newsweek's "How to Get Into College Now", the editors designated twenty-five schools as "New Ivies."
The term "Ivy Plus" is sometimes used to refer to the Ancient Eight plus several other schools for purposes of alumni associations, university affiliations, or endowment comparisons. In his book "Untangling the Ivy League", Zawel writes, "The inclusion of non-Ivy League schools under this term is commonplace for some schools and extremely rare for others. Among these other schools, Massachusetts Institute of Technology and Stanford University are almost always included. The University of Chicago and Duke University are often included as well."

</doc>
<doc id="15018" url="http://en.wikipedia.org/wiki?curid=15018" title="Infusoria">
Infusoria

Infusoria is a collective term for minute aquatic creatures such as ciliates, euglenoids, protozoa, unicellular algae and small invertebrates that exist in freshwater ponds. In modern formal classifications, the term is considered obsolete; the microorganisms previously included in the Infusoria are mostly assigned to the kingdom Protista which itself is a polyphyletic assemblage of groups.
Aquarium use.
Infusoria are used by owners of aquariums to feed fish fry; newly hatched fry of many common aquarium species can be successfully raised on this food during early development due to its size and nutritional content. Many home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own supply cultures or use one of the many commercial cultures available. Infusoria can be cultured by soaking any decomposing vegetative matter, such as papaya skin, in a jar of aged water. The culture will be grown in two to three days, depending on temperature and light received. The water will first turn cloudy, but it will clear up once the infusoria eat the bacteria which caused the cloudiness. At this point, the infusoria will be ready, and will usually be visible to the naked eye as small, white specks swimming in the container.

</doc>
<doc id="15020" url="http://en.wikipedia.org/wiki?curid=15020" title="ISO/IEC 8859">
ISO/IEC 8859

ISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.
ISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.
Introduction.
While the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7 bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.
The ISO/IEC 8859-"n" encodings only contain printable characters, and were designed to be used in conjunction with control characters mapped to the unassigned bytes. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-"n" as their preferred MIME name or, in cases where a preferred MIME name isn't specified, their canonical name. Many people use the terms ISO/IEC 8859-"n" and ISO-8859-"n" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.
Characters.
The ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.
As a rule of thumb, if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it didn't get in. Hence the directional double quotation marks "«" and "»" used for some European languages were included, but not the directional double quotation marks "“" and "”" used for English and some other languages. French didn't get its "œ" and "Œ" ligatures because they could be typed as 'oe'. Ÿ, needed for all-caps text, was left out as well. These characters were, however, included later with ISO/IEC 8859-15, which also introduced the new euro sign character €. Likewise Dutch did not get the 'ĳ' and 'Ĳ' letters, because Dutch speakers had become used to typing these as two letters instead. Romanian did not initially get its ‹Ș›/‹ș› and ‹Ț›/‹ț› (with comma) letters, because these letters were initially unified with ‹Ş›/‹ş› and ‹Ţ›/‹ţ› (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.
Most of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters although the Thai, Hebrew, and Arabic ones do also contain combining characters. However, the standard makes no provision for the scripts of East Asian languages ("CJK"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, but like several other alphabets of the world they aren't encoded in the ISO/IEC 8859 system.
The Parts of ISO/IEC 8859.
ISO/IEC 8859 is divided into the following parts:
Each part of ISO 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1–4, 9, 10, 13–16), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1–4 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.
Table.
At position 0xA0 there's always the non breaking space and 0xAD is mostly the soft hyphen, which only shows at line breaks. Other empty fields are either unassigned or the system used isn't able to display them.
There are new additions as ISO/IEC 8859-7:2003 and ISO/IEC 8859-8:1999 versions. LRM stands for left-to-right mark (U+200E) and RLM stands for right-to-left mark (U+200F).
Relationship to Unicode and the UCS.
Since 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the "U+nnnn" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).
Single-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.
Development status.
The ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of ISO/IEC 10646.

</doc>
<doc id="15031" url="http://en.wikipedia.org/wiki?curid=15031" title="IPCC (disambiguation)">
IPCC (disambiguation)

IPCC may refer to:

</doc>
<doc id="15036" url="http://en.wikipedia.org/wiki?curid=15036" title="Information security">
Information security

Information security, sometimes shortened to InfoSec, is the practice of defending information from unauthorized access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction. It is a general term that can be used regardless of the form the data may take (e.g. electronic, physical).
Overview.
Threats
Computer system threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks, and trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the IT field. Intellectual property is the ownership of property usually consisting of some form of protection. Theft of software is probably the most common in IT businesses today. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile. Cell phones are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization′s website in an attempt to cause loss of confidence to its customers. Information extortion consists of theft of a company′s property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is user carefulness.
Governments, military, corporations, financial institutions, hospitals and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Most of this information is now collected, processed and stored on electronic computers and transmitted across networks to other computers.
Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. Protecting confidential information is a business requirement and in many cases also an ethical and legal requirement. Hence a key concern for organizations today is to derive the optimal information security investment. The renowned Gordon-Loeb Model actually provides a powerful mathematical economic approach for addressing this critical concern.
For the individual, information security has a significant effect on privacy, which is viewed very differently in different cultures.
The field of information security has grown and evolved significantly in recent years. There are many ways of gaining entry into the field as a career. It offers many areas for specialization including securing network(s) and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning and digital forensics.
History.
Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands, but for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g. the UK Secret Office and Deciphering Branch in 1653).
In the mid-19th century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. The British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. In the United Kingdom this led to the creation of the Government Code and Cypher School in 1919. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than men) and where they should be stored as increasingly complex safes and storage facilities were developed. Procedures evolved to ensure documents were destroyed properly and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g. U-570).
The end of the 20th century and early years of the 21st century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. These computers quickly became interconnected through the Internet.
The rapid growth and widespread use of electronic data processing and electronic business conducted through the Internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations – all sharing the common goals of ensuring the security and reliability of information systems.
Definitions.
The definitions of InfoSec suggested in different sources are summarised below (adopted from).
1. "Preservation of confidentiality, integrity and availability of information. Note: In addition, other properties, such as authenticity, accountability, non-repudiation and reliability can also be involved." (ISO/IEC 27000:2009)
2. "The protection of information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide confidentiality, integrity, and availability." (CNSS, 2010)
3. "Ensures that only authorized users (confidentiality) have access to accurate and complete information (integrity) when required (availability)." (ISACA, 2008)
4. "Information Security is the process of protecting the intellectual property of an organisation." (Pipkin, 2000)
5. "...information security is a risk management discipline, whose job is to manage the cost of information risk to the business." (McDermott and Geer, 2001)
6. "A well-informed sense of assurance that information risks and controls are in balance." (Anderson, J., 2003)
7. "Information security is the protection of information and minimises the risk of exposing information to unauthorised parties." (Venter and Eloff, 2003)
8. "Information Security is a multidisciplinary area of study and professional activity which is concerned with the development and implementation of security mechanisms of all available types (technical, organisational, human-oriented and legal) in order to keep information in all its locations (within and outside the organisation's perimeter) and, consequently, information systems, where information is created, processed, stored, transmitted and destroyed, free from threats.
Threats to information and information systems may be categorised and a corresponding security goal may be defined for each category of threats. A set of security goals, identified as a result of a threat analysis, should be revised periodically to ensure its adequacy and conformance with the evolving environment. The currently relevant set of security goals may include: "confidentiality, integrity, availability, privacy, authenticity & trustworthiness, non-repudiation, accountability and auditability."" (Cherdantseva and Hilton, 2013)
Profession.
Information security is a stable and growing profession. Information security professionals are very stable in their employment; more than 80 percent had no change in employer or employment in the past year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.
Basic principles.
Key concepts.
The CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad — confidentiality, integrity and availability — are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) There is continuous debate about extending this classic trio. Other principles such as Accountability have sometimes been proposed for addition – it has been pointed out that issues such as Non-Repudiation do not fit well within the three core concepts.
In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks proposed the nine generally accepted principles: Awareness, Responsibility, Response, Ethics, Democracy, Risk Assessment, Security Design and Implementation, Security Management, and Reassessment. Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security proposed 33 principles. From each of these derived guidelines and practices.
In 2002, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian hexad are a subject of debate amongst security professionals.
In 2013, based on a thorough analysis of Information Assurance and Security (IAS) literature, the IAS-octave was proposed as an extension of the CIA-triad. The IAS-octave includes Confidentiality, Integrity, Availability, Accountability, Auditability, Authenticity/Trustworthiness, Non-repudiation and Privacy. The completeness and accuracy of the IAS-octave was evaluated via a series of interviews with IAS academics and experts. The IAS-octave is one of the dimensions of a Reference Model of Information Assurance and Security (RMIAS), which summarises the IAS knowledge in one all-encompassing model.
Integrity.
In information security, data integrity means maintaining and assuring the accuracy and consistency of data over its entire life-cycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity in addition to data confidentiality.
Availability.
For any information system to serve its purpose, the information must be available when it is needed. This means that the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system essentially forcing it to shut down.
Authenticity.
In computing and information security, it is necessary to ensure that the data, transactions, communications or documents (electronic or physical) are genuine. It is also important for authenticity to validate that both parties involved are who they claim to be. Some information security systems incorporate authentication features such as "digital signatures", which give evidence that the message data is genuine and was sent by someone possessing the proper signing key.
Non-repudiation.
In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction nor can the other party deny having sent a transaction.
It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message and nobody else could have altered it in transit. The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender himself, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity and thus prevents repudiation.
Electronic commerce uses technology such as digital signatures and public key encryption to establish authenticity and non-repudiation.
Risk management.
The "Certified Information Systems Auditor (CISA) Review Manual 2006" provides the following definition of risk management: "Risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization."
There are two things in this definition that may need some clarification. First, the "process" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.
Risk analysis and risk evaluation processes have their limitations since, when security incidents occur, they emerge in a context, and their rarity and even their uniqueness give rise to unpredictable threats. The analysis of these phenomena which are characterized by breakdowns, surprises and side-effects, requires a theoretical approach which is able to examine and interpret subjectively the detail of each incident.
Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.
The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). It should be pointed out that it is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called "residual risk".
A risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.
The research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:
In broad terms, the risk management process consists of:
For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.
Controls.
Selecting proper controls and implementing those will initially help an organization to bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. has defined 133 controls in different areas, but this is not exhaustive. Organizations can implement additional controls according to requirement of the organization. has cut down the number of controls to 113.
Administrative.
Administrative controls (also called procedural controls) consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.
Administrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls. Administrative controls are of paramount importance.
Logical.
Logical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. For example: passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are
logical controls.
An important logical control that is frequently overlooked is the principle of least privilege. The principle of least privilege requires that an individual, program or system process is not granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, or they are promoted to a new position, or they transfer to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges which may no longer be necessary or appropriate.
Physical.
Physical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities. For example: doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.
An important physical control that is frequently overlooked is the separation of duties. Separation of duties ensures that an individual can not complete a critical task by himself. For example: an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator – these roles and responsibilities must be separated from one another.
Defense in depth.
Information security must protect information throughout the life span of the information, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called defense in depth. The strength of any system is no greater than its weakest link. Using a defense in depth strategy, should one defensive measure fail there are other defensive measures in place that continue to provide protection.
Recall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense-in-depth strategy. With this approach, defense-in-depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense-in- depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid and each provides valuable insight into the implementation of a good defense-in-depth strategy.
Security classification for information.
An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification.
The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.
Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information.
The Business Model for Information Security enables security professionals to examine security from systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.
The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:
All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.
Access control.
Access to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected – the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.
Access control is generally considered in three steps: Identification, Authentication, and Authorization.
Identification.
Identification is an assertion of who someone is or what something is. If a person makes the statement "Hello, my name is John Doe" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming "I am the person the username belongs to".
Authentication.
Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe—a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it 
has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly by entering the correct password, the user is providing evidence that they are the person the username belongs to.
There are three different types of information that can be used for authentication:
Strong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose but in our modern world they are no longer adequate. Usernames and passwords are slowly being replaced with more sophisticated authentication mechanisms.
Authorization.
After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms—some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control or it may be derived from a combination of the three approaches.
The non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the Mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.
Examples of common access control mechanisms in use today include role-based access control available in many advanced database management systems—simple file permissions provided in the UNIX and Windows operating systems, Group Policy Objects provided in Windows network systems, Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.
To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. All failed and successful authentication attempts must be logged, and all access to information must leave some type of audit trail.
Also, need-to-know principle needs to be in affect when talking about access control. Need-to-know principle gives access rights to a person to perform their job functions. This principle is used in the government, when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee least amount privileges to prevent employees access and doing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability (C‑I‑A) triad. Need-to-know directly impacts the confidential area of the triad.
Cryptography.
Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user, who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.
Cryptography provides information security with other useful applications as well including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older less secure applications such as telnet and ftp are slowly being replaced with more secure applications such as ssh that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and Email.
Cryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.
Process.
The terms reasonable and prudent person, due care and due diligence have been used in the fields of Finance, Securities, and Law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S.A. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.
In the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the "reasonable and prudent person" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal ethical manner. A prudent person is also diligent (mindful, attentive, and ongoing) in their due care of the business.
In the field of Information Security, Harris
offers the following definitions of due care and due diligence:
"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees." And, [Due diligence are the] "continual activities that make sure the protection mechanisms are continually maintained and operational."
Attention should be made to two important points in these definitions. First, in due care, steps are taken to show - this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities - this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.
Security governance.
The Software Engineering Institute at Carnegie Mellon University, in a publication titled "Governing for Enterprise Security (GES)", defines characteristics of effective security governance. These include:
Incident response plans.
"1 to 3 paragraphs (non technical) that discuss:"
Change management.
Change management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the
processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.
Any change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of Management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.
Not every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.
Change management is usually overseen by a Change Review Board composed of representatives from key business areas, security, networking, systems administrators, Database administration, applications development, desktop support and the help desk. The tasks of the Change Review Board can be facilitated with the use of automated work flow application. The responsibility of the Change Review Board is to ensure the organizations documented change management procedures are followed. The change management process is as follows:
Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.
ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.
Business continuity.
Business continuity is the mechanism by which an organization continues to operate its critical business units, during planned or unplanned disruptions that affect normal business operations, by invoking planned and managed procedures.
Not only is business continuity simply about the business, but it also an IT system and process. Today disasters or disruptions to business are a reality. Whether the disaster is natural or man-made, it affects normal life and so business. Therefore, planning is important.
The planning is merely getting better prepared to face it, knowing fully well that the best plans may fail. Planning helps to reduce cost of recovery, operational overheads and most importantly sail through some smaller ones effortlessly.
For businesses to create effective plans they need to focus upon the following key questions. Most of these are common knowledge, and anyone can do a BCP.
Disaster recovery planning.
While a business continuity plan (BCP) takes a broad approach to dealing with organizational-wide effects of a disaster, a disaster recovery plan (DRP), which is a subset of the business continuity plan, is instead focused on taking the necessary steps to resume normal business operations as quickly as possible. A disaster recovery plan is executed immediately after the disaster occurs and details what steps are to be taken in order to recover critical information technology infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.
Laws and regulations.
"Below is a partial listing of European, United Kingdom, Canadian and US governmental laws and regulations that have, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security."
Information Security Culture.
Employee’s behavior has a big impact to information security in organizations. Cultural concept can help different segments of the organization to concern about the information security within the organization.″Exploring the Relationship between Organizational Culture and Information Security Culture″ provides the following definition of information security culture: ″ISC is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds.″
Information security culture needs to be improved continuously. In ″Information Security Culture from Analysis to Change″, authors commented, ″It′s a never ending process, a cycle of evaluation and change or maintenance.″ To manage the information security culture, five steps should be taken: Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.
Sources of standards.
International Organization for Standardization (ISO) is a consortium of national standards
institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is
the world's largest developer of standards. ISO 15443: "Information technology - Security techniques - A framework
for IT security assurance", ISO/IEC 27002: "Information technology - Security techniques - Code of practice for information security management", ISO-20000: "Information technology - Service management", and ISO/IEC 27001: "Information technology - Security techniques - Information security management systems - Requirements" are of particular interest to information security professionals.
The US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency
within the U.S. Department of Commerce. The NIST Computer Security Division
develops standards, metrics, tests and validation programs as well as publishes standards and guidelines to
increase secure IT planning, implementation, management and operation. NIST is also the custodian of the US Federal Information Processing Standard publications (FIPS).
The Internet Society is a professional membership society with more than 100 organization
and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the
future of the Internet, and is the organization home for the groups responsible for Internet infrastructure standards,
including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.
The Information Security Forum is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.
The Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The Institute developed the This framework describes the range of competencies expected of Information Security and Information Assurance Professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organisations and world-renowned academics and security leaders.
The German Federal Office for Information Security (in German "Bundesamt für Sicherheit in der Informationstechnik (BSI)") BSI-Standards 100-1 to 100-4 are a set of recommendations including "methods, processes, procedures, approaches and measures relating to information security". The BSI-Standard 100-2 "IT-Grundschutz Methodology" describes how an information security management can be implemented and operated. The Standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005 the catalogs were formerly known as "IT Baseline Protection Manual". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4.400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.
At the European Telecommunications Standards Institute a catalog of Information security indicators have been standardized by the Industrial Specification Group (ISG) ISI.
Conclusion.
The never ending process of information security involves ongoing training, assessment, protection, monitoring and detection, incident response and repair, documentation, and review. This makes information security an indispensable part of all the business operations across different domains.

</doc>
<doc id="15044" url="http://en.wikipedia.org/wiki?curid=15044" title="Irish">
Irish

 
Irish may refer to something of, from, or related to Ireland, an island situated off the north-western coast of continental Europe including:

</doc>
<doc id="15047" url="http://en.wikipedia.org/wiki?curid=15047" title="Internalism and externalism">
Internalism and externalism

Internalism and externalism are two opposing ways of explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings. Usually 'internalism' refers to the belief that an explanation can be given of the given subject by pointing to things which are internal to the person or their mind which is considering them. Conversely, externalism holds that it is things about the world which motivate us, justify our beliefs, determine meaning, etc.
Moral philosophy.
Motivation.
In contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary, internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper ""Ought" and Motivation").
These views in moral psychology have various implications. In particular, if motivational internalism is true, then an amoralist is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational "externalist", because the motivational externalist thinks that moral judgments about the right thing to do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire—such as the desire to do the right thing—is required (Brink, 2003),(Rosati, 2006).
Reasons.
There is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for action. An "internal reason" is, roughly, something that one has in light of one's own "subjective motivational set"---one's own commitments, desires (or wants), goals, etc. On the other hand, an "external reason" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not kill oneself no matter what—regardless of whether one wants to die.
Some philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called "internalism about reasons" (or "reasons internalism"). "Externalism about reasons" (or "reasons externalism") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.
Consider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative ("Yes, Sasha has a reason not to steal from that poor person."), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative ("No, Sasha does not have a reason not to steal from that poor person, though others might."). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason "for Sasha's action" not to steal from the poor person next to him only if he currently "wants" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals—that is, part of what Williams calls his "subjective motivational set"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.
Epistemology.
Justification.
In contemporary epistemology, internalism about justification is the idea that everything necessary to provide justification for a belief must be immediately available to an agent's consciousness. Externalism in this context is the view that factors other than those internal to the believer can affect the justificatory status of a belief. One strand of externalism is reliabilism, and the causal theory of knowledge is sometimes considered to be another strand. It is important to distinguish internalism about justification from internalism about knowledge. An internalist about knowledge will likely hold that the conditions that distinguish mere true belief from knowledge are similarly internal to the individual's perspective or grounded in the subject's mental states. Whereas internalism about justification is a widely endorsed view, there is debate about knowledge internalism, due to Edmund Gettier and his Gettier-examples. These are claimed to show that knowledge is not simply justified true belief. In a short but influential paper published in 1963, Gettier produced examples that seemed to show that someone could be justified in believing something which is actually false, and inferring from it a further belief, this belief being coincidentally true. In this way, he claimed that someone could be justified in believing something true but nevertheless not be considered to have knowledge of that thing.
One line of argument in favor of externalism begins with the observation that if what justified our beliefs failed to eliminate significantly the risk of error, then it does not seem that knowledge would be attainable as it would appear that when our beliefs did happen to be correct, this would really be a matter of good fortune. While many will agree with this last claim, the argument seems inconclusive. Setting aside sceptical concerns about the possession of knowledge, Gettier cases have suggested the need to distinguish justification from warrant where warrant is that which distinguishes justified true belief from knowledge by eliminating the kind of accidentality often present in Gettier-type cases. Even if something must significantly reduce the risk of error, it is not clear why justification is what must fill the bill.
One of the more popular arguments for internalism begins with the observation, perhaps first due to Stewart Cohen, that when we imagine subjects completely cut off from their surroundings (thanks to a malicious Cartesian demon, perhaps) we do not think that in cutting these individuals off from their surroundings, these subjects cease to be rational in taking things to be as they appear. The 'new evil demon' argument for internalism (and against externalism) begins with the observation that individuals like us on the inside will be as justified as we are in believing what we believe. As it is part of the story that these individuals' beliefs are not produced by reliable mechanisms or backed by veridical perceptual experiences, the claim that the justification of our beliefs depends upon such things appears to be seriously challenged. Externalists have offered a variety of responses but there is no consensus among epistemologists as to whether these replies are successful (Cohen, 1984; Sosa, 1991).
As a response to skepticism.
In responding to skepticism, Hilary Putnam (1982 ) claims that semantic externalism yields "an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word "water" referred to the substance whose chemical composition is H2O even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling "water" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999 ):
Either I am a BIV, or I am not a BIV.
If I am not a BIV, then when I say "I am not a BIV", it is true.
If I am a BIV, then, when I say "I am not a BIV", it is true (because "brain" and "vat" would only pick out the brains and vats being simulated, not real brains and real vats).
My utterance of "I am not a BIV" is true.
To clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived "Steve." When Steve is given an experience of walking through a park, semantic externalism allows for his thought, "I am walking through a park" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, "I am a brain in a vat," to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.
Apart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words "brain" and "vat" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe "I am not a brain in a vat," then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).
Another attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are "about" things, unlike a BIV's thoughts, which cannot be "about" things (DeRose, 1999 ).
Semantics.
Semantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.
Externalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.
See also:
Philosophy of mind.
Within the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.
The traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various form of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.
As to the traditional discussion on semantic externalism (often dubbed "content externalism"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, "The Meaning of 'Meaning'," (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.
For example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, "I want my mommy," he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, "I want my mommy," will be different from what Tina wants and what she says she wants when she says, "I want my mommy."
Externalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.
Philosophers now tend to distinguish between "wide content" (externalist mental content) and "narrow content" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.
Critics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.
Critics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains H2O. It seems this speaker could know a priori that she thinks that water is wet. This is the thesis of privileged access. It also seems that she could know on the basis of simple thought experiments that she can only think that water is wet if she lives in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.
As mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.
See also
Historiography of science.
Externalism in the historiography of science is the view that the history of science is due to its social context – the socio-political climate and the surrounding economy determines scientific progress.
Internalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity.

</doc>
<doc id="15064" url="http://en.wikipedia.org/wiki?curid=15064" title="Intel 8088">
Intel 8088

The Intel 8088 ("eighty-eighty-eight", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on July 1, 1979, the 8088 had an 8-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range were unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)—only the bus interface unit (BIU) is different. The original IBM PC was based on the 8088.
History and description.
The 8088 was designed in Israel, at Intel's Haifa laboratory, as with a large number of Intel's processors. The 8088 was targeted at economical systems by allowing the use of an 8-bit data path and 8-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then new design office and laboratory in Haifa, Israel.
Variants of the 8088 with more than 5 MHz maximum clock frequency include the 8088-2, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximum frequency of 8 MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8 MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16 MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licenced Dynalogic Hyperion clone, in a move that was regarded as signalling a major new direction for the company.
When announced, the list price of the 8088 was US $124.80.
Differences to the 8086.
The 8088 is architecturally very similar to the 8086. The main difference is that there are only 8 data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer BHE (this is the high order byte select on the 8086 - the 8088 does not have a high order byte on its 8 bit data bus).:5–97 Instead it outputs a maximum mode status, SSO. Combined with the IO/M and DT/R signals, the bus cycles can be decoded (It generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals if a memory access or input/output access is being made has had it sense reversed. The pin on the 8088 is IO/M. On the 8086 part it is IO/M. The reason for the reversal is that it makes the 8088 compatible with the 8085.:5–98
Performance.
Depending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the average performance for the Intel 8088 ranged from approximately 0.33–1 million instructions per second. Meanwhile, the mov "reg,reg" and ALU "reg,reg" instructions taking two and three cycles respectively yielded an "absolute peak" performance of between 1/3 and 1/2 MIPS per MHz, that is, somewhere in the range 3–5 MIPS at 10 MHz.
The speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to 8 bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the 4-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means 4 clocks to transfer 2 bytes, on the 8088 it is 4 clocks per byte. Therefore, for example, a 2-byte shift or rotate instruction, which takes the EU only 2 clock cycles to execute, actually takes eight clocks to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and 
In general, because so many basic instructions execute in fewer than four clocks per instruction byte—including almost all the ALU and data-movement instructions on register operands and some of these on memory operands—it is practically impossible to avoid idling the EU in the 8088 at least 1/4 of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).
A side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.
The 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. (There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU.) Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four clock period bus transfer cycle is not particularly streamlined. (Contrast the two clock period bus cycle of the 6502 CPU and the 80286's three clock period bus cycle with pipelining down to two cycles for most transfers.) Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. (The same is probably not true on the 80286 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.)
Finally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires 4 clock cycles if not taken, but if taken it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.
Intel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100-200 clock cycles each. Many simple multiplications by small constants (besides powers of two, for which shifts can be used) can be done much faster using dedicated short subroutines. (The 80286 and 80386 each greatly increased the execution speed of these multiply and divide instructions.)
Selection for use in the IBM PC.
The original IBM PC was the most influential microcomputer to use the 8088. It used a clock frequency of 4.77 MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some would have preferred the new Motorola 68000, while others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which had been used in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.
IBM chose the 8088 over the 8086 because Intel offered a better price for the former, and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's) which were already well known by many engineers, further reducing cost.
The descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software compatible processors, which are in use today.

</doc>
<doc id="15068" url="http://en.wikipedia.org/wiki?curid=15068" title="Infantry">
Infantry

Infantry is the branch of a military force that fights on foot. As the troops who are intended to engage, fight, and defeat the enemy in face-to-face combat, they bear the brunt of warfare and typically suffer the greatest number of casualties. Historically, as the oldest branch of the combat arms, the infantry are the tip of the spear of a modern army, and continually undergo training that is more physically stressful and psychologically demanding than that of any other branch of the combat arms.
Infantry can enter and maneuver in terrain that is inaccessible to military vehicles and employ crew-served infantry support weapons that provide greater and more sustained firepower. The transport and delivery techniques of modern infantrymen to engage in battle include marching, mechanised transport, airborne (by parachute or by helicopter) and amphibious landing from the sea.
History and etymology.
In English, the 16th-century term Infantry (ca. 1570s) describes soldiers who walk to the battlefield, and there engage, fight, and defeat the enemy in direct combat, usually to take and occupy the terrain. As describing the branch of the combat arms, the term "Infantry" derives from the French Infanterie, which, in turn, is derived from the Italian Fanteria and ultimately from the Latin Infantera; the individual-soldier term Infantryman (1837) was not coined until the 19th century. Historically, before the invention and the introduction of firearms to warfare, the foot soldiers of previous eras—armed with blunt and edged weapons, and a shield—also are considered and identified as infantrymen.
The term arose in Sixteenth-Century Spain, which boasted the first professional standing army seen in Europe since the days of Rome. It was common to appoint royal princes (Infantes) to military commands, and the men under them became known as Infanteria.
In the Western world, during the Græco–Roman Antiquity (8th–7th centuries BC), and during the Middle Ages (AD 476–1453), infantry soldiers were categorized, characterised, and identified according to the type of weapons and armour with which they were armed, thus heavy infantry (hoplite) and light infantry (Greek peltasts, Roman velites). Since the application of firearms to warfare, the classifications of infantrymen have changed to reflect their formations on the battlefield, such as line infantry, and to reflect the modes of transporting them to the battlefield, and the tactics deployed by specific types of combat units, such as mechanized infantry and airborne infantry.
Combat role.
As a branch of the armed forces, the role of the infantry in warfare is to engage, fight, and kill the enemy at close range—using either a firearm (rifle, pistol, machine gun), an edged-weapon (knife, bayonet), or bare hands (close quarters combat)—as required by the mission to hand; thus
Beginning with the Napoleonic Wars of the early 19th century, artillery has become an increasingly dominant force on the battlefield. Since World War I, combat aircraft and armoured vehicles have also become dominant. However, the most effective method for locating all enemy forces on a battlefield is still the infantry patrol, and it is the presence or absence of infantry that ultimately determines whether a particular piece of ground has been captured or held. In 20th and 21st century warfare, infantry functions most effectively as part of a combined arms team including artillery, armour, and combat aircraft. Studies have shown that of all casualties, 50% or more were caused by artillery; about 10% were caused by machine guns; 2–5% by rifle fire; and 1% or less by hand grenades, bayonets, knives, and unarmed combat combined. Several infantry divisions both Allied and Axis in the European theatre of WWII suffered higher than 100% combat and non combat casualties and some above 200%, meaning that the number of service personnel that became casualties was greater than the sum of the divisions' available service positions at full strength.
Organization.
Infantry is notable by its reliance on organized formations to be employed in battle. These have been developed over time, but remain a key element to effective infantry development and deployment. Until the end of the 19th century, infantry units were for the most part employed in closely organized formations up until the actual moment of contact with the enemy. This was necessary to allow commanders to retain control of the unit, especially while maneuvering, as well as allowing officers to retain discipline amongst the ranks.
With the development of machine guns and other weapons with increased firepower, it became necessary to disperse soldiers in infantry units to make them less vulnerable to such weapons. From World War I, it was recognized that infantry were most successfully employed when using their ability to maneuver in constricted terrain, and evade detection in ways not possible for other weapons such as vehicles. This decentralization of command was made possible by improved communications equipment and greater focus on small unit training.
Among the various subtypes of infantry is "Medium infantry." This refers to infantry which are less heavily armed and armored than heavy infantry, but more so than light infantry. In the early modern period, medium infantry were largely eliminated due to discontinued use of body armour up until the 20th century. In the United States Army, Stryker Infantry is considered Medium Infantry, since they are "heavier" than light infantry but "lighter" than mechanized infantry.
Doctrine.
Infantry doctrine is the concise expression of how infantry forces contribute to campaigns, major operations, battles, and engagements. It is a guide to action, not a set of hard and fast rules.
Doctrine provides a very common frame of reference across the military forces, allowing the infantry to function cooperatively in what are now called combined arms operations. Doctrine helps standardise operations, facilitating readiness by establishing common ways of accomplishing infantry tasks. Doctrine links theory, history, experimentation, and practice. Its objective is to foster initiative and creative thinking in the infantry's tactical combat environment.
Doctrine provides the infantry with an authoritative body of statements on how infantry forces conduct operations and provides a common lexicon for use by infantry planners and leaders.
Until the development of effective artillery doctrines, and more recently precision guided air delivered ordnance, the most recent important role of the infantry has been as the primary force of inflicting casualties on the enemy through aimed fire. The infantry is also the only combat arm which can ultimately decide whether any given tactical position is occupied, and it is the presence of infantry that assures control of terrain. While the tactics of employment in battle have changed, the basic missions of the infantry have not.
Retractions to the Infantry Concept: Although it has been argued that infantrymen and infantry tactics are an antiquated and careless use of military manpower and resources, the infantryman has proven quite capable against many units, some much more technological and modern. For instance, light infantry has proven to be extremely effective against tank units by being able to take advantage of a tank's limited field of fire and sight by swarming enemy armor units and utilizing anti-armor rockets at long range or grenades in close quarters. Furthermore, air bombardment that can flatten entire cities has been shown to be completely useless against a dug in infantry force. (see Battle of Stalingrad 1942–1943) Even an occupying enemy police force has sometimes been shown to be a poor match against a clandestine infantry that has secreted itself away in a civilian population. (see French Resistance WWII, Iraqi Insurgency in Fallujah, American Revolution)
Operations.
Attack operations are the most basic role of the infantry, and along with defense, form the main stances of the infantry on the battlefield. Traditionally, in an open battle, or meeting engagement, two armies would maneuver to contact, at which point they would form up their infantry and other units opposite each other. Then one or both would advance and attempt to defeat the enemy force. The goal of an attack remains the same: to advance into an enemy-held "objective," most frequently a hill, river crossing, city or other dominant terrain feature, and dislodge the enemy, thereby establishing control of the objective.
Attacks are often feared by the infantry conducting them because of the high number of casualties suffered while advancing to close with and destroy the enemy while under enemy fire. In mechanized infantry the armored personnel carrier (APC) is considered the assaulting position. These APCs can deliver infantrymen through the front lines to the battle and—in the case of infantry fighting vehicles—contribute supporting firepower to engage the enemy. Successful attacks rely on sufficient force, preparative reconnaissance and battlefield preparation with bomb assets. Retention of discipline and cohesion throughout the attack is paramount to success. A subcategory of attacks is the ambush, where infantrymen lie in wait for enemy forces before attacking at a vulnerable moment. This gives the ambushing infantrymen the combat advantage of surprise, concealment and superior firing positions, and causes confusion. The ambushed unit does not know what it is up against, or where they are attacking from.
Defense operations are the natural counter to attacks, in which the mission is to hold an objective and defeat enemy forces attempting to dislodge the defender. Defensive posture offers many advantages to the infantry, including the ability to use terrain and constructed fortifications to advantage; these reduce exposure to enemy fire compared with advancing forces. Effective defense relies on minimizing losses to enemy fire, breaking the enemy's cohesion before their advance is completed, and preventing enemy penetration of defensive positions.
Patrol is the most common infantry mission. Full-scale attacks and defensive efforts are occasional, but patrols are constant. Patrols consist of small groups of infantry moving about in areas of possible enemy activity to locate the enemy and destroy them when found. Patrols are used not only on the front-lines, but in rear areas where enemy infiltration or insurgencies are possible.
Pursuit is a role that the infantry often assumes. The objective of pursuit operations is the destruction of withdrawing enemy forces which are not capable of effectively engaging friendly units, before they can build their strength to the point where they are effective. Infantry traditionally have been the main force to overrun these units in the past, and in modern combat are used to pursue enemy forces in constricted terrain (urban areas in particular), where faster forces, such as armoured vehicles are incapable of going or would be exposed to ambush.
Escort consists of protecting support units from ambush, particularly from hostile infantry forces. Combat support units (a majority of the military) are not as well armed or trained as infantry units and have a different mission. Therefore, they need the protection of the infantry, particularly when on the move. This is one of the most important roles for the modern infantry, particularly when operating alongside armored vehicles. In this capacity, infantry essentially conducts patrol on the move, scouring terrain which may hide enemy infantry waiting to ambush friendly vehicles, and identifying enemy strong points for attack by the heavier units.
Maneuver operations consume much of an infantry unit's time. Infantry, like all combat arms units, are often maneuvered to meet battlefield needs, and often must do so under enemy attack. The infantry must maintain their cohesion and readiness during the move to ensure their usefulness when they reach their objective. Traditionally, infantry have relied on their own legs for mobility, but mechanised or armoured infantry often uses trucks and armored vehicles for transport. These units can quickly disembark and transition to light infantry, without vehicles, to access terrain which armoured vehicles can't effectively access.
Reconnaissance/intelligence gathering Surveillance operations are often carried out with the employment of small recon units or sniper teams which gather information about the enemy, reporting on characteristics such as size, activity, location, unit and equipment. These infantry units typically are known for their stealth and ability to operate for periods of time within close proximity of the enemy without being detected. They may engage high profile targets, or be employed to hunt down terrorist cells and insurgents within a given area. These units may also entice the enemy to engage a located recon unit, thus disclosing their location to be destroyed by more powerful friendly forces.
Reserve assignments for infantry units involve deployment behind the front, although patrol and security operations are usually maintained in case of enemy infiltration. This is usually the best time for infantry units to integrate replacements into units and to maintain equipment. Additionally, soldiers can be rested and general readiness should improve. However, the unit must be ready for deployment at any point.
Construction can be undertaken either in reserve or on the front, but consists of using infantry troops as labor for construction of field positions, roads, bridges, airfields, and all other manner of structures. The infantry is often given this assignment because of the physical quantity of strong men within the unit, although it can lessen a unit's morale and limit the unit's ability to maintain readiness and perform other missions. More often, such jobs are given to specialist engineering corps.
Base defense – Infantry units are tasked to protect certain areas like command posts or airbases. Units assigned to this job usually have a large number of military police attached to them for control of checkpoints and prisons.
Raid/Hostage Rescue – Infantry units are trained to quickly mobilise, infiltrate, enter and neutralise threat forces when appropriate combat intelligence indicates to secure a location, rescue or capture high profile targets.
Urban Combat – Urban combat poses unique challenges to the combat forces. It is one of the most complicated type of operations an infantry unit will undertake. With many places for the enemy to hide and ambush from, infantry units must be trained in how to enter a city, and systematically clear the buildings, which most likely will be booby trapped, in order to kill or capture enemy personnel within the city. Care must be taken to differentiate innocent civilians who often hide and support the enemy from the nonuniformed armed enemy forces. Civilian and military casualties both are usually very high.
Day to day service.
Because of an infantryman's duties with firearms, explosives, physical and emotional stress, physical violence, casualties and deaths are not uncommon in both war and in peacetime training or operations. It is a highly dangerous and demanding combat service and in World War II military doctors concluded that even physically unwounded soldiers were psychologically worn out after about 200 days of combat.
The physical, mental and environmental operating demands of the infantryman are high. All of the combat necessities such as ammunition, weapon systems, food, water, clothing and shelter are carried on the backs of the infantrymen, at least in light role as opposed to mounted/mechanised. Combat loads of over 36 kg (80 lbs) are standard, and greater loads in excess of 45 kg (100 lbs) are very common. , These heavy loads, combined with long foot patrols of over 40 km a day, in any climate from 43 to in temperature, require the infantryman to be in good physical and mental shape. Infantrymen live, fight and die outdoors in all types of brutal climates, often with no physical shelter. Poor climate conditions adds misery to this already demanding existence. Disease epidemics, frostbite, heat stroke, trench foot, insect and wild animal bites are common along with stress disorders and these have on multiple occsions caused more casualties than enemy action.
Despite the hardships, infantrymen are expected to continue with their combat missions despite death and injury of friends, fear, despair, fatigue and bodily injury.
Some infantry units are considered Special Forces. The earliest Special Forces commando units were more highly trained infantrymen, with special weapons, equipment and missions. Special Forces units recruit heavily from regular infantry units to fill their ranks.
Foreign and domestic militaries typically have a slang term for their infantrymen. In the U.S. military, the slang term among both Marine and Army infantrymen for themselves is "grunt." In the British Army, they are the "squaddies." The infantry is a small close-knit community, and the slang names are terms of endearment that convey mutual respect and shared experiences.
Equipment and training.
In the past infantrymen were just a mass of hastily trained conscripts hastily armed with whatever could be quickly provided. In modern times, the infantryman can be a highly trained and equipped specialist in his own right.
The equipment of infantry forces has evolved along with the development of military technology and tactics in general, but certain constants remain regarding the design and selection of this equipment. Primary types of equipment are weaponry, protective gear, survival gear, and special, mission specific equipment. Infantry tactics have become much more involved, and yet must be learned and rehearsed until they become second nature when the infantry soldier is stumbling with fatigue and in the middle of the "fog of war." Spreading out, making use of cover and concealment, monitoring team-mates and leaders, and watching for the enemy must all become instinctive and simultaneous.
Infantry weapons have included all types of personal weapons, i.e., anything that can be handled by individual soldiers, as well as some light crew-served weapons that can be carried. During operations, especially in modern times, the infantry often scavenge and employ whatever weapons and equipment they can acquire from both friend and foe, in addition to those issued to them by their available supply chain.
Infantry of ancient times through the Renaissance wielded a wide array of non-gunpowder weaponry. Infantry formations used all sorts of melee weapons, such as various types of swords, axes, and maces; shock weapons, such as spears and pikes; and ranged weapons such as javelins, bows, and slings. Their crew-served weapons were the ballista and the battering ram. Infantry of these premodern periods also wore a variety of personal body armour, including chain mail and cuirasses. Many of the premodern infantry weapons evolved over time to counter these advances in body armor, such as the falchion and crossbow, which were designed to pierce chain mail armor and wound the underlying body.
Modern infantrymen may be trained to use equipment in addition to their personal rifles, such as hand guns or pistols, shotguns, machine guns, anti-tank missiles, anti-personnel mines, other incendiary and explosive devices, bayonets, GPS, map and compass, encrypted communications equipment, booby traps, surveillance equipment, night vision equipment, sensitive intelligence documents, classified weapon systems and other sensitive equipment.
Protective equipment and survival gear.
Infantry protective gear includes all equipment designed to protect the soldier against enemy attack. Most protective gear comprises personal armor of some type. Ancient and medieval infantry used shields and wore leather and metal alloys for armour, as defense against both missile and hand-to-hand weapons. With the advent of effective firearms such as the arquebus, large numbers of men could be quickly trained into effective fighting forces, and such armour became thicker while providing less overall coverage to meet the threat of early firearms, which could only pierce this armour at close range. Generally, only pikemen were armoured in this fashion; gunners went largely unarmoured, due to the expense as well as the impracticality of armouring large numbers of men who were not expected to fight in close quarters where it would be most useful. As firearms became more powerful and armour became less useful against gunfire, the ratio of gunners to pikemen increased, until the advent of the bayonet rendered the latter entirely obsolete. While it became clear to most military leaders that the pikeman was now outdated, some armies stubbornly clung to the pike, though pikemen, too, would abandon their armour, until only specialized and prestigious cavalry units retained any significant armour coverage; the infantryman from this point went entirely unarmoured. The return to the use of the helmet was prompted by the need to defend against high explosive fragmentation and concussion, and further developments in materials led to effective bullet-defeating body armour such as Kevlar, within the weight acceptable for infantry use.
Beginning in the Vietnam War, the use of personal body armour has again become widespread among infantry units. Infantrymen must also often carry protective measures against chemical and biological attack, including gas masks, counter-agents, and protective suits. All of these protective measures add to the weight an infantryman must carry, and may decrease combat efficiency. Modern militaries are struggling to balance the value of personal body protection versus the weight burden and ability to move under such weight.
Infantry survival gear includes all of the items soldiers require for day-to-day survival in the combat environment. These include basic environmental protections, medical supplies, food, and sundries. As the amount of equipment a soldier can carry is very limited, efforts have been made to make equipment light and compact. Equipment is carried in tactical gear (such as ALICE), which should be comfortable to wear for extended periods of time, hamper movement as little as possible and be compatible with other things a soldier can be expected to carry, such as field radios and spare ammunition. Infantry have suffered high casualty rates from disease, exposure, and privation—often in excess of the casualties suffered from enemy attacks. Better equipment of troops to protect against these environmental factors greatly reduces these rates of loss. One of the most valuable pieces of gear is the entrenching tool—basically a folding spade—which can be employed not only to dig important defenses, but also in a variety of other daily tasks and even as a weapon.
Specialized equipment consists of a variety of gear which may or may not be carried, depending on the mission and the level of equipment of an army. Communications gear has become a necessity, as it allows effective command of infantry units over greater distances, and communication with artillery and other support units. In some units, individual communications are being used to allow the greatest level of flexibility. Engineering equipment, including explosives, mines, and other gear, is also commonly carried by the infantry or attached specialists. A variety of other gear, often relating to a specific mission, or to the particular terrain in which the unit is employed, can be carried by infantry units.
Other infantry.
Infantry in air forces, such as the Royal Air Force Regiment and the Royal Australian Air Force Airfield Defence Guards, are used primarily for ground-based defence of air bases and other, air force facilities. They also have a number of other, specialist roles, including Chemical, Biological, Radiological and Nuclear (CBRN) defence and training other, air force personnel in basic ground defence tactics.
Naval infantry, commonly known as marines, are a category of infantry that form part of a state’s naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

</doc>
<doc id="15076" url="http://en.wikipedia.org/wiki?curid=15076" title="International Data Encryption Algorithm">
International Data Encryption Algorithm

In cryptography, the International Data Encryption Algorithm (IDEA), originally called Improved Proposed Encryption Standard (IPES), is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991. The algorithm was intended as a replacement for the Data Encryption Standard (DES). IDEA is a minor revision of an earlier cipher, Proposed Encryption Standard (PES).
The cipher was designed under a research contract with the Hasler Foundation, which became part of Ascom-Tech AG. The cipher was patented in a number of countries but was freely available for non-commercial use. The name “IDEA” is also a trademark. The last patents expired in 2012 and IDEA is now patent-free and thus free to use.
IDEA was used in Pretty Good Privacy (PGP) v2.0, and was incorporated after the original cipher used in v1.0, BassOmatic, was found to be insecure. IDEA is an optional algorithm in the OpenPGP standard.
Operation.
IDEA operates on 64-bit blocks using a 128-bit key, and consists of a series of eight identical transformations (a "round", see the illustration) and an output transformation (the "half-round"). The processes for encryption and decryption are similar. IDEA derives much of its security by interleaving operations from different groups — modular addition and multiplication, and bitwise eXclusive OR (XOR) — which are algebraically "incompatible" in some sense. In more detail, these operators, which all deal with 16-bit quantities, are:
After the eight rounds comes a final “half round”, the output transformation illustrated below (the swap of the middle two values cancels out the swap at the end of the last round, so that there is no net swap):
Structure.
The overall structure of IDEA follows the Lai-Massey scheme. XOR is used for both subtraction and addition. IDEA uses a key-dependent half-round function. To work with 16 bit words (meaning four inputs instead of two for the 64 bit block size), IDEA uses the Lai-Massey scheme twice in parallel, with the two parallel round functions being interwoven with each other. To ensure sufficient diffusion, two of the sub-blocks are swapped after each round.
Key schedule.
Each round uses six 16-bit sub-keys, while the half-round uses four, a total of 52 for 8.5 rounds. The first eight sub-keys are extracted directly from the key, with K1 from the first round being the lower sixteen bits; further groups of eight keys are created by rotating the main key left 25 bits between each group of eight. This means that it is rotated less than once per round, on average, for a total of six rotations.
Decryption.
Decryption works like encryption, but the order of the round keys is inverted, and each value of subkeys K1 – K4 is replaced by its inverse for the respective group operation (K5 and K6 of each group should not be changed for decryption).
Security.
The designers analysed IDEA to measure its strength against differential cryptanalysis and concluded that it is immune under certain assumptions. No successful linear or algebraic weaknesses have been reported. s of 2007[ [update]], the best attack which applied to all keys could break IDEA reduced to 6 rounds (the full IDEA cipher uses 8.5 rounds). Note that a "break" is any attack which requires less than 2128 operations; the 6-round attack requires 264 known plaintexts and 2126.8 operations.
Bruce Schneier thought highly of IDEA in 1996, writing, "In my opinion, it is the best and most secure block algorithm available to the public at this time." ("Applied Cryptography", 2nd ed.) However, by 1999 he was no longer recommending IDEA due to the availability of faster algorithms, some progress in its cryptanalysis, and the issue of patents.
In 2011 full 8.5-round IDEA was broken using a meet-in-the-middle attack. Independently in 2012, full 8.5 round IDEA was broken using a narrow-bicliques attack, with a reduction of cryptographic strength of about two bits, similar to the effect of the previous bicliques attack on AES.
Weak keys.
The very simple key schedule makes IDEA subject to a class of weak keys; some keys containing a large number of 0 bits produce weak encryption. These are of little concern in practice, being sufficiently rare that they are unnecessary to avoid explicitly when generating keys randomly. A simple fix was proposed: exclusive-ORing each subkey with a 16-bit constant, such as codice_1.
Larger classes of weak keys were found in 2002.
This is still of negligible probability to be a concern to a randomly chosen key, and some of the problems are fixed by the constant XOR proposed earlier, but the paper is not certain if all of them are. A more comprehensive redesign of the IDEA key schedule may be desirable.
Availability.
A patent application for IDEA was first filed in Switzerland (CH A 1690/90) on May 18, 1990, then an international patent application was filed under the Patent Cooperation Treaty on May 16, 1991. Patents were eventually granted in Austria, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland, the United Kingdom, (European Patent Register entry for , filed May 16, 1991, issued June 22, 1994 and expired May 16, 2011), the United States (U.S. Patent , issued May 25, 1993 and expired January 7, 2012) and Japan (JP 3225440) (expired May 16, 2011).
MediaCrypt AG is now offering a successor to IDEA and focuses on its new cipher (official release on May 2005) IDEA NXT, which was previously called FOX.

</doc>
<doc id="15100" url="http://en.wikipedia.org/wiki?curid=15100" title="Interlingua">
Interlingua

Interlingua (; ISO 639 language codes "ia", "ina") is an international auxiliary language (IAL), developed between 1937 and 1951 by the International Auxiliary Language Association (IALA). It ranks among the top most widely used IALs (along with Esperanto and Ido), and is the most widely used naturalistic IAL: in other words, its vocabulary, grammar and other characteristics are derived from natural languages. Interlingua was developed to combine a simple, mostly regular grammar with a vocabulary common to the widest possible range of languages, making it unusually easy to learn, at least for those whose native languages were sources of Interlingua's vocabulary and grammar. Conversely, it is used as a rapid introduction to many natural languages.
Interlingua literature maintains that (written) Interlingua is comprehensible to the hundreds of millions of people who speak a Romance language, though it is actively spoken by only a few hundred.
The name Interlingua comes from the Latin words "inter", meaning between, and "lingua", meaning tongue or language. These morphemes are identical in Interlingua. Thus, Interlingua would be "between language", or "intermediary language".
Rationale.
The expansive movements of science, technology, trade, diplomacy, and the arts, combined with the historical dominance of the Greek and Latin languages have resulted in a large common vocabulary among European languages. With Interlingua, an objective procedure is used to extract and standardize the most widespread word or words for a concept found in a set of "control languages": English, French, Italian, Spanish and Portuguese, with German and Russian as secondary references. Words from any language are eligible for inclusion, so long as their internationality is shown by their presence in these control languages. Hence, Interlingua includes such diverse word forms as Japanese "geisha" and "samurai", Arabic "califa", Guugu Yimithirr "gangurru" (Interlingua: kanguru), and Finnish "sauna".
Interlingua combines this pre-existing vocabulary with a minimal grammar based on the control languages. People with a good knowledge of a Romance language, or a smattering of a Romance language plus a good knowledge of the "international scientific vocabulary" can frequently understand it immediately on reading or hearing it. Educated speakers of English also enjoy this easy comprehension. The immediate comprehension of Interlingua, in turn, makes it unusually easy to learn. Speakers of other languages can also learn to speak and write Interlingua in a short time, thanks to its simple grammar and regular word formation using a small number of roots and affixes.
Once learned, Interlingua can be used to learn other related languages quickly and easily, and in some studies, even to understand them immediately. Research with Swedish students has shown that, after learning Interlingua, they can translate elementary texts from Italian, Portuguese, and Spanish. In one 1974 study, an Interlingua class translated a Spanish text that students who had taken 150 hours of Spanish found too difficult to understand. Gopsill has suggested that Interlingua's freedom from irregularities allowed the students to grasp the mechanisms of language quickly.
Words in Interlingua retain their original form from the source language; they are altered as little as possible to fit Interlingua's phonotactics. Each word retains its original spelling, pronunciation, and meanings. For this reason, Interlingua is frequently termed a "naturalistic" IAL.
When compared to natural languages, Interlingua most resembles Spanish.
History.
The American heiress Alice Vanderbilt Morris (1874–1950) became interested in linguistics and the international auxiliary language movement in the early 1920s, and in 1924, Morris and her husband, Dave Hennen Morris, established the non-profit International Auxiliary Language Association (IALA) in New York City. Their aim was to place the study of IALs on a scientific basis. Morris developed the research program of IALA in consultation with Edward Sapir, William Edward Collinson, and Otto Jespersen.
International Auxiliary Language Association.
The IALA became a major supporter of mainstream American linguistics, funding, for example, numerous studies by Sapir, Collinson, and Morris Swadesh in the 1930s and 1940s. Alice Morris edited several of these studies and provided much of IALA's financial support. IALA also received support from such prestigious groups as the Carnegie Corporation, the Ford Foundation, the Research Corporation, and the Rockefeller Foundation.
In its early years, IALA concerned itself with three tasks: finding other organizations around the world with similar goals; building a library of books about languages and interlinguistics; and comparing extant IALs, including Esperanto, Esperanto II, Ido, Peano’s Interlingua (Latino sine flexione), Novial, and Interlingue (Occidental). In pursuit of the last goal, it conducted parallel studies of these languages, with comparative studies of national languages, under the direction of scholars at American and European universities. It also arranged conferences with proponents of these IALs, who debated features and goals of their respective languages. With a "concession rule" that required participants to make a certain number of concessions, early debates at IALA sometimes grew from heated to explosive.
At the Second International Interlanguage Congress, held in Geneva in 1931, IALA began to break new ground; 27 recognized linguists signed a testimonial of support for IALA's research program. An additional eight added their signatures at the third congress, convened in Rome in 1933. That same year, Professor Herbert N. Shenton and Dr. Edward L. Thorndike became influential in IALA's work by authoring key studies in the interlinguistic field.
The first steps towards the finalization of Interlingua were taken in 1937, when a committee of 24 eminent linguists from 19 universities published "Some Criteria for an International Language and Commentary". However, the outbreak of World War II in 1939 cut short the intended biannual meetings of the committee.
Development of a new language.
Originally, the association had not set out to create its own language. Its goal was to identify which auxiliary language already available was best suited for international communication, and how to promote it most effectively. However, after ten years of research, more and more members of IALA concluded that none of the existing interlanguages were up to the task. By 1937, the members had made the decision to create a new language, to the surprise of the world's interlanguage community.
To that point, much of the debate had been equivocal on the decision to use naturalistic (e.g., Peano’s Interlingua, Novial and Occidental) or systematic (e.g., Esperanto and Ido) words. During the war years, proponents of a naturalistic interlanguage won out. The first support was Dr. Thorndike's paper; the second was a concession by proponents of the systematic languages that thousands of words were already present in many – or even a majority – of the European languages. Their argument was that systematic derivation of words was a Procrustian bed, forcing the learner to unlearn and re-memorize a new derivation scheme when a usable vocabulary was already available. This finally convinced supporters of the systematic languages, and IALA from that point assumed the position that a naturalistic language would be best.
At the outbreak of World War II, IALA's research activities were moved from Liverpool to New York, where E. Clark Stillman established a new research staff. Stillman, with the assistance of Dr. Alexander Gode, developed a "prototyping" technique – an objective methodology for selecting and standardizing vocabulary based on a comparison of "control languages".
In 1943 Stillman left for war work and Gode became Acting Director of Research. IALA began to develop models of the proposed language, the first of which were presented in Morris's "General Report" in 1945.
From 1946 to 1948, renowned French linguist André Martinet was Director of Research. During this period IALA continued to develop models and conducted polling to determine the optimal form of the final language. In 1946, IALA sent an extensive survey to more than 3,000 language teachers and related professionals on three continents.
Four models were canvassed:
The results of the survey were striking. The two more schematic models were rejected – K overwhelmingly. Of the two naturalistic models, M received somewhat more support than P. IALA decided on a compromise between P and M, with certain elements of C.
Martinet took up a position at Columbia University in 1948, and Gode took on the last phase of Interlingua's development. The vocabulary and grammar of Interlingua were first presented in 1951, when IALA published the finalized "" and the 27,000-word "Interlingua–English Dictionary" (IED). In 1954, IALA published an introductory manual entitled "Interlingua a Prime Vista" ("Interlingua at First Sight").
Interestingly, the Interlingua presented by the IALA is very close to Peano’s Interlingua (Latino sine flexione), both in its grammar and especially in its vocabulary. Accordingly, the very name "Interlingua" was kept, yet a distinct abbreviation was adopted: IA instead of IL.
Success, decline, and resurgence.
An early practical application of Interlingua was the scientific newsletter "Spectroscopia Molecular", published from 1952 to 1980. In 1954, Interlingua was used at the Second World Cardiological Congress in Washington, D.C. for both written summaries and oral interpretation. Within a few years, it found similar use at nine further medical congresses. Between the mid-1950s and the late 1970s, some thirty scientific and especially medical journals provided article summaries in Interlingua. Science Service, the publisher of "Science Newsletter" at the time, published a monthly column in Interlingua from the early 1950s until Gode's death in 1970. In 1967, the powerful International Organization for Standardization, which normalizes terminology, voted almost unanimously to adopt Interlingua as the basis for its dictionaries.
The IALA closed its doors in 1953 but was not formally dissolved until 1956 or later. Its role in promoting Interlingua was largely taken on by Science Service, which hired Gode as head of its newly formed Interlingua Division. Hugh E. Blair, Gode's close friend and colleague, became his assistant. A successor organization, the Interlingua Institute, was founded in 1970 to promote Interlingua in the US and Canada. The new institute supported the work of other linguistic organizations, made considerable scholarly contributions and produced Interlingua summaries for scholarly and medical publications. One of its largest achievements was two immense volumes on phytopathology produced by the American Phytopathological Society in 1976 and 1977.
Interlingua had attracted many former adherents of other international-language projects, notably Occidental and Ido. The former Occidentalist Ric Berger founded The Union Mundial pro Interlingua (UMI) in 1955, and by the late 1950s, interest in Interlingua in Europe had already begun to overtake that in North America.
Beginning in the 1980s UMI has held international conferences every two years (typical attendance at the earlier meetings was 50 to 100) and launched a publishing programme that eventually produced over 100 volumes. Other Interlingua-language works were published by university presses in Sweden and Italy, and in the 1990s, Brazil and Switzerland. Several Scandinavian schools undertook projects that used Interlingua as a means of teaching the international scientific and intellectual vocabulary.
In 2000, the Interlingua Institute was dissolved amid funding disputes with the UMI; the American Interlingua Society, established the following year, succeeded the institute and responded to new interest emerging in Mexico.
In the Soviet bloc.
Interlingua was spoken and promoted in the Soviet bloc, despite attempts to suppress the language. In East Germany, government officials confiscated the letters and magazines that the UMI sent to Walter Raédler, the Interlingua representative there.
In Czechoslovakia, Július Tomin published his first article on Interlingua in the Slovak magazine "Príroda a spoločnosť" (Nature and Society) in 1971, after which he received several anonymous threatening letters. He went on to become the Czech Interlingua representative, teach Interlingua in the school system, and publish a series of articles and books.
Interlingua today.
Today, interest in Interlingua has expanded from the scientific community to the general public. Individuals, governments, and private companies use Interlingua for learning and instruction, travel, online publishing, and communication across language barriers. Interlingua is promoted internationally by the Union Mundial pro Interlingua. Periodicals and books are produced by many national organizations, such as the Societate American pro Interlingua, the Svenska Sällskapet för Interlingua, and the Union Brazilian pro Interlingua.
Samples.
From an essay by Alexander Gode:
Community.
It is not certain how many people have an active knowledge of Interlingua. As noted above, Interlingua is the most widely spoken naturalistic auxiliary language.
Interlingua's greatest advantage is that it is the most widely "understood" international auxiliary language by virtue of its naturalistic (as opposed to schematic) grammar and vocabulary, allowing those familiar with a Romance language, and educated speakers of English, to read and understand it without prior study.
Interlingua has active speakers on all continents, especially in South America and in Eastern and Northern Europe, most notably Scandinavia; also in Russia and Ukraine. In Africa, Interlingua has official representation in the Republic of the Congo. There are copious Interlingua web pages, including editions of Wikipedia and Wiktionary, and a number of periodicals, including "Panorama in Interlingua" from the Union Mundial pro Interlingua (UMI) and magazines of the national societies allied with it. There are several active mailing lists, and Interlingua is also in use in certain Usenet newsgroups, particularly in the europa.* hierarchy. Interlingua is presented on CDs, radio, and television. In recent years, samples of Interlingua have also been seen in music and anime.
Interlingua is taught in many high schools and universities, sometimes as a means of teaching other languages quickly, presenting interlinguistics, or introducing the international vocabulary. The University of Granada in Spain, for example, offers an Interlingua course in collaboration with the Centro de Formación Continua.
Every two years, the UMI organizes an international conference in a different country. In the year between, the Scandinavian Interlingua societies co-organize a conference in Sweden. National organizations such as the Union Brazilian pro Interlingua also organize regular conferences.
Phonology and orthography.
Phonology.
Interlingua is primarily a written language, and the pronunciation is not entirely settled. The sounds in parentheses are not used by all speakers.
Interlingua alphabet.
Interlingua uses the 26 letters of the ISO basic Latin alphabet with no diacritics. The alphabet, pronunciation in IPA & letter name in Interlingua are:
Orthography and pronunciation.
Interlingua has a largely phonemic orthography. For the most part, consonants are pronounced as in English, while the vowels are like Spanish. Double consonants are pronounced as single. Interlingua has five falling diphthongs, /ai/, /au/, /ei/, /eu/, and /oi/, although /ei/ and /oi/ are rare.
Stress.
The "general rule" is that stress falls on the vowel before the last consonant (e.g., "lingua", 'language', "esser", 'to be', "requirimento", 'requirement'), and where that is not possible, on the first vowel ("via", 'way', "io crea", 'I create'). There are a few exceptions, and the following rules account for most of them:
Speakers may pronounce all words according to the general rule mentioned above. For example, "kilometro" is acceptable, although "kilometro" is more common.
Loanwords.
Unassimilated foreign loanwords, or borrowed words, are pronounced and spelled as in their language of origin. Their spelling may contain diacritics, or accent marks. If the diacritics do not affect pronunciation, they are removed.
Phonotactics.
Interlingua has no explicitly defined phonotactics. However, the prototyping procedure for determining Interlingua words, which strives for internationality, should in general lead naturally to words that are easy for most learners to pronounce. In the process of forming new words, an ending cannot always be added without a modification of some kind in between. A good example is the plural "-s", which is always preceded by a vowel to prevent the occurrence of a hard-to-pronounce consonant cluster at the end. If the singular does not end in a vowel, the final "-s" becomes "-es."
Vocabulary.
Words in Interlingua may be taken from any language, as long as their internationality is verified by their presence in seven "control" languages: Spanish, Portuguese, Italian, French, and English, with German and Russian acting as secondary controls. These are the most widely spoken Romance, Germanic, and Slavic languages, respectively. Because of their close relationship, Spanish and Portuguese are treated as one unit. The largest number of Interlingua words are of Latin origin, with the Greek and Germanic languages providing the second and third largest number. The remainder of the vocabulary originates in Slavic and non-Indo-European languages.
Eligibility.
A word, that is a form with meaning, is eligible for the Interlingua vocabulary if it is verified by at least three of the four primary control languages. Either secondary control language can substitute for a primary language. Any word of Indo-European origin found in a control language can contribute to the eligibility of an international word. In some cases, the archaic or "potential" presence of a word can contribute to its eligibility.
A word can be potentially present in a language when a derivative is present, but the word itself is not. English "proximity", for example, gives support to Interlingua "proxime", meaning 'near, close'. This counts as long as one or more control languages actually have this basic root word, which the Romance languages all do. Potentiality also occurs when a concept is represented as a compound or derivative in a control language, the morphemes that make it up are themselves international, and the combination adequately conveys the meaning of the larger word. An example is Italian "fiammifero" (lit. flamebearer), meaning "match, lucifer", which leads to Interlingua "flammifero", or "match". This word is thus said to be potentially present in the other languages although they may represent the meaning with a single morpheme.
Words do not enter the Interlingua vocabulary solely because cognates exist in a sufficient number of languages. If their meanings have become different over time, they are considered different words for the purpose of Interlingua eligibility. If they still have one or more meanings in common, however, the word can enter Interlingua with this smaller set of meanings.
If this procedure did not produce an international word, the word for a concept was originally taken from Latin (see below). This only occurred with a few grammatical particles.
Form.
The form of an Interlingua word is considered an "international prototype" with respect to the other words. On the one hand, it should be neutral, free from characteristics peculiar to one language. On the other hand, it should maximally capture the characteristics common to all contributing languages. As a result, it can be transformed into any of the contributing variants using only these language-specific characteristics. If the word has any derivatives that occur in the source languages with appropriate parallel meanings, then their morphological connection must remain intact; for example, the Interlingua word for 'time' is spelled "tempore" and not "*tempus" or "*tempo" in order to match it with its derived adjectives, such as "temporal".
The language-specific characteristics are closely related to the sound laws of the individual languages; the resulting words are often close or even identical to the most recent form common to the contributing words. This sometimes corresponds with that of Vulgar Latin. At other times, it is much more recent or even contemporary. It is never older than the classical period.
An illustration.
The French "œil", Italian "occhio", Spanish "ojo", and Portuguese "olho" appear quite different, but they descend from a historical form "oculus". German "Auge", Dutch "oog" and English "eye" (cf. Czech and Polish "oko", Ukrainian "око" "(óko)") are related to this form in that all three descend from Proto-Indo-European "*okʷ". In addition, international derivatives like "ocular" and "oculista" occur in all of Interlingua's control languages. Each of these forms contributes to the eligibility of the Interlingua word. German and English base words do not influence the form of the Interlingua word, because their Indo-European connection is considered too remote. Instead, the remaining base words and especially the derivatives determine the form "oculo" found in Interlingua.
Notes on Interlingua vocabulary.
New words can be derived internally – that is, from existing Interlingua words – or extracted from the control languages in the manner of the original vocabulary. Internal word-building, though freer than in the control languages, is more limited than in schematic languages.
Originally, a word was taken from Latin if the usual procedure did not produce a sufficiently international word. More recently, modern alternatives have become generally accepted. For example, the southern Romance "comprar", meaning 'to buy', has replaced "emer", because the latter occurs only in derivatives in the control languages. Similarly, the modern form "troppo", 'too' or 'too much', has replaced "nimis", and "ma" 'but' has largely replaced "sed".
Grammar.
Interlingua has been developed to omit any grammatical feature that is absent from even one control language. Thus, Interlingua has no noun–adjective agreement by gender, case, or number (cf. Spanish and Portuguese "gatas negras" or Italian "gatte nere", 'black female cats'), because this is absent from English, and it has no progressive verb tenses (English "I am reading"), because they are absent from French. Conversely, Interlingua distinguishes singular nouns from plural nouns because all the control languages do.
The definite article "le" is invariable, as in English. Nouns have no grammatical gender. Plurals are formed by adding "-s", or "-es" after a final consonant. Personal pronouns take one form for the subject and one for the direct object and reflexive. In the third person, the reflexive is always "se". Most adverbs are derived regularly from adjectives by adding "-mente", or "-amente" after a "-c". An adverb can be formed from any adjective in this way.
Verbs take the same form for all persons ("io, tu, illa vive", 'I live', 'you live', 'she lives'). The indicative ("pare", 'appear', 'appears') is the same as the imperative ("pare!" 'appear!'), and there is no subjunctive. Three common verbs usually take short forms in the present tense: "es" for 'is', 'am', 'are;' "ha" for 'has', 'have;' and "va" for 'go', 'goes'. A few irregular verb forms are available, but rarely used.
There are four simple tenses (present, past, future, and conditional), three compound tenses (past, future, and conditional), and the passive voice. The compound structures employ an auxiliary plus the infinitive or the past participle (e.g., "Ille ha arrivate", 'He has arrived'). Simple and compound tenses can be combined in various ways to express more complex tenses (e.g., "Nos haberea morite", 'We would have died').
Word order is subject–verb–object, except that a direct object pronoun or reflexive pronoun comes before the verb ("Io les vide", 'I see them'). Adjectives may precede or follow the nouns they modify, but they most often follow it. The position of adverbs is flexible, though constrained by common sense.
The grammar of Interlingua has been described as similar to that of the Romance languages, but greatly simplified, primarily under the influence of English. More recently, Interlingua's grammar has been likened to the simple grammars of Japanese and particularly Chinese.
Criticisms and controversies.
Some opponents argue that, being based on a few European languages, Interlingua is best suited for speakers of European languages. Others contend that Interlingua has spelling irregularities that, while internationally recognizable in written form, increase the time needed to fully learn the language, especially for those unfamiliar with Indo-European languages. A related point of criticism is that Interlingua's credential as being Standard Average European is too weak outside the Romance languages. Some opponents see the Germanic, Slavic, and Celtic languages, in particular, as having little influence.
Proponents argue that Interlingua's source languages include not only Romance languages but English, German, and Russian as well. Moreover, the source languages are widely spoken internationally, and large numbers of their words also appear in other languages – still more when derivative forms and loan translations are included. Tests had shown that if a larger number of source languages were used, the results would be about the same. So, IALA selected a much simpler extraction procedure for Interlingua with little adverse effect on its internationality.
Flags and symbols.
As with Esperanto, there have been proposals for a flag of Interlingua; the proposal by Czech translator Karel Podrazil is recognized by multilingual sites. It consists of a white four-pointed star extending to the edges of the flag and dividing it into an upper blue and lower red half. The star is symbolic of the four cardinal directions, and the two halves symbolize Romance and non-Romance speakers of Interlingua who understand each other.
Another symbol of Interlingua is a globe surrounded by twelve stars on a black or blue background, echoing the twelve stars of the Flag of Europe (because the source languages of Interlingua are purely European). Novial [ Wikipedia] marks Interlingua with the Flag of the Europe itself.

</doc>
<doc id="15180" url="http://en.wikipedia.org/wiki?curid=15180" title="Intergovernmentalism">
Intergovernmentalism

Intergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration.
Definition.
Intergovernmentalism treats states, and national governments in particular, as the primary actors in the integration process. Intergovernmentalist approaches claim to be able to explain both periods of radical change in the European Union (because of converging governmental preferences) and periods of inertia (due to diverging national interests). Intergovernmentalism is distinguishable from realism and neorealism because of its recognition of both the significance of institutionalisation in international politics and the impact of domestic politics upon governmental preferences.
Regional integration.
European Integration.
The best-known example of regional integration is the European Union (EU), an economic and political intergovernmental organisation of 28 member states, all in Europe. The EU operates through a system of supranational independent institutions and intergovernmental negotiated decisions by the member states. Institutions of the EU include the European Commission, the Council of the European Union, the European Council, the Court of Justice of the European Union, the European Central Bank, the Court of Auditors, and the European Parliament. The European Parliament is elected every five years by EU citizens. The EU's "de facto" capital is Brussels.
The EU has developed a single market through a standardised system of laws that apply in all member states. Within the Schengen Area (which includes 22 EU and 4 non-EU European states) passport controls have been abolished. EU policies favour the free movement of people, goods, services, and capital within its boundaries, enact legislation in justice and home affairs, and maintain common policies on trade, agriculture, fisheries and regional development. 
A monetary union, the eurozone, was established in 1999 and is composed of 17 member states. Through the Common Foreign and Security Policy the EU has developed a role in external relations and defence. Permanent diplomatic missions have been established around the world. The EU is represented at the United Nations, the World Trade Organisation, the G8 and the G-20.
Intergovernmentalism represents a way for limiting the conferral of powers upon supranational institutions, halting the emergence of common policies. in the current institutional system of the EU, the European Council and the Council play the role of the institutions which have the last word about decisions and policies of the EU, institutionalizing a de facto intergovernmental control over the EU as a whole, with the possibility to give more power to a small group of states. This extreme consequence can create the condition of supremacy of someone over someone else violating the principle of a “Union of Equals”.
African Integration.
The African Union (AU, or, in its other official languages, UA) is a continental intergovernmental union, similar but less integrated to the EU, consisting of 54 African states. The only all-African state not in the AU is Morocco. The AU was established on 26 May 2001 in Addis Ababa, Ethiopia, and launched on 9 July 2002 in South Africa to replace the Organisation of African Unity (OAU). The most important decisions of the AU are made by the Assembly of the African Union, a semi-annual meeting of the heads of state and government of its member states. The AU's secretariat, the African Union Commission, is based in Addis Ababa, Ethiopia.
Overview.
The objectives of the AU are:
The African Union is made up of both political and administrative bodies. The highest decision-making organ is the Assembly of the African Union, made up of all the heads of state or government of member states of the AU. The Assembly is chaired by Hailemariam Desalegn, Prime Minister of Ethiopia, elected at the 20th ordinary meeting of the Assembly in January 2013. The AU also has a representative body, the Pan African Parliament, which consists of 265 members elected by the national parliaments of the AU member states. Its president is Bethel Nnaemeka Amadi.
Other political institutions of the AU include:

</doc>
<doc id="15187" url="http://en.wikipedia.org/wiki?curid=15187" title="In vivo">
In vivo

Studies that are in vivo (Latin for "within the living"; often not italicized in English) are those in which the effects of various biological entities are tested on whole, living organisms usually animals including humans, and plants as opposed to a partial or dead organism, or those done "in vitro" ("within the glass"), i.e., in a laboratory environment using test tubes, petri dishes etc. Examples of investigations "in vivo" include: the pathogenesis of disease by comparing the effects of bacterial infection with the effects of purified bacterial toxins; the development of antibiotics, antiviral drugs, and new drugs generally; and new surgical procedures. Consequently animal testing and clinical trials are major elements of "in vivo" research. "In vivo" testing is often employed over "in vitro" because it is better suited for observing the overall effects of an experiment on a living subject. 
The English microbiologist Professor Harry Smith and his colleagues in the mid-1950s showed the importance of "in vivo" studies. They found that sterile filtrates of serum from animals infected with Bacillus anthracis were lethal for other animals, whereas extracts of culture fluid from the same organism grown "in vitro" were not. This discovery of anthrax toxin through the use of "in vivo" experiments had a major impact on studies of the pathogenesis of infectious disease.
The maxim "in vivo veritas" ("in a living thing [there is] truth") is used to describe this type of testing and is a play on "in vino veritas", ("in wine [there is] truth") a well-known proverb.
"In vivo" vs. "ex vivo" research.
In microbiology "in vivo" is often used to refer to experimentation done in live isolated cells rather than in a whole organism, for example, cultured cells derived from biopsies. In this situation, the more specific term is "ex vivo". Once cells are disrupted and individual parts are tested or analyzed, this is known as "in vitro".
Methods of use.
According to Christopher Lipinski and Andrew Hopkins, "Whether the aim is to discover drugs or to gain knowledge of biological systems, the nature and properties of a chemical tool cannot be considered independently of the system it is to be tested in. Compounds that bind to isolated recombinant proteins are one thing; chemical tools that can perturb cell function another; and pharmacological agents that can be tolerated by a live organism and perturb its systems are yet another. If it were simple to ascertain the properties required to develop a lead discovered "in vitro" to one that is active "in vivo", drug discovery would be as reliable as drug manufacturing."

</doc>
<doc id="15191" url="http://en.wikipedia.org/wiki?curid=15191" title="Inquisition">
Inquisition

The Inquisition is a group of institutions within the judicial system of the Roman Catholic Church whose aim is to combat heresy. It started in 12th-century France to combat religious sectarianism, in particular the Cathars and the Waldensians. Other groups which were investigated later include the Spiritual Franciscans, the Hussites (followers of Jan Hus) and Beguines. Beginning in the 1250s, inquisitors were generally chosen from members of the Dominican Order, to replace the earlier practice of using local clergy as judges. The term Medieval Inquisition covers these courts up through the 14th century.
In the Late Middle Ages and early Renaissance, the concept and scope of the Inquisition was significantly expanded in response to the Protestant Reformation and the Catholic Counter-Reformation. Its geographic scope was expanded to other European countries, resulting in the Spanish Inquisition and Portuguese Inquisition. Those two kingdoms in particular operated inquisitorial courts throughout their respective empires (Spanish and Portuguese) in the Americas (resulting in the Peruvian Inquisition and Mexican Inquisition), Asia, and Africa. One particular focus of the Spanish and Portuguese inquisitions was the issue of Jewish anusim and Muslim converts to Catholicism, partly because these minority groups were more numerous in Spain and Portugal than in many other parts of Europe, and partly because they were often considered suspect due to the assumption that they had secretly reverted to their previous religions.
Except within the Papal States, the institution of the Inquisition was abolished in the early 19th century, after the Napoleonic wars in Europe and after the Spanish American wars of independence in the Americas. The institution survived as part of the Roman Curia, but in 1904 was given the new name of "Supreme Sacred Congregation of the Holy Office". In 1965 it became the Congregation for the Doctrine of the Faith.
Definition and purpose.
The term "Inquisition" comes from Medieval Latin "inquisitio", which referred to any court process that was based on Roman law, which had gradually come back into usage in the late medieval period. Today, the English term "Inquisition" can apply to any one of several institutions which worked against heretics (or other offenders against canon law) within the judicial system of the Roman Catholic Church. Although the term "Inquisition" is usually applied to ecclesiastical courts of the Catholic Church, nonetheless it has several different usages:
Generally, the Inquisition was concerned only with the heretical behaviour of Catholic adherents or converts and did not concern itself with those outside that religion such as Jews or Muslims.
When a suspect was convicted of unrepentant heresy, the inquisitorial tribunal was required by law to hand the person over to the secular authorities for final sentencing, at which point a magistrate would determine the penalty, which was usually burning at the stake although the penalty varied based on local law. The laws were inclusive of proscriptions against certain religious crimes (heresy, etc.), and the punishments included death by burning, although imprisonment for life or banishment would usually be used. Thus the inquisitors generally knew what would be the fate of anyone so remanded, and cannot be considered to have divorced the means of determining guilt from its effects.
The 1578 handbook for inquisitors spelled out the purpose of inquisitorial penalties: "... quoniam punitio non refertur primo & per se in correctionem & bonum eius qui punitur, sed in bonum publicum ut alij terreantur, & a malis committendis avocentur." Translation from the Latin: "... for punishment does not take place primarily and per se for the correction and good of the person punished, but for the public good in order that others may become terrified and weaned away from the evils they would commit."
Historical background.
Before 1100, the Catholic Church had already suppressed what they believed to be heresy, usually through a system of ecclesiastical proscription or imprisonment, but without using torture
and seldom resorting to executions. Such punishments had a number of ecclesiastical opponents, although some countries punished heresy with the death penalty.
In the 12th century, to counter the spread of Catharism, prosecution of heretics became more frequent. The Church charged councils composed of bishops and archbishops with establishing inquisitions (see Episcopal Inquisition). The first Inquisition was temporarily established in Languedoc (south of France) in 1184. The murder in 1208 of Pope Innocent's papal legate Pierre de Castelnau sparked the Albigensian Crusade (1209–1229). The Inquisition was permanently established in 1229. It was centered under the Dominicans in Rome and later at Carcassonne in Languedoc.
Medieval Inquisition.
Historians use the term "Medieval Inquisition" to describe the various inquisitions that started around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). These inquisitions responded to large popular movements throughout Europe considered apostate or heretical to Christianity, in particular the Cathars in southern France and the Waldensians in both southern France and northern Italy. Other Inquisitions followed after these first inquisition movements. Legal basis for some inquisitorial activity came from Pope Innocent IV's papal bull "Ad extirpanda" of 1252, which explicitly authorized (and defined the appropriate circumstances for) the use of torture by the Inquisition for eliciting confessions from heretics. By 1256 inquisitors were given absolution if they used instruments of torture.
In the 13th century, Pope Gregory IX (reigned 1227–1241) assigned the duty of carrying out inquisitions to the Dominican Order and Franciscan Order. Most inquisitors were friars who taught theology and/or law in the universities. They used inquisitorial procedures, a common legal practice adapted from the earlier Ancient Roman court procedures. They judged heresy along with bishops and groups of "assessors" (clergy serving in a role that was roughly analogous to a jury or legal advisers), using the local authorities to establish a tribunal and to prosecute heretics. After 1200, a Grand Inquisitor headed each Inquisition. Grand Inquisitions persisted until the mid 19th century.
Early Modern European history.
With the sharpening of debate and of conflict between the Protestant Reformation and the Catholic Counter-Reformation, Protestant societies came to see/use the Inquisition as a terrifying "Other" trope, while staunch Catholics regarded the Holy Office as a necessary bulwark against the spread of reprehensible heresies.
Witch-trials.
The prosecution of witchcraft generally became more prominent throughout the late medieval and Renaissance era, perhaps driven partly by the upheavals of the era - the Black Death, Hundred Years War, and a gradual cooling of the climate which modern scientists call the Little Ice Age (between about the 15th and 19th centuries). Witches were sometimes blamed. Pope Innocent VIII, in his papal bull "Summis desiderantes affectibus" (5 December 1484), called for measures against magicians and witches in Germany. The grip of freezing weather, failing crops, rising crime, and mass starvation were blamed on witches.
A similar theme is found in the "Malleus Maleficarum" written in 1486, which stated that witchcraft was to blame for bad weather. These remarks are included in Part 2, Chapter XV, which is entitled: "How they Raise and Stir up Hailstorms and Tempests, and Cause Lightning to Blast both Men and Beasts":
Although men as well as women could be open to this charge, the title of the book itself is feminine in gender and Kramer wrote in section I that: "all witchcraft comes from carnal lust which is in women insatiable". In 1490, shortly after the book's initial publication, the Catholic Church ruled that the "Malleus Maleficarum" was false, and in 1538 the Spanish Inquisition cautioned against using it. Spreading from Tyrol, where it originated, to other Germanic States, it helped to fuel the witchhunts in Protestant countries in the seventeenth century as well.
Most of Medieval Western and Central Europe had long-standing Catholic standardisation mixed with some survivals of earlier non-Christian practices such as the use of charms or incantations, with intermittent localized occurrences of different ideas (such as Catharism or Platonism) and sometimes recurring anti-Semitic or anti-Judaic activity. These parochial beliefs and practices were commonly used as the basis for charges of witchcraft or heresy.
With the Protestant Reformation, Catholic authorities became much more ready to suspect heresy in any new ideas,
including those of Renaissance humanism, previously strongly supported by many at the top of the Church hierarchy. The extirpation of heretics became a much broader and more complex enterprise, complicated by the politics of territorial Protestant powers, especially in northern Europe. The Catholic Church could no longer exercise direct influence in the politics and justice-systems of lands which officially adopted Protestantism. Thus war (the French Wars of Religion, the Thirty Years War), massacre (the St. Bartholomew's Day massacre) and the missional and propaganda work (by the Sacra congregatio de propaganda fide) of the Counter-Reformation came to play larger roles in these circumstances, and the Roman law type of a "judicial" approach to heresy represented by the Inquisition became less important overall.
Spanish Inquisition.
Portugal and Spain in the late Middle Ages consisted largely of multicultural territories of Muslim and Jewish influence, reconquered from Islamic control, and the new Christian authorities could not assume that all their subjects would suddenly become and remain orthodox Roman Catholics. So the Inquisition in Iberia, in the lands of the Reconquista counties and kingdoms like Leon, Castile and Aragon, had a special socio-political basis as well as more fundamental religious motives.
In some parts of Spain towards the end of the 14th century, there was a wave of violent anti-Judaism, encouraged by the preaching of Ferrand Martinez, Archdeacon of Ecija. In the pogroms of June 1391 in Seville, hundreds of Jews were killed, and the synagogue was completely destroyed. The number of people killed was also high in other cities, such as Córdoba, Valencia and Barcelona.
One of the consequences of these pogroms was the mass conversion of thousands of surviving Jews. Forced baptism was contrary to the law of the Catholic Church, and theoretically anybody who had been forcibly baptized could legally return to Judaism. However, this was very narrowly interpreted. Legal definitions of the time theoretically acknowledged that a forced baptism was not a valid sacrament, but confined this to cases where it was literally administered by physical force. A person who had consented to baptism under threat of death or serious injury was still regarded as a voluntary convert, and accordingly forbidden to revert to Judaism. After the public violence, many of the converted "felt it safer to remain in their new religion." Thus, after 1391, a new social group appeared and were referred to as "conversos" or "New Christians".
King Ferdinand II of Aragon and Queen Isabella I of Castile established the Spanish Inquisition in 1478. In contrast to the previous inquisitions, it operated completely under royal Christian authority, though staffed by clergy and orders, and independently of the Holy See. It operated in Spain and in all Spanish colonies and territories, which included the Canary Islands, the Spanish Netherlands, the Kingdom of Naples, and all Spanish possessions in North, Central, and South America. It primarily targeted forced converts from Islam (Moriscos, Conversos and "secret Moors") and from Judaism (Conversos, Crypto-Jews and Marranos) — both groups still resided in Spain after the end of the Islamic control of Spain — who came under suspicion of either continuing to adhere to their old religion or of having fallen back into it.
In 1492 all Jews who had not converted were expelled from Spain; those who converted became subject to the Inquisition. (Jews were not heretics, but "Catholics" who practised the Jewish faith were regarded as heretics).
Henry Kamen and Thomas Madden have written of myths that exaggerate the horrors of the Spanish Inquisition.
Inquisition in the Spanish overseas empire.
In the Americas, King Philip II set up three tribunals (each formally titled "Tribunal del Santo Oficio de la Inquisición") in 1569, one in Mexico, Cartagena de Indias (in modern day Colombia) and Peru. The Mexican office administered Mexico (central and southeastern Mexico), Nueva Galicia (northern and western Mexico), the Audiencias of Guatemala (Guatemala, Chiapas, El Salvador, Honduras, Nicaragua, Costa Rica), and the Spanish East Indies. The Peruvian Inquisition, based in Lima, administered all the Spanish territories in South America and Panama.
Portuguese Inquisition.
The Portuguese Inquisition formally started in Portugal in 1536 at the request of the King of Portugal, João III. Manuel I had asked Pope Leo X for the installation of the Inquisition in 1515, but only after his death (1521) did Pope Paul III acquiesce. At its head stood a "Grande Inquisidor", or General Inquisitor, named by the Pope but selected by the Crown, and always from within the royal family. The Portuguese Inquisition principally targeted the Sephardic Jews, whom the state forced to convert to Christianity. Spain had expelled its Sephardic population in 1492; after 1492 many of these Spanish Jews left Spain for Portugal, but eventually were targeted there as well.
The Portuguese Inquisition held its first "auto-da-fé" in 1540. The Portuguese inquisitors mostly targeted the Jewish New Christians (i.e. "conversos" or "marranos"). The Portuguese Inquisition expanded its scope of operations from Portugal to Portugal's colonial possessions, including Brazil, Cape Verde, and Goa, where it continued as a religious court, investigating and trying cases of breaches of the tenets of orthodox Roman Catholicism until 1821. King João III (reigned 1521–57) extended the activity of the courts to cover censorship, divination, witchcraft and bigamy. Originally oriented for a religious action, the Inquisition exerted an influence over almost every aspect of Portuguese society: political, cultural and social.
The Goa Inquisition, an inquisition largely aimed at Catholic converts from Hinduism or Islam who were thought to have returned to their original ways, started in Goa in 1560. In addition, the Inquisition prosecuted non-converts who broke prohibitions against the observance of Hindu or Muslim rites or interfered with Portuguese attempts to convert non-Christians to Catholicism. Aleixo Dias Falcão and Francisco Marques set it up in the palace of the Sabaio Adil Khan.
According to Henry Charles Lea, between 1540 and 1794, tribunals in Lisbon, Porto, Coimbra and Évora resulted in the burning of 1,175 persons, the burning of another 633 in effigy, and the penancing of 29,590. But documentation of 15 out of 689 autos-da-fé has disappeared, so these numbers may slightly understate the activity.
Roman Inquisition.
In 1542 Pope Paul III established the Congregation of the Holy Office of the Inquisition as a permanent congregation staffed with cardinals and other officials. It had the tasks of maintaining and defending the integrity of the faith and of examining and proscribing errors and false doctrines; it thus became the supervisory body of local Inquisitions. Arguably the most famous case tried by the Roman Inquisition involved Galileo Galilei in 1633.
The penances and sentences for those who confessed or were found guilty were pronounced together in a public ceremony at the end of all the processes. This was the "sermo generalis" or "auto-da-fé".
Penances (not matters for the civil authorities) might consist of a pilgrimage, a public scourging, a fine, or the wearing of a cross. The wearing of two tongues of red or other brightly colored cloth, sewn onto an outer garment in an "X" pattern, marked those who were under investigation. The penalties in serious cases were confiscation of property to the inquisition or imprisonment. This led to the possibility of false charges over confiscation with those over a certain income, particularly rich maranos. Following the French invasion of 1798, the new authorities sent 3,000 chests containing over 100,000 Inquisition documents to France from Rome.
Ending of the Inquisition in the 19th and 20th centuries.
The wars of independence of the former Spanish colonies in the Americas concluded with the abolition of the Inquisition in every quarter of Hispanic America between 1813 and 1825.
In Portugal, in the wake of the Liberal Revolution of 1820, the "General Extraordinary and Constituent Courts of the Portuguese Nation" abolished the Portuguese inquisition in 1821.
The last execution of the Inquisition was in Spain in 1826. This was the execution by garroting of the school teacher Cayetano Ripoll for purportedly teaching Deism in his school. In Spain the practices of the Inquisition were finally outlawed in 1834.
In Italy, after the restoration of the Pope as the ruler of the Papal States in 1814, the activity of the Papal States Inquisition continued on until the mid-19th century, notably in the well-publicised Mortara Affair (1858–1870). In 1908 the name of the Congregation became "The Sacred Congregation of the Holy Office", which in 1965 further changed to "Congregation for the Doctrine of the Faith", as retained to the present day[ [update]].
Statistics.
Beginning in the 19th century, historians have gradually compiled statistics drawn from the surviving court records, from which estimates have been calculated by adjusting the recorded number of convictions by the average rate of document loss for each time period. Gustav Henningsen and Jaime Contreras studied the records of the Spanish Inquisition, which list 44,674 cases of which 826 resulted in executions "in person" and 778 "in effigy" (i.e. a straw dummy was burned in place of the person). William Monter estimated there were 1000 executions between 1530–1630 and 250 between 1630–1730. Jean-Pierre Dedieu studied the records of Toledo's tribunal, which put 12,000 people on trial. For the period prior to 1530, Henry Kamen estimated there were about 2,000 executions in all of Spain's tribunals. Italian Renaissance history professor and Inquisition expert Carlo Ginzburg had his doubts about using statistics to reach a judgment about the period. “In many cases, we don’t have the evidence, the evidence has been lost,” said Ginzburg.
References.
Notes
Bibliography

</doc>
<doc id="15208" url="http://en.wikipedia.org/wiki?curid=15208" title="Isaac Albéniz">
Isaac Albéniz

Isaac Manuel Francisco Albéniz y Pascual (]; 29 May 1860 – 18 May 1909) was a Spanish pianist and composer best known for his piano works based on folk music idioms. Transcriptions of many of his pieces, such as "Asturias (Leyenda)", "Granada, Sevilla, Cádiz, Córdoba", "Cataluña", and the "Tango in D", are important pieces for classical guitar, though he never composed for the guitar. The personal papers of Isaac Albéniz are preserved, among other institutions, in the Biblioteca de Catalunya.
Life.
Born in Camprodon, province of Girona, to Ángel Albéniz (a customs official) and his wife, Dolors Pascual, Albéniz was a child prodigy who first performed at the age of four. At age seven, after apparently taking lessons from Antoine François Marmontel, he passed the entrance examination for piano at the Paris Conservatoire, but he was refused admission because he was believed to be too young. By the time he had reached 12, he had made many attempts to run away from home.
His concert career began at the age of nine when his father toured both Isaac and his sister, Clementina, throughout northern Spain. A popular myth is that at the age of 12 Albéniz stowed away in a ship bound for Buenos Aires. He then made his way via Cuba to the United States, giving concerts in New York and San Francisco and then travelled to Liverpool, London and Leipzig. By age 15, he had already given concerts worldwide. This over-dramatized story is not entirely false. Albéniz did travel the world as a performer; however, he was accompanied by his "father", who as a customs agent was required to travel frequently. This can be attested by comparing Isaac's concert dates with his father's travel itinerary.
In 1876, after a short stay at the Leipzig Conservatory, he went to study at the Royal Conservatory of Brussels after King Alfonso's personal secretary, Guillermo Morphy, obtained him a royal grant. Count Morphy thought highly of Albéniz, who would later dedicate "Sevilla" to Morphy's wife when it premiered in Paris in January 1886.
In 1880 Albéniz went to Budapest to study with Franz Liszt, only to find out that Liszt was in Weimar, Germany.
In 1883 he met the teacher and composer Felip Pedrell, who inspired him to write Spanish music such as the "Chants d'Espagne". The first movement (Prelude) of that suite, later retitled after the composer's death as "Asturias (Leyenda)", is probably most famous today as part of the classical guitar repertoire, even though it was originally composed for piano. (Many of Albéniz's other compositions were also transcribed for guitar, notably by Francisco Tárrega.) At the 1888 Universal Exposition in Barcelona, the piano manufacturer Érard sponsored a series of 20 concerts featuring Albéniz's music.
The apex of Albéniz's concert career is considered to be 1889 to 1892 when he had concert tours throughout Europe. During the 1890s Albéniz lived in London and Paris. For London he wrote some musical comedies which brought him to the attention of the wealthy Francis Money-Coutts, 5th Baron Latymer. Money-Coutts commissioned and provided him with librettos for the opera "Henry Clifford" and for a projected trilogy of Arthurian operas. The first of these, "Merlin" (1898–1902), was thought to have been lost but has recently been reconstructed and performed. Albéniz never completed "Lancelot" (only the first act is finished, as a vocal and piano score), and he never began "Guinevere", the final part.
In 1900 he started to suffer from Bright's disease and returned to writing piano music. Between 1905 and 1908 he composed his final masterpiece, "Iberia" (1908), a suite of twelve piano "impressions".
In 1883 the composer married his student Rosina Jordana. They had three children: Blanca (who died in 1886), Laura (a painter), and Alfonso (who played for Real Madrid in the early 1900s before embarking on a career as a diplomat). Two other children died in infancy.
Albéniz died from his kidney disease on 18 May 1909 at age 48 in Cambo-les-Bains. Only a few weeks before his death, the government of France awarded Albéniz its highest honor, the Grand-Croix de la Légion d'honneur. He is buried at the Montjuïc Cemetery, Barcelona.
Work.
Early works.
Albéniz's early works were mostly "salon style" music. Albéniz's first published composition, "Marcha Militar", appeared in 1868. A number of works written before this are now lost. He continued composing in traditional styles ranging from Rameau, Bach, Beethoven, Chopin and Liszt until the mid-1880s. He also wrote at least five zarzuelas, of which all but two are now lost.
Middle period – Spanish influences.
During the late 1880s, the strong influence of Spanish style is evident in Albéniz's music. In 1883 Isaac Albéniz met the teacher and composer Felipe Pedrell. Pedrell was a leading figure in the development of nationalist Spanish music. In his book "The Music of Spain", Gilbert Chase describes Pedrell's influence on Albéniz: "What Albéniz derived from Pedrell was above all a spiritual orientation, the realization of the wonderful values inherent in Spanish music." Felipe Pedrell inspired Isaac Albéniz to write Spanish music such as the "Suite española", Op. 47, noted for its delicate, intricate melody and abrupt dynamic changes.
In addition to the Spanish spirit infused in Albéniz's music, he incorporated other qualities as well. In her biography of Albéniz, Pola Baytelman discerns four characteristics of the music from the middle period as follows: 1. The dance rhythms of Spain, of which there are a wide variety. 2. The use of cante jondo, which means deep or profound singing. It is the most serious and moving variety of flamenco or Spanish gypsy song, often dealing with themes of death, anguish, or religion. 3. The use of exotic scales also associated with flamenco music. The Phrygian mode is the most prominent in Albéniz's music, although he also used the Aeolian and Mixolydian modes as well as the whole-tone scale. 4. The transfer of guitar idioms into piano writing. 
Following his marriage, Albéniz settled in Madrid and produced a substantial quantity of music in a relatively short period. By 1886 he had written over 50 piano pieces. The Albéniz biographer Walter A. Clark says that pieces from this period received enthusiastic reception in the composer's many concerts. Chase describes music from this period, Taking the guitar as his instrumental model, and drawing his inspiration largely from the peculiar traits of Andalusian folk music – but without using actual folk themes – Albéniz achieves a stylization of Spanish traditional idioms that while thoroughly artistic, gives a captivating impression of spontaneous improvisation... "Cordoba" is the piece that best represents the style of Albéniz in this period, with its hauntingly beautiful melody, set against the acrid dissonances of the plucked accompaniment imitating the notes of the Moorish guslas. Here is the heady scent of jasmines amid the swaying palm trees, the dream fantasy of an Andalusian "Arabian Nights" in which Albéniz loved to let his imagination dwell.
Later period.
While Albéniz's crowning achievement, "Iberia", was written in the last years of his life in France, many of its preceding works are well-known and of great interest. The five pieces in "Chants d'Espagne", ("Songs of Spain", published in 1892) are a solid example of the compositional ideas he was exploring in the "middle period" of his life. The suite shows what Albéniz biographer Walter Aaron Clark describes as the "first flowering of his unique creative genius", and the beginnings of compositional exploration that became the hallmark of his later works. This period also includes his operatic works – "Merlin, Henry Clifford", and "Pepita Jiménez". His orchestral works of this period include "Spanish Rhapsody" (1887) and "Catalonia" (1899), dedicated to Ramon Casas, who had painted his full-length portrait in 1894.
Albéniz on his own music.
Perhaps the best source on the works is Albéniz himself. He is quoted as commenting on his earlier period works as: There are among them a few things that are not completely worthless. The music is a bit infantile, plain, spirited; but in the end, the people, our Spanish people, are something of all that. I believe that the people are right when they continue to be moved by "Cordoba, Mallorca", by the copla of the "Sevillanas", by the "Serenata", and "Granada". In all of them I now note that there is less musical science, less of the grand idea, but more color, sunlight, flavor of olives. That music of youth, with its little sins and absurdities that almost point out the sentimental affectation ... appears to me like the carvings in the Alhambra, those peculiar arabesques that say nothing with their turns and shapes, but which are like the air, like the sun, like the blackbirds or like the nightingales of its gardens. They are more valuable than all else of Moorish Spain, which though we may not like it, is the true Spain.
Impact.
Albéniz's influence on the future of Spanish music was profound. His activities as conductor, performer and composer significantly raised the profile of Spanish music abroad and encouraged Spanish music and musicians in his own country.
Albéniz's works have become an important part of the repertoire of the classical guitar, many of which have been transcribed by Miguel Llobet and others. "Asturias (Leyenda)" in particular is heard most often on the guitar, as are "Granada, Sevilla, Cadiz, "Cataluña", Cordoba" and the "Tango in D". Gordon Crosskey and Cuban-born guitarist Manuel Barrueco have both made solo guitar arrangements of six of the eight-movement "Suite española". Selections from "Iberia" have rarely been attempted on solo guitar but have been very effectively performed by guitar ensembles, such as the performance by John Williams and Julian Bream of "Iberia's " opening "Evocation". The Doors incorporated "Asturias" into their song "Spanish Caravan"; also, Iron Maiden's To Tame a Land uses the introduction of the piece for the song bridge. More recently, a guitar version of "Granada" functions as something of a love theme in Woody Allen's 2008 film "Vicky Cristina Barcelona". The 2008 horror film "Mirrors" incorporates the theme from "Asturias" into its score.
In 1997 the "Fundación Isaac Albéniz" was founded to promote Spanish music and musicians and to act as a research centre for Albéniz and Spanish music in general.
In film.
A film about his life entitled "Albéniz" was made in 1947. It was produced in Argentina.
List of selected works.
Symphonic versions of "Iberia" have been arranged by Enrique Fernández Arbós, Carlos Surinach, and Peter Breiner. There is a piano and orchestra version of "Rapsodia española" by Cristóbal Halffter.

</doc>
<doc id="15225" url="http://en.wikipedia.org/wiki?curid=15225" title="Ivar Aasen">
Ivar Aasen

Ivar Andreas Aasen (]; 5 August 1813 – 23 September 1896) was a Norwegian philologist, lexicographer, playwright, and poet. He is best known for having created one of Norway's official languages, Nynorsk.
Background.
Aasen was born at Åsen in Ørsta (then Ørsten), in the district of Sunnmøre, on the west coast of Norway. His father, a small peasant-farmer named Ivar Jonsson, died in 1826. He was brought up to farmwork, but he assiduously cultivated all his leisure in reading. An early interest of his was botany. When he was eighteen, he opened an elementary school in his native parish. In 1833 he entered the household of H. C. Thoresen, the husband of the eminent writer Magdalene Thoresen, in Herøy (then Herø), and there he picked up the elements of Latin. Gradually, and by dint of infinite patience and concentration, the young peasant became master of many languages, and began the scientific study of their structure. Ivar single-handedly created a new language for Norway to become the "literary" language.
Career.
About 1841 he had freed himself from all the burden of manual labour, and could occupy his thoughts with the dialect of his native district, Sunnmøre; his first publication was a small collection of folk songs in the Sunnmøre dialect (1843). His remarkable abilities now attracted general attention, and he was helped to continue his studies undisturbed. His "Grammar of the Norwegian Dialects" (Danish: "Det Norske Folkesprogs Grammatik", 1848) was the result of much labour, and of journeys taken to every part of the country. Aasen's famous "Dictionary of the Norwegian Dialects" (Danish: "Ordbog over det Norske Folkesprog") appeared in its original form in 1850, and from this publication dates all the wide cultivation of the popular language in Norwegian, since Aasen really did no less than construct, out of the different materials at his disposal, a popular language or definite "folke-maal" (people's language) for Norway. By 1853, he had created the norm for utilizing his new language, which he called Landsmaal, meaning country language. With certain modifications, the most important of which were introduced later by Aasen himself, but also through a latter policy aiming to merge this Norwegian language with Dano-Norwegian, this language has become "Nynorsk" ("New Norwegian"), the second of Norway's two official languages (the other being "Bokmål", the Dano-Norwegian descendant of the Danish language used in Norway at Aasen's time). An unofficial variety of Norwegian more close to Aasen's language is still found in Høgnorsk ("High Norwegian"). Today, some consider Nynorsk on equal footing with bokmål, as bokmål tends to be used more in radio and television and most newspapers, whereas New Norse (Nynorsk) is used equally in government work as well as approximately 17% of schools. Although it is not as common as its brother language, it needs to be looked upon as a viable language, as a large minority of Norwegians use it as their primary language including many scholars and authors. New Norse is both a written and spoken language.
Aasen composed poems and plays in the composite dialect to show how it should be used; one of these dramas, "The Heir" (1855), was frequently acted, and may be considered as the pioneer of all the abundant dialect-literature of the last half-century of the 1800s, from Vinje to Garborg. In 1856, he published "Norske Ordsprog", a treatise on Norwegian proverbs. Aasen continuously enlarged and improved his grammars and his dictionary. He lived very quietly in lodgings in Oslo (then Christiania), surrounded by his books and shrinking from publicity, but his name grew into wide political favour as his ideas about the language of the peasants became more and more the watch-word of the popular party. In 1864, he published his definitive grammar of Nynorsk and in 1873 he published the definitive dictionary.
Quite early in his career, in 1842, he had begun to receive a grant to enable him to give his entire attention to his philological investigations; and the Storting (Norwegian parliament), conscious of the national importance of his work, treated him in this respect with more and more generosity as he advanced in years. He continued his investigations to the last, but it may be said that, after the 1873 edition of his "Dictionary" (with a new title: Danish: "Norsk Ordbog"), he added but little to his stores. Aasen holds perhaps an isolated place in literary history as the one man who has invented, or at least selected and constructed, a language which has pleased so many thousands of his countrymen that they have accepted it for their schools, their sermons and their songs. He died in Christiania on 23 September 1896, and was buried with public honours.
The Ivar Aasen Centre.
Ivar Aasen-tunet, an institution devoted to the Nynorsk language, opened in June 2000. Their web page includes most of Aasens' texts numerous other examples of Nynorsk literature (in Nettbiblioteket), and some articles, also in English, about language history in Norway.
2013 Language year.
"Språkåret 2013" (The Language Year 2013) celebrated Ivar Aasen's 200 year anniversary, as well as the 100 year anniversary of Det Norske Teateret. The year's main focus was to celebrate linguistic diversity in Norway. In a poll released in connection with the celebration, 56% of Norwegians said they held positive views of Aasen, while 7% held negative views. On Aasen's 200 anniversary, 5 August 2013, "Bergens Tidende", which is normally published mainly in bokmål, published an edition fully in nynorsk in memory of Aasen.
Bibliography.
Aasen published a wide range of material, some of it released posthumously.

</doc>
<doc id="15236" url="http://en.wikipedia.org/wiki?curid=15236" title="ICANN">
ICANN

The Internet Corporation for Assigned Names and Numbers (ICANN ) is a nonprofit organization that is responsible for the coordination of maintenance and methodology of several databases of unique identifiers related to the namespaces of the Internet, and ensuring the network's stable and secure operation.
Most visibly, much of its work has concerned the Internet's global Domain Name System, including policy development for internationalization of the DNS system, introduction of new generic top-level domains (TLDs), and the operation of root name servers. The numbering facilities ICANN manages include the Internet Protocol address spaces for IPv4 and IPv6, and assignment of address blocks to regional Internet registries. ICANN also maintains registries of Internet protocol identifiers.
ICANN performs the actual technical maintenance work of the central Internet address pools and DNS Root registries pursuant to the IANA function contract.
ICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; to promote competition; to achieve broad representation of the global Internet community; and to develop policies appropriate to its mission through bottom-up, consensus-based processes.
ICANN was created on September 18, 1998, and incorporated on September 30, 1998. It is headquartered in the Playa Vista section of Los Angeles, California. On September 29, 2006, ICANN signed a new agreement with the United States Department of Commerce (DOC) that moves the organization further towards a solely multistakeholder governance model.
On October 1, 2009 the U.S. Department of Commerce gave up its control of ICANN, completing ICANN's transition.
History.
Before the establishment of ICANN, the IANA function of administering registries of Internet protocol identifiers (including the distributing top-level domains and IP addresses) was performed by Jon Postel, a Computer Science researcher who had been involved in the creation of ARPANET, first at UCLA and then at the University of Southern California's Information Sciences Institute (ISI). In 1997 Postel testified before Congress that this had come about as a "side task" to this research work The Information Sciences Institute was funded by the U.S. Department of Defense, as was SRI International's Network Information Center, which also performed some assigned name functions.
As the Internet grew and expanded globally, the U.S. Department of Commerce initiated a process to establish a new organization to take over the IANA functions. On January 30, 1998, the National Telecommunications and Information Administration (NTIA), an agency of the U.S. Department of Commerce, issued for comment, "A Proposal to Improve the Technical Management of Internet Names and Addresses." The proposed rule making, or "", was published in the Federal Register on February 20, 1998, providing opportunity for public comment. NTIA received more than 650 comments as of March 23, 1998, when the comment period closed.
The Green Paper proposed certain actions designed to privatize the management of Internet names and addresses in a manner that allows for the development of robust competition and facilitates global participation in Internet management. The Green Paper proposed for discussion a variety of issues relating to DNS management including private sector creation of a new not-for-profit corporation (the "new corporation") managed by a globally and functionally representative Board of Directors. ICANN was formed in response to this policy. ICANN manages the Internet Assigned Numbers Authority (IANA) under contract to the United States Department of Commerce (DOC) and pursuant to an agreement with the IETF.
ICANN was incorporated in California on September 30, 1998, with entrepreneur and philanthropist Esther Dyson as founding chairwoman. It is qualified to do business in the District of Columbia. ICANN was established in California due to the presence of Jon Postel, who was a founder of ICANN and was set to be its first CTO prior to his unexpected death. ICANN formerly operated from the same Marina del Rey building where Postel formerly worked, which is home to an office of the Information Sciences Institute at the University of Southern California. However, ICANN's headquarters is now located in the nearby Playa Vista section of Los Angeles.
Per its original , primary responsibility for policy formation in ICANN was to be delegated to three supporting organizations (Address Supporting Organization, Domain Name Supporting Organization, and Protocol Supporting Organization), each of which was to develop and recommend substantive policies and procedures for the management of the identifiers within their respective scope. They were also required to be financially independent from ICANN. As expected, the Regional Internet Registries and the IETF agreed to serve as the Address Supporting Organization and Protocol Supporting Organization respectively, and ICANN issued a call for interested parties to propose the structure and composition of the Domain Name Supporting Organization. On 4 March 1999, the ICANN Board, based in part on the DNSO proposals received, decided instead on an alternate construction for the DNSO which delineated specific constituencies bodies within ICANN itself, thus adding primary responsibility for DNS policy development to ICANN's existing duties of oversight and coordination.
On July 26, 2006, the United States government renewed the contract with ICANN for performance of the IANA function for an additional one to five years. The context of ICANN's relationship with the U.S. government was clarified on September 29, 2006 when ICANN signed a new Memorandum of understanding with the United States Department of Commerce (DOC). This document does give the DoC a final, unilateral oversight over some of the ICANN operations.
In July 2008, the U.S. Department of Commerce reiterated an earlier statement that it has "no plans to transition management of the authoritative root zone file to ICANN". The letter also stresses the separate roles of the IANA and VeriSign.
On October 1, 2009 the U.S. Department of Commerce gave up its control of ICANN.
Notable events.
On March 18, 2002, publicly elected At-Large Representative for North America board member Karl Auerbach sued ICANN in Superior Court in California to gain access to ICANN's accounting records without restriction. Auerbach won.
In September and October 2003, ICANN played a crucial role in the conflict over VeriSign's "wild card" DNS service Site Finder. After an open letter from ICANN issuing an ultimatum to VeriSign, later supported by the IAB, the company voluntarily shut down the service on October 4, 2003. Following this action, VeriSign filed a lawsuit against ICANN on February 27, 2004, claiming that ICANN had overstepped its authority. In this lawsuit, VeriSign sought to reduce ambiguity about ICANN's authority. The antitrust component of VeriSign's claim was dismissed in August 2004. VeriSign's broader challenge that ICANN overstepped its contractual rights is currently outstanding. A proposed settlement already approved by ICANN's board would resolve VeriSign's challenge to ICANN in exchange for the right to increase pricing on .com domains. At the meeting of ICANN in Rome, which took place from March 2 to March 6, 2004, ICANN agreed to ask approval of the US Department of Commerce for the Waiting List Service of VeriSign.
On May 17, 2004, ICANN published a proposed budget for the year 2004-05. It included proposals to increase the openness and professionalism of its operations, and greatly increased its proposed spending from US $8.27 million to $15.83 million. The increase was to be funded by the introduction of new top-level domains, charges to domain registries, and a fee for some domain name registrations, renewals and transfers (initially USD 0.20 for all domains within a country-code top-level domain, and USD 0.25 for all others). The Council of European National Top Level Domain Registries (CENTR), which represents the Internet registries of 39 countries, rejected the increase, accusing ICANN of a lack of financial prudence and criticizing what it describes as ICANN's "unrealistic political and operational targets". Despite the criticism, the registry agreement for the top-level domains jobs and travel includes a US $2 fee on every domain the licensed companies sell or renew.
After a second round of negotiations in 2004, the TLDs eu, asia, travel, jobs, mobi, and cat were introduced in 2005.
On February 28, 2006, ICANN's board approved a settlement with VeriSign in the lawsuit resulting from SiteFinder that involved allowing VeriSign (the registry) to raise its registration fees by up to 7% a year. This was criticised by some people in the US House of Representatives' Small Business committee.
In February 2007, ICANN began the steps to remove accreditation of one of their registrars, RegisterFly amid charges and lawsuits involving fraud, and criticism of ICANN's handling of the situation. ICANN has been the subject of criticism as a result of its handling of RegisterFly, and the harm caused to thousands of clients as a result of what has been called ICANN's "laissez faire attitude toward customer allegations of fraud". Backend cybercrime detection within the ICANN sphere of influence is also lacking.
On May 23, 2008, ICANN issued Enforcement Notices against 10 Accredited Registrars and announced this through a press release entitled: "Worst Spam Offenders" Notified by ICANN, Compliance system working to correct Whois and other issues. This was largely in response to a report issued by KnujOn called The 10 Worst Registrars in terms of spam advertised junk product sites and compliance failure. The mention of the word spam in the title of the ICANN memo is somewhat misleading since ICANN does not address issues of spam or email abuse. Website content and usage are not within ICANN's mandate. However the KnujOn Report details how various registrars have not complied with their contractual obligations under the Registrar Accreditation Agreement (RAA). The main point of the KnujOn research was to demonstrate the relationships between compliance failure, illicit product traffic, and spam. The report demonstrated that out of 900 ICANN accredited Registrars fewer than 20 held 90% of the web domains advertised in spam. These same Registrars were also most frequently cited by KnujOn as failing to resolve complaints made through the Whois Data Problem Reporting System (WDPRS).
On June 26, 2008, the ICANN Board started a new process of TLD naming policy to take a "significant step forward on the introduction of new generic top-level domains." This program envisions the availability of many new or already proposed domains, as well a new application and implementation process.
On October 1, 2008, ICANN issued Breach Notices against Joker and Beijing Innovative Linkage Technology Ltd. after further researching reports and complaints issued by KnujOn. These notices gave the Registrars 15 days to fix their Whois investigation efforts.
In 2010, ICANN approved a major review of its policies with respect to accountability, transparency, and public participation by the Berkman Center for Internet and Society at Harvard University. This external review was in support of the work of ICANN's Accountability and Transparency Review team.
On February 3, 2011, ICANN announced that it had distributed the last batch of its remaining IPv4 addresses to the world’s five Regional Internet Registries, the organizations that manage IP addresses in different regions. These Registries began assigning the final IPv4 addresses within their regions until they ran out completely.
On June 20, 2011, the ICANN board voted to end most restrictions on the names of generic top-level domains (gTLD). Companies and organizations became able to choose essentially arbitrary top level Internet domain names. The use of non-Latin characters (such as Cyrillic, Arabic, Chinese, etc.) will also be allowed in gTLDs. ICANN began accepting applications for new gTLDS on January 12, 2012. The initial price to apply for a new gTLD is $185,000. The renewal or the annual fee of the domain will further be $25,000. It is anticipated that many corporations will apply for gTLDs based on their brands. ICANN expects the new rules to significantly change the face of the Internet. Peter Thrush, chairman of ICANN's board of directors stated after the vote: "Today's decision will usher in a new Internet age. We have provided a platform for the next generation of creativity and inspiration. Unless there is a good reason to restrain it, innovation should be allowed to run free." Some would argue that the innovative freedom Peter Thrush refers to starts at $185,000, and thus is not accessible to everyone.
The 2013 NSA spying scandal has led to ICANN endorsing the Montevideo Statement.
Structure.
At present ICANN is formally organized as a non-profit corporation "for charitable and public purposes" under the California Nonprofit Public Benefit Corporation Law. It is managed by a 16-member Board of Directors composed of eight members selected by a nominating committee on which all the constituencies of ICANN are represented; six representatives of its Supporting Organizations, sub-groups that deal with specific sections of the policies under ICANN's purview; an At-Large seat filled by an At-Large Organization; and the President / CEO, appointed by the Board.
There are currently three Supporting Organizations. The Generic Names Supporting Organization (GNSO) deals with policy making on generic top-level domains (gTLDs). The Country Code Names Supporting Organization (ccNSO) deals with policy making on country-code top-level domains (ccTLDs). The Address Supporting Organization (ASO) deals with policy making on IP addresses.
ICANN also relies on some advisory committees and other advisory mechanisms to receive advice on the interests and needs of stakeholders that do not directly participate in the Supporting Organizations. These include the Governmental Advisory Committee (GAC), which is composed of representatives of a large number of national governments from all over the world; the At-Large Advisory Committee (ALAC), which is composed of individual Internet users from around the world selected by each of the and Nominating Committee; the Root Server System Advisory Committee, which provides advice on the operation of the DNS root server system; the Security and Stability Advisory Committee (SSAC), which is composed of Internet experts who study security issues pertaining to ICANN's mandate; and the Technical Liaison Group (TLG), which is composed of representatives of other international technical organizations that focus, at least in part, on the Internet.
Governmental Advisory Committee.
The Governmental Advisory Committee has representatives from 111 states (108 UN members, the Holy See, Cook Islands, Niue and Taiwan), Hong Kong, Bermuda, Montserrat, the European Commission and the African Union Commission.
In addition the following organizations are GAC Observers:
Democratic input.
In the Memorandum of Understanding that set up the relationship between ICANN and the U.S. government, ICANN was given a mandate requiring that it operate "in a bottom up, consensus driven, democratic manner." However, the attempts that ICANN have made to set up an organizational structure that would allow wide input from the global Internet community did not produce results amenable to the current Board. As a result, the At-Large constituency and direct election of board members by the global Internet community were soon abandoned.
ICANN holds periodic public meetings rotated between continents for the purpose of encouraging global participation in its processes. Resolutions of the ICANN Board, preliminary reports, and minutes of the meetings, are published on the ICANN website, sometimes in real time. However there are criticisms from ICANN constituencies including the and the At-Large Advisory Committee (ALAC) that there is not enough public disclosure and that too many discussions and decisions take place out of sight of the public.
In the early 2000s, there had been speculation that the United Nations might signal a takeover of ICANN, followed by a negative reaction from the US government and worries about a division of the Internet. The World Summit on the Information Society in Tunisia in November 2005 agreed not to get involved in the day-to-day and technical operations of ICANN. However it also agreed to set up an international Internet Governance Forum, with a consultative role on the future governance of the Internet. ICANN's Government Advisory Committee is currently set up to provide advice to ICANN regarding public policy issues and has participation by many of the world's governments.
Some have attempted to argue that ICANN was never given the authority to decide policy, e.g., choose new TLDs or shut out other interested parties who refuse to pay ICANN's US$185,000 fee, but was to be a technical caretaker. Critics suggest that ICANN should not be allowed to impose business rules on market participants, and that all TLDs should be added on a first-come, first-served basis and the market should be the arbiter of who succeeds and who does not.
Activities.
One task that ICANN was asked to do was to address the issue of domain name ownership resolution for generic top-level domains (gTLDs). ICANN's attempt at such a policy was drafted in close cooperation with the World Intellectual Property Organization (WIPO), and the result has now become known as the Uniform Dispute Resolution Policy (UDRP). This policy essentially attempts to provide a mechanism for rapid, cheap and reasonable resolution of domain name conflicts, avoiding the traditional court system for disputes by allowing cases to be brought to one of a set of bodies that arbitrate domain name disputes. According to ICANN policy, a domain registrant must agree to be bound by the UDRP—they cannot get a domain name without agreeing to this.
A look at the UDRP decision patterns has led some to conclude that compulsory domain name arbitration is less likely to give a fair hearing to domain name owners asserting defenses under the First Amendment and other laws, compared to the federal courts of appeal in particular.
Proposed elimination of public DNS whois.
The initial report of ICANN's Expert Working Group has recommended that the present form of Whois, a utility that allows anyone to know who has registered a domain name on the Internet, be scrapped. It recommends it be replaced with a system that keeps most registration information secret (or "gated") from most Internet users, and only discloses information for "permissible purposes". ICANN's list of permissible purposes includes Domain name research, Domain name sale and purchase, Regulatory enforcement, Personal data protection, Legal actions, and Abuse mitigation. Whois has been a key tool of investigative journalists interested in determining who was disseminating information on the Internet. The use of whois by the free press is not included in the list of permissible purposes in the initial report.
Criticism.
Since its creation, ICANN has been the subject of criticism and controversy. In 2000, professor Michael Froomkin of the University of Miami School of Law argued that ICANN's relationship with the U.S. Department of Commerce is illegal, in violation of either the Constitution or federal statutes. In 2009, the new "Affirmation of Commitments" agreement between ICANN and the U.S. Department of Commerce, that aimed to create international oversight, ran into criticism.
In December 2011, the Federal Trade Commission stated ICANN had long failed to provide safeguards that protect consumers from online swindlers.
Also in 2011, seventy-nine companies, including The Coca-Cola Company, Hewlett-Packard, Samsung and others, signed a petition against ICANN's new TLD program (sometimes referred to as a "commercial landgrab"), in a group organized by the Association of National Advertisers. As of September 2014, this group, the Coalition for Responsible Internet Domain Oversight, that opposes the rollout of ICANN's TLD expansion program, has been joined by 102 associations and 79 major companies. Partly as a response to this criticism, ICANN initiated an effort to protect trademarks in domain name registrations, which eventually culminated in the establishment of the Trademark Clearinghouse.
IBSA proposal (2011).
One controversial proposal, resulting from a September 2011 summit between India, Brazil, and South Africa (IBSA), would seek to move Internet governance into a "UN Committee on Internet-Related Policy" (UN-CIRP). The move was a reaction to a perception that the principles of the 2005 Tunis Agenda for the Information Society have not been met. The statement called for the subordination of independent technical organizations such as ICANN and the ITU to a political organization operating under the auspices of the United Nations. After outrage from India’s civil society and media, the Indian government backed away from the proposal.
Montevideo Statement on the Future of Internet Cooperation (2013).
On 7 October 2013 the Montevideo Statement on the Future of Internet Cooperation was released by the leaders of a number of organizations involved in coordinating the Internet's global technical infrastructure, loosely known as the "I*" (or "I-star") group. Among other things, the statement "expressed strong concern over the undermining of the trust and confidence of Internet users globally due to recent revelations of pervasive monitoring and surveillance" and "called for accelerating the globalization of ICANN and IANA functions, towards an environment in which all stakeholders, including all governments, participate on an equal footing". This desire to move away from a United States centric approach is seen as a reaction to the ongoing NSA surveillance scandal. The statement was signed by the heads of the Internet Corporation for Assigned Names and Numbers (ICANN), the Internet Engineering Task Force, the Internet Architecture Board, the World Wide Web Consortium, the Internet Society, and the five regional Internet address registries (African Network Information Center, American Registry for Internet Numbers, Asia-Pacific Network Information Centre, Latin America and Caribbean Internet Addresses Registry, and Réseaux IP Européens Network Coordination Centre).
Global Multistakeholder Meeting on the Future of Internet Governance (2013).
In October 2013, Fadi Chehadé, current President and CEO of ICANN, met with Brazilian President Dilma Rousseff in Brasilia. Upon Chehadé's invitation, the two announced that Brazil would host an international summit on Internet governance in April 2014. The announcement came after the 2013 disclosures of mass surveillance by the U.S. government, and President Rousseff's speech at the opening session of the 2013 United Nations General Assembly, where she strongly criticized the American surveillance program as a "breach of international law". The "Global Multistakeholder Meeting on the Future of Internet Governance (NET mundial)" will include representatives of government, industry, civil society, and academia. At the IGF VIII meeting in Bali in October 2013 a commenter noted that Brazil intends the meeting to be a "summit" in the sense that it will be high level with decision-making authority. The organizers of the "NET mundial" meeting have decided that an online forum called "/1net", set up by the I* group, will be a major conduit of non-governmental input into the three committees preparing for the meeting in April.
The Obama administration that had joined critics of ICANN in 2011 announced in March 2014 that they intended to transition away from oversight of the IANA functions contract. The current contract that the United States Department of Commerce has with ICANN will expire in 2015, in its place the NTIA will transition oversight of the IANA functions to the 'global multistakeholder community'.
NetMundial Initiative (2014).
The NetMundial Initiative is a plan for international governance of the Internet that was first proposed at the Global Multistakeholder Meeting on the Future of Internet Governance (GMMFIG) conference (23–24 April 2014)
and later developed into the NetMundial Initiative by ICANN CEO Fadi Chehade along with representatives of the World Economic Forum (WEF)
and the Brazilian Internet Steering Committee (Comitê Gestor da Internet no Brasil), commonly referred to as "CGI.br".
The meeting produced a nonbinding statement in favor of consensus-based decision-making. It reflected a compromise and did not harshly condemn mass surveillance or include the words "net neutrality", despite initial support for that from Brazil. The final resolution says ICANN should be under international control by September 2015.
A minority of governments, including Russia, China, Iran and India, were unhappy with the final resolution and wanted multi-lateral management for the Internet, rather than broader multi-stakeholder management.
A month later, the Panel On Global Internet Cooperation and Governance Mechanisms (convened by the Internet Corporation for Assigned Names and Numbers (ICANN) and the World Economic Forum (WEF) with assistance from The Annenberg Foundation), supported and included the NetMundial statement in its own report.
In June 2014, France strongly attacked ICANN, saying ICANN is not a fit venue for Internet governance and that alternatives should be sought.

</doc>
<doc id="15291" url="http://en.wikipedia.org/wiki?curid=15291" title="Intercourse">
Intercourse

Intercourse may refer to:

</doc>
<doc id="15294" url="http://en.wikipedia.org/wiki?curid=15294" title="Islamabad Capital Territory">
Islamabad Capital Territory

The Islamabad Capital Territory (ICT) (Urdu: وفاقی دارالحکومت‎) is one of the two federal territories of Pakistan. It includes Islamabad, the capital city of Pakistan, and covers an area of 1,165.5 km² (450 mi²) of which 906 km² (349.8 mi²) is Islamabad proper. It is represented in the National Assembly by two constituencies, namely NA-48 and NA-49.
History.
The land was acquired from Khyber Pakhtunkhwa and Punjab in 1960, for the purpose of establishing Pakistan's new capital. According to the 1960 master plan, the ICT included the city of Rawalpindi, and was to be composed of the following parts:
However, the city of Rawalpindi was eventually excluded from the ICT on its creation in the 1980s. The remainder of the territory is now subdivided into five zones, with Zone I designated to house all residential, industrial and government institutions. 
Punjab is located to the south of the ICT, and Khyber Pakhtunkhwa is located to the north west.
Introduction.
Islamabad was designed and built to be a modern capital for Pakistan. It was established in 1960, on the orders of then President General Ayub Khan.
Islamabad nestles against the backdrop of the Margalla Hills at the northern end of Potohar Plateau. Its climate is healthy, relatively pollution free, plentiful in water resources and lush green. It is a modern and carefully planned residential city with wide roads and avenues, many public buildings and well-organised bazaars, markets, and shopping centres.
Government.
Islamabad is situated between the province of Khyber Pakhtunkhwa and Punjab but Islamabad is not a part of any province. The federal Government of Pakistan controls it and is one of the two federal Territories of Pakistan (the other being FATA) which is directly governed by the Federal Government of Pakistan.
Union Councils.
Islamabad is administratively divided into two segments, namely Islamabad Urban and Islamabad rural. The rural area is further divided into 32 Union Councils, comprising some 133 villages, and city aera is further divided into 47 Union Councils.The table below lists the 79 Union Councils, each Union Council is named after the main town.
Area and population.
The city is divided into eight basic zone types: Administrative zone, Diplomatic Enclave zone, Residential Areas, Educational Sectors, Industrial Sectors, Commercial Areas, Rural Areas, Green Areas.
Each sector has its own shopping area, a green belt (which goes across the whole sector in a straight line) and public park. The population of the city is around 950,000 people of which 66% is urban. It has an area of about 910 square kilometres. The city lies at latitudes 33° 49' north and longitudes 72° 24' east with altitudes ranging from 457 to 610 meters.
Tribes.
While urban Islamabad is home to people from all over Pakistan as well as expatriates, in the rural areas a number of Pothohari speaking tribal communities can still be recognized. The main ones are:
Dhanial.
They are one of the largest tribes living in the areas on the Potohar plateau and Lower Himalayas. This tribe traces their lineage to Ali ibn Abi-Talib. Most of the Dhanyals are settled in the cities of Rawalpindi and Islamabad and in the Muree Hills. Other branches of the tribe live in Azad Kashmir, Abbottabad, Sialkot and Hazara. Their main villages in the Capital Territory are Tumair Chirah Pehont Kirpa Pind Begwal and Merabegwal.
Abbasi.
Abbasi inhibits in almost all the villages of Islamabad. But they are majority in bhara koh, Pindorian, Tarlai, Ali pur.
Mir.
Mir clan,basically from Occupied Kashmir also resides here from last many years in different parts of the city.
Awan.
Awans around the town of Golra Sharif,village Malpur Islamabad and the village Sohan. Some are also found in the villages of Tarlai Kalan and Malot, as well as along the border with Khyber-Pakhtunkhwa.
Dhamial.
Dhamial rajputs are one of the largest tribes of Pothohar. There ancestor Sultan Dhami Khan was ruler of Pothohar and they originate from Kot Dhamiak. They also claim to be the bloodline of Hazrat Yousaf(AS). There are about 3 villages in capital Islamabad like Sher Dhamial, Hun Dhamial, Saab Dhamial and village Dhamial etc.
Mughal.
The Mughal are another large tribe, who claim Mughal ancestry as descendents of various Central Asian Turco-Mongol armies that invaded Iran and South Asia such as those of Genghis Khan, Timur, Babur and beyond.
Gakhar or Kayanis.
The region is home a large community of Gakhar or Kayanis, who at one time were rulers of the region. Rawalpindi had been the capital of the Gakhars. The last Gakhar ruler Sultan Muqrab Khan was defeated by the Sikhs. Ghakhar villages include Malpur, Rehara, Chatta (kund rajgan), Malot, Seevra, Bharakau, New Parian, New Malper,Chuchkal and Dodocha.
Gujar.
The Gujar are found throughout rural ICT. The largest Gujar settlement was the village of Dhok Chaudhary Jevan, which now part of Sector E 7. Most Gujars are now living in Chaudhary Umar Abad. Other Gujar villages include Ahdi Paswal, Turnol and Talhar.
Jat.
The Jat can be found in the Union Council Kuri in the village of Mohrian and Tarlai Kalan where the Thathal clan, and Chatha clan populate the village of Bakhtawar Chatha. Other Jat villages include Thandapani, Nilore, Tumair, Darkalla and Alipur.
Rajputs.
The ICT territory is home a large community of Rajputs, who once were rulers of the region. Among the clans of Rajputs, the Thathal or [THOTHAL] are found in the village of Tarlai KalanPhulgran Harno Thanda Pani and Mohrian. The Matyal can be encountered in Gagri, Union Council Sihala, while the Janjuas are found in Union Council Bhara Kahu, jagiot, chanol. Other Rajput villages include Bhangreel Kalan, Bhangreel Khurd, Kortara, Takht Pari, Shadi Dhamial, Mohra Amir, Sood Gangal, Mohri Khumbal, Hoon Dhamial,[Khadrapar],Chaper Mir Khanal] Hreno Thanda Pani, Thamir, bangyal and Bhima Kanait.
Clans include the Minhas, [Thathal] Dhamial, Bangial, Ranial, Chohan, Bhains, Baghial, Khumbal, Gangal, Bashan, Janjua, Nagiyal Rajput and Hon Rajputs.
Climate.
The average humidity level is 55%, with an average rainfall of 1450 millimeters each year. The maximum average temperature is 29 °C and the minimum average temperature attained here during the year is generally around 11 °C. Reminiscent of tropical climate, Islamabad retains mild winters and has never recorded snowfall.
Education.
Islamabad has the highest literacy rate of Pakistan at 85%. and also has some of Pakistan's major universities, including Quaid-i-Azam University, the International Islamic University, and the National University of Sciences and Technology.
Private School Network Islamabad is working for Private Educational Institutions. The president of PSN is Dr.Muhammad Afzal Babur from Bhara Kahu. PSN is divided in 8 zone in islamabad,In Tarlai Zone Chaudhary Faisal Ali from Faisal Academy Tarlai Kalan is Zonal General Sectary of PSN.]].
Quaid-e-Azam University has several faculties. The institute is located in a semi-hilly area, east of the Secretariat buildings and near the base of Margalla Hills. This Post-Graduate institute is spread over 1,705 acre. The nucleus of the campus has been designed as an axial spine with a library as its center.
Other universities include the following:
External links.
 travel guide from Wikivoyage

</doc>
<doc id="15304" url="http://en.wikipedia.org/wiki?curid=15304" title="IDE">
IDE

IDE or Ide may refer to:

</doc>
<doc id="15309" url="http://en.wikipedia.org/wiki?curid=15309" title="Intellivision">
Intellivision

The Intellivision is a home video game console released by Mattel in 1979. Development of the console began in 1978, less than a year after the introduction of its main competitor, the Atari 2600. The word "intellivision" is a portmanteau of "intelligent television". Over 3 million Intellivision units were sold and a total of 125 games were released for the console.
In 2009, video game website IGN named the Intellivision the No. 14 greatest video game console of all time. It remained Mattel's only video game console until the release of the HyperScan in 2006.
History and development.
The Intellivision was developed by Mattel Electronics, a subsidiary of Mattel formed expressly for the development of electronic games. The console was test marketed in Fresno, California, in 1979 with a total of four games available, and was released nationwide in 1980 with a price tag of US$299 and a pack-in game: "Las Vegas Poker & Blackjack". Though not the first system to challenge Atari, it was the first to pose a serious threat to Atari's dominance. A series of advertisements featuring George Plimpton were produced that demonstrated the superiority of the Intellivision's graphics and sound to those of the Atari 2600, using side-by-side game comparisons.
One of the slogans of the television advertisements stated that Intellivision was "the closest thing to the real thing"; one example in an advertisement compared golf games. The other console's games had a blip sound and cruder graphics, while the Intellivision featured a realistic swing sound and striking of the ball, and graphics that suggested a more 3D look. There was also an advertisement comparing the Atari 2600 to it, featuring the slogan "I didn't know".
Like Atari, Mattel marketed their console to a number of retailers as a rebadged unit. These models include the Radio Shack TandyVision, the GTE-Sylvania Intellivision, and the Sears Super Video Arcade. The Sears model was a specific coup for Mattel, as Sears was already selling a rebadged Atari 2600 unit, and in doing so made a big contribution to Atari's success.
In its first year, Mattel sold 175,000 Intellivision consoles, and the library grew to 35 games. At this time, all Intellivision games were developed by an outside firm, APh Technological Consulting. The company recognized that what had been seen as a secondary product line might be a big business. Realizing that potential profits are much greater with first party software, Mattel formed its own in-house software development group.
The original five members of that Intellivision team were manager Gabriel Baum, Don Daglow, Rick Levine, Mike Minkoff and John Sohl. Levine and Minkoff, a long-time Mattel Toys veteran, both came over from the hand-held Mattel games engineering team. To keep these programmers from being hired away by rival Atari, their identity and work location was kept a closely guarded secret. In public, the programmers were referred to collectively as the Blue Sky Rangers.
By 1982, sales were soaring. Over two million Intellivision consoles had been sold by the end of the year, earning Mattel a $100,000,000 profit. Third-party Atari developers Activision, and Imagic began releasing games for the Intellivision, as did hardware rival Coleco. Mattel created "M Network" branded games for Atari and Coleco's systems. The most popular titles sold over a million units each. The Intellivision was also introduced in Japan by Bandai in 1982.
The original 5-person Mattel game development team had grown to 110 people under now-Vice President Baum, while Daglow led Intellivision development and top engineer Minkoff directed all work on all other platforms.
Keyboard Component.
Intellivision's packaging and promotional materials, as well as television commercials, promised that with the addition of a soon-to-be-available accessory called the "Keyboard Component", originally portrayed in TV ads as a larger box with an opening in the top that the Intellivision fit into. This would turn the Intellivision into a fully functional home computer system.
The unit would bring the system's available RAM up to a full 64kB, a large amount for the time, and would have provided both a built-in cassette drive for data storage and a connection for an optional 40-column thermal printer. The cassette drive would be able to provide both data storage and an audio track simultaneously, allowing for interactive audio recording and playback under computer control, and a secondary 6502 microprocessor inside the Keyboard Component would be programmed to handle all of these extra capabilities independently of the Intellivision's CP1610 CPU. The unit would even provide an extra cartridge slot, allowing the original Intellivision to remain permanently docked with the Keyboard Component while still being able to play standard game cartridges.
Unfortunately, while the Keyboard Component was an ambitious piece of engineering for its time, it suffered from reliability problems and proved to be expensive to produce. Originally slated to be available in 1981, the Keyboard Component was repeatedly delayed as the engineers tried to find ways to overcome the reliability issues and reduce manufacturing costs.
The Keyboard Component's repeated delays became so notorious around Mattel headquarters that comedian Jay Leno, when performing at Mattel's 1981 Christmas party, got his biggest titter of the evening with the line: "You know what the three big lies are, don't you? 'The check is in the mail,' 'I'll still respect you in the morning,' and 'The Keyboard will be out in spring.'"
Complaints from consumers who had chosen to buy the Intellivision specifically on the promise of a "Coming Soon!" personal-computer upgrade that seemed as if it would never materialize eventually caught the attention of the Federal Trade Commission (FTC), who started investigating Mattel Electronics for fraud and false advertising. Mattel said that the Keyboard Component was a real product still being test-marketed and even released a small number of Keyboard Components to a handful of retail stores, along with a handful of software titles in order to support this claim. The FTC eventually ordered Mattel to pay a $10,000/day fine until the promised computer upgrade was in full retail distribution. To protect themselves from the ongoing fines, the Keyboard Component was officially canceled in the fall of 1982 and the Entertainment Computer System (ECS) module offered up in its place.
While approximately four thousand Keyboard Components were manufactured before the module was canceled and recalled, it is not clear how many of them actually found their way into the hands of Intellivision customers. Today, very few of them still exist; when the Keyboard Component was officially canceled, part of Mattel's settlement with the FTC involved offering to buy back all of the existing Keyboard Components from dissatisfied customers. Any customer who opted to keep theirs was required to sign a waiver indicating their understanding that no more software would be written for the system and which absolved Intellivision of any future responsibility for technical support. Several of the units were later used by Mattel Electronics engineers when it was discovered that, with a few minor modifications, a Keyboard Component could be used as an Intellivision software-development system in place of the original hand-built development boards.
The Keyboard Component debacle was ranked as #11 on GameSpy's 25 Dumbest Moments in Gaming.
Entertainment Computer System (ECS).
In mid-1981, Mattel's upper management was becoming concerned that the Keyboard Component division would never be able to produce a sellable product. As a result, Mattel Electronics set up a competing internal engineering team whose stated mission was to produce an inexpensive add-on called the BASIC Development System, or BDS, to be sold as an educational device to introduce kids to the concepts of computer programming.
The rival BDS engineering group, who had to keep the project's real purpose a secret among themselves, fearing that if David Chandler, the head of the Keyboard Component team, found out about it he would use his influence to get the project killed, eventually came up with a much less expensive alternative. Originally dubbed the Lucky, from LUCKI: Low User-Cost Keyboard Interface, it lacked many of the sophisticated features envisioned for the original Keyboard Component. Gone, for example, was the full 64kB of RAM and the secondary 6502 CPU; instead, the ECS offered a mere 2kB RAM expansion, a built-in BASIC that was marginally functional, plus a much-simplified cassette and thermal-printer interface.
Ultimately, this fulfilled the original promises of turning the Intellivision into a computer, making it possible to write programs and store them to tape, and interfacing with a printer well enough to allow Mattel to claim that they had delivered the promised computer upgrade and stop the FTC's mounting fines. It even offered, via an additional AY-3-8910 sound chip inside the ECS module and an optional 49-key Music Synthesizer keyboard, the possibility of turning the Intellivision into a multi-voice synthesizer which could be used to play or learn music.
In the fall of 1982, the LUCKI, now renamed the Entertainment Computer System (ECS), was presented at the annual sales meeting, officially ending the ill-fated Keyboard Component project. A new advertising campaign was aired in time for the 1982 Christmas season, and the ECS itself was shown to the public at the January 1983 Consumer Electronic Show (CES) in Las Vegas at the Las Vegas Convention Center. A few months later, the ECS hit the market, and the FTC agreed to drop the $10K/day fines.
Unfortunately, by the time the ECS made its retail debut, an internal shake-up at the top levels of Mattel Electronics' management had caused the company's focus to shift away from hardware add-ons in favor of software, and the ECS received very little further marketing push. Further hardware developments, including a planned Program Expander that would have added another 16K of RAM and a more intricate, fully featured Extended-BASIC to the system, were halted, and in the end less than a dozen software titles were released for the ECS.
Intellivoice.
In 1982, Mattel introduced a new peripheral for the Intellivision: The Intellivoice, a voice synthesis device which produces speech when used with certain games. The Intellivoice was original in two respects: not only was this capability unique to the Intellivision system at the time (although a similar device was available for the Odyssey²), but the speech-supporting games written for Intellivoice actually made the speech an integral part of the gameplay.
Unfortunately, the amount of speech that could be compressed into a 4K or 8K ROM cartridge was limited, and the system did not sell as well as Mattel had hoped; while the initial orders were as high as 300,000 units for the Intellivoice module and its initial game-cartridge offerings, interest in future titles dropped rapidly until the fourth and last Intellivoice title, "", sold a mere 90,000 units. A fifth game, a children's title called "Magic Carousel", was shelved, and in August 1983 the Intellivoice system was quietly phased out.
The four titles available for the Intellivoice system, in order of their release, were:
A fifth title, "Intellivision World Series Major League Baseball", developed as part of the Entertainment Computer System series, also supports the Intellivoice if both the ECS and Intellivoice are connected concurrently. Unlike the Intellivoice-specific games, however, "World Series Major League Baseball" is also playable without the Intellivoice module (but not without the ECS.)
A further homebrew title, "Space Hunt", also uses the male Intellivoice sounds (especially on its main title screen). This game is a spin-off clone of "Astrosmash", which uses graphics loaned from "Utopia" and the "TRON" game series.
Intellivision II.
In addition to the ECS module, 1982 also saw the introduction of a redesigned model, called the Intellivision II (featuring detachable controllers and sleeker case), the System Changer (which played Atari 2600 games on the Intellivision II), and a music keyboard add-on for the ECS.
Like the ECS, Intellivision II was designed first and foremost to be inexpensive to manufacture. Among other things, the raised bubble keypad of the original hand controller was replaced by a flat membrane keyboard surface. However, because many Intellivision games had been designed for users to play by feeling the buttons without looking down, some of these games were far less playable on Intellivision II.
Instead of an internal power supply like the original system had, the Intellivision II would use an external AC adapter. Its main drawback, however, was that it was a non-standard power supply — running on 16.2V — meant that if the AC adapter was lost or damaged, the system could be rendered useless, as replacement power supplies for that particular voltage requirement were not readily available. It is unknown whether Intellivision II AC adapters were sold separately.
Mattel also changed the Intellivision II's internal ROM program (called the EXEC) in an attempt to lock out unlicensed 3rd party titles. To make room for the lock-out code while retaining compatibility with existing titles, some portions of the EXEC code were moved in a way that changed their timing. While most games were unaffected, a couple of the more popular titles, "Shark! Shark!", and "Space Spartans", had certain sound effects that the Intellivision II reproduced differently than intended, although the games remained playable. "Electric Company Word Fun" did not run at all and INTV's later release "Super Pro Football" has minor display glitches at the start, both due to the modified EXEC. Mattel's attempt to lock out competitors' software titles was only temporarily successful, as the 3rd-party game manufacturers quickly figured out how to get around it.
Intellivision III.
Mattel planned to release Intellivision III, a more powerful console with a price above $200, for Christmas 1983. The company canceled the console after the ColecoVision beat the Atari 5200 in the market for higher-performance consoles, and after home computers became as inexpensive as game consoles.
Competition and market crash.
Amid the flurry of new hardware, there was trouble for the Intellivision. New game systems (ColecoVision, Emerson Arcadia 2001, Atari 5200, and Vectrex, all in 1982) were further subdividing the market, and the video game crash put pressure on the entire industry. The Intellivision team rushed to finish a major new round of games, including "BurgerTime" and the ultra-secret 3D glasses game "Hover Force". Although "BurgerTime" was a popular game on the Intellivision and was programmed by Blue Sky Ranger Ray Kaestner in record time, the five-month manufacturing cycle meant that the game did not appear until the late spring of 1983, after the video game crash had severely damaged game sales.
In the spring of 1983, Mattel went from aggressively hiring game programmers to laying them off within a two-week period. By August, there were massive layoffs, and the price of the Intellivision II (which launched at $150 earlier that year) was lowered to $69. Mattel Electronics posted a $300 million loss. Early in 1984, the division was closed — the first high-profile victim of the crash.
Intellivision game sales continued when a liquidator purchased all rights to the Intellivision and its software from Mattel, as well as all remaining inventory. After much of the existing software inventory had been sold, former Mattel Marketing executive Terry Valeski bought all rights to Intellivision and started a new venture. The new company, INTV Corp., continued to sell old stock via retail and mail order. When the old stock of Intellivision II consoles ran out, they introduced a new console dubbed INTV System III. This unit was actually a cosmetic rebadge of the original Intellivision console (this unit was later renamed the Super Pro System.) In addition to manufacturing new consoles, INTV Corp. also continued to develop new games, releasing a few new titles each year. Eventually, the system was discontinued in 1991.
Rereleases.
Intellivision games became readily available again when Keith Robinson, an early Intellivision programmer responsible for the game "TRON: Solar Sailer", purchased the software rights and founded a new company, Intellivision Productions. As a result, games originally designed for the Intellivision are available on PCs and modern-day consoles including the PlayStation 2, Xbox, and Nintendo GameCube in the "Intellivision Lives!" package, though all are now out of print at retail. However, the Xbox version is available for purchase as a downloadable game through Xbox Live Game Marketplace's Xbox Originals service for the Xbox 360. VH1 Classic and MTV Networks released 6 Intellivision games to iOS. A few licensed Intellivision games are available through the GameTap subscription gaming service. Also, several LCD handheld and direct-to-TV games have been released in recent years.
On March 24, 2010, Microsoft launched the Game Room service for Xbox Live and Games for Windows Live. This service includes support for Intellivision titles and allows players to compete against one another for high scores via online leaderboards. At the 2011 Consumer Electronics Show, Microsoft announced a version of Game Room for Windows Phone, promising a catalog of 44 Intellivision titles.
On October 1, 2014, AtGames Digital Media, Inc., under license from Intellivision Productions, Inc., released the Intellivision Flashback Classic Console, a miniature sized Intellivision console with two original sized controllers. It comes with 60 Intellivision games built into ROM.
Reviews and game guides.
Ken Uston published "Ken Uston's Guide to Buying and Beating the Home Video Games" in 1982 as a guide to potential buyers of console systems/cartridges, as well as a brief strategy guide to numerous cartridge games then in existence. He described Intellivision as "the most mechanically reliable of the systems… The controller (used during "many hours of experimentation") worked with perfect consistency. The unit never had overheating problems, nor were loose wires or other connections encountered." However, Uston rated the controls and control system as "below average" and the worst of the consoles he tested (including Atari 2600, Magnavox Odyssey², Astrovision, and Fairchild Channel F).
Jeff Rovin lists "Intellivision" as one of the seven major suppliers of videogames in 1982, and mentions it as "the unchallenged king of graphics", however stating that the controllers can be "difficult to operate", the fact that if a controller breaks the entire unit must be shipped off for repairs (since they did not detach at first), and that the overlays "are sometimes so stubborn as to tempt one's patience" .
Technical specifications.
Game controller.
The Intellivision controller featured:
The controller was ranked the fourth worst video game controller by IGN editor Craig Harris.

</doc>
<doc id="15323" url="http://en.wikipedia.org/wiki?curid=15323" title="Internet Protocol">
Internet Protocol

The Internet Protocol (IP) is the principal communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. Its routing function enables internetworking, and essentially establishes the Internet.
IP has the task of delivering packets from the source host to the destination host solely based on the IP addresses in the packet headers. For this purpose, IP defines packet structures that encapsulate the data to be delivered. It also defines addressing methods that are used to label the datagram with source and destination information.
Historically, IP was the connectionless datagram service in the original "Transmission Control Program" introduced by Vint Cerf and Bob Kahn in 1974; the other being the connection-oriented Transmission Control Protocol (TCP). The Internet protocol suite is therefore often referred to as TCP/IP.
The first major version of IP, Internet Protocol Version 4 (IPv4), is the dominant protocol of the Internet. Its successor is Internet Protocol Version 6 (IPv6).
Function.
The Internet Protocol is responsible for addressing hosts and for routing datagrams (packets) from a source host to a destination host across one or more IP networks. For this purpose, the Internet Protocol defines the format of packets and provides an addressing system that has two functions: identifying hosts; and providing a logical location service.
Datagram construction.
Each datagram has two components: a header and a payload. The IP header is tagged with the source IP address, the destination IP address, and other meta-data needed to route and deliver the datagram. The payload is the data that is transported. This method of nesting the data payload in a packet with a header is called encapsulation.
IP addressing and routing.
IP addressing entails the assignment of IP addresses and associated parameters to host interfaces. The address space is divided into networks and subnetworks, involving the designation of network or routing prefixes. IP routing is performed by all hosts, but most importantly by routers, which transport packets across network boundaries. Routers communicate with one another via specially designed routing protocols, either interior gateway protocols or exterior gateway protocols, as needed for the topology of the network.
IP routing is also common in local networks. For example, many Ethernet switches support IP multicast operations. These switches use IP addresses and Internet Group Management Protocol to control multicast routing but use MAC addresses for the actual routing.
Reliability.
The design of the Internet protocols is based on the end-to-end principle. The network infrastructure is considered inherently unreliable at any single network element or transmission medium and assumes that it is dynamic in terms of availability of links and nodes. No central monitoring or performance measurement facility exists that tracks or maintains the state of the network. For the benefit of reducing network complexity, the intelligence in the network is purposely mostly located in the end nodes of data transmission. Routers in the transmission path forward packets to the next known, directly reachable gateway matching the routing prefix for the destination address.
As a consequence of this design, the Internet Protocol only provides best effort delivery and its service is characterized as unreliable. In network architectural language, it is a connectionless protocol, in contrast to connection-oriented modes of transmission. Various error conditions may occur, such as data corruption, packet loss, duplication and out-of-order delivery. Because routing is dynamic, meaning every packet is treated independently, and because the network maintains no state based on the path of prior packets, different packets may be routed to the same destination via different paths, resulting in out-of-order sequencing at the receiver.
Internet Protocol Version 4 (IPv4) provides safeguards to ensure that the IP packet header is error-free. A routing node calculates a checksum for a packet. If the checksum is bad, the routing node discards the packet. The routing node does not have to notify either end node, although the Internet Control Message Protocol (ICMP) allows such notification. By contrast, in order to increase performance, and since current link layer technology is assumed to provide sufficient error detection, the IPv6 header has no checksum to protect it.
All error conditions in the network must be detected and compensated by the end nodes of a transmission. The upper layer protocols of the Internet protocol suite are responsible for resolving reliability issues. For example, a host may cache network data to ensure correct ordering before the data is delivered to an application.
Link capacity and capability.
The dynamic nature of the Internet and the diversity of its components provide no guarantee that any particular path is actually capable of, or suitable for, performing the data transmission requested, even if the path is available and reliable. One of the technical constraints is the size of data packets allowed on a given link. An application must assure that it uses proper transmission characteristics. Some of this responsibility lies also in the upper layer protocols. Facilities exist to examine the maximum transmission unit (MTU) size of the local link and Path MTU Discovery can be used for the entire projected path to the destination. The IPv4 internetworking layer has the capability to automatically fragment the original datagram into smaller units for transmission. In this case, IP provides re-ordering of fragments delivered out of order.
The Transmission Control Protocol (TCP) is an example of a protocol that adjusts its segment size to be smaller than the MTU. The User Datagram Protocol (UDP) and the Internet Control Message Protocol (ICMP) disregard MTU size, thereby forcing IP to fragment oversized datagrams.
Version history.
In May 1974, the Institute of Electrical and Electronic Engineers (IEEE) published a paper entitled "A Protocol for Packet Network Intercommunication". The paper's authors, Vint Cerf and Bob Kahn, described an internetworking protocol for sharing resources using packet switching among network nodes. A central control component of this model was the "Transmission Control Program" that incorporated both connection-oriented links and datagram services between hosts. The monolithic Transmission Control Program was later divided into a modular architecture consisting of the Transmission Control Protocol at the transport layer and the Internet Protocol at the network layer. The model became known as the "Department of Defense (DoD) Internet Model" and "Internet Protocol Suite", and informally as "TCP/IP".
The Internet Protocol is one of the elements that define the Internet. The dominant internetworking protocol in the Internet Layer in use today is IPv4; the number 4 is the protocol version number carried in every IP datagram. IPv4 is described in RFC 791 (1981).
The successor to IPv4 is IPv6. Its most prominent modification from version 4 is the addressing system. IPv4 uses 32-bit addresses (c. 4 billion, or , addresses) while IPv6 uses 128-bit addresses (c. 340 undecillion, or addresses). Although adoption of IPv6 has been slow, as of June 2008[ [update]], all United States government systems have demonstrated basic infrastructure support for IPv6 (if only at the backbone level). IPv6 was a result of several years of experimentation and dialog during which various protocol models were proposed, such as TP/IX (RFC 1475), PIP (RFC 1621) and TUBA (TCP and UDP with Bigger Addresses, RFC 1347).
IP versions 0 to 3 were experimental versions, used between 1977 and 1979. The following Internet Experiment Note (IEN) documents describe versions of the Internet Protocol prior to the modern version of IPv4:
Version 5 was used by the Internet Stream Protocol, an experimental streaming protocol.
Other protocol proposals named "IPv9" and "IPv8" briefly surfaced, but had no affiliation with any international standards body, and have had no support.
On April 1, 1994, the IETF published an April Fool's Day joke about IPv9.
Security.
During the design phase of the ARPANET and the early Internet, the security aspects and needs of a public, international network could not be adequately anticipated. Consequently, many Internet protocols exhibited vulnerabilities highlighted by network attacks and later security assessments. In 2008, a thorough security assessment and proposed mitigation of problems was published. The Internet Engineering Task Force (IETF) has been pursuing further studies.

</doc>
<doc id="15346" url="http://en.wikipedia.org/wiki?curid=15346" title="Institute of National Remembrance">
Institute of National Remembrance

Institute of National Remembrance – Commission for the Prosecution of Crimes against the Polish Nation (Polish: "Instytut Pamięci Narodowej – Komisja Ścigania Zbrodni przeciwko Narodowi Polskiemu"; IPN) is a Polish government-affiliated research institute with lustration prerogatives and prosecution powers founded by specific legislation. It specialises in the legal and historical sciences and in particular the recent history of Poland. IPN investigates both Nazi and Communist crimes committed in Poland between 1939 and 1989, documents its findings and disseminates the results of its investigations to the public.
The Institute was established by the Polish Parliament on 18 December 1998. The Institute started its activities on 1 July 2000.
According to a new law which went into effect on 15 March 2007, IPN was to be mandated to carry out lustration procedures prescribed by Polish law. However, key articles of that law were judged unconstitutional by Poland's constitutional court on 11 May 2007, so the role of IPN in the lustration process is at present unclear.
The IPN is a founding member organisation of the Platform of European Memory and Conscience.
Purpose.
IPN's main areas of activity and mission statement include:
IPN collects, archives and organises documents about the Polish communist security apparatus (22 July 1944 to 31 December 1989).
Organisation.
IPN was created by special legislation on 18 December 1998. IPN is governed by the chairman. This chairman is chosen by a supermajority (60%) of the Polish Parliament (Sejm) with the approval of the Senate of Poland on a request by a Collegium of IPN. The chairman has a 5-year term of office. The first chairman of the IPN was Leon Kieres, elected by the Sejm for five years in 8 June 2000 (term 30 June 2000 – 29 December 2005). The second chairman was Janusz Kurtyka, elected on 9 December 2005 with a term that started 29 December 2005 until his death in the Smolensk airplane crash on 10 April 2010. Franciszek Gryciuk was acting chairman from 2010 to 2011, when the current chairman, Łukasz Kamiński, was elected by the Sejm.
The IPN is divided into:
On 29 April 2010, acting president Bronislaw Komorowski signed into law a parliamentary act that reformed the Institute of National Remembrance.
Activities.
Research.
The research conducted by IPN from December 2000 falls into four main topical areas:
Among the most widely reported cases investigated by the IPN thus far is the Jedwabne Pogrom, a pogrom of Polish Jews "committed directly by Poles, but inspired by the Germans" in 1941. A selection of other cases include: 
Education.
IPN is involved in dissemination of its research results in the form of publications (particularly the IPN Bulletin (Biuletyn IPN „pamięć.pl”) and "Remembrance and Justice" periodicals), exhibitions, seminars, panel discussions, film reviews, workshops and school lessons. Since December 2000 IPN has organized over 30 academic conferences (particularly the Warsaw Congress of Science organized every year in September); 22 exhibitions in various museums and educational competitions involving thousands of students. "IPN Bulletin" is of an informative and popular-scientific character and contains articles pertaining to the history of Poland in the years 1939–1990 as well as describes the current IPN activities. "Remembrance and Justice" appears every half a year and is a scientific historical magazine. IPN also publishes books which are usually edited as collections of documents, reports and memories, but also scientific elaborations (78 of such publications have appeared till April 2007).
The Public Education Office co-operates on a permanent basis with the Ministry of National Education and Sport, having signed a Co-operation Agreement in 2001. IPN gives opinions of curricula and textbooks on history that are used in Polish schools and is involved in teacher training activities. The IPN also co-organizes postgraduate diploma studies on history at the Jagiellonian University and the University of Maria Curie-Skłodowska.
Boardgames.
The Institution of National Remembrance has created several boardgames to help educate people about recent polish history
Lustration.
On 18 December 2006 Polish law regulating IPN was changed and came into effect on 15 March 2007. This change gave IPN new lustration powers. However, key articles of that law were judged unconstitutional by Poland's Constitutional Court on 11 May 2007, making the role of IPN in lustration unclear and putting the whole process into question.
Criticism.
Role in lustration and Wildstein list.
One of the most controversial aspects of IPN is a by-product of its role in collecting and publishing previously secret archives from the Polish communist security apparatus, the Służba Bezpieczeństwa: revealing secret agents and collaborators (a process called "lustration"). One incident which drew criticism involved the so-called Wildstein list; a partial list of names of people who allegedly worked for the communist era Polish intelligence service, which was copied from IPN archives (without IPN permission) in 2004 by journalist Bronisław Wildstein and published in the Internet in 2005. The list gained much attention in Polish media and politics, and during that time IPN security procedures and handling of the matter came under criticism.
IPN presidential election.
The election of a new IPN president in December 2005 was controversial. Janusz Kurtyka, the incumbent IPN president, was contested by Andrzej Przewoźnik. Przewoźnik's candidature received a severe setback after documents were found which suggested his possible co-operation with Służba Bezpieczeństwa, the Communist Poland's internal intelligence agency and secret police. Przewoźnik was cleared of the accusations only after he had lost the election.
Przewoźnik and Kurtyka both died in the 2010 Polish Air Force Tu-154 crash.
Praise.
IPN actions have also attracted support. In 2006 an open letter was published, declaring that:
"History of Solidarity and anti-communist resistance in Poland cannot be damaged by scientific studies and resulting increase in our knowledge of the past. History of opposition to totalitarianism belongs to millions of Poles and not to one social or political group which usurps the right to decide which parts of national history should be discussed and which forgotten."
This letter was signed by a former Prime Minister of Poland, Jan Olszewski; the Mayor of Zakopane, Piotr Bąk; Polish-American Professor and member of the United States Holocaust Memorial Council Marek Jan Chodakiewicz; Professors Maria Dzielska, Piotr Franaszek and Tomasz Gąsowski of the Jagiellonian University; Professor Marek Czachor of Gdańsk University of Technology, journalist and writer Marcin Wolski; Solidarity co-founder Anna Walentynowicz and dozens of others.

</doc>
<doc id="15357" url="http://en.wikipedia.org/wiki?curid=15357" title="Imperial Airways">
Imperial Airways

Imperial Airways was the early British commercial long range air transport company, operating from 1924 to 1939 and serving parts of Europe but principally the British Empire routes to South Africa, India and the Far East, including Malaya and Hong Kong. There were local partnership companies; Qantas (Queensland and Northern Territory Aerial Services Ltd) in Australia and TEAL (Tasman Empire Airways Ltd) in New Zealand.
Imperial Airways was merged into the British Overseas Airways Corporation (BOAC) in 1939, which in turn merged with the British European Airways Corporation to form British Airways.
Background.
The establishment of Imperial Airways occurred in the context of facilitating overseas settlement by making travel to and from the colonies quicker, and that flight would also speed up colonial government and trade that was until then dependent upon ships. The launch of the airline followed a burst of air route surveying in the British Empire after the First World War, and after some experimental (and often dangerous) long-distance flying to the margins of Empire.
Formation.
Imperial Airways was created against a background of stiff competition from French and German airlines that enjoyed heavy government subsidies and following the advice of the government's 'Hambling Committee' (formally known as the 'C.A.T Subsidies Committee'). The committee produced a report on 15 February 1923 recommending that four of the largest existing airlines, The Instone Air Line Company, owned by shipping magnate Samuel Instone, Noel Pemberton Billing's British Marine Air Navigation (part of the Supermarine flying-boat company), the Daimler Airway, under the management of George Edward Woods and Handley Page Transport Co Ltd., should be merged.
It was hoped that this would create a company which could compete against French and German competition and would be strong enough to develop Britain's external air services while minimizing government subsidies for duplicated services. With this in view, a £1m subsidy over ten years was offered to encourage the merger. Agreement was made between the President of the Air Council and the British, Foreign and Colonial Corporation on 3 December 1923 for the company, under the title of the 'Imperial Air Transport Company' to acquire existing air transport services in the UK. The agreement set out the government subsidies for the new company: £137,000 in the first year diminishing to £32,000 in the tenth year as well as minimum mileages to be achieved and penalties if these weren't met.
Imperial Airways Limited was formed on 31 March 1924 with equipment from each contributing concern: British Marine Air Navigation Company Ltd, the Daimler Airway, Handley Page Transport Ltd and the Instone Air Line Ltd. The government had appointed two directors Hambling (who was also President of the Institute of Bankers) and Major J. W. Hills a former Treasury Financial Secretary.
The land operations were based at Croydon Airport to the south of London. IAL immediately discontinued its predecessors' service to points north of London, the airline being focused on international and imperial service rather than domestic. Thereafter the only IAL aircraft operating 'North of Watford' were charter flights.
Industrial troubles with the pilots delayed the start of services until 26 April 1924, when a daily London–Paris route was opened with a de Havilland DH.34. Thereafter the task of expanding the routes between England and the Continent began, with Southampton–Guernsey on 1 May 1924, London-Brussels–Cologne on 3 May, London–Amsterdam on 2 June 1924, and a summer service from London–Paris–Basle–Zürich on 17 June 1924. The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Empire services.
Route proving.
Between 16 November 1925 and 13 March 1926 Alan Cobham made an Imperial Airways’ route survey flight from the UK to Cape Town and back in the Armstrong Siddeley Jaguar–powered de Havilland DH.50J floatplane "G-EBFO". The outward route was London–Paris–Marseille–Pisa–Taranto–Athens–Sollum–Cairo–Luxor–Assuan–Wadi Halfa–Atbara–Khartoum–Malakal–Mongalla–Jinja–Kisumu–Tabora–Abercorn–Ndola–Broken Hill–Livingstone–Bulawayo–Pretoria–Johannesburg–Kimberley–Blomfontein–Cape Town. On his return Cobham was awarded the Air Force Cross for his services to aviation.
On 30 June 1926 Alan Cobham took off from the River Medway at Rochester in "G-EBFO" to make an Imperial Airways route survey for a service to Melbourne, arriving on 15 August. He left Melbourne on 29 August and, after completing 28,000 miles in 320 hours flying time over 78 days, he alighted on the Thames at Westminster on 1 October. Cobham was met by the Secretary of State for Air, Sir Samuel Hoare, and was subsequently knighted by HM King George V.
27 December 1926 Imperial Airways de Havilland DH.66 Hercules "G-EBMX City of Delhi" left Croydon for a survey flight to India. The flight reached Karachi on 6 January and Delhi on 8 January. The aircraft was named by Lady Irwin, wife of the Viceroy, on 10 January 1927. The return flight left on 1 February 1927 and arrived at Heliopolis, Cairo on 7 February. The flying time from Croydon to Delhi was 62 hours 27 minutes and Delhi to Heliopolis 32 hours 50 minutes.
The Eastern Route.
Regular services on the Cairo to Basra route began on 12 January 1927 using DH.66 aircraft, replacing the previous RAF mail flight. Following 2 years of negotiations with the Persian authorities regarding overflight rights, a London to Karachi service started on 30 March 1929, taking 7 days and consisting of a flight from London to Basle, a train to Genoa and a Short S.8 Calcutta flying boats to Alexandria, a train to Cairo and finally a DH.66 flight to Karachi. The route was extended as far as Delhi on 29 December 1929. The route across Europe and the Mediterranean changed many times over the next few years but almost always involved a rail journey.
In April 1931 an experimental London-Australia air mail flight took place; the mail was transferred at the Dutch East Indies, and took 26 days in total to reach Sydney. For the passenger flight leaving London on 1 October 1932, the Eastern route was switched from the Persian to the Arabian side of the Persian Gulf, and Handley Page HP 42 airliners were introduced on the Cairo to Karachi sector. The move saw the establishment of an airport and rest house, Al Mahatta Fort, in the Trucial State of Sharjah now part of the United Arab Emirates.
On 29 May 1933 an England to Australia survey flight took off, operated by Imperial Airways Armstrong Whitworth Atalanta G-ABTL "Astraea". Major H G Brackley, Imperial Airways’ Air Superintendent, was in charge of the flight. "Astraea" flew Croydon-Paris-Lyons-Rome-Brindidsi-Athens-Alexandria-Cairo where it followed the normal route to Karachi then onwards to Jodhpur-Delhi-Calcutta-Akyab-Rangoon-Bangkok-Prachuab-Alor Star-Singapore-Palembang-Batavia-Sourabaya-Bima-Koepang-Bathurst Island-Darwin-Newcastle Waters-Camooweal-Cloncurry-Longreach-Roma-Toowoomba reaching Eagle Farm, Brisbane on 23 June. Sydney was visited on 26 June, Canberra on 28 June and Melbourne on 29 June.
There followed a rapid eastern extension. The first London to Calcutta service departed on 1 July 1933, the first London to Rangoon service on 23 September 1933, the first London to Singapore service on 9 December 1933, and the first London to Brisbane service on 8 December 1934, with QANTAS responsible for the Singapore to Brisbane sector. (The 1934 start was for mail; passenger flights to Brisbane began the following April.) The first London to Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
The Africa Route.
On 28 February 1931 a weekly service began between London and Mwanza on Lake Victoria in Tanganyika as part of the proposed route to Cape Town. On 9 December 1931 the Imperial Airways’ service for Central Africa was extended experimentally to Cape Town for the carriage of Christmas mail. The aircraft used on the last sector, DH66 G-AARY "City of Karachi" arrived in Cape Town on 21 December 1931. On 20 January 1932 a mail-only route to London to Cape Town was opened. On 27 April this route was opened to passengers and took 10 days. In early 1933 Atalantas replaced the DH.66s on the Kisumu to Cape Town sector of the London to Cape Town route. On 9 February 1936 the trans-Africa route was opened by Imperial Airways between Khartoum and Kano in Nigeria. This route was extended to Lagos on 15 October 1936.
Short Empire Flying Boats.
In 1937 with the introduction of Short Empire flying boats built at Short Brothers, Imperial Airways could offer a through-service from Southampton to the Empire. The journey to the Cape was via Marseille, Rome, Brindisi, Athens, Alexandria, Khartoum, Port Bell, Kisumu and onwards by land-based craft to Nairobi, Mbeya and eventually Cape Town. Survey flights were also made across the Atlantic and to New Zealand. By mid-1937 Imperial had completed its thousandth service to the Empire. Starting in 1938 Empire flying boats also flew between Britain and Australia via India and the Middle East.
In March 1939 three Shorts a week left Southhampton for Australia, reaching Sydney after ten days of flying and nine overnight stops. Three more left for South Africa, taking six flying days to Durban.
Passengers.
Imperial's aircraft were small, most seating fewer than twenty passengers; about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research. To begin with only the wealthy could afford to fly, but passenger lists gradually diversified. Travel experiences related to flying low and slow, and were reported enthusiastically in newspapers, magazines and books. There was opportunity for sightseeing from the air and at stops.
Crews.
Imperial Airways stationed its all-male flight deck crew, cabin crew and ground crew along the length of its routes. Specialist engineers and inspectors – and ground crew on rotation or leave – travelled on the airline without generating any seat revenue. Several air crew lost their lives in accidents. At the end of the 1930s crew numbers approximated 3,000. All crew were expected to be ambassadors for Britain and the British Empire.
Air Mail.
In 1934 the Government began negotiations with Imperial Airways to establish a service (Empire Air Mail Scheme) to carry mail by air on routes served by the airline. Indirectly these negotiations led to the dismissal in 1936 of Sir Christopher Bullock, the Permanent Under-Secretary at the Air Ministry, who was found by a Board of Inquiry to have abused his position in seeking a position on the board of the company while these negotiations were in train. The Government, including the Prime Minister, regretted the decision to dismiss him, later finding that, in fact, no corruption was alleged and sought Bullock's reinstatement which he declined.
The Empire Air Mail Programme began in July 1937, delivering anywhere for 1½ d./oz. By mid-1938 a hundred tons of mail had been delivered to India and a similar amount to Africa. In the same year, construction was started on the Empire Terminal in Victoria, London, designed by A. Lakeman and with a statue by Eric Broadbent, "Speed Wings Over the World" gracing the portal above the main entrance. From the terminal there were train connections to Imperial's flying boats at Southampton and coaches to its landplane base at Croydon Airport. The terminal operated as recently as 1980.
To help promote use of the Air Mail service, in June and July 1939, Imperial Airways participated with Pan American Airways in providing a special "around the world" service; Imperial carried the souvenir mail from Foynes, Ireland, to Hong Kong, out of the eastbound New York to New York route. Pan American provided service from New York to Foynes (departing 24 June, via the first flight of Northern FAM 18) and Hong Kong to San Francisco (via FAM 14), and United Airlines carried it on the final leg from San Francisco to New York, arriving on 28 July.
Captain H.W.C. Alger was the pilot for the inaugural air mail flight carrying mail from England to Australia for the first time on the Short Empire flyingboat "Castor" for Imperial Airways' Empires Air Routes, in 1937.
In January 2016, 80 years later, the Crete2Cape Vintage Air Rally will be flying this old route with ten vintage aeroplanes - a celebration of the skill and determination of these early aviators.
Aircraft.
Imperial Airways operated many types of aircraft from its formation in 1 April 1924 until 1 April 1940 when all aircraft still in service were transferred to BOAC.

</doc>
<doc id="15388" url="http://en.wikipedia.org/wiki?curid=15388" title="Arabian mythology">
Arabian mythology

Arabian mythology is the set of ancient, pre-Islamic beliefs held by the Arab people. Prior to Islam, the "Kaaba" of Mecca was covered in symbols representing the myriad demons, djinn, demigods, or simply tribal gods and other assorted deities which represented the polytheistic culture of pre-Islamic Arabia. The shrine was dedicated to the god Hubal and also contained images the three chief goddesses Al-lāt, Al-‘Uzzá, and Manāt. It has been inferred from this plurality that this mythology flourished in an exceptionally broad context. Many of the physical descriptions of the pre-Islamic gods are traced to idols, especially near the Kaaba, which is believed to have contained up to 360 of them.
Gods.
Allah in pre-Islamic Arabia.
In pre-Islamic Arabia, Allah was used by polythiestic Arabs including the Meccans as a reference to possibly a creator god or a supreme deity of their pantheon. In pre-Islamic Arabia, Allah was not used to refer to the sole divinity as it is in Islam. The notion of the term may have been vague in the Meccan religion. Muhammad's father's name was ʿAbd-Allāh meaning "the slave of Allāh". Pre-Islamic Christians, Jews and the monotheistic Arabs called Hanifs used the term 'Bismillah', 'in the name of Allah' and the name Allah to refer to their supreme deity in Arabic stone inscriptions centuries before Islam.
Meccan gods.
The three chief goddesses of Meccan Arabian mythology were Al-lāt, Al-‘Uzzá, and Manāt. Each is associated with certain domains and had shrines with idols located near Taif which have been destroyed. Allāt (Arabic: اللات‎) or Al-lāt is the goddess associated with the underworld. Al-‘Uzzá (Arabic: العزى‎) "The Mightiest One" or "The Strong" was an Arabian fertility goddess. She was called upon for protection and victory before war. Manāt (Arabic: مناة‎) was the goddess of fate; the Book of Idols describes her as the most ancient of all these idols. An idol of Manāt was erected on the seashore in the vicinity of al-Mushallal in Qudayd, between Medina and Mecca. The Aws and the Khazraj, as well as the inhabitants of Medina and Mecca and their vicinities, venerated Manāt and performed sacrifices before her idol, including offering their children. Pilgrimages of some Arabs, including the Aws, Khazraj, Yathrib and others, were not considered completed until they visited Manāt and shaved their heads.
Hubal (Arabic: هبل‎) was one of the most notable gods in Mecca where an image of him was worshipped at the Kaaba. The sanctuary was dedicated to Hubal, who was worshipped as the greatest of the 360 idols the Kaaba contained, which probably represented the days of the year. An idol of Hubal, said to have been near the Kaaba, is described as shaped like a human with the right hand severed and replaced with a golden hand. Manaf (Arabic: مناف‎), was another god of Meccans. He was related to women and menstruation.

</doc>
<doc id="15422" url="http://en.wikipedia.org/wiki?curid=15422" title="List of Internet top-level domains">
List of Internet top-level domains

This is a list of Internet top-level domain (TLDs). A top-level domain is a domain name in the Domain Name System that is a direct subdomain of the DNS root zone. The official list of all top-level domains is maintained by the Internet Assigned Numbers Authority (IANA). In an initiative to expand the domain name space, IANA is considering approval of several proposed top-level domains. s of February 2015[ [update]], the root domain contains 810 top-level domains, while a few have been retired and are no longer functional.
Original top-level domains.
Seven generic top-level domains were created early in that development of the Internet, and pre-date the creation of ICANN in 1998.
Country code top-level domains.
"Note:" the country code domain system was created in the early days of the Domain Name System, and pre-dates ICANN.
Internationalized country code top-level domains.
Source:
Internationalized generic top-level domains.
All of these TLDs are internationalized domain names (IDN) and support second-level IDNs.
Test TLDs.
ICANN created a set of top-level Internationalized domain names in October 2007 for the purpose of testing the use of IDNA in the root zone and within those domains. These testing domains were abolished on 31 October 2013. Each of these TLDs encoded a word meaning "test" in the respective language.
Each of these domains contained only one site with the word "example" encoded in the respective script and language. These "example.test" sites were test wikis used by ICANN.

</doc>
<doc id="15662" url="http://en.wikipedia.org/wiki?curid=15662" title="Geography of Jamaica">
Geography of Jamaica

Jamaica lies 90 mi south of Cuba and 118 mi west of Haiti. At its greatest extent, Jamaica is 146 mi long, and its width varies between 34 and. With an area of 10,911 km2, Jamaica is the largest island of the Commonwealth Caribbean and the third largest of the Greater Antilles, after Cuba and Hispaniola. Many small islands are located along the south coast of Jamaica, such as the Port Royal Cays. Southwest of mainland Jamaica lies Pedro Bank, an area of shallow seas, with a number of cays (low islands or reefs), extending generally east to west for over 160 km. To the southeast lies Morant Bank, with the Morant Cays, 51 km from Morant Point, the easternmost point of mainland Jamaica. Alice Shoal, 260 km southwest of the main island of Jamaica, falls within the Jamaica–Colombia Joint Regime.
Geology and landforms.
Jamaica and the other islands of the Antilles evolved from an arc of ancient volcanoes that rose from the sea millions of years ago. During periods of submersion, thick layers of limestone were laid down over the old igneous and metamorphic rock. In many places, the limestone is thousands of feet thick. The country can be divided into three landform regions: the eastern mountains, the central valleys and plateaus, and the coastal plains.
The highest area is the Blue Mountains. These eastern mountains are formed by a central ridge of metamorphic rock running northwest to southeast from which many long spurs jut to the north and south. For a distance of over 3 km, the crest of the ridge exceeds 1800 m. The highest point is Blue Mountain Peak at 7402 ft. The Blue Mountains rise to these elevations from the coastal plain in the space of about 16 km, thus producing one of the steepest general gradients in the world. In this part of the country, the old metamorphic rock reveals itself through the surrounding limestone. To the north of the Blue Mountains lies the strongly tilted limestone plateau forming the John Crow Mountains. This range rises to elevations of over 1000 m. To the west, in the central part of the country, are two high rolling plateaus: the Dry Harbour Mountains to the north and the Manchester Plateau to the south. Between the two, the land is rugged and here, also, the limestone layers are broken by the older rocks. Streams that rise in the region flow outward and sink soon after reaching the limestone layers.
The limestone plateau covers two-thirds of the country, so that karst formations dominate the island. Karst is formed by the erosion of the limestone in solution. Sinkholes, caves and caverns, disappearing streams, hummocky hills, and terra rosa (residual red) soils in the valleys are distinguishing features of a karst landscape; all these are present in Jamaica. To the west of the mountains is the rugged terrain of the Cockpit Country, one of the world's most dramatic examples of karst topography.
The Cockpit Country is pockmarked with steep-sided hollows, as much as 120 m deep in places, which are separated by conical hills and ridges. On the north, the main defining feature is the fault-based "Escarpment", a long ridge that extends from Flagstaff in the west, through Windsor in the centre, to Campbells and the start of the Barbecue Bottom Road (B10). The Barbecue Bottom Road, which runs north-south, high along the side of a deep, fault-based valley in the east, is the only drivable route across the Cockpit Country. However, there are two old, historical trails that cross further west, the Troy Trail, and the Quick Step Trail, both of which are seldom used as of 2006 and difficult to find. In the southwest, near Quick Step, is the district known as the "Land of Look Behind," so named because Spanish horsemen venturing into this region of hostile runaway slaves were said to have ridden two to a mount, one rider facing to the rear to keep a precautionary watch. Where the ridges between sinkholes in the plateau area have dissolved, flat-bottomed basins or valleys have been formed that are filled with terra rosa soils, some of the most productive on the island. The largest basin is the Vale of Clarendon, 80 km long and 32 km wide. Queen of Spains Valley, Nassau Valley, and Cave Valley were formed by the same process.
Coasts.
The coastline of Jamaica is one of many contrasts. The northeast shore is severely eroded by the ocean. There are many small inlets in the rugged coastline, but no coastal plain of any extent. A narrow strip of plains along the northern coast offers calm seas and white sand beaches. Behind the beaches is a flat raised plain of uplifted coral reef.
The southern coast has small stretches of plains lined by black sand beaches. These are backed by cliffs of limestone where the plateaus end. In many stretches with no coastal plain, the cliffs drop 300 m straight to the sea. In the southwest, broad plains stretch inland for a number of kilometres. The Black River courses 70 km through the largest of these plains. The swamplands of the Great Morass and the Upper Morass fill much of the plains. The western coastline contains the island's finest beaches.
Climate.
Two types of climate are found in Jamaica. An upland tropical climate prevails on the windward side of the mountains, whereas a semiarid climate predominates on the leeward side. Warm trade winds from the east and northeast bring rainfall throughout the year. The rainfall is heaviest from May to October, with peaks in those two months. The average rainfall is 1960 mm per year. Rainfall is much greater in the mountain areas facing the north and east, however. Where the higher elevations of the John Crow Mountains and the Blue Mountains catch the rain from the moisture-laden winds, rainfall exceeds 5080 mm per year. Since the southwestern half of the island lies in the rain shadow of the mountains, it has a semiarid climate and receives fewer than 760 mm of rainfall annually.
Temperatures in Jamaica are fairly constant throughout the year, averaging 25 to in the lowlands and 15 to at higher elevations. Temperatures may dip to below 10 °C at the peaks of the Blue Mountains. The island receives, in addition to the northeast trade winds, refreshing onshore breezes during the day and cooling offshore breezes at night. These are known on Jamaica as the "Doctor Breeze" and the "Undertaker's Breeze," respectively.
Jamaica lies in the Atlantic hurricane belt; as a result, the island sometimes experiences significant storm damage. Powerful hurricanes which have hit the island directly causing death and destruction include Hurricane Charlie in 1951 and Hurricane Gilbert in 1988. Several other powerful hurricanes have passed near to the island with damaging effects. In 1980, for example, Hurricane Allen destroyed nearly all Jamaica's banana crop. Hurricane Ivan (2004) swept past the island causing heavy damage and a number of deaths; in 2005, Hurricanes Dennis and Emily brought heavy rains to the island. A Category 4 hurricane, Hurricane Dean, caused some deaths and heavy damage to Jamaica in August 2007.
The first recorded hurricane to hit Jamaica was in 1519. The island has been struck by tropical cyclones regularly. During two of the coldest periods in the last 250 years (1780s and 1810s), the frequency of hurricanes in the Jamaica region was unusually high. Another peak of activity occurred in the 1910s, the coldest decade of the 20th century. On the other hand, hurricane formation was greatly diminished from 1968 to 1994, which for some reason coincides with the great Sahel drought.
Vegetation and wildlife.
Although most of Jamaica's native vegetation has been stripped in order to make room for cultivation, some areas have been left virtually undisturbed since the time of Columbus. Indigenous vegetation can be found along the northern coast from Rio Bueno to Discovery Bay, in the highest parts of the Blue Mountains, and in the heart of the Cockpit Country.
As in the case of vegetation, considerable loss of wildlife has occurred beginning with the settlement of native peoples in the region millennia ago. For example, the only pinniped ever known to the Caribbean, the Caribbean monk seal once occurred in Jamaican waters and has now been driven to extinction.

</doc>
<doc id="15700" url="http://en.wikipedia.org/wiki?curid=15700" title="Transport in Jersey">
Transport in Jersey

This article details the variety of means of transport in Jersey, Channel Islands.
Air transport.
Airports:
Rail transport.
Historically there were public railway services in the island, provided by two railway companies:
During the German military occupation 1940–1945, light railways were re-established by the Germans for the purpose of supplying coastal fortifications. A one-metre gauge line was laid down following the route of the former Jersey Railway from Saint Helier to La Corbière, with a branch line connecting the stone quarry at Ronez in Saint John. A 60cm line ran along the west coast, and another was laid out heading east from Saint Helier to Gorey. The first line was opened in July 1942, the ceremony being disrupted by passively resisting Jersey spectators. The German railway infrastructure was dismantled after the Liberation in 1945.
Two railways operate at the Pallot Heritage Steam Museum; a standard gauge heritage steam railway, and a narrow gauge pleasure line operated by steam-outline diesel motive power.
Road transport.
Highways:
<br>"total:"
577 km (1995)
<br>"paved:"
NA km
<br>"unpaved:"
NA km
Buses.
Buses are operated by CT Plus Jersey, a local subsidiary of HCT Group. Bus service routes radiate from the Liberation Station in St Helier.
In 2012, it was announced that CT Plus would take over the operation of the bus service, commencing on 2 January 2013, ending 10 years of Connex service in Jersey. This new service is called LibertyBus.
Cycling.
Jersey has a well sign posted Island Cycle Network. A traffic-free route for cyclists and pedestrians links Saint Helier to La Corbière and a branch of this route ends up at St Peter's village passing Jersey Airport.
Driving.
Driving is on the left hand side. The maximum speed limit throughout the entire island is 40 mph (64 km/h), with slower limits on certain stretches of road, such as 20/30 mph (32/48 km/h) in built up areas and 15 mph (24 km/h) on roads designated as "green lanes".
Visitors wishing to drive must possess a Certificate of Insurance or an International Green Card, a valid Driving Licence or International Driving Permit (UK International Driving Permits are not valid). Photocopies are not acceptable. A nationality plate must be displayed on the back of visiting vehicles.
It is an offence to hold a mobile phone whilst driving a moving vehicle. Where fitted, all passengers inside a vehicle must wear a seat belt at all times, regardless of whether they are sitting in the front or the rear.
The penalties for drinking and driving in Jersey are up to £2,000 fine or 6 months in prison for the first offence plus unlimited disqualification of driving licence. It is an offence to drive whilst under the influence of drugs. Since July 2014 it has also been illegal to smoke in any vehicle carrying passengers under the age of 18. 
Parking.
Single yellow lines indicate that parking is prohibited and is liable to a fine. 
Paycards are used to pay for parking throughout Jersey with the exception of the harbour, airport and waterfront car parks where a pay upon exit scheme is operated. Paycards require scratching off the appropriate day, date, month, and time.
Payment by paycards is required for parking wherever the paycard symbol is displayed. Some paycard locations, such as the lay-bys in Victoria Avenue, and car parks in St Brelade's Bay are seasonal.
There are four main residents’ and business parking zones within St Helier. 
In some roads on the outskirts of St Helier and in the harbours, and also in some car parks in St Brelade, parking is free but controlled by parking discs (time wheels) – obtainable from the Town Hall for a small charge.
Sea transport.
Seaports and harbours:
Saint Helier is the island's main port, others include Gorey, Saint Aubin, La Rocque, and Bonne Nuit. It is 33.6mi distant from Granville, Manche, 142.9mi from Southampton, 131.3mi from Poole, and 22.9mi from St Malo.
On 20 August 2013, Huelin-Renouf, which had operated a "lift-on lift-off" container service for 80 years between the Port of Southampton and the Port of Jersey, ceased trading. Senator Alan Maclean, a Jersey politician had previously tried to save the 90-odd jobs furnished by the company to no avail. On 20 September, it was announced that Channel Island Lines would continue this service, and would purchase the MV Huelin Dispatch from Associated British Ports who in turn had purchased them from the receiver in the bankruptcy. The new operator was to be funded by Rockayne Limited, a closely held association of Jersey businesspeople.
Passenger-only access to France is provided by ferry service, to either Barneville-Carteret, Granville or Dielette.
A service to St Malo was provided by , but is now operated by its sister service, Condor Ferries, which runs the Commodore Goodwill, a large ro-ro vessel to Portsmouth, and has multiple ro-ro connections to Poole, Weymouth, and St Malo. It was announced on 4 October 2012 that Condor Logistics would close its operations with the loss of about 180 jobs (110 in the UK, 50 in Jersey and 20 in Guernsey). The move was blamed on changes to low-value consignment relief affecting the Channel Islands.

</doc>
<doc id="15736" url="http://en.wikipedia.org/wiki?curid=15736" title="Johannes Kepler">
Johannes Kepler

Johannes Kepler (]; December 27, 1571 – November 15, 1630) was a German mathematician, astronomer, and astrologer. A key figure in the 17th century scientific revolution, he is best known for his laws of planetary motion, based on his works "Astronomia nova", "Harmonices Mundi", and "Epitome of Copernican Astronomy". These works also provided one of the foundations for Isaac Newton's theory of universal gravitation.
During his career, Kepler was a mathematics teacher at a seminary school in Graz, Austria, where he became an associate of Prince Hans Ulrich von Eggenberg. Later he became an assistant to astronomer Tycho Brahe, and eventually the imperial mathematician to Emperor Rudolf II and his two successors Matthias and Ferdinand II. He was also a mathematics teacher in Linz, Austria, and an adviser to General Wallenstein. Additionally, he did fundamental work in the field of optics, invented an improved version of the refracting telescope (the Keplerian Telescope), and mentioned the telescopic discoveries of his contemporary Galileo Galilei.
Kepler lived in an era when there was no clear distinction between astronomy and astrology, but there was a strong division between astronomy (a branch of mathematics within the liberal arts) and physics (a branch of natural philosophy). Kepler also incorporated religious arguments and reasoning into his work, motivated by the religious conviction and belief that God had created the world according to an intelligible plan that is accessible through the natural light of reason. Kepler described his new astronomy as "celestial physics", as "an excursion into Aristotle's "Metaphysics"", and as "a supplement to Aristotle's "On the Heavens"", transforming the ancient tradition of physical cosmology by treating astronomy as part of a universal mathematical physics.
Early years.
Johannes Kepler was born on December 27, the feast day of St. John the Evangelist, 1571, at the Free Imperial City of Weil der Stadt (now part of the Stuttgart Region in the German state of Baden-Württemberg, 30 km west of Stuttgart's center). His grandfather, Sebald Kepler, had been Lord Mayor of that town but, by the time Johannes was born, he had two brothers and one sister and the Kepler family fortune was in decline. His father, Heinrich Kepler, earned a precarious living as a mercenary, and he left the family when Johannes was five years old. He was believed to have died in the Eighty Years' War in the Netherlands. His mother Katharina Guldenmann, an inn-keeper's daughter, was a healer and herbalist. Born prematurely, Johannes claimed to have been weak and sickly as a child. Nevertheless, he often impressed travelers at his grandfather's inn with his phenomenal mathematical faculty.
He was introduced to astronomy at an early age, and developed a love for it that would span his entire life. At age six, he observed the Great Comet of 1577, writing that he "was taken by [his] mother to a high place to look at it." At age nine, he observed another astronomical event, a lunar eclipse in 1580, recording that he remembered being "called outdoors" to see it and that the moon "appeared quite red". However, childhood smallpox left him with weak vision and crippled hands, limiting his ability in the observational aspects of astronomy.
In 1589, after moving through grammar school, Latin school, and seminary at Maulbronn, Kepler attended Tübinger Stift at the University of Tübingen. There, he studied philosophy under Vitus Müller and theology under Jacob Heerbrand (a student of Philipp Melanchthon at Wittenberg), who also taught Michael Maestlin while he was a student, until he became Chancellor at Tübingen in 1590. He proved himself to be a superb mathematician and earned a reputation as a skillful astrologer, casting horoscopes for fellow students. Under the instruction of Michael Maestlin, Tübingen's professor of mathematics from 1583 to 1631, he learned both the Ptolemaic system and the Copernican system of planetary motion. He became a Copernican at that time. In a student disputation, he defended heliocentrism from both a theoretical and theological perspective, maintaining that the Sun was the principal source of motive power in the Universe. Despite his desire to become a minister, near the end of his studies Kepler was recommended for a position as teacher of mathematics and astronomy at the Protestant school in Graz. He accepted the position in April 1594, at the age of 23.
Graz (1594–1600).
"Mysterium Cosmographicum".
Johannes Kepler's first major astronomical work, "Mysterium Cosmographicum" ("The Cosmographic Mystery"), was the first published defense of the Copernican system. Kepler claimed to have had an epiphany on July 19, 1595, while teaching in Graz, demonstrating the periodic conjunction of Saturn and Jupiter in the zodiac: he realized that regular polygons bound one inscribed and one circumscribed circle at definite ratios, which, he reasoned, might be the geometrical basis of the Universe. After failing to find a unique arrangement of polygons that fit known astronomical observations (even with extra planets added to the system), Kepler began experimenting with 3-dimensional polyhedra. He found that each of the five Platonic solids could be uniquely inscribed and circumscribed by spherical orbs; nesting these solids, each encased in a sphere, within one another would produce six layers, corresponding to the six known planets—Mercury, Venus, Earth, Mars, Jupiter, and Saturn. By ordering the solids correctly—octahedron, icosahedron, dodecahedron, tetrahedron, cube—Kepler found that the spheres could be placed at intervals corresponding (within the accuracy limits of available astronomical observations) to the relative sizes of each planet’s path, assuming the planets circle the Sun. Kepler also found a formula relating the size of each planet’s orb to the length of its orbital period: from inner to outer planets, the ratio of increase in orbital period is twice the difference in orb radius. However, Kepler later rejected this formula, because it was not precise enough.
As he indicated in the title, Kepler thought he had revealed God’s geometrical plan for the Universe. Much of Kepler’s enthusiasm for the Copernican system stemmed from his theological convictions about the connection between the physical and the spiritual; the Universe itself was an image of God, with the Sun corresponding to the Father, the stellar sphere to the Son, and the intervening space between to the Holy Spirit. His first manuscript of "Mysterium" contained an extensive chapter reconciling heliocentrism with biblical passages that seemed to support geocentrism.
With the support of his mentor Michael Maestlin, Kepler received permission from the Tübingen university senate to publish his manuscript, pending removal of the Bible exegesis and the addition of a simpler, more understandable description of the Copernican system as well as Kepler’s new ideas. "Mysterium" was published late in 1596, and Kepler received his copies and began sending them to prominent astronomers and patrons early in 1597; it was not widely read, but it established Kepler’s reputation as a highly skilled astronomer. The effusive dedication, to powerful patrons as well as to the men who controlled his position in Graz, also provided a crucial doorway into the patronage system.
Though the details would be modified in light of his later work, Kepler never relinquished the Platonist polyhedral-spherist cosmology of "Mysterium Cosmographicum". His subsequent main astronomical works were in some sense only further developments of it, concerned with finding more precise inner and outer dimensions for the spheres by calculating the eccentricities of the planetary orbits within it. In 1621, Kepler published an expanded second edition of "Mysterium", half as long again as the first, detailing in footnotes the corrections and improvements he had achieved in the 25 years since its first publication.
In terms of the impact of "Mysterium", it can be seen as an important first step in modernizing the theory proposed by Nicolaus Copernicus in his "De Revolutionibus". Whilst Copernicus sought to advance a helio-centric system in this book, he resorted to Ptolemaic devices (viz., epicycles and eccentric circles) in order to explain the change in planets' orbital speed, and also continued to use as a point of reference the center of the earth's orbit rather than that of the sun "as an aid to calculation and in order not to confuse the reader by diverging too much from Ptolemy." Modern astronomy owes much to "Mysterium Cosmographicum", despite flaws in its main thesis, "since it represents the first step in cleansing the Copernican system of the remnants of the Ptolemaic theory still clinging to it." 
Marriage to Barbara Müller.
In December 1595, Kepler was introduced to Barbara Müller, a 23-year-old widow (twice over) with a young daughter, Gemma van Dvijneveldt, and he began courting her. Müller, heiress to the estates of her late husbands, was also the daughter of a successful mill owner. Her father Jobst initially opposed a marriage despite Kepler's nobility; though he had inherited his grandfather's nobility, Kepler's poverty made him an unacceptable match. Jobst relented after Kepler completed work on "Mysterium", but the engagement nearly fell apart while Kepler was away tending to the details of publication. However, church officials—who had helped set up the match—pressured the Müllers to honor their agreement. Barbara and Johannes were married on April 27, 1597.
In the first years of their marriage, the Keplers had two children (Heinrich and Susanna), both of whom died in infancy. In 1602, they had a daughter (Susanna); in 1604, a son (Friedrich); and in 1607, another son (Ludwig).
Other research.
Following the publication of "Mysterium" and with the blessing of the Graz school inspectors, Kepler began an ambitious program to extend and elaborate his work. He planned four additional books: one on the stationary aspects of the Universe (the Sun and the fixed stars); one on the planets and their motions; one on the physical nature of planets and the formation of geographical features (focused especially on Earth); and one on the effects of the heavens on the Earth, to include atmospheric optics, meteorology, and astrology.
He also sought the opinions of many of the astronomers to whom he had sent "Mysterium", among them Reimarus Ursus (Nicolaus Reimers Bär)—the imperial mathematician to Rudolph II and a bitter rival of Tycho Brahe. Ursus did not reply directly, but republished Kepler's flattering letter to pursue his priority dispute over (what is now called) the Tychonic system with Tycho. Despite this black mark, Tycho also began corresponding with Kepler, starting with a harsh but legitimate critique of Kepler's system; among a host of objections, Tycho took issue with the use of inaccurate numerical data taken from Copernicus. Through their letters, Tycho and Kepler discussed a broad range of astronomical problems, dwelling on lunar phenomena and Copernican theory (particularly its theological viability). But without the significantly more accurate data of Tycho's observatory, Kepler had no way to address many of these issues.
Instead, he turned his attention to chronology and "harmony," the numerological relationships among music, mathematics and the physical world, and their astrological consequences. By assuming the Earth to possess a soul (a property he would later invoke to explain how the sun causes the motion of planets), he established a speculative system connecting astrological aspects and astronomical distances to weather and other earthly phenomena. By 1599, however, he again felt his work limited by the inaccuracy of available data—just as growing religious tension was also threatening his continued employment in Graz. In December of that year, Tycho invited Kepler to visit him in Prague; on January 1, 1600 (before he even received the invitation), Kepler set off in the hopes that Tycho's patronage could solve his philosophical problems as well as his social and financial ones.
Prague (1600–1612).
Work for Tycho Brahe.
On February 4, 1600, Kepler met Tycho Brahe and his assistants Franz Tengnagel and Longomontanus at Benátky nad Jizerou (35 km from Prague), the site where Tycho's new observatory was being constructed. Over the next two months he stayed as a guest, analyzing some of Tycho's observations of Mars; Tycho guarded his data closely, but was impressed by Kepler's theoretical ideas and soon allowed him more access. Kepler planned to test his theory from "Mysterium Cosmographicum" based on the Mars data, but he estimated that the work would take up to two years (since he was not allowed to simply copy the data for his own use). With the help of Johannes Jessenius, Kepler attempted to negotiate a more formal employment arrangement with Tycho, but negotiations broke down in an angry argument and Kepler left for Prague on April 6. Kepler and Tycho soon reconciled and eventually reached an agreement on salary and living arrangements, and in June, Kepler returned home to Graz to collect his family.
Political and religious difficulties in Graz dashed his hopes of returning immediately to Brahe; in hopes of continuing his astronomical studies, Kepler sought an appointment as mathematician to Archduke Ferdinand. To that end, Kepler composed an essay—dedicated to Ferdinand—in which he proposed a force-based theory of lunar motion: "In Terra inest virtus, quae Lunam ciet" ("There is a force in the earth which causes the moon to move"). Though the essay did not earn him a place in Ferdinand's court, it did detail a new method for measuring lunar eclipses, which he applied during the July 10 eclipse in Graz. These observations formed the basis of his explorations of the laws of optics that would culminate in "Astronomiae Pars Optica".
On August 2, 1600, after refusing to convert to Catholicism, Kepler and his family were banished from Graz. Several months later, Kepler returned, now with the rest of his household, to Prague. Through most of 1601, he was supported directly by Tycho, who assigned him to analyzing planetary observations and writing a tract against Tycho's (by then deceased) rival, Ursus. In September, Tycho secured him a commission as a collaborator on the new project he had proposed to the emperor: the "Rudolphine Tables" that should replace the "Prutenic Tables" of Erasmus Reinhold. Two days after Tycho's unexpected death on October 24, 1601, Kepler was appointed his successor as imperial mathematician with the responsibility to complete his unfinished work. The next 11 years as imperial mathematician would be the most productive of his life.
Advisor to Emperor Rudolph II.
Kepler's primary obligation as imperial mathematician was to provide astrological advice to the emperor. Though Kepler took a dim view of the attempts of contemporary astrologers to precisely predict the future or divine specific events, he had been casting well-received detailed horoscopes for friends, family, and patrons since his time as a student in Tübingen. In addition to horoscopes for allies and foreign leaders, the emperor sought Kepler's advice in times of political trouble. Rudolph was actively interested in the work of many of his court scholars (including numerous alchemists) and kept up with Kepler's work in physical astronomy as well.
Officially, the only acceptable religious doctrines in Prague were Catholic and Utraquist, but Kepler's position in the imperial court allowed him to practice his Lutheran faith unhindered. The emperor nominally provided an ample income for his family, but the difficulties of the over-extended imperial treasury meant that actually getting hold of enough money to meet financial obligations was a continual struggle. Partly because of financial troubles, his life at home with Barbara was unpleasant, marred with bickering and bouts of sickness. Court life, however, brought Kepler into contact with other prominent scholars (Johannes Matthäus Wackher von Wackhenfels, Jost Bürgi, David Fabricius, Martin Bachazek, and Johannes Brengger, among others) and astronomical work proceeded rapidly.
"Astronomiae Pars Optica".
As he slowly continued analyzing Tycho's Mars observations—now available to him in their entirety—and began the slow process of tabulating the "Rudolphine Tables", Kepler also picked up the investigation of the laws of optics from his lunar essay of 1600. Both lunar and solar eclipses presented unexplained phenomena, such as unexpected shadow sizes, the red color of a total lunar eclipse, and the reportedly unusual light surrounding a total solar eclipse. Related issues of atmospheric refraction applied to "all" astronomical observations. Through most of 1603, Kepler paused his other work to focus on optical theory; the resulting manuscript, presented to the emperor on January 1, 1604, was published as "Astronomiae Pars Optica" ("The Optical Part of Astronomy"). In it, Kepler described the inverse-square law governing the intensity of light, reflection by flat and curved mirrors, and principles of pinhole cameras, as well as the astronomical implications of optics such as parallax and the apparent sizes of heavenly bodies. He also extended his study of optics to the human eye, and is generally considered by neuroscientists to be the first to recognize that images are projected inverted and reversed by the eye's lens onto the retina. The solution to this dilemma was not of particular importance to Kepler as he did not see it as pertaining to optics, although he did suggest that the image was later corrected "in the hollows of the brain" due to the "activity of the Soul." Today, "Astronomiae Pars Optica" is generally recognized as the foundation of modern optics (though the law of refraction is conspicuously absent). With respect to the beginnings of projective geometry, Kepler introduced the idea of continuous change of a mathematical entity in this work. He argued that if a focus of a conic section were allowed to move along the line joining the foci, the geometric form would morph or degenerate, one into another. In this way, an ellipse becomes a parabola when a focus moves toward infinity, and when two foci of an ellipse merge into one another, a circle is formed. As the foci of a hyperbola merge into one another, the hyperbola becomes a pair of straight lines. He also assumed that if a straight line is extended to infinity it will meet itself at a single point at infinity, thus having the properties of a large circle. This idea was later utilized by Pascal, Leibniz, Monge, and Poncelet, among others, and became known as geometric continuity and as the Law or Principle of Continuity.
The Supernova of 1604.
In October 1604, a bright new evening star (SN 1604) appeared, but Kepler did not believe the rumors until he saw it himself. Kepler began systematically observing the nova. Astrologically, the end of 1603 marked the beginning of a fiery trigon, the start of the ca. 800-year cycle of great conjunctions; astrologers associated the two previous such periods with the rise of Charlemagne (ca. 800 years earlier) and the birth of Christ (ca. 1600 years earlier), and thus expected events of great portent, especially regarding the emperor. It was in this context, as the imperial mathematician and astrologer to the emperor, that Kepler described the new star two years later in his "De Stella Nova". In it, Kepler addressed the star's astronomical properties while taking a skeptical approach to the many astrological interpretations then circulating. He noted its fading luminosity, speculated about its origin, and used the lack of observed parallax to argue that it was in the sphere of fixed stars, further undermining the doctrine of the immutability of the heavens (the idea accepted since Aristotle that the celestial spheres were perfect and unchanging). The birth of a new star implied the variability of the heavens. In an appendix, Kepler also discussed the recent chronology work of the Polish historian Laurentius Suslyga; he calculated that, if Suslyga was correct that accepted timelines were four years behind, then the Star of Bethlehem—analogous to the present new star—would have coincided with the first great conjunction of the earlier 800-year cycle.
"Astronomia nova".
The extended line of research that culminated in "Astronomia nova" ("A New Astronomy")—including the first two laws of planetary motion—began with the analysis, under Tycho's direction, of Mars' orbit. Kepler calculated and recalculated various approximations of Mars' orbit using an equant (the mathematical tool that Copernicus had eliminated with his system), eventually creating a model that generally agreed with Tycho's observations to within two arcminutes (the average measurement error). But he was not satisfied with the complex and still slightly inaccurate result; at certain points the model differed from the data by up to eight arcminutes. The wide array of traditional mathematical astronomy methods having failed him, Kepler set about trying to fit an ovoid orbit to the data.
Within Kepler's religious view of the cosmos, the Sun (a symbol of God the Father) was the source of motive force in the solar system. As a physical basis, Kepler drew by analogy on William Gilbert's theory of the magnetic soul of the Earth from "De Magnete" (1600) and on his own work on optics. Kepler supposed that the motive power (or motive "species") radiated by the Sun weakens with distance, causing faster or slower motion as planets move closer or farther from it. Perhaps this assumption entailed a mathematical relationship that would restore astronomical order. Based on measurements of the aphelion and perihelion of the Earth and Mars, he created a formula in which a planet's rate of motion is inversely proportional to its distance from the Sun. Verifying this relationship throughout the orbital cycle, however, required very extensive calculation; to simplify this task, by late 1602 Kepler reformulated the proportion in terms of geometry: "planets sweep out equal areas in equal times"—Kepler's second law of planetary motion.
He then set about calculating the entire orbit of Mars, using the geometrical rate law and assuming an egg-shaped ovoid orbit. After approximately 40 failed attempts, in early 1605 he at last hit upon the idea of an ellipse, which he had previously assumed to be too simple a solution for earlier astronomers to have overlooked. Finding that an elliptical orbit fit the Mars data, he immediately concluded that "all planets move in ellipses, with the sun at one focus"—Kepler's first law of planetary motion. Because he employed no calculating assistants, however, he did not extend the mathematical analysis beyond Mars. By the end of the year, he completed the manuscript for "Astronomia nova", though it would not be published until 1609 due to legal disputes over the use of Tycho's observations, the property of his heirs.
"Dioptrice", "Somnium" manuscript, and other work.
In the years following the completion of "Astronomia Nova", most of Kepler's research was focused on preparations for the "Rudolphine Tables" and a comprehensive set of ephemerides (specific predictions of planet and star positions) based on the table (though neither would be completed for many years). He also attempted (unsuccessfully) to begin a collaboration with Italian astronomer Giovanni Antonio Magini. Some of his other work dealt with chronology, especially the dating of events in the life of Jesus, and with astrology, especially criticism of dramatic predictions of catastrophe such as those of Helisaeus Roeslin.
Kepler and Roeslin engaged in a series of published attacks and counter-attacks, while physician Philip Feselius published a work dismissing astrology altogether (and Roeslin's work in particular). In response to what Kepler saw as the excesses of astrology on the one hand and overzealous rejection of it on the other, Kepler prepared "Tertius Interveniens" [Third-party Interventions]. Nominally this work—presented to the common patron of Roeslin and Feselius—was a neutral mediation between the feuding scholars, but it also set out Kepler's general views on the value of astrology, including some hypothesized mechanisms of interaction between planets and individual souls. While Kepler considered most traditional rules and methods of astrology to be the "evil-smelling dung" in which "an industrious hen" scrapes, there was an "occasional grain-seed, indeed, even a pearl or a gold nugget" to be found by the conscientious scientific astrologer. Conversely, Sir Oliver Lodge observed that Kepler was somewhat disdainful of astrology, as Kepler was "continually attacking and throwing sarcasm at astrology, but it was the only thing for which people would pay him, and on it after a fashion he lived."
In the first months of 1610, Galileo Galilei—using his powerful new telescope—discovered four satellites orbiting Jupiter. Upon publishing his account as "Sidereus Nuncius" [Starry Messenger], Galileo sought the opinion of Kepler, in part to bolster the credibility of his observations. Kepler responded enthusiastically with a short published reply, "Dissertatio cum Nuncio Sidereo" [Conversation with the Starry Messenger]. He endorsed Galileo's observations and offered a range of speculations about the meaning and implications of Galileo's discoveries and telescopic methods, for astronomy and optics as well as cosmology and astrology. Later that year, Kepler published his own telescopic observations of the moons in "Narratio de Jovis Satellitibus", providing further support of Galileo. To Kepler's disappointment, however, Galileo never published his reactions (if any) to "Astronomia Nova".
After hearing of Galileo's telescopic discoveries, Kepler also started a theoretical and experimental investigation of telescopic optics using a telescope borrowed from Duke Ernest of Cologne. The resulting manuscript was completed in September 1610 and published as "Dioptrice" in 1611. In it, Kepler set out the theoretical basis of double-convex converging lenses and double-concave diverging lenses—and how they are combined to produce a Galilean telescope—as well as the concepts of real vs. virtual images, upright vs. inverted images, and the effects of focal length on magnification and reduction. He also described an improved telescope—now known as the "astronomical" or "Keplerian telescope"—in which two convex lenses can produce higher magnification than Galileo's combination of convex and concave lenses.
Around 1611, Kepler circulated a manuscript of what would eventually be published (posthumously) as "Somnium" [The Dream]. Part of the purpose of "Somnium" was to describe what practicing astronomy would be like from the perspective of another planet, to show the feasibility of a non-geocentric system. The manuscript, which disappeared after changing hands several times, described a fantastic trip to the moon; it was part allegory, part autobiography, and part treatise on interplanetary travel (and is sometimes described as the first work of science fiction). Years later, a distorted version of the story may have instigated the witchcraft trial against his mother, as the mother of the narrator consults a demon to learn the means of space travel. Following her eventual acquittal, Kepler composed 223 footnotes to the story—several times longer than the actual text—which explained the allegorical aspects as well as the considerable scientific content (particularly regarding lunar geography) hidden within the text.
Work in mathematics and physics.
As a New Year's gift that year, he also composed for his friend and some-time patron, Baron Wackher von Wackhenfels, a short pamphlet entitled "Strena Seu de Nive Sexangula" ("A New Year's Gift of Hexagonal Snow"). In this treatise, he published the first description of the hexagonal symmetry of snowflakes and, extending the discussion into a hypothetical atomistic physical basis for the symmetry, posed what later became known as the Kepler conjecture, a statement about the most efficient arrangement for packing spheres. Kepler was one of the pioneers of the mathematical applications of infinitesimals; see Law of Continuity.
Personal and political troubles.
In 1611, the growing political-religious tension in Prague came to a head. Emperor Rudolph—whose health was failing—was forced to abdicate as King of Bohemia by his brother Matthias. Both sides sought Kepler's astrological advice, an opportunity he used to deliver conciliatory political advice (with little reference to the stars, except in general statements to discourage drastic action). However, it was clear that Kepler's future prospects in the court of Matthias were dim.
Also in that year, Barbara Kepler contracted Hungarian spotted fever, then began having seizures. As Barbara was recovering, Kepler's three children all fell sick with smallpox; Friedrich, 6, died. Following his son's death, Kepler sent letters to potential patrons in Württemberg and Padua. At the University of Tübingen in Württemberg, concerns over Kepler's perceived Calvinist heresies in violation of the Augsburg Confession and the Formula of Concord prevented his return. The University of Padua—on the recommendation of the departing Galileo—sought Kepler to fill the mathematics professorship, but Kepler, preferring to keep his family in German territory, instead travelled to Austria to arrange a position as teacher and district mathematician in Linz. However, Barbara relapsed into illness and died shortly after Kepler's return.
Kepler postponed the move to Linz and remained in Prague until Rudolph's death in early 1612, though between political upheaval, religious tension, and family tragedy (along with the legal dispute over his wife's estate), Kepler could do no research. Instead, he pieced together a chronology manuscript, "Eclogae Chronicae", from correspondence and earlier work. Upon succession as Holy Roman Emperor, Matthias re-affirmed Kepler's position (and salary) as imperial mathematician but allowed him to move to Linz.
Linz and elsewhere (1612–1630).
In Linz, Kepler's primary responsibilities (beyond completing the "Rudolphine Tables") were teaching at the district school and providing astrological and astronomical services. In his first years there, he enjoyed financial security and religious freedom relative to his life in Prague—though he was excluded from Eucharist by his Lutheran church over his theological scruples. His first publication in Linz was "De vero Anno" (1613), an expanded treatise on the year of Christ's birth; he also participated in deliberations on whether to introduce Pope Gregory's reformed calendar to Protestant German lands; that year he also wrote the influential mathematical treatise "Nova stereometria doliorum vinariorum", on measuring the volume of containers such as wine barrels, published in 1615.
Second marriage.
On October 30, 1613, Kepler married the 24-year-old Susanna Reuttinger. Following the death of his first wife Barbara, Kepler had considered 11 different matches over two years (a decision process formalized later as the marriage problem). He eventually returned to Reuttinger (the fifth match) who, he wrote, "won me over with love, humble loyalty, economy of household, diligence, and the love she gave the stepchildren." The first three children of this marriage (Margareta Regina, Katharina, and Sebald) died in childhood. Three more survived into adulthood: Cordula (b. 1621); Fridmar (b. 1623); and Hildebert (b. 1625). According to Kepler's biographers, this was a much happier marriage than his first.
"Epitome of Copernican Astronomy", calendars, and the witch trial of his mother.
Since completing the "Astronomia nova", Kepler had intended to compose an astronomy textbook. In 1615, he completed the first of three volumes of "Epitome astronomiae Copernicanae" ("Epitome of Copernican Astronomy"); the first volume (books I-III) was printed in 1617, the second (book IV) in 1620, and the third (books V-VII) in 1621. Despite the title, which referred simply to heliocentrism, Kepler's textbook culminated in his own ellipse-based system. The "Epitome" became Kepler's most influential work. It contained all three laws of planetary motion and attempted to explain heavenly motions through physical causes. Though it explicitly extended the first two laws of planetary motion (applied to Mars in "Astronomia nova") to all the planets as well as the Moon and the Medicean satellites of Jupiter, it did not explain how elliptical orbits could be derived from observational data.
As a spin-off from the "Rudolphine Tables" and the related "Ephemerides", Kepler published astrological calendars, which were very popular and helped offset the costs of producing his other work—especially when support from the Imperial treasury was withheld. In his calendars—six between 1617 and 1624—Kepler forecast planetary positions and weather as well as political events; the latter were often cannily accurate, thanks to his keen grasp of contemporary political and theological tensions. By 1624, however, the escalation of those tensions and the ambiguity of the prophecies meant political trouble for Kepler himself; his final calendar was publicly burned in Graz.
In 1615, Ursula Reingold, a woman in a financial dispute with Kepler's brother Christoph, claimed Kepler's mother Katharina had made her sick with an evil brew. The dispute escalated, and in 1617 Katharina was accused of witchcraft; witchcraft trials were relatively common in central Europe at this time. Beginning in August 1620, she was imprisoned for fourteen months. She was released in October 1621, thanks in part to the extensive legal defense drawn up by Kepler. The accusers had no stronger evidence than rumors. Katharina was subjected to "territio verbalis", a graphic description of the torture awaiting her as a witch, in a final attempt to make her confess. Throughout the trial, Kepler postponed his other work to focus on his "harmonic theory". The result, published in 1619, was "Harmonices Mundi" ("Harmony of the World").
"Harmonices Mundi".
Kepler was convinced "that the geometrical things have provided the Creator with the model for decorating the whole world". In "Harmony", he attempted to explain the proportions of the natural world—particularly the astronomical and astrological aspects—in terms of music. The central set of "harmonies" was the "musica universalis" or "music of the spheres", which had been studied by Pythagoras, Ptolemy and many others before Kepler; in fact, soon after publishing "Harmonices Mundi", Kepler was embroiled in a priority dispute with Robert Fludd, who had recently published his own harmonic theory.
Kepler began by exploring regular polygons and regular solids, including the figures that would come to be known as Kepler's solids. From there, he extended his harmonic analysis to music, meteorology, and astrology; harmony resulted from the tones made by the souls of heavenly bodies—and in the case of astrology, the interaction between those tones and human souls. In the final portion of the work (Book V), Kepler dealt with planetary motions, especially relationships between orbital velocity and orbital distance from the Sun. Similar relationships had been used by other astronomers, but Kepler—with Tycho's data and his own astronomical theories—treated them much more precisely and attached new physical significance to them.
Among many other harmonies, Kepler articulated what came to be known as the third law of planetary motion. He then tried many combinations until he discovered that (approximately) ""The square of the periodic times are to each other as the cubes of the mean distances"." Although he gives the date of this epiphany (March 8, 1618), he does not give any details about how he arrived at this conclusion. However, the wider significance for planetary dynamics of this purely kinematical law was not realized until the 1660s. When conjoined with Christiaan Huygens' newly discovered law of centrifugal force, it enabled Isaac Newton, Edmund Halley, and perhaps Christopher Wren and Robert Hooke to demonstrate independently that the presumed gravitational attraction between the Sun and its planets decreased with the square of the distance between them. This refuted the traditional assumption of scholastic physics that the power of gravitational attraction remained constant with distance whenever it applied between two bodies, such as was assumed by Kepler and also by Galileo in his mistaken universal law that gravitational fall is uniformly accelerated, and also by Galileo's student Borrelli in his 1666 celestial mechanics. William Gilbert, after experimenting with magnets, decided that the center of the Earth was a huge magnet. His theory led Kepler to think that a magnetic force from the Sun drove planets in their own orbits. This was an interesting explanation for planetary motion, but unfortunately for Kepler, it was wrong. Before scientists could find the right answer, they needed to know more about motion.
"Rudolphine Tables" and his last years.
In 1623, Kepler at last completed the "Rudolphine Tables", which at the time was considered his major work. However, due to the publishing requirements of the emperor and negotiations with Tycho Brahe's heir, it would not be printed until 1627. In the meantime, religious tension — the root of the ongoing Thirty Years' War — once again put Kepler and his family in jeopardy. In 1625, agents of the Catholic Counter-Reformation placed most of Kepler's library under seal, and in 1626 the city of Linz was besieged. Kepler moved to Ulm, where he arranged for the printing of the "Tables" at his own expense.
In 1628, following the military successes of the Emperor Ferdinand's armies under General Wallenstein, Kepler became an official advisor to Wallenstein. Though not the general's court astrologer per se, Kepler provided astronomical calculations for Wallenstein's astrologers and occasionally wrote horoscopes himself. In his final years, Kepler spent much of his time traveling, from the imperial court in Prague to Linz and Ulm to a temporary home in Sagan, and finally to Regensburg. Soon after arriving in Regensburg, Kepler fell ill. He died on November 15, 1630, and was buried there; his burial site was lost after the Swedish army destroyed the churchyard. Only Kepler's self-authored poetic epitaph survived the times:
Reception of his astronomy.
Kepler's laws were not immediately accepted. Several major figures such as Galileo and René Descartes completely ignored Kepler's "Astronomia nova." Many astronomers, including Kepler's teacher, Michael Maestlin, objected to Kepler's introduction of physics into his astronomy. Some adopted compromise positions. Ismael Boulliau accepted elliptical orbits but replaced Kepler's area law with uniform motion in respect to the empty focus of the ellipse, while Seth Ward used an elliptical orbit with motions defined by an equant.
Several astronomers tested Kepler's theory, and its various modifications, against astronomical observations. Two transits of Venus and Mercury across the face of the sun provided sensitive tests of the theory, under circumstances when these planets could not normally be observed. In the case of the transit of Mercury in 1631, Kepler had been extremely uncertain of the parameters for Mercury, and advised observers to look for the transit the day before and after the predicted date. Pierre Gassendi observed the transit on the date predicted, a confirmation of Kepler's prediction. This was the first observation of a transit of Mercury. However, his attempt to observe the transit of Venus just one month later was unsuccessful due to inaccuracies in the Rudolphine Tables. Gassendi did not realize that it was not visible from most of Europe, including Paris. Jeremiah Horrocks, who observed the 1639 Venus transit, had used his own observations to adjust the parameters of the Keplerian model, predicted the transit, and then built apparatus to observe the transit. He remained a firm advocate of the Keplerian model.
"Epitome of Copernican Astronomy" was read by astronomers throughout Europe, and following Kepler's death it was the main vehicle for spreading Kepler's ideas. Between 1630 and 1650, it was the most widely used astronomy textbook, winning many converts to ellipse-based astronomy. However, few adopted his ideas on the physical basis for celestial motions. In the late 17th century, a number of physical astronomy theories drawing from Kepler's work—notably those of Giovanni Alfonso Borelli and Robert Hooke—began to incorporate attractive forces (though not the quasi-spiritual motive species postulated by Kepler) and the Cartesian concept of inertia. This culminated in Isaac Newton's "Principia Mathematica" (1687), in which Newton derived Kepler's laws of planetary motion from a force-based theory of universal gravitation.
Historical and cultural legacy.
Beyond his role in the historical development of astronomy and natural philosophy, Kepler has loomed large in the philosophy and historiography of science. Kepler and his laws of motion were central to early histories of astronomy such as Jean Etienne Montucla’s 1758 "Histoire des mathématiques" and Jean-Baptiste Delambre's 1821 "Histoire de l’astronomie moderne". These and other histories written from an Enlightenment perspective treated Kepler's metaphysical and religious arguments with skepticism and disapproval, but later Romantic-era natural philosophers viewed these elements as central to his success. William Whewell, in his influential "History of the Inductive Sciences" of 1837, found Kepler to be the archetype of the inductive scientific genius; in his "Philosophy of the Inductive Sciences" of 1840, Whewell held Kepler up as the embodiment of the most advanced forms of scientific method. Similarly, Ernst Friedrich Apelt—the first to extensively study Kepler's manuscripts, after their purchase by Catherine the Great—identified Kepler as a key to the "Revolution of the sciences". Apelt, who saw Kepler's mathematics, aesthetic sensibility, physical ideas, and theology as part of a unified system of thought, produced the first extended analysis of Kepler's life and work.
Modern translations of a number of Kepler's books appeared in the late-nineteenth and early-twentieth centuries, the systematic publication of his collected works began in 1937 (and is nearing completion in the early 21st century), and Max Caspar's Kepler biography was published in 1948. However, Alexandre Koyré's work on Kepler was, after Apelt, the first major milestone in historical interpretations of Kepler's cosmology and its influence. In the 1930s and 1940s, Koyré, and a number of others in the first generation of professional historians of science, described the "Scientific Revolution" as the central event in the history of science, and Kepler as a (perhaps the) central figure in the revolution. Koyré placed Kepler's theorization, rather than his empirical work, at the center of the intellectual transformation from ancient to modern world-views. Since the 1960s, the volume of historical Kepler scholarship has expanded greatly, including studies of his astrology and meteorology, his geometrical methods, the role of his religious views in his work, his literary and rhetorical methods, his interaction with the broader cultural and philosophical currents of his time, and even his role as an historian of science.
The debate over Kepler's place in the Scientific Revolution has also produced a wide variety of philosophical and popular treatments. One of the most influential is Arthur Koestler's 1959 "The Sleepwalkers", in which Kepler is unambiguously the hero (morally and theologically as well as intellectually) of the revolution. Philosophers of science—such as Charles Sanders Peirce, Norwood Russell Hanson, Stephen Toulmin, and Karl Popper—have repeatedly turned to Kepler: examples of incommensurability, analogical reasoning, falsification, and many other philosophical concepts have been found in Kepler's work. Physicist Wolfgang Pauli even used Kepler's priority dispute with Robert Fludd to explore the implications of analytical psychology on scientific investigation. A well-received, if fanciful, historical novel by John Banville, "Kepler" (1981), explored many of the themes developed in Koestler's non-fiction narrative and in the philosophy of science. Somewhat more fanciful is a recent work of nonfiction, "Heavenly Intrigue" (2004), suggesting that Kepler murdered Tycho Brahe to gain access to his data. Kepler has acquired a popular image as an icon of scientific modernity and a man before his time; science popularizer Carl Sagan described him as "the first astrophysicist and the last scientific astrologer".
The German composer Paul Hindemith wrote an opera about Kepler entitled "Die Harmonie der Welt", and a symphony of the same name was derived from music for the opera.
In Austria, Kepler left behind such a historical legacy that he was one of the motifs of a silver collector's coin: the 10-euro Johannes Kepler silver coin, minted on September 10, 2002. The reverse side of the coin has a portrait of Kepler, who spent some time teaching in Graz and the surrounding areas. Kepler was acquainted with Prince Hans Ulrich von Eggenberg personally, and he probably influenced the construction of Eggenberg Castle (the motif of the obverse of the coin). In front of him on the coin is the model of nested spheres and polyhedra from "Mysterium Cosmographicum".
In 2009, NASA named the Kepler Mission for Kepler's contributions to the field of astronomy.
In New Zealand's Fiordland National Park there is also a range of mountains named after Kepler, called the Kepler Mountains, and a Three Day Walking Trail known as the Kepler Track through the mountains of the same name.
Veneration.
Kepler is honored together with Nicolaus Copernicus with a feast day on the liturgical calendar of the Episcopal Church (USA) on May 23.
External links.
From the at the Library of Congress:

</doc>
<doc id="15739" url="http://en.wikipedia.org/wiki?curid=15739" title="Jewellery">
Jewellery

Jewellery or jewelry () consists of small decorative items worn for personal adornment, such as brooches, rings, necklaces, earrings, and bracelets. Jewellery may be attached to the body or the clothes, and the term is restricted to durable ornaments, excluding flowers for example. For many centuries metal, often combined with gemstones, has been the normal material for jewellery, but other materials such as shells and other plant materials may be used. It is one of the oldest type of archaeological artefact – with 100,000-year-old beads made from "Nassarius" shells thought to be the oldest known jewellery. The basic forms of jewellery vary between cultures but are often extremely long-lived; in European cultures the most common forms of jewellery listed above have persisted since ancient times, while other forms such as adornments for the nose or ankle, important in other cultures, are much less common. Historically, the most widespread influence on jewellery in terms of design and style have come from Asia.
Jewellery may be made from a wide range of materials. Gemstones and similar materials such as amber and coral, precious metals, beads, and shells have been widely used, and enamel has often been important. In most cultures jewellery can be understood as a status symbol, for its material properties, its patterns, or for meaningful symbols. Jewellery has been made to adorn nearly every body part, from hairpins to toe rings, and even genital jewellery. The patterns of wearing jewellery between the sexes, and by children and older people can vary greatly between cultures, but adult women have been the most consistent wearers of jewellery; in modern European culture the amount worn by adult males is relatively low compared with other cultures and other periods in European culture.
The word "jewellery" itself is derived from the word "jewel", which was anglicized from the Old French ""jouel", and beyond that, to the Latin word "jocale"", meaning plaything. In British English, New Zealand English, Hiberno-English, Australian English, and South African English it is spelled "jewellery," while the spelling is "jewelry" in American English. Both are used in Canadian English.
Form and function.
Humans have used jewellery for a number of different reasons:
Most cultures at some point have had a practice of keeping large amounts of wealth stored in the form of jewellery. Numerous cultures store wedding dowries in the form of jewellery or make jewellery as a means to store or display coins. Alternatively, jewellery has been used as a currency or trade good; an example being the use of slave beads.
Many items of jewellery, such as brooches and buckles, originated as purely functional items, but evolved into decorative items as their functional requirement diminished.
Jewellery can also symbolise group membership (as in the case, of the Christian crucifix or the Jewish Star of David) or status (as in the case of chains of office, or the Western practice of married people wearing wedding rings).
Wearing of amulets and devotional medals to provide protection or ward off evil is common in some cultures. These may take the form of symbols (such as the ankh), stones, plants, animals, body parts (such as the Khamsa), or glyphs (such as stylised versions of the Throne Verse in Islamic art).
Materials and methods.
In creating jewellery, gemstones, coins, or other precious items are often used, and they are typically set into precious metals. Alloys of nearly every metal known have been encountered in jewellery. Bronze, for example, was common in Roman times. Modern fine jewellery usually includes gold, white gold, platinum, palladium, titanium, or silver. Most contemporary gold jewellery is made of an alloy of gold, the purity of which is stated in karats, indicated by a number followed by the letter "K". American gold jewellery must be of at least 10K purity (41.7% pure gold), (though in the UK the number is 9K (37.5% pure gold) and is typically found up to 18K (75% pure gold). Higher purity levels are less common with alloys at 22 K (91.6% pure gold), and 24 K (99.9% pure gold) being considered too soft for jewellery use in America and Europe. These high purity alloys, however, are widely used across Asia, the Middle East and Africa. Platinum alloys range from 900 (90% pure) to 950 (95.0% pure). The silver used in jewellery is usually sterling silver, or 92.5% fine silver. In costume jewellery, stainless steel findings are sometimes used.
Other commonly used materials include glass, such as fused-glass or enamel; wood, often carved or turned; shells and other natural animal substances such as bone and ivory; natural clay; polymer clay; Hemp and other twines have been used as well to create jewellery that has more of a natural feel. However, any inclusion of lead or lead solder will give an English Assay office (the building which gives English jewellery its stamp of approval, the Hallmark) the right to destroy the piece, however it is very rare for the assay office to do so.
Beads are frequently used in jewellery. These may be made of glass, gemstones, metal, wood, shells, clay and polymer clay. Beaded jewellery commonly encompasses necklaces, bracelets, earrings, belts and rings. Beads may be large or small; the smallest type of beads used are known as seed beads, these are the beads used for the "woven" style of beaded jewellery. Another use of seed beads is an embroidery technique where seed beads are sewn onto fabric backings to create broad collar neck pieces and beaded bracelets. Bead embroidery, a popular type of handwork during the Victorian era, is enjoying a renaissance in modern jewellery making. Beading, or beadwork, is also very popular in many African and indigenous North American cultures.
Silversmiths, goldsmiths, and lapidaries methods include forging, casting, soldering or welding, cutting, carving and "cold-joining" (using adhesives, staples and rivets to assemble parts).
Diamonds.
Diamonds were first mined in India. Pliny may have mentioned them, although there is some debate as to the exact nature of the stone he referred to as "Adamas"; In 2005, Australia, Botswana, Russia and Canada ranked among the primary sources of gemstone diamond production.
The British crown jewels contain the Cullinan Diamond, part of the largest gem-quality rough diamond ever found (1905), at 3,106.75 carats (621.35 g).
Now popular in engagement rings, this usage dates back to the marriage of Maximilian I to Mary of Burgundy in 1477.
Other gemstones.
Many precious and semiprecious stones are used for jewellery. Among them are:
Some gemstones (like pearls, coral, and amber) are classified as organic, meaning that they are produced by living organisms. Others are inorganic, meaning that they are generally composed of and arise from minerals.
Some gems, for example, amethyst, have become less valued as methods of extracting and importing them have progressed. Some man-made gems can serve in place of natural gems, such as cubic zirconia, which can be used in place of diamond.
Metal finishes.
For platinum, gold, and silver jewellery, there are many techniques to create finishes. The most common are high-polish, satin/matte, brushed, and hammered. High-polished jewellery is the most common and gives the metal a highly reflective, shiny look. Satin, or matte finish reduces the shine and reflection of the jewellery, and this is commonly used to accentuate gemstones such as diamonds. Brushed finishes give the jewellery a textured look and are created by brushing a material (similar to sandpaper) against the metal, leaving "brush strokes." Hammered finishes are typically created by using a rounded steel hammer and hammering the jewellery to give it a wavy texture.
Some jewellery is plated to give it a shiny, reflective look or to achieve a desired colour. Sterling silver jewellery may be plated with a thin layer of 0.999 fine silver (a process known as flashing) or may be plated with rhodium or gold. Base metal costume jewellery may also be plated with silver, gold, or rhodium for a more attractive finish.
Impact on society.
Jewellery has been used to denote status. In ancient Rome, only certain ranks could wear rings; later, sumptuary laws dictated who could wear what type of jewellery. This was also based on rank of the citizens of that time. Cultural dictates have also played a significant role. For example, the wearing of earrings by Western men was considered effeminate in the 19th century and early 20th century. More recently, the display of body jewellery, such as piercings, has become a mark of acceptance or seen as a badge of courage within some groups but is completely rejected in others. Likewise, hip hop culture has popularised the slang term bling-bling, which refers to ostentatious display of jewellery by men or women.
Conversely, the jewellery industry in the early 20th century launched a campaign to popularise wedding rings for men, which caught on, as well as engagement rings for men, which did not, going so far as to create a false history and claim that the practice had medieval roots. By the mid-1940s, 85% of weddings in the U.S. featured a double-ring ceremony, up from 15% in the 1920s. Religion has also played a role in societies influence. Islam, for instance, considers the wearing of gold by men as a social taboo, and many religions have edicts against excessive display. In Christianity, the New Testament gives injunctions against the wearing of gold, in the writings of the apostles Paul and Peter. In Revelation 17, "the great whore" or false religious system, is depicted as being "decked with gold and precious stones and pearls, having a golden cup in her hand." (Rev. 17:4) For Muslims it is considered haraam for a man to wear gold and women are restricted to ear jewellery.
History.
The history of jewellery is long and goes back many years, with many different uses among different cultures. It has endured for thousands of years and has provided various insights into how ancient cultures worked.
Prehistory.
The first signs of jewellery came from the people in Africa. Perforated beads suggesting shell jewellery made from sea snail shells have been found dating to 75,000 years ago at Blombos Cave. In Kenya, at Enkapune Ya Muto, beads made from perforated ostrich egg shells have been dated to more than 40,000 years ago. In Russia, a stone bracelet and marble ring are attributed to a similar age.
Later, the European early modern humans had crude necklaces and bracelets of bone, teeth, berries, and stone hung on pieces of string or animal sinew, or pieces of carved bone used to secure clothing together. In some cases, jewellery had shell or mother-of-pearl pieces. In southern Russia, carved bracelets made of mammoth tusk have been found. The Venus of Hohle Fels features a perforation at the top, showing that it was intended to be worn as a pendant.
Around seven-thousand years ago, the first sign of copper jewellery was seen. In October 2012 the Museum of Ancient History in Lower Austria revealed that they had found a grave of a female jewellery worker – forcing archaeologists to take a fresh look at prehistoric gender roles after it appeared to be that of a female fine metal worker – a profession that was previously thought to have been carried out exclusively by men.
Egypt.
The first signs of established jewellery making in Ancient Egypt was around 3,000–5,000 years ago. The Egyptians preferred the luxury, rarity, and workability of gold over other metals. In Predynastic Egypt jewellery soon began to symbolise power and religious power in the community. Although it was worn by wealthy Egyptians in life, it was also worn by them in death, with jewellery commonly placed among grave goods.
In conjunction with gold jewellery, Egyptians used coloured glass, along with semi-precious gems. The colour of the jewellery had significance. Green, for example, symbolised fertility. Lapis lazuli and silver had to be imported from beyond the country’s borders.
Egyptian designs were most common in Phoenician jewellery. Also, ancient Turkish designs found in Persian jewellery suggest that trade between the Middle East and Europe was not uncommon. Women wore elaborate gold and silver pieces that were used in ceremonies.
Europe and the Middle East.
Mesopotamia.
By approximately 5,000 years ago, jewellery-making had become a significant craft in the cities of Mesopotamia. The most significant archaeological evidence comes from the Royal Cemetery of Ur, where hundreds of burials dating 2900–2300 BC were unearthed; tombs such as that of Puabi contained a multitude of artefacts in gold, silver, and semi-precious stones, such as lapis lazuli crowns embellished with gold figurines, close-fitting collar necklaces, and jewel-headed pins. In Assyria, men and women both wore extensive amounts of jewellery, including amulets, ankle bracelets, heavy multi-strand necklaces, and cylinder seals.
Jewellery in Mesopotamia tended to be manufactured from thin metal leaf and was set with large numbers of brightly coloured stones (chiefly agate, lapis, carnelian, and jasper). Favoured shapes included leaves, spirals, cones, and bunches of grapes. Jewellers created works both for human use and for adorning statues and idols. They employed a wide variety of sophisticated metalworking techniques, such as cloisonné, engraving, fine granulation, and filigree.
Extensive and meticulously maintained records pertaining to the trade and manufacture of jewellery have also been unearthed throughout Mesopotamian archaeological sites. One record in the Mari royal archives, for example, gives the composition of various items of jewellery: 1 necklace of flat speckled chalcedony beads including: 34 flat speckled chalcedony bead, [and] 35 gold fluted beads, in groups of five.
1 necklace of flat speckled chalcedony beads including: 39 flat speckled chalcedony beads, [with] 41 fluted beads in a group that make up the hanging device.
1 necklace with rounded lapis lazuli beads including: 28 rounded lapis lazuli beads, [and] 29 fluted beads for its clasp.
Greece.
The Greeks started using gold and gems in jewellery in 1600 BC, although beads shaped as shells and animals were produced widely in earlier times. Around 1500 BC, the main techniques of working gold in Greece included casting, twisting bars, and making wire. Many of these sophisticated techniques were popular in the Mycenaean period, but unfortunately this skill was lost at the end of the Bronze Age. The forms and shapes of jewellery in ancient Greece such as the armring (13th century BC), brooch (10th century BC) and pins (7th century BC), have varied widely since the Bronze Age as well. Other forms of jewellery include wreaths, earrings, necklace and bracelets. A good example of the high quality that gold working techniques could achieve in Greece is the ‘Gold Olive Wreath’ (4th century BC), which is modeled on the type of wreath given as a prize for winners in athletic competitions like the Olympic Games. Jewellery dating from 600 to 475 BC is not well represented in the archaeological record, but after the Persian wars the quantity of jewellery again became more plentiful. One particularly popular type of design at this time was a bracelet decorated with snake and animal-heads Because these bracelets used considerably more metal, many examples were made from bronze. By 300 BC, the Greeks had mastered making coloured jewellery and using amethysts, pearl, and emeralds. Also, the first signs of cameos appeared, with the Greeks creating them from Indian Sardonyx, a striped brown pink and cream agate stone. Greek jewellery was often simpler than in other cultures, with simple designs and workmanship. However, as time progressed, the designs grew in complexity and different materials were soon used.
Jewellery in Greece was hardly worn and was mostly used for public appearances or on special occasions. It was frequently given as a gift and was predominantly worn by women to show their wealth, social status, and beauty. The jewellery was often supposed to give the wearer protection from the “Evil Eye” or endowed the owner with supernatural powers, while others had a religious symbolism. Older pieces of jewellery that have been found were dedicated to the Gods.
They worked two styles of pieces: cast pieces and pieces hammered out of sheet metal. Fewer pieces of cast jewellery have been recovered. It was made by casting the metal onto two stone or clay moulds. The two halves were then joined together, and wax, followed by molten metal, was placed in the centre. This technique had been practised since the late Bronze Age. The more common form of jewellery was the hammered sheet type. Sheets of metal would be hammered to thickness and then soldered together. The inside of the two sheets would be filled with wax or another liquid to preserve the metal work. Different techniques, such as using a stamp or engraving, were then used to create motifs on the jewellery. Jewels may then be added to hollows or glass poured into special cavities on the surface.
The Greeks took much of their designs from outer origins, such as Asia, when Alexander the Great conquered part of it. In earlier designs, other European influences can also be detected. When Roman rule came to Greece, no change in jewellery designs was detected. However, by 27 BC, Greek designs were heavily influenced by the Roman culture. That is not to say that indigenous design did not thrive. Numerous polychrome butterfly pendants on silver foxtail chains, dating from the 1st century, have been found near Olbia, with only one example ever found anywhere else.
Rome.
Although jewellery work was abundantly diverse in earlier times, especially among the barbarian tribes such as the Celts, when the Romans conquered most of Europe, jewellery was changed as smaller factions developed the Roman designs. The most common artefact of early Rome was the brooch, which was used to secure clothing together. The Romans used a diverse range of materials for their jewellery from their extensive resources across the continent. Although they used gold, they sometimes used bronze or bone, and in earlier times, glass beads & pearl. As early as 2,000 years ago, they imported Sri Lankan sapphires and Indian diamonds and used emeralds and amber in their jewellery. In Roman-ruled England, fossilised wood called jet from Northern England was often carved into pieces of jewellery. The early Italians worked in crude gold and created clasps, necklaces, earrings, and bracelets. They also produced larger pendants that could be filled with perfume.
Like the Greeks, often the purpose of Roman jewellery was to ward off the “Evil Eye” given by other people. Although women wore a vast array of jewellery, men often only wore a finger ring. Although they were expected to wear at least one ring, some Roman men wore a ring on every finger, while others wore none. Roman men and women wore rings with an engraved gem on it that was used with wax to seal documents, a practice that continued into medieval times when kings and noblemen used the same method. After the fall of the Roman Empire, the jewellery designs were absorbed by neighbouring countries and tribes.
Middle Ages.
Post-Roman Europe continued to develop jewellery making skills. The Celts and Merovingians in particular are noted for their jewellery, which in terms of quality matched or exceeded that of Byzantium. Clothing fasteners, amulets, and, to a lesser extent, signet rings, are the most common artefacts known to us. A particularly striking Celtic example is the Tara Brooch. The Torc was common throughout Europe as a symbol of status and power. By the 8th century, jewelled weaponry was common for men, while other jewellery (with the exception of signet rings) seemed to become the domain of women. Grave goods found in a 6th–7th century burial near Chalon-sur-Saône are illustrative. A young girl was buried with: 2 silver fibulae, a necklace (with coins), bracelet, gold earrings, a pair of hair-pins, comb, and buckle. The Celts specialised in continuous patterns and designs, while Merovingian designs are best known for stylised animal figures. They were not the only groups known for high quality work. Note the Visigoth work shown here, and the numerous decorative objects found at the Anglo-Saxon Ship burial at Sutton Hoo Suffolk, England are a particularly well-known example. On the continent, cloisonné and garnet were perhaps the quintessential method and gemstone of the period.
The Eastern successor of the Roman Empire, the Byzantine Empire, continued many of the methods of the Romans, though religious themes came to predominate. Unlike the Romans, the Franks, and the Celts, however, Byzantium used light-weight gold leaf rather than solid gold, and more emphasis was placed on stones and gems. As in the West, Byzantine jewellery was worn by wealthier females, with male jewellery apparently restricted to signet rings. Woman's jewellery had some peculiarities like kolts that decorated headband.
Like other contemporary cultures, jewellery was commonly buried with its owner.
Renaissance.
The Renaissance and exploration both had significant impacts on the development of jewellery in Europe. By the 17th century, increasing exploration and trade led to increased availability of a wide variety of gemstones as well as exposure to the art of other cultures. Whereas prior to this the working of gold and precious metal had been at the forefront of jewellery, this period saw increasing dominance of gemstones and their settings. An example of this is the Cheapside Hoard, the stock of a jeweller hidden in London during the Commonwealth period and not found again until 1912. It contained Colombian emerald, topaz, amazonite from Brazil, spinel, iolite, and chrysoberyl from Sri Lanka, ruby from India, Afghan lapis lazuli, Persian turquoise, Red Sea peridot, as well as Bohemian and Hungarian opal, garnet, and amethyst. Large stones were frequently set in box-bezels on enamelled rings. Notable among merchants of the period was Jean-Baptiste Tavernier, who brought the precursor stone of the Hope Diamond to France in the 1660s.
When Napoleon Bonaparte was crowned as Emperor of the French in 1804, he revived the style and grandeur of jewellery and fashion in France. Under Napoleon’s rule, jewellers introduced "parures", suites of matching jewellery, such as a diamond tiara, diamond earrings, diamond rings, a diamond brooch, and a diamond necklace. Both of Napoleon’s wives had beautiful sets such as these and wore them regularly. Another fashion trend resurrected by Napoleon was the cameo. Soon after his cameo decorated crown was seen, cameos were highly sought. The period also saw the early stages of costume jewellery, with fish scale covered glass beads in place of pearls or conch shell cameos instead of stone cameos. New terms were coined to differentiate the arts: jewellers who worked in cheaper materials were called "bijoutiers", while jewellers who worked with expensive materials were called "joailliers", a practice which continues to this day.
Romanticism.
Starting in the late 18th century, Romanticism had a profound impact on the development of western jewellery. Perhaps the most significant influences were the public’s fascination with the treasures being discovered through the birth of modern archaeology and a fascination with Medieval and Renaissance art. Changing social conditions and the onset of the Industrial Revolution also led to growth of a middle class that wanted and could afford jewellery. As a result, the use of industrial processes, cheaper alloys, and stone substitutes led to the development of paste or costume jewellery. Distinguished goldsmiths continued to flourish, however, as wealthier patrons sought to ensure that what they wore still stood apart from the jewellery of the masses, not only through use of precious metals and stones but also though superior artistic and technical work. One such artist was the French goldsmith Françoise Désire Froment Meurice. A category unique to this period and quite appropriate to the philosophy of romanticism was mourning jewellery. It originated in England, where Queen Victoria was often seen wearing jet jewellery after the death of Prince Albert, and it allowed the wearer to continue wearing jewellery while expressing a state of mourning at the death of a loved one.
In the United States, this period saw the founding in 1837 of Tiffany & Co. by Charles Lewis Tiffany. Tiffany's put the United States on the world map in terms of jewellery and gained fame creating dazzling commissions for people such as the wife of Abraham Lincoln. Later, it would gain popular notoriety as the setting of the film Breakfast at Tiffany's. In France, Pierre Cartier founded Cartier SA in 1847, while 1884 saw the founding of Bulgari in Italy. The modern production studio had been born and was a step away from the former dominance of individual craftsmen and patronage.
This period also saw the first major collaboration between East and West. Collaboration in Pforzheim between German and Japanese artists led to Shakudō plaques set into Filigree frames being created by the Stoeffler firm in 1885). Perhaps the grand finalé – and an appropriate transition to the following period – were the masterful creations of the Russian artist Peter Carl Fabergé, working for the Imperial Russian court, whose Fabergé eggs and jewellery pieces are still considered as the epitome of the goldsmith’s art.
Art Nouveau.
In the 1890s, jewellers began to explore the potential of the growing Art Nouveau style and the closely related German Jugendstil, British (and to some extent American) Arts and Crafts Movement, Catalan Modernisme, Austro-Hungarian Sezession, Italian "Liberty", etc.
Art Nouveau jewellery encompassed many distinct features including a focus on the female form and an emphasis on colour, most commonly rendered through the use of enamelling techniques including basse-taille, champleve, cloisonné, and plique-à-jour. Motifs included orchids, irises, pansies, vines, swans, peacocks, snakes, dragonflies, mythological creatures, and the female silhouette.
René Lalique, working for the Paris shop of Samuel Bing, was recognised by contemporaries as a leading figure in this trend. The Darmstadt Artists' Colony and Wiener Werkstätte provided perhaps the most significant input to the trend, while in Denmark Georg Jensen, though best known for his Silverware, also contributed significant pieces. In England, Liberty & Co. and the British arts & crafts movement of Charles Robert Ashbee contributed slightly more linear but still characteristic designs. The new style moved the focus of the jeweller's art from the setting of stones to the artistic design of the piece itself. Lalique's dragonfly design is one of the best examples of this. Enamels played a large role in technique, while sinuous organic lines are the most recognisable design feature.
The end of World War I once again changed public attitudes, and a more sober style developed.
Art Deco.
Growing political tensions, the after-effects of the war, and a reaction against the perceived decadence of the turn of the 20th century led to simpler forms, combined with more effective manufacturing for mass production of high-quality jewellery. Covering the period of the 1920s and 1930s, the style has become popularly known as Art Deco. Walter Gropius and the German Bauhaus movement, with their philosophy of "no barriers between artists and craftsmen" led to some interesting and stylistically simplified forms. Modern materials were also introduced: plastics and aluminium were first used in jewellery, and of note are the chromed pendants of Russian-born Bauhaus master Naum Slutzky. Technical mastery became as valued as the material itself. In the West, this period saw the reinvention of granulation by the German Elizabeth Treskow, although development of the re-invention has continued into the 1990s. It is based on the basic shapes.
Asia.
In Asia, the Indian subcontinent has the longest continuous legacy of jewellery making anywhere, with a history of over 5,000 years. One of the first to start jewellery making were the peoples of the Indus Valley Civilization, in what is now predominately modern-day Pakistan and part of northern and western India. Early jewellery making in China started around the same period, but it became widespread with the spread of Buddhism around 2,000 years ago.
China.
The Chinese used silver in their jewellery more than gold. Blue kingfisher feathers were tied onto early Chinese jewellery and later, blue gems and glass were incorporated into designs. However, jade was preferred over any other stone. The Chinese revered jade because of the human-like qualities they assigned to it, such as its hardness, durability, and beauty. The first jade pieces were very simple, but as time progressed, more complex designs evolved. Jade rings from between the 4th and 7th centuries BC show evidence of having been worked with a compound milling machine, hundreds of years before the first mention of such equipment in the west.
In China, the most uncommon piece of jewellery is the earring, which was worn neither by men nor women. Amulets were common, often with a Chinese symbol or dragon. Dragons, Chinese symbols, and phoenixes were frequently depicted on jewellery designs.
The Chinese often placed their jewellery in their graves. Most Chinese graves found by archaeologists contain decorative jewellery.
Indian subcontinent.
The Indian subcontinent (encompassing India, Pakistan and other countries of South Asia) has a long jewellery history, which went through various changes through cultural influence and politics for more than 5,000–8,000 years. Because India had an abundant supply of precious metals and gems, it prospered financially through export and exchange with other countries. While European traditions were heavily influenced by waxing and waning empires, India enjoyed a continuous development of art forms for some 5,000 years. One of the first to start jewellery making were the peoples of the Indus Valley Civilization (encompassing present-day Pakistan and north and northwest India). By 1500 BC, the peoples of the Indus Valley were creating gold earrings and necklaces, bead necklaces, and metallic bangles. Before 2100 BC, prior to the period when metals were widely used, the largest jewellery trade in the Indus Valley region was the bead trade. Beads in the Indus Valley were made using simple techniques. First, a bead maker would need a rough stone, which would be bought from an eastern stone trader. The stone would then be placed into a hot oven where it would be heated until it turned deep red, a colour highly prized by people of the Indus Valley. The red stone would then be chipped to the right size and a hole bored through it with primitive drills. The beads were then polished. Some beads were also painted with designs. This art form was often passed down through the family. Children of bead makers often learned how to work beads from a young age. Persian style also played a big role in India’s jewellery. Each stone had its own characteristics related to Hinduism.
Jewellery in the Indus Valley was worn predominantly by females, who wore numerous clay or shell bracelets on their wrists. They were often shaped like doughnuts and painted black. Over time, clay bangles were discarded for more durable ones. In present-day India, bangles are made out of metal or glass. Other pieces that women frequently wore were thin bands of gold that would be worn on the forehead, earrings, primitive brooches, chokers, and gold rings. Although women wore jewellery the most, some men in the Indus Valley wore beads. Small beads were often crafted to be placed in men and women’s hair. The beads were about one millimetre long.
A female skeleton (presently on display at the National Museum, New Delhi, India) wears a carlinean bangle (bracelet) on her left hand. "Kada" is a special kind of bracelet and is widely popular in Indian culture. They symbolizes animals like peacock, elephant, etc.
According to Hindu belief, gold and silver are considered as sacred metals. Gold is symbolic of the warm sun, while silver suggests the cool moon. Both are the quintessential metals of Indian jewellery. Pure gold does not oxidise or corrode with time, which is why Hindu tradition associates gold with immortality. Gold imagery occurs frequently in ancient Indian literature. In the Vedic Hindu belief of cosmological creation, the source of physical and spiritual human life originated in and evolved from a golden womb (hiranyagarbha) or egg (hiranyanda), a metaphor of the sun, whose light rises from the primordial waters. 
Jewellery had great status with India’s royalty; it was so powerful that they established laws, limiting wearing of jewellery to royalty. Only royalty and a few others to whom they granted permission could wear gold ornaments on their feet. This would normally be considered breaking the appreciation of the sacred metals. Even though the majority of the Indian population wore jewellery, Maharajas and people related to royalty had a deeper connection with jewellery. The Maharaja's role was so important that the Hindu philosophers identified him as central to the smooth working of the world. He was considered as a divine being, a deity in human form, whose duty was to uphold and protect dharma, the moral order of the universe.
Navaratna (nine gems)is a powerful jewel frequently worn by a Maharaja (Emperor). It is an amulet, which comprises diamond, pearl, ruby, sapphire, emerald, topaz, cat’s eye, coral, and hyacinth (red zircon). Each of these stones is associated with a celestial deity, represented the totality of the Hindu universe when all nine gems are together. The diamond is the most powerful gem among the nine stones. There were various cuts for the gemstone. Indian Kings bought gemstones privately from the sellers. Maharaja and other royal family members value gem as Hindu God. They exchanged gems with people to whom they were very close, especially the royal family members and other intimate allies. “Only the emperor himself, his intimate relations, and select members of his entourage were permitted to wear royal turban ornament. As the empire matured, differing styles of ornament acquired the generic name of sarpech, from sar or sir, meaning head, and pech, meaning fastener.”
India was the first country to mine diamonds, with some mines dating back to 296 BC. India traded the diamonds, realising their valuable qualities. Historically, diamonds have been given to retain or regain a lover’s or ruler’s lost favour, as symbols of tribute, or as an expression of fidelity in exchange for concessions and protection. Mughal emperors and Kings used the diamonds as a means of assuring their immortality by having their names and wordly titles inscribed upon them. Moreover, it has played and continues to play a pivotal role in Indian social, political, economic, and religious event, as it often has done elsewhere. In Indian history, diamonds have been used to acquire military equipment, finance wars, foment revolutions, and tempt defections. They have contributed to the abdication or the decapitation of potentates. They have been used to murder a representative of the dominating power by lacing his food with crushed diamond. Indian diamonds have been used as security to finance large loans needed to buttress politically or economically tottering regimes. Victorious military heroes have been honoured by rewards of diamonds and also have been used as ransom payment for release from imprisonment or abduction. 
Today, many of the jewellery designs and traditions are used, and jewellery is commonplace in Indian ceremonies and weddings.
North and South America.
Jewellery played a major role in the fate of the Americas when the Spanish established an empire to seize South American gold. Jewellery making developed in the Americas 5,000 years ago in Central and South America. Large amounts of gold was easily accessible, and the Aztecs, Mixtecs, Mayans, and numerous Andean cultures, such as the Mochica of Peru, created beautiful pieces of jewellery.
With the Mochica culture, goldwork flourished. The pieces are no longer simple metalwork, but are now masterful examples of jewellery making. Pieces are sophisticated in their design, and feature inlays of turquoise, mother of pearl, spondylus shell, and amethyst. The nose and ear ornaments, chest plates, small containers and whistles are considered masterpieces of ancient Peruvian culture.
Among the Aztecs, only nobility wore gold jewellery, as it showed their rank, power, and wealth. Gold jewellery was most common in the Aztec Empire and was often decorated with feathers from Quetzal birds and others. In general, the more jewellery an Aztec noble wore, the higher his status or prestige. The Emperor and his High Priests, for example, would be nearly completely covered in jewellery when making public appearances. Although gold was the most common and a popular material used in Aztec jewellery, jade, turquoise, and certain feathers were considered more valuable. In addition to adornment and status, the Aztecs also used jewellery in sacrifices to appease the gods. Priests also used gem-encrusted daggers to perform animal and human sacrifices.
Another ancient American civilization with expertise in jewellery making were the Maya. At the peak of their civilization, the Maya were making jewellery from jade, gold, silver, bronze, and copper. Maya designs were similar to those of the Aztecs, with lavish headdresses and jewellery. The Maya also traded in precious gems. However, in earlier times, the Maya had little access to metal, so they made the majority of their jewellery out of bone or stone. Merchants and nobility were the only few that wore expensive jewellery in the Maya region, much the same as with the Aztecs.
In North America, Native Americans used shells, wood, turquoise, and soapstone, almost unavailable in South and Central America. The turquoise was used in necklaces and to be placed in earrings. Native Americans with access to oyster shells, often located in only one location in America, traded the shells with other tribes, showing the great importance of the body adornment trade in Northern America.
Native American.
Native American jewellery is the personal adornment, often in the forms of necklaces, earrings, bracelets, rings, pins, brooches, labrets, and more, made by the Indigenous peoples of the United States. Native American jewellery reflects the cultural diversity and history of its makers. Native American tribes continue to develop distinct aesthetics rooted in their personal artistic visions and cultural traditions. Artists create jewellery for adornment, ceremonies, and trade. Lois Sherr Dubin writes, "[i]n the absence of written languages, adornment became an important element of Indian [Native American] communication, conveying many levels of information." Later, jewellery and personal adornment "...signaled resistance to assimilation. It remains a major statement of tribal and individual identity."
Metalsmiths, beaders, carvers, and lapidaries combine a variety of metals, hardwoods, precious and semi-precious gemstones, beadwork, quillwork, teeth, bones, hide, vegetal fibres, and other materials to create jewellery. Contemporary Native American jewellery ranges from hand-quarried and processed stones and shells to computer-fabricated steel and titanium jewellery.
Pacific.
Jewellery making in the Pacific started later than in other areas because of recent human settlement. Early Pacific jewellery was made of bone, wood, and other natural materials, and thus has not survived. Most Pacific jewellery is worn above the waist, with headdresses, necklaces, hair pins, and arm and waist belts being the most common pieces.
Jewellery in the Pacific, with the exception of Australia, is worn to be a symbol of either fertility or power. Elaborate headdresses are worn by many Pacific cultures and some, such as the inhabitants of Papua New Guinea, wear certain headdresses once they have killed an enemy. Tribesman may wear boar bones through their noses.
Island jewellery is still very much primal because of the lack of communication with outside cultures. Some areas of Borneo and Papua New Guinea are yet to be explored by Western nations. However, the island nations that were flooded with Western missionaries have had drastic changes made to their jewellery designs. Missionaries saw any type of tribal jewellery as a sign of the wearer's devotion to paganism. Thus many tribal designs were lost forever in the mass conversion to Christianity.
Australia is now the number one supplier of opals in the world. Opals had already been mined in Europe and South America for many years prior, but in the late 19th century, the Australian opal market became predominant. Australian opals are only mined in a few select places around the country, making it one of the most profitable stones in the Pacific.
The New Zealand Māori traditionally had a strong culture of personal adornment, most famously the hei-tiki. Hei-tikis are traditionally carved by hand from bone, nephrite, or bowenite.
Nowadays a wide range of such traditionally inspired items such as bone carved pendants based on traditional fishhooks "hei matau" and other greenstone jewellery are popular with young New Zealanders of all backgrounds – for whom they relate to a generalized sense of New Zealand identity. These trends have contributed towards a worldwide interest in traditional Māori culture and arts.
Other than jewellery created through Māori influence, modern jewellery in New Zealand is multicultural and varied.
Modern.
Most modern commercial jewellery continues traditional forms and styles, but designers such as Georg Jensen have widened the concept of wearable art. The advent of new materials, such as plastics, Precious Metal Clay (PMC), and colouring techniques, has led to increased variety in styles. Other advances, such as the development of improved pearl harvesting by people such as Mikimoto Kōkichi and the development of improved quality artificial gemstones such as moissanite (a diamond simulant), has placed jewellery within the economic grasp of a much larger segment of the population.
The "jewellery as art" movement was spearheaded by artisans such as Robert Lee Morris and continued by designers such as Gill Forsbrook in the UK. Influence from other cultural forms is also evident. One example of this is bling-bling style jewellery, popularised by hip-hop and rap artists in the early 21st century, e.g. grills, a type of jewellery worn over the teeth.
The late 20th century saw the blending of European design with oriental techniques such as Mokume-gane. The following are innovations in the decades straddling the year 2000: "Mokume-gane, hydraulic die forming, anti-clastic raising, fold-forming, reactive metal anodising, shell forms, PMC, photoetching, and [use of] CAD/CAM."
Artisan jewellery continues to grow as both a hobby and a profession. With more than 17 United States periodicals about beading alone, resources, accessibility, and a low initial cost of entry continues to expand production of hand-made adornments. Some fine examples of artisan jewellery can be seen at The Metropolitan Museum of Art in New York City.
The increase in numbers of students choosing to study jewellery design and production in Australia has grown in the past 20 years, and Australia now has a thriving contemporary jewellery community. Many of these jewellers have embraced modern materials and techniques, as well as incorporating traditional workmanship.
Masonic.
Freemasons attach jewels to their detachable collars when in Lodge to signify a Brothers Office held with the Lodge. For example, the square represents the Master of the Lodge and the dove represents the Deacon.
Body modification.
Jewellery used in body modification can be simple and plain or dramatic and extreme. The use of simple silver studs, rings, and earrings predominates. Common jewellery pieces such as, earrings are a form of body modification, as they are accommodated by creating a small hole in the ear.
Padaung women in Myanmar place large golden rings around their necks. From as early as five years old, girls are introduced to their first neck ring. Over the years, more rings are added. In addition to the twenty-plus pounds of rings on her neck, a woman will also wear just as many rings on her calves too. At their extent, some necks modified like this can reach 10 – long. The practice has obvious health impacts, however, and has in recent years declined from cultural norm to tourist curiosity. Tribes related to the Paduang, as well as other cultures throughout the world, use jewellery to stretch their earlobes or enlarge ear piercings. In the Americas, labrets have been worn since before first contact by Innu and First Nations peoples of the northwest coast. Lip plates are worn by the African Mursi and Sara people, as well as some South American peoples.
In the late 20th century, the influence of modern primitivism led to many of these practices being incorporated into western subcultures. Many of these practices rely on a combination of body modification and decorative objects, thus keeping the distinction between these two types of decoration blurred.
In many cultures, jewellery is used as a temporary body modifier, with, in some cases, hooks or even objects as large as bike bars being placed into the recipient's skin. Although this procedure is often carried out by tribal or semi-tribal groups, often acting under a trance during religious ceremonies, this practice has seeped into western culture. Many extreme-jewellery shops now cater to people wanting large hooks or spikes set into their skin. Most often, these hooks are used in conjunction with pulleys to hoist the recipient into the air. This practice is said to give an erotic feeling to the person and some couples have even performed their marriage ceremony whilst being suspended by hooks.
Jewellery market.
According to a 2007 KPMG study, the largest jewellery market is the United States with a market share of 30.8%, Japan, India, China, and the Middle East each with 8–9%, and Italy with 5%. The authors of the study predict a dramatic change in market shares by 2015, where the market share of the United States will have dropped to around 25%, and China and India will increase theirs to over 13%. The Middle East will remain more or less constant at 9%, whereas Europe's and Japan's marketshare will be halved and become less than 4% for Japan, and less than 3% for the biggest individual European countries, Italy and the UK.

</doc>
<doc id="15767" url="http://en.wikipedia.org/wiki?curid=15767" title="Jon Postel">
Jon Postel

Jonathan Bruce Postel (; August 6, 1943 – October 16, 1998) was an American computer scientist who made many significant contributions to the development of the Internet, particularly with respect to standards. He is known principally for being the Editor of the Request for Comment (RFC) document series, and for administering the Internet Assigned Numbers Authority (IANA) until his death. In his lifetime he was known as the God of the Internet for his comprehensive influence on the medium.
The Internet Society's Postel Award is named in his honor, as is the Postel Center at Information Sciences Institute , University of Southern California. His obituary was written by Vint Cerf and published as RFC 2468 in remembrance of Postel and his work. In 2012, Postel was inducted into the Internet Hall of Fame by the Internet Society.
Career.
Postel attended Van Nuys High School, and then UCLA where he earned his B.S. (1966) as well as his M.A. (1968) in Engineering. He then went on to complete his Ph.D. there in Computer Science in 1974, with Dave Farber as his thesis advisor.
While at UCLA, he was involved in early work on the ARPANET. He worked briefly at the MITRE Corporation, then helped set up the Network Information Center at SRI. In March 1977, he joined the Information Sciences Institute at the University of Southern California. Postel was the RFC Editor from 1969 until his death, and wrote and edited many important RFCs, including RFC 791, RFC 792 and RFC 793, which define the basic protocols of the Internet protocol suite, and RFC 2223, "Instructions to RFC Authors". He wrote or co-authored more than 200 RFCs.
Postel served on the Internet Architecture Board and its predecessors for many years. He was the Director of the names and number assignment clearinghouse, the Internet Assigned Numbers Authority (IANA), from its inception. He was the first member of the Internet Society, and was on its Board of Trustees. He was the original and long-time .us Top-Level Domain administrator. He also managed the Los Nettos Network.
All of the above were part-time activities he assumed in conjunction with his primary position as Director of the Computer Networks Division ("") of the Information Sciences Institute at the University of Southern California.
DNS Root Authority test, U.S. response.
On January 28, 1998, Postel, as a test, emailed eight of the twelve operators of Internet's regional root nameservers on his own authority and instructed them to change the root zone server from then SAIC subsidiary Network Solutions (NSI)'s A.ROOT-SERVERS.NET (198.41.0.4) to IANA's DNSROOT.IANA.ORG (198.32.1.98). The operators complied with Postel's instructions, thus dividing control of Internet naming between the non-government operators with IANA and the 4 remaining U.S. Government roots at NASA, DoD, and BRL with NSI. Though usage of the Internet was not interrupted, he soon received orders from senior government officials to undo this change, which he did. Within a week, the US NTIA issued "A proposal to improve technical management of Internet names and addresses", including changes to authority over the Internet DNS root zone, which ultimately, and controversially, increased U.S. control.
Legacy.
On October 16, 1998, Postel died of heart problems in Los Angeles, nine months after the DNS Root Authority incident.
The significance of Jon Postel's contributions to building the Internet, both technical and personal, were such that a memorial recollection of his life forms part of the core technical literature sequence of the Internet in the form of RFC 2468 "I Remember IANA", written by Vinton Cerf.
Postel's law.
Perhaps his most famous legacy is from RFC 760, which includes a robustness principle often called "Postel's law": "an implementation should be conservative in its sending behavior, and liberal in its receiving behavior" (reworded in RFC 1122 as "Be liberal in what you accept, and conservative in what you send").
In digital circuits, this principle has long been an important aspect of what is known as the static discipline.
Trivia.
 also means "email" in Albanian.

</doc>
<doc id="15786" url="http://en.wikipedia.org/wiki?curid=15786" title="July">
July

July ( ) is the seventh month of the year (between June and August) in the Julian and Gregorian Calendars and one of seven months with the length of 31 days. It was named by the Roman Senate in honor of the Roman general, Julius Caesar, it being the month of his birth. Prior to that, it was called Quintilis.
It is on average the warmest month in most of the Northern hemisphere (where it is the second month of summer) and the coldest month in much of the Southern hemisphere (where it is the second month of winter). The second half of the year commences in July. In the Southern hemisphere, July is the seasonal equivalent of January in the Northern hemisphere. 
July starts on the same day of the week as April in every year, and January in leap years. In a common year no other month ends on the same day as July, while in a leap year July ends on the same day of the week as January. October of the previous year starts on the same day of the week as July of the current year as a common year and May of the previous year starts on the same day of the week as July of the current year as a leap year. February and October of the previous year end on the same day of the week as July of the current year as a common year and May of the previous year ends on the same day of the week as July of the current year as a leap year. In years immediately before common years, July starts on the same day of the week as September and December of the following year and in years immediately before leap years, July starts on the same day of the week as June of the following year. In years immediately before common years, July ends on the same day of the week as April and December of the following year and in years immediately before leap years, July ends on the same day of the week as September of the following year. In common years immediately after other common years, July starts and finishes on the same day of the week as January of the previous year.
In the Northern Hemisphere:

</doc>
<doc id="15791" url="http://en.wikipedia.org/wiki?curid=15791" title="January 26">
January 26

January 26 is the day of the year in the Gregorian calendar.

</doc>
<doc id="15798" url="http://en.wikipedia.org/wiki?curid=15798" title="June 17">
June 17

June 17 is the day of the year in the Gregorian calendar.

</doc>
<doc id="15806" url="http://en.wikipedia.org/wiki?curid=15806" title="June 12">
June 12

June 12 is the day of the year in the Gregorian calendar.

</doc>
<doc id="15817" url="http://en.wikipedia.org/wiki?curid=15817" title="June 19">
June 19

June 19 is the day of the year in the Gregorian calendar.

</doc>
<doc id="15830" url="http://en.wikipedia.org/wiki?curid=15830" title="John Lee Hooker">
John Lee Hooker

John Lee Hooker (August 22, 1917 – June 21, 2001) was an American blues singer, songwriter and guitarist. He was born in Mississippi, the son of a sharecropper, and rose to prominence performing an electric guitar-style adaptation of Delta blues. Hooker often incorporated other elements, including talking blues and early North Mississippi Hill country blues. He developed his own driving-rhythm boogie style, distinct from the 1930s–1940s piano-derived boogie-woogie style. Some of his best known songs include "Boogie Chillen'" (1948), "Crawling King Snake" (1949), "Dimples" (1956), "Boom Boom" (1962), and "One Bourbon, One Scotch, One Beer" (1966) – the first being the most popular race record of 1949.
Early life.
There is some debate as to the year of Hooker's birth in Coahoma County, Mississippi, the youngest of the eleven children of William Hooker (1871–1923), a sharecropper and Baptist preacher, and Minnie Ramsey (born 1875, date of death unknown); according to his official website, he was born on August 22, 1917.
Hooker and his siblings were home-schooled. They were permitted to listen only to religious songs, with his earliest exposure being the spirituals sung in church. In 1921, his parents separated. The next year, his mother married William Moore, a blues singer who provided Hooker with his first introduction to the guitar (and whom John would later credit for his distinctive playing style). 
John's stepfather was his first outstanding blues influence. William Moore was a local blues guitarist who learned in Shreveport, Louisiana to play a droning, one-chord blues that was strikingly different from the Delta blues of the time. Around 1923 his biological father died. At the age of 14, John Lee Hooker ran away from home, reportedly never seeing his mother or stepfather again.
Throughout the 1930s, Hooker lived in Memphis, Tennessee where he worked on Beale Street at The New Daisy Theatre and occasionally performed at house parties. He worked in factories in various cities during World War II, drifting until he found himself in Detroit in 1948 working at Ford Motor Company. He felt right at home near the blues venues and saloons on Hastings Street, the heart of black entertainment on Detroit's east side. In a city noted for its pianists, guitar players were scarce. Performing in Detroit clubs, his popularity grew quickly and, seeking a louder instrument than his acoustic guitar, he bought his first electric guitar.
Career.
Hooker's recording career began in 1948 when his agent placed a demo, made by Hooker, with the Bihari brothers, owners of the Modern Records label. The company initially released an up-tempo number, "Boogie Chillen'", which became Hooker's first hit single. Though they were not songwriters, the Biharis often purchased or claimed co-authorship of songs that appeared on their labels, thus securing songwriting royalties for themselves, in addition to their own streams of income.
Sometimes these songs were older tunes that Hooker renamed, as with B.B. King's "Rock Me Baby", anonymous jams "B.B.'s Boogie", or songs by employees (bandleader Vince Weaver). The Biharis used a number of pseudonyms for songwriting credits: Jules was credited as "Jules Taub"; Joe as "Joe Josea"; and Sam as "Sam Ling". One song by John Lee Hooker, "Down Child", is solely credited to "Taub", with Hooker receiving no credit. Another, "Turn Over a New Leaf" is credited to Hooker and Ling.
In 1949, Hooker was recorded performing in an informal setting for Detroit jazz enthusiasts. His repertoire included down-home and spiritual tunes that he would not record commercially.
The recorded set has been made available in the album "Jack O'Diamonds".
Despite being illiterate, Hooker was a prolific lyricist. In addition to adapting the occasionally traditional blues lyric (such as "if I was chief of police, I would run her right out of town..."), he freely invented many songs from scratch. Recording studios in the 1950s rarely paid black musicians more than a pittance, so Hooker would spend the night wandering from studio to studio, coming up with new songs or variations on his songs for each studio. Because of his recording contract, he would record these songs under obvious pseudonyms such as John Lee Booker, notably for Chess Records and Chance Records in 1951/52, as Johnny Lee for De Luxe Records in 1953/54 as John Lee, and even John Lee Cooker, or as Texas Slim, Delta John, Birmingham Sam and his Magic Guitar, Johnny Williams, or The Boogie Man.
His early solo songs were recorded under Bernie Besman. John Lee Hooker rarely played on a standard beat, changing tempo to fit the needs of the song. This often made it difficult to use backing musicians who were not accustomed to Hooker's musical vagaries. As a result, Besman would record Hooker, in addition to playing guitar and singing, stomping along with the music on a wooden pallet. For much of this time period he recorded and toured with Eddie Kirkland, who was still performing until his death in a car accident in 2011. Later sessions for the VeeJay label in Chicago used studio musicians on most of his recordings, including Eddie Taylor, who could handle his musical idiosyncrasies very well. His biggest UK hit, "Boom Boom", (originally released on VeeJay) was recorded with a horn section.
Later life.
He appeared and sang in the 1980 movie "The Blues Brothers". Due to Hooker's improvisational style, his performance was filmed and sound-recorded live at the scene at Chicago's Maxwell Street Market, in contrast to the usual "playback" technique used in most film musicals. Hooker was also a direct influence in the look of John Belushi's character Jake Blues.
In 1989, he joined with a number of musicians, including Carlos Santana and Bonnie Raitt to record the album "The Healer", for which he and Santana won a Grammy Award. Hooker recorded several songs with Van Morrison, including "Never Get Out of These Blues Alive", "The Healing Game", and "I Cover the Waterfront". He also appeared on stage with Van Morrison several times, some of which was released on the live album "A Night in San Francisco". The same year he appeared as the title character on Pete Townshend's "".
On December 19, 1989, Hooker appeared with The Rolling Stones and Eric Clapton to perform "Boogie Chillen'"in Atlantic City, N.J., as part of The Rolling Stones "Steel Wheels" tour. The show was broadcast live on cable television on a pay-per-view basis.
Hooker recorded over 100 albums. He lived the last years of his life in Long Beach, California. In 1997, he opened a nightclub in San Francisco's Fillmore District called "John Lee Hooker's Boom Boom Room", after one of his hits.
Death.
Hooker fell ill just before a tour of Europe in 2001 and died in his sleep on June 21 at the age of 83, two months before his 84th birthday. He was interred at the Chapel of the Chimes in Oakland, California.
His last live in the studio recording on guitar and vocal was of a song he wrote with Pete Sears called "Elizebeth", featuring members of his Coast to Coast Blues Band with Sears on piano. It was recorded on January 14, 1998 at Bayview Studios in Richmond, California. The last song Hooker recorded before his death was "Ali D'Oro", a collaboration with the Italian soul singer Zucchero, in which Hooker sang the chorus "I lay down with an angel." He is survived by eight children, nineteen grandchildren, eighteen great-grandchildren, a nephew, and fiance Sidora Dazi. He has two children that followed in his footsteps, Zakiya Hooker and John Lee Hooker, Jr.
Among his many awards, Hooker has a star on the Hollywood Walk of Fame and in 1991 he was inducted into the Rock and Roll Hall of Fame. Two of his songs, "Boogie Chillen" and "Boom Boom" were included in the list of The Rock and Roll Hall of Fame's 500 Songs that Shaped Rock and Roll. "Boogie Chillen" was included as one of the Songs of the Century. He was also inducted in 1980 into the Blues Hall of Fame. In 2000, Hooker was awarded the Grammy Lifetime Achievement Award.
Music and legacy.
Hooker's guitar playing is closely aligned with piano boogie-woogie. He would play the walking bass pattern with his thumb, stopping to emphasize the end of a line with a series of trills, done by rapid hammer-ons and pull-offs. The songs that most epitomize his early sound are "Boogie Chillen", about being 17 and wanting to go out to dance at the Boogie clubs, "Baby, Please Don't Go", a blues standard first recorded by Big Joe Williams, and "Tupelo Blues", a song about the flooding of Tupelo, Mississippi, in April 1936.
He maintained a solo career, popular with blues and folk music fans of the early 1960s and crossed over to white audiences, giving an early opportunity to the young Bob Dylan. As he got older, he added more and more people to his band, changing his live show from simply Hooker with his guitar to a large band, with Hooker singing.
His vocal phrasing was less closely tied to specific bars than most blues singers. This casual, rambling style had been gradually diminishing with the onset of electric blues bands from Chicago but, even when not playing solo, Hooker retained it in his sound.
Though Hooker lived in Detroit during most of his career, he is not associated with the Chicago-style blues prevalent in large northern cities, as much as he is with the southern rural blues styles, known as delta blues, country blues, folk blues, or front porch blues. His use of an electric guitar tied together the Delta blues with the emerging post-war electric blues.
His songs have been covered by Buddy Guy, Cream, AC/DC, ZZ Top, Led Zeppelin, Tom Jones, Bruce Springsteen, Jimi Hendrix, Eric Clapton, Nick Cave & The Bad Seeds, Van Morrison, The Yardbirds, The Animals, The Doors, The White Stripes, MC5, George Thorogood, R. L. Burnside, The J. Geils Band, Big Head Todd and the Monsters, The Gories, Cat Power, and The Jon Spencer Blues Explosion.
Awards and recognition.
Grammy Awards:
Discography.
Singles.
Hooker issued a large number of singles, with almost a hundred releases by 1960.
<br>Here are ten of his early classic recordings:
Albums.
Listed below are the original albums with notable reissues.
The Detroit Years (recordings 1948-1955)
The Chicago Years (recordings 1955-1964)
The Folk Years (recordings 1959-1963)
The ABC Years (recordings 1965-1974)
The Rosebud Years (recordings 1975-2001)

</doc>
<doc id="15842" url="http://en.wikipedia.org/wiki?curid=15842" title="June 29">
June 29

June 29 is the day of the year in the Gregorian calendar.

</doc>
<doc id="15849" url="http://en.wikipedia.org/wiki?curid=15849" title="July 4">
July 4

July 4 is the day of the year in the Gregorian calendar. The Aphelion, the point in the year when the Earth is farthest from the Sun, occurs around this date.

</doc>
<doc id="15870" url="http://en.wikipedia.org/wiki?curid=15870" title="John Lynch (New Hampshire)">
John Lynch (New Hampshire)

John H. Lynch (born November 25, 1952) is an American businessman and politician who served as the 80th Governor of the U.S. State of New Hampshire. Lynch was first elected Governor in 2004, defeating first-term incumbent Governor Craig Benson - the first time an incumbent Governor was denied a second term in 78 years. Lynch won re-election in landslide victories in 2006 and 2008, and comfortably won a historic fourth term in 2010. Lynch is the most popular governor in New Hampshire history and, while in office, consistently ranked among the nation's most popular governors.
Education, career, and personal life.
Lynch was born in Waltham, Massachusetts, the fifth of William and Margaret Lynch's six children. Lynch earned his Bachelor of Arts degree from the University of New Hampshire in 1974, a Master of Business Administration from Harvard Business School, and a Juris Doctor from Georgetown University Law Center.
During his business career, Lynch served as Director of Admissions at Harvard Business School and President of The Lynch Group, a business consulting firm in Manchester, New Hampshire. Lynch served as CEO of Knoll Inc., a national furniture manufacturer, where he transformed the company previously losing $50 million a year, to making a profits of nearly $240 million yearly. Under his leadership, Knoll created new jobs, gave factory workers annual bonuses, established a scholarship program for the children of employees, created retirement plans for employees who didn’t have any, and gave workers stock in the company. Before announcing his run for Governor, Lynch was serving as Chairman of the University System of New Hampshire Board of Trustees.
Lynch and his wife, Dr. Susan Lynch, a pediatrician and childhood obesity activist, reside in an 11000 sqft home atop Gould Hill in Hopkinton, New Hampshire. The multi-million dollar home offers a tennis court, cabana, and swimming pool among other amenities, and views extending to Mount Washington. The Lynches have three children: Jacqueline, Julia and Hayden. Jacqueline recently graduated from Bucknell University. Julia graduated from Dartmouth College in 2011. Hayden currently attends Dartmouth as a member of the class of 2016.
Governor of New Hampshire.
Electoral history.
In June 2004, Lynch launched his campaign for Governor of New Hampshire. Lynch spent the five months preceding the election relentlessly attacking Governor Craig Benson, the first-term Republican incumbent, for what Lynch claimed was a lack of integrity following a long series of scandals during Benson's tenure. Lynch further accused Benson of creating a “culture of corruption” and cronyism at the State House. On September 15, Lynch won the democratic primary and on November 2, Lynch defeated Benson 51% to 49%. Lynch was the first challenger to defeat a first-term incumbent in New Hampshire since 1926. On January 6, 2005, Lynch was inaugurated as the 80th Governor of New Hampshire.
On November 7, 2006, Lynch was re-elected Governor in a 74% to 26% landslide victory over Republican challenger Jim Coburn. Lynch's 74% of the vote was the largest margin of victory ever in a New Hampshire gubernatorial race. Lynch’s coatails carried his party to control of both chambers of the State Legislature and both of New Hampshire's two U.S. House seats.
On November 4, 2008, Lynch was elected to a third term in another landslide victory. Lynch defeated Republican challenger Joseph Kenney, a State Senator and U.S. Marine, 70% to 28%, with 2% of the vote won by the Libertarian candidate. Lynch’s coattails again carried his party to victory. Democrats maintained control of the State Legislature and both U.S. House seats, and gained a U.S. Senate seat.
On November 2, 2010, Lynch was elected to a historic fourth term as Governor of New Hampshire, in a victory over former State Health and Human Service’s Commissioner John Stephen, 53% to 45%. Lynch was the only Democrat elected to statewide office. As had happened in many states throughout the U.S. during the 2010 midterm elections, Democrats suffered heavy losses. Democrats lost control of both chambers of the State Legislature, control of the Executive Council and both of the U.S. House seats.
According to the Concord Monitor, when Lynch was inaugurated on January 6, 2011, he became “the state's longest-serving governor in nearly two centuries. John Taylor Gilman was the last governor to serve longer than six years, serving 14 one-year terms as governor between 1794 and 1816. (The state switched to two-year terms in 1877)" New Hampshire and neighboring Vermont are the only two States in the U.S. that use two-year terms.
On September 15, 2011, Lynch announced he would not seek a historic fifth term as Governor. During the announcement Lynch said "I feel like I have the passion and the energy to keep doing this work for a long, long time, but democracy demands periodic change. To refresh and revive itself, democracy needs new leaders and new ideas." On January 3, 2013 Lynch was succeeded by fellow Democrat Maggie Hassan, marking the first time a Democrat succeeded a Democrat as the state's Governor since the 19th century.
Tenure.
Taxes.
As a candidate for Governor, Lynch took "The Pledge" not to enact any broad-based taxes, especially a Sales or Income tax. As Governor, Lynch kept his promise. Lynch does not support an amendment to the State Constitution banning an income tax.
In 2007, Lynch signed into law the Research and Development Tax credit which, for the following five years, appropriated $1,000,000 for companies to write off qualifying “manufacturing research and development” expenditures. In 2012, during his final State of the State address, Lynch proposed doubling the tax credit citing its success in creating jobs, and slammed lawmakers for slashing funding to the states Community College system to fund a 10 cent reduction in the Tobacco tax.
In June 2010, Lynch signed a Budget balancing measure that repealed the state's LLC tax.
Crime.
Lynch worked with the state Attorney General, police chiefs, and lawmakers to pass sex offender laws; increase the state police force; and increase the number of state prosecutors. New Hampshire was rated the "Safest State" in the Nation in 2008 and 2009. New Hampshire again boasts the nation’s lowest murder rate and the second-lowest rates for aggravated assault, according to "CQ Press". Lynch issued the following statement after the announcement of the award in 2009:
I am proud that working together we continue to keep New Hampshire the ‘Safest State’ in the nation. Our low crime rate has long been a part of what makes this such a great place to live and work, and it is important that we work to maintain our high quality of life. With this recognition, we should take time to thank the hard-working men and women of New Hampshire law enforcement who work every day to help keep us all safe.
Death penalty.
Lynch upheld the Death Penalty while in office, stating “there are crimes so heinous that the death penalty is warranted.” The New Hampshire House of Representatives passed legislation in March 2009 to abolish the death penalty, which Lynch threatened to veto. Due to the veto threat, the Senate tabled the legislation in April of that year. In June, Lynch compromised with legislators and signed legislation to form the New Hampshire Commission to Study the Death Penalty. In December 2010, the Commission recommended, by a 12 to 10 vote, to retain the Death Penalty. However, the panel unanimously recommended against expanding it. In 2011, Lynch signed legislation to expand the Death Penalty to include home invasions.
Natural disaster response.
In April 2006, Lynch was awarded the "National Chairman of Volunteers" Award for Volunteer Excellence by the American Red Cross, due to his leadership during the 2005 floods.
Same-sex marriage.
On June 3, 2009, Lynch signed a same-sex marriage bill into law, despite being personally opposed to gay marriages, making New Hampshire the fifth state in the United States to allow such unions.
Historic popularity.
Throughout his eight year tenure, Lynch enjoyed very high approval ratings, often being ranked among the most popular of U.S. Governors. According to the WMUR/Granite State Poll conducted by the University of New Hampshire, just three months after taking office in January 2005, Lynch's approval rating surpassed 50% and stayed upwards of 55% throughout his tenure. Likewise, between February 2006 and February 2009 his approval rating was above 70%. In April 2012, Lynch's approval rating was again above 70% making him the second most popular Governor in the United States, behind New York Governor Andrew Cuomo. Lynch enjoyed bipartisan support and is the most popular Governor in the state's history.
Presidential endorsements.
During the 2008 Democratic presidential primaries, Lynch was one of eight superdelegates from New Hampshire. Lynch remained neutral during the New Hampshire primary because as Governor he needed to "focus on being a good host to the primary", according to a statement by spokesman Colin Manning. At an event on June 27, 2008 in Unity, New Hampshire, Lynch formally endorsed Barack Obama for President.
Lynch endorsed President Barack Obama in the 2012 Presidential Election.

</doc>
<doc id="15990" url="http://en.wikipedia.org/wiki?curid=15990" title="January 23">
January 23

January 23 is the day of the year in the Gregorian calendar.

</doc>
<doc id="16001" url="http://en.wikipedia.org/wiki?curid=16001" title="Gyula Andrássy">
Gyula Andrássy

Count Gyula Andrássy de Csíkszentkirály et Krasznahorka (8 March 1823 – 18 February 1890) was a Hungarian statesman, who served as Prime Minister of Hungary (1867–1871) and subsequently as Foreign Minister of Austria-Hungary (1871–1879). A conservative, his foreign policies looked to expanding the Empire into Southeast Europe, preferably with British and German support, and without alienating Turkey. He saw Russia as the main adversary, because of its own expansionist policies toward Slavic and Orthodox areas. He distrusted Slavic nationalist movements as a threat to his multi-ethnic empire.
Biography.
The son of Count Károly Andrássy and Etelka Szapáry, he was born in Oláhpatak (now in Rožňava District, Slovakia), Kingdom of Hungary. The son of a liberal father who belonged to the political opposition, at a time when opposing the government was very dangerous, Andrássy at a very early age threw himself into the political struggles of the day, adopting at the outset the patriotic side.
Career.
Count István Széchenyi was the first adequately to appreciate his capacity. In 1845 Andrássy was appointed as president of the society for the regulation of the waters of the Upper Tisza River.
In 1846, he attracted attention by publishing highly critical articles of the government in Lajos Kossuth's paper, the "Pesti Hírlap." He was elected as one of the Radical candidates to the Diet of 1848.
When the Croats under Josip Jelačić attempted have Međimurje, which was then part of Hungary, returned to Croatia, Andrássy entered military service. He was commander of the gentry of his county, and served with distinction at the battles of Pákozd and Schwechat, as Arthur Görgey's adjutant (1848).
Toward the end of the war, Andrássy was sent to Constantinople by the revolutionary government. He was seeking to obtain the neutrality of the Ottoman Empire, if not their support, during the struggle with Croatia.
After the catastrophe of Világos, where the Hungarians were defeated, Andrássy emigrated to London and then to Paris. On 21 September 1851, he was hanged in effigy by the Austrian government for his share in the Hungarian revolt.
In exile for ten years, he studied politics in what was then the centre of European diplomacy. He discerned the weakness of the second French empire beneath its imposing exterior.
Andrássy returned to Hungary in 1858, but his position was still difficult. He had never petitioned for an amnesty, and had steadily rejected all the overtures both of the Austrian government and of the Magyar Conservatives (who would have accepted something short of full autonomy for the kingdom.) He enthusiastically supported Ferenc Deák's party.
On 21 December 1865, he was chosen vice-president of the Diet. In March 1866, he was elected as president of the sub-committee appointed by the parliamentary commission to draw up the Austro-Hungarian Compromise of 1867 between Austria and Hungary. He originated the idea of the "Delegations" of powers. 
It was said at that time that he was the only member of the commission who could persuade the court of the justice of the national claims.
After the Battle of Königgrätz, he was formally consulted by Emperor Franz Joseph for the first time. He recommended the re-establishment of the constitution and the appointment of a responsible foreign and defence ministry.
On 17 February 1867 the king appointed him as the first prime minister of the Hungarian half of the newly formed Dual Monarchy of Austria-Hungary. The obvious first choice had been Ferenc Deák, one of the architects of the Compromise, but he stepped down in favour of Andrássy. Deák described him as "the providential statesman given to Hungary by the grace of God."
As premier, Andrássy by his firmness, amiability and dexterity as a debater, soon won for himself a commanding position. Yet his position continued to be difficult, inasmuch as the authority of Deák dwarfed that of all the party leaders, however eminent.
Andrássy chose for himself the departments of war and foreign affairs. It was he who reorganized the Honvéd system (state army), and he used often to say that the regulation of the military border districts was the most difficult labour of his life.
On the outbreak of the Franco-Prussian War of 1870, Andrássy resolutely defended the neutrality of the Austrian monarchy, and in his speech on 28 July 1870 warmly protested against the assumption that it was in the interests of Austria to seek to recover the position she had held in Germany before 1863. On the fall of Beust (6 November 1871), Andrássy stepped into his place. His tenure of the chancellorship was epoch-making.
Hitherto the empire of the Habsburgs had never been able to dissociate itself from its Holy Roman traditions. But its loss of influence in Italy and Germany, and the consequent formation of the Dual State, had at length indicated the proper, and, indeed, the only field for its diplomacy in the future – the Near East, where the process of the crystallization of the Balkan peoples into nationalities was still incomplete. The question was whether these nationalities were to be allowed to become independent or were only to exchange the tyranny of the sultan for the tyranny of the tsar or the Habsburg emperor.
Hitherto Austria had been content either to keep out the Russians or share the booty with them. She was now, moreover, in consequence of her misfortunes deprived of most of her influence in the councils of Europe.
It was Andrassy who recovered for her proper place in the European concert. First he approached the German emperor; then more friendly relations were established with the courts of Italy and Russia by means of conferences at Berlin, Vienna, St Petersburg and Venice.
The "Andrássy Note".
The recovered influence of Austria was evident in the negotiations which followed the outbreak of serious disturbances in Bosnia in 1875.
The three courts of Vienna, Berlin and St Petersburg reached an understanding as to their attitude in the Eastern question, and their views were embodied in the dispatch, known as the "Andrássy Note", sent on 30 December 1875 by Andrássy to Count Beust, the Austrian ambassador to the Court of St James.
In it he pointed out that the efforts of the powers to localize the revolt seemed in danger of failure, that the rebels were still holding their own, and that the Ottoman promises of reform, embodied in various firmans, were no more than vague statements of principle which had never had, and were probably not intended to have, any local application. In order to avert the risk of a general conflagration, therefore, he urged that the time had come for concerted action of the powers for the purpose of pressing the Porte to fulfil its promises.
A sketch of the more essential reforms followed: the recognition rather than the toleration of the Christian religion; the abolition of the system of farming the taxes; and, in Bosnia and Herzegovina, where the religious was complicated by an agrarian question, the conversion of the Christian peasants into free proprietors, to rescue them from their double subjection to the Muslim Ottoman landowners.
In Bosnia and Herzegovina elected provincial councils were to be established, life-term judges appointed and individual liberties guaranteed.
Finally, a mixed commission of Muslims and Christians was to be empowered to watch over the carrying out of these reforms.
The fact that the sultan would be responsible to Europe for the realization of his promises would serve to allay the natural suspicions of the insurgents. To this plan both Britain and France gave a general assent, and the Andrássy Note was adopted as the basis of negotiations.
When war became inevitable between Russia and the Porte, Andrássy arranged with the Russian court that, in case Russia prevailed, the status quo should not be changed to the detriment of the Austrian monarchy. When, however, the Treaty of San Stefano threatened a Russian hegemony in the Near East, Andrássy concurred with the German and British courts that the final adjustment of matters must be submitted to a European congress.
At the Congress of Berlin in 1878 he was the principal Austrian plenipotentiary, and directed his efforts to diminish the gains of Russia and aggrandize the Dual Monarchy. Before the Congress opened on 13 June, negotiations between Andrássy and the British Foreign Secretary Marquess of Salisbury had already "ended on 6 June by Britain agreeing to all the Austrian proposals relative to Bosnia-Herzegovina about to come before the congress while Austria would support British demands".
In addition to the occupation and administration of Bosnia-Herzegovina, Andrássy also obtained the right to station garrisons in the Sanjak of Novi Pazar, which remained under Ottoman administration. The Sanjak preserved the separation of Serbia and Montenegro, and the Austro-Hungarian garrisons there would open the way for a dash to Salonika that "would bring the western half of the Balkans under permanent Austrian influence". "High [Austro-Hungarian] military authorities desired [an ...] immediate major expedition with Salonika as its objective".
This occupation was most unpopular in Hungary, both for financial reasons and because of the strong philo-Turk sentiments of the Magyars.
On 28 September 1878 the Finance Minister, Koloman von Zell, threatened to resign if the army, behind which stood the Archduke Albert, were allowed to advance to Salonika. In the session of the Hungarian Parliament of 5 November 1878 the Opposition proposed that the Foreign Minister should be impeached for violating the constitution by his policy during the Near East Crisis and by the occupation of Bosnia-Herzegovina. The motion was lost by 179 to 95. By the Opposition rank and file the gravest accusations were raised against Andrassy.
On 10 October 1878 the French diplomat Melchior de Vogüé described the situation as follows:
Particularly in Hungary the dissatisfaction caused by this 'adventure' has reached the gravest proportions, prompted by that strong conservative instinct which animates the Magyar race and is the secret of its destinies. This vigorous and exclusive instinct explains the historical phenomenon of an isolated group, small in numbers yet dominating a country inhabited by a majority of peoples of different races and conflicting aspirations, and playing a role in European affairs out of all proportions to its numerical importance or intellectual culture. This instinct is to-day awakened and gives warning that it feels the occupation of Bosnia-Herzegovina to be a menace which, by introducing fresh Slav elements into the Hungarian political organism and providing a wider field and further recruitment of the Croat opposition, would upset the unstable equilibrium in which the Magyar domination is poised.
Andrássy felt constrained to bow before the storm, and he placed his resignation in the emperor's hands (8 October 1879). The day before his retirement he signed the offensive-defensive alliance with Germany, which placed the foreign relations of Austria-Hungary once more on a stable footing.
Later life.
After his retirement, Andrássy continued to take an active part in public affairs both in the Delegations and in the Upper House. In 1885 he warmly supported the project for the reform of the House of Magnates, but on the other hand he jealously defended the inviolability of the Composition of 1867, and on 5 March 1889 in his place in the Upper House spoke against any particularist tampering with the common army. In the last years of his life he regained his popularity, and his death on 18 February 1890, aged 66, was mourned as a national calamity. There is a plaque dedicated to him in the town of Volosko where he died (between Rijeka and Opatija in present-day Croatia). It is located just above the restaurant Amfora.
He was the first Magyar statesman who, for centuries, had occupied a European position. It has been said that he united in himself the Magyar magnate with the modern gentleman. His motto was: "It is hard to promise, but it is easy to perform." If Deák was the architect, Andrássy certainly was the master-builder of the modern Hungarian state.
Family.
By his wife, the countess Katinka Kendeffy, whom he married in Paris in 1856, Count Andrássy left two sons, and one daughter, Ilona (b. 1858). Both the sons gained distinction in Hungarian politics.
The eldest, Tivadar Andrássy (Theodore Andreas) (born 10 July 1857), was elected vice-president of the Lower House of the Hungarian parliament in 1890. The younger, Gyula (born 30 June 1860), also had a successful political career.
Gyula Andrassy's granddaughter, Klara, married the Hungarian nobleman and industrialist Prince Karoly Odescalchi. 
According to a very common legend, Count Andrássy had a long lasting romance with Queen Elisabeth (Sissy), wife of Emperor and King Franz-Josef of Austria-Hungary, and fathered their only son, Archduke Rudolf. There is no evidence for this story except for the strong sympathy and devotion of both Sissy and Rudolf towards Hungary, its culture and national customs (they were both fluent in Hungarian and regarded Hungarian poetry highly).
Count Andrassy had four granddaughters, Klara above, Barbara, married Marquis Pallavicini,Katalin married Count Mihaly Karolyi and Ilona.war widow of Prince Paul Esterhazy, remarried Count Jozsef Cziraky.

</doc>
<doc id="16111" url="http://en.wikipedia.org/wiki?curid=16111" title="John Sayles">
John Sayles

John Thomas Sayles (born September 28, 1950) is an American independent film director, screenwriter, editor, actor and author. He has twice been nominated for the Academy Award for Best Original Screenplay for "Passion Fish" (1992) and "Lone Star" (1996).
Early life.
Sayles was born in Schenectady, New York, the son of Mary ("née" Rausch), a teacher, and Donald John Sayles, a school administrator. Both of Sayles's parents were of half-Irish descent and were Catholic. He attended Williams College.
Career.
Like Martin Scorsese and James Cameron, among others, Sayles began his film career working with Roger Corman. In 1979, Sayles funded his first film, "Return of the Secaucus 7", with $30,000 he had in the bank from writing scripts for Corman. He set the film in a large house so that he did not have to travel to or get permits for different locations, set it over a three-day weekend to limit costume changes, and wrote about people his age so that he could have his friends act in it. The film received near-unanimous critical acclaim at the time and has held its reputation. In November 1997, the National Film Preservation Board announced that "Return of the Secaucus 7" would be one of the 25 films selected that year for preservation in the National Film Registry at the Library of Congress.
In 1983, after the films "Baby It's You" (starring Rosanna Arquette) and "Lianna" (a story in which a married woman becomes discontented with her marriage and falls in love with another woman), Sayles received a MacArthur Fellowship. He used the money to partially fund the fantasy "The Brother from Another Planet", a film about a black, three-toed slave who escapes from another planet and finds himself at home among the people of Harlem.
In 1989, Sayles created and wrote the pilot episode for the short-lived television show "Shannon's Deal" about a down-and-out Philadelphia lawyer played by Jamey Sheridan. Sayles received a 1990 Edgar Award for his teleplay for the pilot. The show ran for 16 episodes before being canceled in 1991.
Sayles has funded most of his films by writing genre scripts, such as "Piranha", "Alligator", "The Howling" and "The Challenge". Having collaborated with Joe Dante on "Piranha" and "The Howling", Sayles acted in Dante's movie, "Matinee". In deciding whether to take a job, Sayles reports that he is interested mostly in whether there is the germ of an idea for a movie which he would want to watch. Sayles gets the rest of his funding by working as a script doctor; he did rewrites for "Apollo 13", and "Mimic".
A genre script, called "Night Skies", inspired what would eventually become the film "E.T. the Extra-Terrestrial". That film's director, Steven Spielberg, later commissioned Sayles to write a script (unused) for the fourth "Jurassic Park" film.
He has written and directed his own films, including "Lone Star", "Passion Fish", "Eight Men Out", "The Secret of Roan Inish," and "Matewan". He serves on the advisory board for the Austin Film Society.
Maggie Renzi has been John Sayles' long-time companion (and collaborator), but they have not married. Renzi has produced most of his films since "Lianna". They met as students at Williams College.
Sayles works with a regular repertory of actors, most notably Chris Cooper, David Strathairn, and Gordon Clapp, each of whom has appeared in at least four of his films.
In early 2003, Sayles signed the Not In Our Name "Statement of Conscience" (along with Noam Chomsky, Steve Earle, Brian Eno, Jesse Jackson, Viggo Mortensen, Bonnie Raitt, Oliver Stone, Marisa Tomei, Susan Sarandon and others) which opposed the invasion of Iraq.
In February 2009, Sayles was reported to be writing an HBO series based on the early life of Anthony Kiedis of the Red Hot Chili Peppers. The drama, tentatively titled "Scar Tissue", centers on Kiedis's early years living in West Hollywood with his father. At that time, Kiedis's father, known as Spider, sold drugs (according to legend, his clients included The Who and Led Zeppelin) and mingled with rock stars on the Sunset Strip, all while aspiring to get into show business.
In February 2010, Sayles began shooting his 17th feature film, the historical war drama "Amigo", in the Philippines. The film is a fictional account of events during the Philippine–American War, with a cast that includes Joel Torre, Chris Cooper, and Garret Dillahunt.
His novel "A Moment in the Sun", set during the same period as "Amigo", in the Philippines, Cuba, and the US, was released in 2011 by McSweeney’s. It includes an account of the Wilmington Insurrection of 1898 in North Carolina, the only coup d'état in United States history in which a duly elected government was overthrown.
Awards/nominations.
Films.
Awards for "Honeydripper":
Award for "Silver City":
Awards for "Sunshine State":
Awards for "Limbo":
Awards for "Men with Guns/Hombres armados":
Awards for "Lone Star":
Awards for "The Secret of Roan Inish":
Awards for "Passion Fish":
Awards for "City of Hope":
Awards for "Matewan":
Awards for "The Brother from Another Planet":
Awards for "Return of the Secaucus 7":
Other recognition.
Sayles' first published story, "I-80 Nebraska", won an O. Henry Award; his novel, "Union Dues", was nominated for a National Book Award as well as the National Book Critics Circle Award.
In 1983, Sayles received the John D. MacArthur Award, given to 20 Americans in diverse fields each year for their innovative work. He has also been the recipient of the Eugene V. Debs Award, the John Steinbeck Award and the John Cassavetes Award. He was honored with the Ian McLellan Hunter Award for Lifetime Achievement by the Writers Guild of America (1999).
Recurring collaborators.
Actors who have regularly worked with Sayles include Maggie Renzi, David Strathairn, Joe Morton, Chris Cooper, Mary McDonnell, Vincent Spano, Kevin Tighe, Josh Mostel, Tom Wright, Gordon Clapp and Angela Bassett.

</doc>
<doc id="16115" url="http://en.wikipedia.org/wiki?curid=16115" title="James Spader">
James Spader

James Todd Spader (born February 7, 1960) is an American actor. He is best known for portraying eccentric characters in such films as the action science fiction film "Stargate" (1994), the controversial psychological thriller "Crash" (1996) and the erotic romance "Secretary" (2002).
His most well-known television roles are those of attorney Alan Shore in "The Practice" and its spin-off "Boston Legal" (for which he won three Emmy Awards), and Robert California in the comedy-mockumentary "The Office". He stars as high-profile criminal Raymond "Red" Reddington in the crime drama "The Blacklist" and played the villainous robot Ultron in Marvel Studios' 2015 superhero film "".
Early life.
Spader was born in Boston, Massachusetts, the son of Jean (née Fraser) and Stoddard Greenwood "Todd" Spader, both teachers. One of Spader's great-great-grandfathers was William Ingersoll Bowditch (of the historic William Ingersoll Bowditch House); Spader's ancestors also include deaf educator Laurent Clerc, mathematician Nathaniel Bowditch, American Revolution general Joshua Babcock, and Lieutenant-Governor of Nova Scotia Paul Mascarene.
During his early education, Spader attended The Pike School (where his mother taught Art) and enrolled in the Brooks School (where his father taught) in North Andover, Massachusetts for one year. He later transferred to Phillips Academy but dropped out in the 11th grade to pursue acting at the Michael Chekhov School in New York City. Before becoming a full-time actor, he held a variety of jobs, including yoga instructor, busboy, truck driver, stable boy, and railroad car loader.
Career.
Spader's first major film role was in the film "Endless Love" (1981), and his first starring role was in "Tuff Turf" (1985). However, he did not rise to stardom until 1986, when he played the rich, arrogant playboy Steff in "Pretty in Pink". He co-starred in "Mannequin" (1987) and the film adaptation of "Less Than Zero" (1987), in which he played a drug dealer named Rip. Supporting roles in films such as "Baby Boom" (1987) and "Wall Street" (1987) followed until his breakthrough in "Sex, Lies, and Videotape" (1989), in which he played a sexual voyeur who complicates the lives of three Baton Rouge, Louisiana residents. For this performance, he received the Best Actor Award at the Cannes Film Festival.
Spader's roles in the early 1990s included a young, affluent widower opposite Susan Sarandon in the romantic drama "White Palace" (1990), Rob Lowe in the Noir drama "Bad Influence" (1990) John Cusack's best friend in the drama "True Colors" (1991), and a poker-playing drifter in "The Music of Chance" (1993). In 1994, he starred as Egyptologist Daniel Jackson in the sci-fi film "Stargate". In 1996, he played car accident fetishist James Ballard in the controversial Canadian film "Crash" and assassin Lee Woods in "2 Days in the Valley". In 1997, Spader guest starred in the "Seinfeld" episode "The Apology", as an angry recovering alcoholic who refuses to apologize to George for making fun of him. In 2000, he played a drug-addicted detective tracking down a serial killer in "The Watcher". In 2002, he starred as a sadistic boss in the critically acclaimed film "Secretary".
From 2004 to 2008, Spader starred as Alan Shore in the series "Boston Legal", in which he reprised his role from the television series "The Practice". He won the Emmy Award for Outstanding Lead Actor in a Drama Series in 2004 for his portrayal on "The Practice" and won it again in 2005 and 2007 for "Boston Legal". With the 2005 win, he became one of only a few actors to win an Emmy award while playing the same character in two different series. Even rarer, he won a second consecutive Emmy while playing the same character in two different series. He also won the Satellite Award for Best Actor in a Series, Comedy or Musical for "Boston Legal" in 2006.
In October 2006, Spader narrated "", the first episode of Discovery Channel's documentary series "Discovery Atlas". He has also done the voice-over in several television commercials for Acura. He starred in "Race", a play written and directed by David Mamet, which opened on December 6, 2009 at the Ethel Barrymore Theatre on Broadway. The show closed on August 21, 2010 after 297 performances. In March 2011, he was named to star in the film "By Virtue Fall", written and to be directed by Sheldon Turner. s of 2011[ [update]], the movie was in pre-production.
Spader guest starred as Robert California in "Search Committee", the season 7 finale of "The Office". On June 27, 2011, it was announced that he would join the cast on a permanent basis. He planned to stay only through the eighth season, and while the original plan was just to do the guest appearance, executive producer Paul Lieberstein said: "those two scenes became a season".
Spader stars in the NBC series "The Blacklist", which premiered on NBC September 23, 2013. He portrays Raymond "Red" Reddington, one of the FBI's most wanted fugitives. He also played villainous robot Ultron in "" (2015).
Personal life.
Spader met his first wife, decorator Victoria Kheel, while working in a yoga studio after he moved to New York City in the 1980s. They married in 1987 and have two sons, Elijah and Sebastian. Spader filed for divorce from Kheel in 2004. He began dating his former "Alien Hunter" (2003) co-star, Leslie Stefanson, in 2002; they have one son, Nathaneal (born August 2008).

</doc>
<doc id="16129" url="http://en.wikipedia.org/wiki?curid=16129" title="John Brunner (novelist)">
John Brunner (novelist)

John Kilian Houston Brunner (24 September 1934 – 26 August 1995) was a British author of science fiction novels and stories. His 1968 novel "Stand on Zanzibar", about an overpopulated world, won the 1969 Hugo Award for best science fiction novel, and the BSFA award the same year. "The Jagged Orbit" won the BSFA award in 1970.
Life.
Brunner was born in Preston Crowmarsh, near Wallingford in Oxfordshire, and went to school at St Andrew's Prep School, Pangbourne, then to Cheltenham College. He wrote his first novel, "Galactic Storm", at 17, and published it under the pen-name Gill Hunt, but he did not start writing full-time until 1958. He served as an officer in the Royal Air Force from 1953 to 1955, and married Marjorie Rosamond Sauer on 12 July 1958.
Brunner had an uneasy relationship with British new wave writers, who often considered him too American in his settings and themes. He attempted to shift to a more mainstream readership in the early 1980s, without success. Before his death, most of his books had fallen out of print. Brunner accused publishers of a conspiracy against him, although he was difficult to deal with (his wife had handled his publishing relations before she died).
Brunner's health began to decline in the 1980s and worsened with the death of his wife in 1986. He remarried, to Li Yi Tan, on 27 September 1991. He died of a heart attack in Glasgow on 25 August 1995, while attending the World Science Fiction Convention there.
Literary works.
At first writing conventional space opera, Brunner later began to experiment with the novel form. His 1968 novel "Stand on Zanzibar" exploits the fragmented organizational style John Dos Passos invented for his "USA trilogy", but updates it in terms of the theory of media popularised by Marshall McLuhan.
"The Jagged Orbit" (1969) is set in a United States dominated by weapons proliferation and interracial violence, and has 100 numbered chapters varying in length from a single syllable to several pages in length. "The Sheep Look Up" (1972) depicts ecological catastrophe in America. Brunner is credited with coining the term "worm" and predicting the emergence of computer viruses in his 1975 novel "The Shockwave Rider", in which he used the term to describe software which reproduces itself across a computer network. Together with "Stand on Zanzibar", these novels have been called the "Club of Rome Quartet", named after the Club of Rome whose 1972 report "The Limits to Growth" warned of the dire effects of overpopulation.
Brunner's pen names include K. H. Brunner, Gill Hunt, John Loxmith, Trevor Staines, Ellis Quick, Henry Crosstrees Jr., and Keith Woodcott.
In addition to his fiction, Brunner wrote poetry and many unpaid articles in a variety of publications, particularly fanzines, but also 13 letters to the "New Scientist" and an article about the educational relevance of science fiction in "Physics Education". Brunner was an active member of the organisation Campaign for Nuclear Disarmament and wrote the words to "The H-Bomb's Thunder", which was sung on the Aldermaston Marches. He was a linguist, translator, and Guest of Honour at the first European Science Fiction Convention "Eurocon-1" in Trieste in 1972.
Film and TV.
John Brunner wrote the screenplay for the 1967 science fiction film "The Terrornauts" by Amicus Productions.
Two of his short stories, "Some Lapse of Time" and "The Last Lonely Man", were adapted as TV plays in the BBC science fiction series "Out of the Unknown", in series 1 (1965) and series 3 (1969) respectively.

</doc>
<doc id="16132" url="http://en.wikipedia.org/wiki?curid=16132" title="Joseph Yoakum">
Joseph Yoakum

Joseph Elmer Yoakum (February 22, 1889 – December 25, 1972) was a self-taught landscape artist of African-American and Native American descent, who drew landscapes in a highly individual style. He was 76 when he started to record his memories in the form of imaginary landscapes, and he produced over 2,000 drawings during the last decade of his life. His work is an example of what is sometimes called Outsider Art (formerly, drawings and paintings of the insane).
Early life.
His official records note that Yoakum was born in Missouri, but he told a story of being born in Arizona, in 1888, as a Navajo Indian on the Window Rock Navajo reservation. Taking pride in his invented native heritage, Yoakum would pronounce "Navajo" as "Na-va-JOE" (as in "Joseph"). His father was a Cherokee Indian, and his mother was a former slave of mixed Cherokee, African-American, and French-American descent. He spent his early childhood on a Missouri farm.
Yoakum left home when he was nine years old to join the Great Wallace Circus. As a billposter, he also traveled across the U.S. with Buffalo Bill's Wild West Show and the Ringling Brothers, among the five different circuses. He later traveled to Europe as a stowaway.
In 1908, he returned to Missouri and started a family with his girlfriend Myrtle Julian, with whom he had his first son in 1909; the couple married in 1910. Yoakum was drafted into army service in 1918 and worked in the 805th Pioneer Infantry repairing roads and railroads.
After the war, he traveled around the U.S. working odd jobs, but he never returned to his family. He later remarried and moved to Chicago. In 1946, Yoakum was committed to a psychiatric hospital there. He soon left and by the early 1950s, he was drawing on a regular basis.
Artistic work.
Yoakum was again living and painting in Chicago by 1962. Tom Brand, owner of Galaxy Press on the south side of Chicago, in 1968 had some printing to deliver to a coffee shop called "The Whole". While there he noticed the colored pencil drawings of Yoakum and was immediately taken by them. Brand had an account with the Ed Sherbyn Gallery on the north side of Chicago, and he persuaded Sherbyn to exhibit Yoakum's works and even printed his own poster for this show. Norman Mark of "The Chicago Daily News" wrote an article about Yoakum called "My drawings are a spiritual unfoldment"; this article was printed on the back of the poster. Brand informed his artist friends (including Whitney Halstead) about Yoakum and encouraged them to visit the Whole coffee shop. Halstead, an artist and instructor at the School of the Art Institute of Chicago, became the greatest promoter of Yoakum's work during his lifetime. He believed that his story was "more invention than reality... in part myth, Yoakum's life as he would have wished to have lived it."
In 1967, Yoakum was discovered by the mainstream art community through John Hopgood, an instructor at the Chicago State College, who saw Yoakum's work hanging in his studio window and purchased twenty-two pictures. A group of students including Roger Brown, Gladys Nillson, Jim Nutt, and Barbara Rossi, and teachers at the School of the Art Institute of Chicago, including Ray Yoshida and Whitney Halstead, took an interest in promoting his work. In 1972, just one month before his death, Yoakum was given a one-man show at the Whitney Museum in New York City.
He started drawing familiar places, such as "Green Valley Ashville Kentucky", as a method to capture his memories. However, he shifted towards imaginary landscapes in places he had never visited, like "Mt Cloubelle of West India" or "Mt Mowbullan in Dividing Range near Brisbane Australia". Drawing outlines with ballpoint pen, rarely making corrections, he colored his drawings within the lines using watercolors and pastels. He became known for his organic forms, always using two lines to designate land masses.
During the final four months of his life was marked by a use of pure abstraction, as in his illustration "Flooding of Sock River through Ash Grove Mo [Missouri] on July 4, 1914 in that [waters] drove many persons from Homes I were with the Groupe leiving ["sic"] their homes for safety". That painting was one of his autobiographical works.

</doc>
<doc id="16139" url="http://en.wikipedia.org/wiki?curid=16139" title="Joe Pass">
Joe Pass

Joe Pass (born "Joseph Anthony Jacobi Passalacqua", January 13, 1929 – May 23, 1994) was an American virtuoso jazz guitarist of Sicilian descent. He is generally considered to be one of the greatest jazz guitarists of the 20th century. His extensive use of walking basslines, melodic counterpoint during improvisation, use of a chord-melody style of playing and outstanding knowledge of chord inversions and progressions opened up new possibilities for the jazz guitar and had a profound influence on later guitarists.
Early life.
Born in New Brunswick, New Jersey, Joe Pass, the son of Mariano Passalacqua, a Sicilian-born steel mill worker, was raised in Johnstown, Pennsylvania. He received his first guitar, a Harmony model bought for $17, on his 9th birthday. Pass' father recognized early that his son had "a little something happening" and pushed him constantly to pick up tunes by ear, play pieces not written specifically for the instrument, practice scales and not to "leave any spaces" - that is, to fill in the sonic space between the notes of the melody.
As early as 14, Pass started getting gigs and was playing with bands fronted by Tony Pastor and Charlie Barnet, honing his guitar skills and learning the music business. He began traveling with small jazz groups and eventually moved from Pennsylvania to New York City. In a few years, he developed a heroin addiction and spent much of the 1950s in prison. Pass managed to emerge from narcotics addiction through a two-and-a-half-year stay in the Synanon rehabilitation program. During that time he "didn't do a lot of playing". In 1962 he recorded "Sounds of Synanon". It was about this time that Pass received his trademark Gibson ES-175 guitar as a gift, which he subsequently used for touring and recording for many years.
Discovery and subsequent career.
Pass recorded a series of albums during the 1960s for the Pacific Jazz label, including the early classics "Catch Me," "12-String Guitar," "For Django," and "Simplicity." In 1963, Pass received "Downbeat" magazine's "New Star Award." Pass was also featured on Pacific Jazz recordings by Gerald Wilson, Bud Shank, and Les McCann. Pass toured with George Shearing in 1965. During the 1960s however, he did mostly TV and recording session work in Los Angeles.
He was a sideman with Louis Bellson, Frank Sinatra, Sarah Vaughan, Joe Williams, Della Reese, Johnny Mathis, and worked on TV shows including "The Tonight Show Starring Johnny Carson," "The Merv Griffin Show," "The Steve Allen Show," and others. In the early 1970s, Pass and guitarist Herb Ellis were performing together regularly at Donte's jazz club in Los Angeles. This collaboration led to Pass and Ellis recording the very first album on the new Concord Jazz label, entitled simply "Jazz/Concord" (#CJS-1), along with bassist Ray Brown and drummer Jake Hanna. In the early 1970s, Pass also collaborated on a series of music books, and his "Joe Pass Guitar Style" (written with Bill Thrasher) is considered a leading improvisation textbook for students of jazz.
Norman Granz, the producer of Jazz at the Philharmonic and the founder of Verve Records signed Pass to Granz's new Pablo Records label in 1970. In 1974, Pass released his landmark solo album "Virtuoso" on Pablo Records. Also in 1974, Pablo Records released the album "The Trio" featuring Pass, Oscar Peterson, and Niels-Henning Ørsted Pedersen. He performed with them on many occasions throughout the 1970s and 1980s. At the Grammy Awards of 1975, "The Trio" won the Grammy Award for Best Jazz Performance by a Group. As part of the Pablo Records "stable," Pass also recorded with Benny Carter, Milt Jackson, Herb Ellis, Zoot Sims, Duke Ellington, Dizzy Gillespie, Ella Fitzgerald, Count Basie, and others.
Pass and Ella Fitzgerald recorded six albums together on Pablo Records, toward the end of Fitzgerald's career: "Take Love Easy" (1973), "Fitzgerald and Pass... Again" (1976), "Hamburg Duets - 1976" (1976), "Sophisticated Lady" (1975, 1983), 
"Speak Love" (1983), and "Easy Living" (1986).
In 1994, Joe Pass died from liver cancer in Los Angeles, California at the age of 65. Prior to his death, he had recorded an album of instrumental versions of Hank Williams songs with country guitarist Roy Clark.
Speaking about "Nuages: Live at Yoshi's, Volume 2," Jim Ferguson wrote: "The follow up to 1993's "Joe Pass & Co. Live At Yoshi's," this release was colored by sad circumstances: both bassist Monty Budwig and Pass were stricken with fatal illnesses. Nevertheless, all concerned, including drummer Colin Bailey and second guitarist John Pisano, play up to their usual high levels... Issued posthumously, this material is hardly sub-standard. Bristling with energy throughout, it helps document the final stages in the career of a player who, arguably, was the greatest mainstream guitarist since Wes Montgomery."
Legacy.
In addition to his ensemble performances, the jazz community regards Joe Pass as an influential solo guitarist. "New York Magazine" said of him, "Joe Pass looks like somebody's uncle and plays guitar like nobody's business. He's called "the world's greatest" and often compared to Paganini for his virtuosity. There is a certain purity to his sound that makes him stand out easily from other first-rate jazz guitarists." His solo style was marked by an advanced linear technique, sophisticated harmonic sense, counterpoint between improvised lead lines, bass figures and chords, spontaneous modulations, and transitions from fast tempos to rubato passages. He would regularly add what he called "color tones" to his compositions, to give what he believed was a more sophisticated and "funkier" sound. He would often use melodic counterpoint during improvisation, move lines and chords chromatically or play melodies by solely shifting chords, and descending augmented arpeggios at the end of phrases.
Pass' early style (influenced by guitarist Django Reinhardt and saxophonist Charlie Parker), was marked by fast single-note lines and a flowing melodic sense. Pass had the unusual lifelong habit of breaking his guitar picks and playing only with the smaller part. As Pass made the transition from ensemble to solo guitar performance, he preferred to abandon the pick altogether, and play fingerstyle. He found this enabled him to execute his harmonic concepts more effectively. His series of solo albums, "Virtuoso" (volumes 1 through 4) are a demonstration of Pass' refined technique.
 "He weaves his own fast-moving chords and filigree work so nimbly that it is hard to believe fingers can physically shift to quickly. Slight moustached, fairly balding, he frowns over his fretwork like a worried head waiter with more guests than tables but the sound that comes out could only be the confident product of years of devotion to the instrument... But it is when he plays completely solo, which he does for half of each set, that he comes into his own, because without hindrance of the rhythm section he can completely orchestrate each number. Sometimes it is by contrasting out of tempo sections with fast-moving interludes, sometimes by switching mood from wistful to lightly swinging, sometimes by alternating single-note lines with chords or simultaneous bass line and melody-the possibilities seem endless. Luckily, there is a new L.P. by him which captures all this on vinyl, as someone has had the unusual good sense to record him all alone. It is called "Virtuoso" and rightly so." - Miles Kington on Pass in an October 1974 article in "The Times".
Epiphone has produced an edition of the "Emperor" line of archtop electric-acoustic guitar in his honour. Previously Ibanez had a Joe Pass model jazz guitar, as they continue to for influential jazz guitarists George Benson and Pat Metheny.

</doc>
<doc id="16141" url="http://en.wikipedia.org/wiki?curid=16141" title="Jazz guitar">
Jazz guitar

The term jazz guitar may refer to either a type of guitar or to the variety of guitar playing styles used in the various genres which are commonly termed "jazz". The jazz-type guitar was born as a result of using electric amplification to increase the volume of conventional acoustic guitars.
Conceived in the early 1930s, the electric guitar became a necessity as jazz musicians sought to amplify their sound. Arguably, no other musical instrument had greater influence on how music evolved since the beginning of the twentieth century. Although the earliest guitars used in jazz were acoustic and acoustic guitars are still sometimes used in jazz, most jazz guitarists since the 1940s have performed on an electrically amplified guitar or electric guitar.
Traditionally, jazz electric guitarists use an archtop with a relatively broad hollow sound-box, violin-style f-holes, a "floating bridge", and a magnetic pickup. Solid body guitars, mass-produced since the early 1950s, are also used.
Jazz guitar playing styles include "comping" with jazz chord voicings (and in some cases walking bass lines) and "blowing" (improvising) over jazz chord progressions with jazz-style phrasing and ornaments. Comping refers to playing chords underneath a song's melody or another musician's solo improvisations. When jazz guitar players improvise, they may use the scales, modes, and arpeggios associated with the chords in a tune's chord progression.
History.
1900-mid-1930s.
The stringed, chord-playing rhythm be heard in groups which included military band-style instruments such as brass, saxes, clarinets, and drums, such as early jazz groups. As the acoustic guitar became a more popular instrument in the early 20th century, guitar-makers began building louder guitars which would be useful in a wider range of settings.
The Gibson L5, an acoustic archtop guitar which was first produced in 1923, was an early “jazz”-style guitar which was used by early jazz guitarists such as Eddie Lang. By the 1930s, the guitar began to displace the banjo as the primary chordal rhythm instrument in jazz music, because the guitar could be used to voice chords of greater harmonic complexity, and it had a somewhat more muted tone that blended well with the upright bass, which, by this time, had almost completely replaced the tuba as the dominant bass instrument in jazz music.
Late 1930s-1960s.
During the late 1930s and through the 1940s—the heyday of big band jazz and swing music—the guitar was an important rhythm section instrument. Some guitarists, such as Freddie Green of Count Basie’s band, developed a guitar-specific style of accompaniment. Few of the big bands, however, featured amplified guitar solos, which were done instead in the small combo context. The most important jazz guitar soloists of this period included the Manouche virtuoso Django Reinhardt, Oscar Moore who was featured with Nat “King” Cole’s trio, and Charlie Christian of Benny Goodman's band and sextet, who was a major influence despite his early death at 25.
It was not until the large-scale emergence of small combo jazz in the post-WWII period that the guitar took off as a versatile instrument, which was used both in the rhythm section and as a featured melodic instrument and solo improviser. In the hands of George Barnes, Kenny Burrell, Herb Ellis, Barney Kessel, Jimmy Raney, and Tal Farlow, who had absorbed the language of bebop, the guitar began to be seen as a “serious” jazz instrument. Improved electric guitars such as Gibson’s ES-175 (released in 1949), gave players a larger variety of tonal options. In the 1940s through the 1960s, players such as Wes Montgomery, Joe Pass, and Jim Hall laid the foundation of what is now known as "jazz guitar" playing.
1970s.
As jazz-rock fusion emerged in the early 1970s, many players switched to the more rock-oriented solid body guitars. Other jazz guitarists, like Grant Green and Wes Montgomery, turned to applying their skills to pop-oriented styles that fused jazz with soul and R&B, such as soul jazz-styled organ trios. Younger jazz musicians rode the surge of electric popular genres such as blues, rock, and funk to reach new audiences. Guitarists in the fusion realm fused the post-bop harmonic and melodic language of musicians such as John Coltrane, McCoy Tyner, Ornette Coleman, and Miles Davis with a hard-edged (and usually very loud) rock tone created by iconic guitarists such as Cream's Eric Clapton who'd redefined the sound of the guitar for those unfamiliar with the black blues players of Chicago and, before that, the Delta region of the Mississippi upon whom his style was based. With John Mayall's Bluesbreakers, Clapton turned up the volume on a sound already pioneered by Buddy Guy, Freddie King, B.B. King and others that was fluid, with heavy finger vibratos, string bending, and speed through powerful Marshall amplifiers.
Fusion players such as John McLaughlin adopted the fluid, powerful sound of rock guitarists such as Clapton and Jimi Hendrix. McLaughlin was a master innovator, incorporating hard jazz with the new sounds of Clapton, Hendrix, Beck and others. McLaughlin later formed the Mahavisnhu Orchestra, an historically important fusion band that played to sold out venues in the early 1970s and as a result, produced an endless progeny of fusion guitarist. Guitarists such as Pat Martino, Al Di Meola, Larry Coryell, John Abercrombie, John Scofield and Mike Stern (the latter two both alumni of the Miles Davis band) fashioned a new language for the guitar which introduced jazz to a new generation of fans. Like the rock-blues icons that preceded them, fusion guitarists usually played their solid body instruments through stadium rock-style amplification, and signal processing "effects" such as simulated distortion, wah-wah, octave splitters, compression, and flange pedals. In addition, they also simply turned up to full volume in order to create natural overdrive such as the blues rock players.
1980s-2000s.
By the early 1980s, the radical experiments of early 1970s-era fusion gave way to a more radio-friendly sounds of smooth jazz. Guitarist Pat Metheny mixed the sounds of blues, country, and “world” music, along with rock and jazz, playing both a flat-top acoustic guitar and an electric guitar with a softer, more mellow tone which was sweetened with a shimmering effect known as “chorusing". During the 1980s, a neo-traditional school of jazz sought to reconnect with the past. In keeping with such an aesthetic, young guitarists of this era sought a clean and round tone, and they often played traditional hollow-body arch-top guitars without electronic effects, frequently through vacuum tube amplifiers.
As players such as Bobby Broom, Peter Bernstein, Howard Alden, Russell Malone, and Mark Whitfield revived the sounds of traditional jazz guitar, there was also a resurgence of archtop luthierie (guitar-making). By the early 1990s many small independent luthiers began making archtop guitars. In the 2000s, jazz guitar playing continues to change. Some guitarists incorporate a Latin jazz influence, acid jazz-style dance club music uses samples from Wes Montgomery, and guitarists such as Bill Frisell continue to defy categorization.
Types of guitars.
Archtop guitars.
 While jazz can be played on any type of guitar, from an acoustic instrument to a solid-bodied electric guitar such as a Fender Stratocaster, the full-depth archtop guitar has become known as the prototypical "jazz guitar." Archtop guitars are steel-string acoustic guitars with a big soundbox, arched top, violin-style f-holes, a "floating bridge" and magnetic or piezoelectric pickups. Early makers of jazz guitars included Gibson, Epiphone, D'Angelico and Stromberg.
The earliest guitars used in jazz were acoustic, later superseded by a typical electric configuration of two humbucking pickups. In the 1990s, there was a resurgence of interest among jazz guitarists in acoustic archtop guitars with floating pickups.
The original acoustic archtop guitars were designed to enhance volume: for that reason they were constructed for use with relatively heavy guitar strings. Even after electrification became the norm, jazz guitarists continued to fit strings of 0.012" gauge or heavier for reasons of tone, and also prefer flatwound strings.
The characteristic arched top can be made of a solid piece of wood that is carved into the arched shape, or a piece of laminated wood (essentially a type of plywood) that is pressed into shape. Spruce is often used for tops, and maple for backs.
Archtop guitars can be mass-produced, such as the Ibanez Artcore series, or handmade by luthiers such as Robert Benedetto.
Musical ingredients.
Rhythm.
Jazz rhythm guitar often consists of very textural, odd-meter playing that includes generous use of exotic, difficult-to-fret chords. In 4/4 timing, it is common to play 2.5 beat intervals such as on the 2 and then the half beat or "and" after 4. Chords are not generally played in a repetitive rhythmic fashion, like a rock rhythm guitarist would play.
Harmony.
Jazz guitarists use their knowledge of harmony and jazz theory to create jazz chord "voicings," which emphasize the 3rd and 7th notes of the chord. Some more sophisticated chord voicings also include the 9th, 11th, and 13th notes of the chord. In some modern jazz styles, dominant 7th chords in a tune may contain altered 9ths (either flattened by a semitone, which is called a "flat 9th", or sharpened by a semitone, which is called a "sharp 9th"); 11ths (sharpened by a semitone, which is called a "sharp 11th"); 13ths (typically flattened by a semitone, which is called a "flat 13th").
Jazz guitarists need to learn about a range of different chords, including major 7th, major 6th, minor 7th, minor/major 7th, dominant 7th, diminished, half-diminished, and augmented chords. As well, they need to learn about chord transformations (e.g., altered chords, such as "alt dominant chords" described above), chord substitutions, and re-harmonization techniques. Some jazz guitarists use their knowledge of jazz scales and chords to provide a walking bass-style accompaniment.
Jazz guitarists learn to perform these chords over the range of different chord progressions used in jazz, such as the II-V-I progression, the jazz-style blues progression, the minor jazz-style blues form, the "rhythm changes" progression, and the variety of chord progressions used in jazz ballads, and jazz standards. Guitarists may also learn to use the chord types, strumming styles, and effects pedals (e.g., chorus effect or fuzzbox) used in 1970s-era jazz-Latin, jazz-funk, and jazz-rock fusion music.
Melody.
Jazz guitarists integrate the basic building blocks of scales and arpeggio patterns into balanced rhythmic and melodic phrases that make up a cohesive solo. Jazz guitarists often try to imbue their melodic phrasing with the sense of natural breathing and legato phrasing used by horn players such as saxophone players. As well, a jazz guitarists' solo improvisations have to have a rhythmic drive and "timefeel" that creates a sense of "swing" and "groove." The most experienced jazz guitarists learn to play with different "timefeels" such as playing "ahead of the beat" or "behind the beat," to create or release tension.
Another aspect of the jazz guitar style is the use of stylistically appropriate ornaments, such as grace notes, slides, and muted notes. Each subgenre or era of jazz has different ornaments that are part of the style of that subgenre or era. Jazz guitarists usually learn the appropriate ornamenting styles by listening to prominent recordings from a given style or jazz era. Some jazz guitarists also borrow ornamentation techniques from other jazz instruments, such as Wes Montgomery's borrowing of playing melodies in parallel octaves, which is a jazz piano technique. Jazz guitarists also have to learn how to add in passing tones, use "guide tones" and chord tones from the chord progression to structure their improvisations.
In the 1970s and 1980s, with jazz-rock fusion guitar playing, jazz guitarists incorporated rock guitar soloing approaches, such as riff-based soloing and usage of pentatonic and blues scale patterns. Some guitarists used Jimi Hendrix-influenced distortion and wah-wah effects to get a sustained, heavy tone, or even used rapid-fire guitar shredding techniques, such as tapping and tremolo bar bending. Guitarist Al Di Meola, who started his career with Return to Forever in 1974, was one of the first guitarists to perform in a "shred" style, a technique later used in rock and heavy metal playing. Di Meola used alternate-picking to perform very rapid sequences of notes in his solos.
Improvisation.
When jazz guitar players improvise, they use the scales, modes, and arpeggios associated with the chords in a tune's chord progression. The approach to improvising has changed since the earliest eras of jazz guitar. During the Swing era, many soloists improvised "by ear" by embellishing the melody with ornaments and passing notes. However, during the bebop era, the rapid tempo and complicated chord progressions made it increasingly harder to play "by ear." Along with other improvisers, such as saxes and piano players, bebop-era jazz guitarists began to improvise over the chord changes using scales (whole tone scale, chromatic scale, etc.) and arpeggios. Jazz guitar players tend to improvise around chord/scale relationships, rather than reworking the melody, possibly due to their familiarity with chords resulting from their comping role. A source of melodic ideas for improvisation is transcribing improvised solos from recordings. This provides jazz guitarists with a source of "licks", melodic phrases and ideas they incorporate either intact or in variations, and is an established way of learning from the previous generations of players.
Playing styles.
Big band rhythm.
In jazz big bands, popular during the 1930s and 1940s, the guitarist is considered an integral part of the rhythm section (guitar, drums and bass). They usually played a regular four chords to the bar, although an amount of harmonic improvisation is possible. Freddie Green, guitarist in the Count Basie orchestra, was a noted exponent this style. The harmonies are often minimal; for instance, the root note is often omitted on the assumption that it will be supplied by the bassist.
Small group comping.
When jazz guitarists play chords underneath a song's melody or another musician's solo improvisations, it is called "comping", short for "accompanying" The accompanying style in most jazz styles differs from the way chordal instruments accompany in many popular styles of music. In many popular styles of music, such as rock and pop, the rhythm guitarist usually performs the chords in rhythmic fashion which sets out the beat or groove of a tune. In contrast, in many modern jazz styles within smaller, the guitarist plays much more sparsely, intermingling periodic chords and delicate voicings into pauses in the melody or solo, and using periods of silence. Jazz guitarists commonly use a wide variety of inversions when comping, rather than only using standard voicings.
Chord-melody and unaccompanied soloing.
In this style, the guitarist aims to render an entire song — harmony, melody and bass — in something like the way a classical guitarist or pianist can. Chord roots cannot be left to the bassist in this style. Chords themselves can be used sparsely or more densely, depending on both the individual player and his or her arrangement of a particular piece. In the sparse style, a full chord is often played only at the beginning of a melodic phrase. The denser chordal textures, in contrast, approach chord soloing (see below). A third approach is to maintain a steady, busy bass-line, like a New Orleans pianist. Here, no more than two or three notes are played at a time, and the full harmony is indicated by arpeggiation. Exponents of this style often come from a country, folk or ragtime background, such as Chet Atkins, although it is also sometimes employed by straight-ahead jazz practitioners, for instance Martin Taylor. Chord-melody is often played with a plectrum (see Tal Farlow, George Benson and others); whereas fingerstyle, as practised by Joe Pass, George van Eps, Ted Greene, Robert Conti, Lenny Breau or hybrid picking as practised by Ed Bickert, Laszlo Sirsom and others allows for a more complex, polyphonic approach to unaccompanied soloing.
"Blowing" or single-note soloing.
Charlie Christian and Django Reinhardt are generally held to have initiated the use of the guitar to play melodies and improvisations over other instruments, the former using an early form of amplification, the latter playing forcefully on an acoustic guitar. Over the years, jazz guitarists have been able to solo in standard jazz idioms, such as bebop, cool jazz and so on, while in also absorbing influences from rock guitarists, such as the use of electronic effects.
Chord soloing.
Jazz guitarists are not limited to single note improvisation. When working with accompaniment, chord solos are created by improvising chords (harmony) and melody simultaneously, usually in the upper register on strings 1,2,3 and 4. Wes Montgomery was noted for playing successive choruses in single notes, then octaves and finally a chord solo - this can be heard in his improvisation on the standard Lover Man (Oh, Where Can You Be?). When playing without accompaniment, jazz guitarists may create chord solos by playing bass, melody and chords, individually or simultaneously, on any or all strings - such as the work of Lenny Breau, Joe Pass, Martin Taylor and others. This technique can be also be incorporated into unaccompanied soloing: for instance Django Reinhardt's "improvisations", as he called his solo pieces.
References.
http://www.acousticfingerstyle.com/ArchtopGuitars.htm
http://www.allmusic.com/artist/p102096
http://www.allmusic.com/artist/p7121

</doc>
<doc id="16148" url="http://en.wikipedia.org/wiki?curid=16148" title="John Engler">
John Engler

John Mathias Engler (born October 12, 1948) is an American politician and member of the Republican Party who served as the 46th Governor of Michigan from 1991 to 2003.
Engler has spent most of his adult life in government. He was serving in the Michigan Senate when he enrolled at Thomas M. Cooley Law School, and graduated with a Juris Doctor degree, having served as a Michigan State Senator since 1979. He was elected Senate Majority Leader in 1984 and served there until elected governor in 1990.
Engler serves on the Board of Advisors of the Russell Kirk Center for Cultural Renewal, an educational organization that continues the intellectual legacy of noted conservative icon and Michigan native Russell Kirk. Engler also serves on the Board of Trustees of the Marguerite Eyer Wilbur Foundation, which funds many Kirk Center programs.
Engler is a member of the Annie E Casey Foundation Board of Trustees and a member of the Board of Directors of Universal Forest Products Inc. headquartered in Grand Rapids, Michigan.
In 2013, Engler joined the advisory board of Blackford Capital's Michigan Prosperity Fund as chairman. 
Early life and education.
Engler, a Roman Catholic, was born in Mount Pleasant, Michigan on October 12, 1948 to Mathias John Engler and his wife, Agnes Marie (née Neyer), but grew up on a cattle farm near Beal City. He attended Michigan State University and graduated with a degree in agricultural economics in 1971, and Thomas M. Cooley Law School, graduating with a Doctor of Jurisprudence degree, in 1981. He was elected to the Michigan House of Representatives as a State Representative in 1970 at the age of 22. He served in the House from 1971–78. His campaign manager in that first election was a college friend, Dick Posthumus. He later became the 1st Republican youth vice-chair for the Michigan Republican Party, defeating future U.S. Senator Spencer Abraham. Posthumus later went on to be elected State Senator, Senate Majority Leader and Lieutenant Governor. He was Engler's running mate in the 1998 election and served from 1999 to 2003. 
Career.
Governorship.
His administration was characterized by privatization of state services, income tax reduction, a sales tax increase, educational reform, welfare reform and major reorganization of executive branch departments. In 1996 he was elected Chairman of the Republican Governors Association and in 2001 he was elected to head the National Governors Association.
1996 Presidential election.
During the 1996 presidential campaign, Engler was considered by many political commentators and experts to be a serious potential vice presidential running mate for Republican nominee Bob Dole. Eventually, however, Dole instead selected Jack Kemp, a former congressman and HUD Secretary.
2000 Presidential election.
Engler endorsed Texas Governor George W. Bush in the 2000 Republican primary. After Bush secured the GOP nomination, Engler's name began to surface as a possible running mate for Bush. In his book "Decision Points", Bush says that Engler was someone he was "close" with and could "work well with." Ultimately, Engler was passed over for the running mate position in favor of Dick Cheney. After the election, Engler's close political ally, Spencer Abraham, who narrowly lost his re-election bid for the Senate to Debbie Stabenow, was chosen as Bush's Secretary of Energy.
2002 Elections and post-gubernatorial work.
Engler's lieutenant governor, Dick Posthumus, sought to succeed Engler in the 2002 gubernatorial race. Posthumus lost a close race to the state's Attorney General, Democrat Jennifer Granholm. After leaving the governor's mansion in January 2003, Engler served as President of the state and local government sector of Electronic Data Systems. He left that post August 31, 2004. 
In September 2004, Engler was elected President & CEO of the National Association of Manufacturers. Engler's six plus year tenure at the NAM ended in January 2011. In January 2011, Engler was named the President of the Business Roundtable.
Election results.
In 1990, State Senate Majority Leader John Engler challenged Governor James Blanchard in his bid for a third term. Political observers viewed his bid as a long shot, and he trailed Blanchard by double digits in the polls the weekend before the election. However, on election day Engler pulled off the upset, defeating Blanchard by approximately 17,000 votes – less than one percentage point. In 1994 Engler ran for his second term. The Democrats nominated former Representative Howard Wolpe, who had close ties to labor movement – a potent force in Democratic politics in Michigan. Engler bested Wolpe 61%–39%, and the state Republican Party made significant gains. Spencer Abraham picked up the Senate seat of retiring Democrat Donald W. Riegle, Jr.. Republicans gained a seat to break a tie in the state House of Representatives and take a 56–54 majority, while also picking up a seat in the U.S. House of Representatives. Republican Candice Miller won an upset victory to win the post of Secretary of State. 
Michigan voters re-elected Engler to his third and final term in 1998. He won a landslide victory over lawyer Geoffrey Fieger. Engler took 1,883,005 votes – 62 % of the total – to Fieger's 38 percent and 1,143,574 votes. Engler's landslide helped the state Republican Party to gain six seats in the state House of Representatives, taking control of the chamber they had lost two years previously with a 58–52 margin, as well as picking up an additional seat in the State Senate, for a 23–15 majority. Republicans also gained a seat on the technically non-partisan state Supreme Court, holding a 4–3 majority over the Democrats.
Personal life.
Engler was married to Colleen House Engler in 1975. Colleen Engler served in the Michigan House of Representatives and ran for lieutenant governor of Michigan in 1986. She filed for divorce in 1986.
Engler married Michelle DeMunbrun, a Texas attorney, on December 8, 1990. The couple have triplet daughters born November 13, 1994. As First Lady, Michelle Engler chaired the Michigan Community Service Commission which gained national recognition under her leadership for its innovative work in expanding volunteer opportunities across Michigan. A significant contribution to the Commission's work came from the inspiring example set by former Michigan Governor George Romney. Michelle Engler was named to the Federal Home Loan Mortgage Corporation (Freddie Mac) board in 2001 by President George W. Bush and re-appointed in 2002.

</doc>
<doc id="16273" url="http://en.wikipedia.org/wiki?curid=16273" title="James Stewart">
James Stewart

James Maitland "Jimmy" Stewart (May 20, 1908 – July 2, 1997) was an American film and stage actor, known for his distinctive drawl voice and down-to-earth persona. Over the course of his career, he starred in many films widely considered classics. He was known for portraying the American middle class man with everyday life struggles.
Stewart was nominated for five Academy Awards, winning one in competition and receiving one Lifetime Achievement award. Stewart was named the third greatest male screen legend in cinema history by the American Film Institute. He was a major Metro-Goldwyn-Mayer contract star. He also had a noted military career and was a World War II and Vietnam War veteran, who rose to the rank of Brigadier General in the United States Air Force Reserve.
The actor Cary Grant said of Stewart's acting technique:He had the ability to talk naturally. He knew that in conversations people "do" often interrupt one another and it's not always so easy to get a thought out. It took a little time for the sound men to get used to him, but he had an "enormous" impact. And then, some years later, Marlon came out and did the same thing all over again—but what people forget is that Jimmy did it first.
Early life and career.
James Maitland Stewart was born on May 20, 1908, in Indiana, Pennsylvania, the son of Elizabeth Ruth Jackson (1875–1953) and Alexander Maitland Stewart (1871–1961), who owned a hardware store. Stewart had Scottish and Irish ancestry, and was raised as a Presbyterian. He was descended from veterans of the American Revolution, the War of 1812, and the American Civil War. The eldest of three children (he had two younger sisters, Virginia and Mary), he was expected to continue his father's business, which had been in the family for three generations. His mother was an excellent pianist but his father discouraged Stewart's request for lessons. When his father accepted a gift of an accordion from a guest, young Stewart quickly learned to play the instrument, which became a fixture offstage during his acting career. As the family grew, music continued to be an important part of family life.
Stewart attended Mercersburg Academy prep school, graduating in 1928. He was active in a variety of activities. He played on the football and track teams, was art editor of the "KARUX" yearbook, and a member of the choir club, glee club, and John Marshall Literary Society. During his first summer break, Stewart returned to his hometown to work as a brick loader for a local construction company and on highway and road construction jobs where he painted lines on the roads. Over the following two summers, he took a job as an assistant with a professional magician. He made his first appearance onstage at Mercersburg, as Buquet in the play "The Wolves".
A shy child, Stewart spent much of his after-school time in the basement working on model airplanes, mechanical drawing and chemistry—all with a dream of going into aviation. It was a dream greatly enhanced by the legendary 1927 flight of Charles Lindbergh, whose progress 19-year-old Stewart, then stricken with scarlet fever, was himself avidly following from home; thus foreshadowing his starring movie role as Lindbergh 30 years later.
However, he abandoned visions of being a pilot when his father insisted that instead of the United States Naval Academy he attend Princeton University. Stewart enrolled at Princeton in 1928 as a member of the class of 1932. He excelled at studying architecture, so impressing his professors with his thesis on an airport design that he was awarded a scholarship for graduate studies; but he gradually became attracted to the school's drama and music clubs, including the Princeton Triangle Club. His acting and accordion talents at Princeton led him to be invited to the University Players, an intercollegiate summer stock company in West Falmouth, Massachusetts, on Cape Cod. The company had been organized in 1928 and would run until 1932, with Joshua Logan, Bretaigne Windust and Charles Leatherbee as directors. Stewart performed in bit parts in the Players' productions in Cape Cod during the summer of 1932, after he graduated.
The troupe had previously included Henry Fonda, who married Margaret Sullavan on Christmas Day 1931, while the players were in Baltimore, Maryland, for an 18-week winter season. Sullavan, who had rejoined the Players in Baltimore in November 1931 at the close of the post-Broadway tour of "A Modern Virgin", left the Players for good at the end of "The Trial of Mary Dugan" in Baltimore in March 1932. By the time Stewart joined the University Players on Cape Cod after his graduation from Princeton in 1932, Fonda and Sullavan's brief marriage had ended. Stewart and Fonda became great friends over the summer of 1932 when they shared an apartment with Joshua Logan and Myron McCormick. When Stewart came to New York at the end of the summer stock season, which had included the Broadway tryout of "Goodbye Again", he shared an apartment with Fonda, who had by then finalized his divorce from Sullavan. Along with fellow University Players Alfred Dalrymple and Myron McCormick, Stewart debuted on Broadway in the brief run of "Carry Nation" and a few weeks later – again with McCormick and Dalrymple – as a chauffeur in the comedy "Goodbye Again", in which he had two lines. "The New Yorker" noted, "Mr. James Stewart's chauffeur... comes on for three minutes and walks off to a round of spontaneous applause."
The play was a moderate success, but times were hard. Many Broadway theaters had been converted to movie houses and the Depression was reaching bottom. "From 1932 through 1934", Stewart later recalled, "I'd only worked three months. Every play I got into folded." By 1934, he was given more substantial stage roles, including the modest hit "Page Miss Glory" and his first dramatic stage role in Sidney Howard's "Yellow Jack", which convinced him to continue his acting career. However, Stewart and Fonda, still roommates, were both struggling. In the fall of 1934, Fonda's success in "The Farmer Takes a Wife" took him to Hollywood. Finally, Stewart attracted the interest of MGM scout Bill Grady who saw Stewart on the opening night of "Divided by Three", a glittering première with many luminaries in attendance, including Irving Berlin, Moss Hart and Fonda, who had returned to New York for the show. With Fonda's encouragement, Stewart agreed to take a screen test, after which he signed a contract with MGM in April 1935, as a contract player for up to seven years at $350 a week.
Upon Stewart's arrival by train in Los Angeles, Fonda greeted him at the station and took him to Fonda's studio-supplied lodging, next door to Greta Garbo. Stewart's first job at the studio was as a participant in screen tests with newly arrived starlets. At first, he had trouble being cast in Hollywood films owing to his gangling looks and shy, humble screen presence. Aside from an unbilled appearance in a Shemp Howard comedy short called "Art Trouble" in 1934, his first film was the poorly received Spencer Tracy vehicle, "The Murder Man" (1935). "Rose Marie" (1936), an adaptation of a popular operetta, was more successful. After having mixed success in films, he received his first intensely dramatic role in 1936's "After the Thin Man", and played Jean Harlow's character's frustrated boyfriend in the Clark Gable vehicle "Wife vs. Secretary" earlier that same year.
On the romantic front, he dated newly divorced Ginger Rogers. The romance soon cooled, however, and by chance Stewart encountered Margaret Sullavan again. Stewart found his footing in Hollywood thanks largely to Sullavan, who campaigned for Stewart to be her leading man in the 1936 romantic comedy "Next Time We Love". She rehearsed extensively with him, having a noticeable effect on his confidence. She encouraged Stewart to feel comfortable with his unique mannerisms and boyish charm and use them naturally as his own style. Stewart was enjoying Hollywood life and had no regrets about giving up the stage, as he worked six days a week in the MGM factory. In 1936, he acquired big-time agent Leland Hayward, who would eventually marry Sullavan. Hayward started to chart Stewart's career, deciding that the best path for him was through loan-outs to other studios.
Prewar success.
In 1938, Stewart had a brief, tumultuous romance with Hollywood queen Norma Shearer, whose husband, Irving Thalberg, head of production at MGM, had died two years earlier. Stewart began a successful partnership with director Frank Capra in 1938, when he was loaned out to Columbia Pictures to star in "You Can't Take It With You". Capra had been impressed by Stewart's minor role in "Navy Blue and Gold" (1937). The director had recently completed several popular movies, including "It Happened One Night" (1934), and was looking for the right actor to suit his needs—other recent actors in Capra's films such as Clark Gable, Ronald Colman, and Gary Cooper did not quite fit. Not only was Stewart just what he was looking for, but Capra also found Stewart understood that prototype intuitively and required very little directing. Later Capra commented, "I think he's probably the best actor who's ever hit the screen."
"You Can't Take It With You", starring Capra's "favorite actress", comedienne Jean Arthur, won the 1938 Best Picture Academy Award. The following year saw Stewart work with Capra and Arthur again in the political comedy-drama "Mr. Smith Goes to Washington". Stewart replaced intended star Gary Cooper in the film, playing an idealist thrown into the political arena. Upon its October 1939 release, the film garnered critical praise and became a box-office success. Stewart was nominated for the first of five Academy Awards for Best Actor. Stewart's father was still trying to talk him into leaving Hollywood and its sinful ways and to return to his home town to lead a decent life. Stewart took a secret trip to Europe to take a break and returned home in 1939 just as Germany invaded Poland.
You hear so much about the old movie moguls and the impersonal factories where there is no freedom. MGM was a wonderful place where decisions were made on my behalf by my superiors. What's wrong with that?
James Stewart
"Destry Rides Again", also released in 1939, became Stewart's first western film, a genre with which he would become identified later in his career. In this western parody, he is a pacifist lawman and Marlene Dietrich is the dancing saloon girl who comes to love him, but doesn't get him. In the film, Dietrich sings her famous song "See What the Boys in the Back Room Will Have". Off-screen, Dietrich did get her man, but the romance was short-lived. "Made for Each Other" (1939) had Stewart sharing the screen with irrepressible Carole Lombard in a melodrama that garnered good reviews for both stars, but did less well with the public. "Newsweek" wrote that they were "perfectly cast in the leading roles." Between movies, Stewart began a radio career and became a distinctive voice on the Lux Radio Theater's "The Screen Guild Theater" and other shows. So well-known had his slow drawl become that comedians began impersonating him.
In 1940, Stewart and Sullavan reunited for two films. The first, the Ernst Lubitsch romantic comedy, "The Shop Around the Corner", starred Stewart and Sullavan as co-workers unknowingly involved in a pen-pal romance but who cannot stand each other in real life (this was subsequently remade into the musical, "In the Good Old Summertime" with Judy Garland and Van Johnson, and later as the romantic comedy "You've Got Mail" with Tom Hanks and Meg Ryan). It was Stewart's fifth film of the year and that rare film shot in sequence; it was completed in only 27 days. "The Mortal Storm", directed by Frank Borzage, was one of the first blatantly anti-Nazi films to be produced in Hollywood and featured Sullavan and Stewart as friends and then lovers caught in turmoil upon Hitler's rise to power, literally hunted down by their own friends.
Stewart also starred with Cary Grant and Katharine Hepburn in George Cukor's classic "The Philadelphia Story" (1940). His performance as an intrusive, fast-talking reporter earned him his only Academy Award in a competitive category (Best Actor, 1941), and he beat out his good friend Henry Fonda ("The Grapes of Wrath"). Stewart thought his performance "entertaining and slick and smooth" but lacking the "guts" of "Mr. Smith." Stewart gave the Oscar statuette to his father, who displayed it for many years in a case inside the front door of his hardware store, alongside other family awards and military medals.
During the months before he began military service, Stewart appeared in a series of screwball comedies with varying levels of success. He followed the mediocre "No Time for Comedy" (1940) with Rosalind Russell and "Come Live with Me" (1941) with Hedy Lamarr with the Judy Garland musical, "Ziegfeld Girl", and the George Marshall romantic comedy "Pot o' Gold" featuring Paulette Goddard. Stewart was drafted in late 1940, a situation that coincided with the lapse in his MGM contract, marking a turning point in Stewart's career, with 28 movies to his credit at that point.
Military service.
The Stewart and Jackson families had deep military roots as both grandfathers had fought in the Civil War, and his father had served during both the Spanish–American War and World War I. Stewart considered his father to be the biggest influence on his life, so it was not surprising that, when another war came, he too was eager to serve. Members of his family had previously been in the infantry, but Stewart chose to become a military flier.
An early interest in flying led Stewart to gain his Private Pilot certificate in 1935 and Commercial Pilot certificate in 1938. He often flew cross-country to visit his parents in Pennsylvania, navigating by the railroad tracks. Nearly two years before the December 1941 attack on Pearl Harbor, Stewart had accumulated over 400 hours of flying time.
Considered a highly proficient pilot, he entered a cross-country race as a co-pilot in 1939. Stewart, along with musician/composer Hoagy Carmichael, saw the need for trained war pilots, and joined with other Hollywood celebrities to invest in Thunderbird Field, a pilot-training school built and operated by Southwest Airways in Glendale, Arizona. This airfield became part of the United States Army Air Forces training establishment and trained more than 10,000 pilots during World War II, and is now the home of Thunderbird School of Global Management.
In October 1940, Stewart was drafted into the United States Army but was rejected for failing to meet height and weight requirements for new recruits—Stewart was five pounds (2.3 kg) under the standard. To get up to 148 pounds, he sought out the help of Metro-Goldwyn-Mayer's muscle man and trainer Don Loomis, who was noted for his ability to add or subtract pounds in his studio gymnasium. Stewart subsequently attempted to enlist in the Air Corps, but still came in under the weight requirement, although he persuaded the enlistment officer to run new tests, this time passing the weigh-in, with the result that Stewart enlisted and was inducted in the Army on March 22, 1941. He became the first major American movie star to wear a military uniform in World War II.
Stewart enlisted as a private but as both a college graduate and a licensed commercial pilot applied for an Air Corps commission and pilot rating. Soon to be 33, he was almost six years beyond the maximum age restriction for aviation cadet training, the normal path of commissioning for pilots. Stewart received his commission as a second lieutenant on January 19, 1942, shortly after the attack on Pearl Harbor, while a corporal at Moffett Field, California. He also received a pilot rating, although the circumstances are unclear, since he did not participate in the standard pilot training program. Stewart's first assignment was an appearance at a March of Dimes rally in Washington, D.C., but Stewart desired assignment to an operational unit rather than serve as a recruiting symbol. He applied for and was granted advanced training in multi-engined aircraft. Stewart was posted to nearby Mather Field to instruct in both single- and twin-engined aircraft.
Public appearances by Stewart were limited engagements scheduled by the Army Air Forces. "Stewart appeared several times on network radio with Edgar Bergen and Charlie McCarthy. Shortly after Pearl Harbor, he performed with Orson Welles, Edward G. Robinson, Walter Huston and Lionel Barrymore in an all-network radio program called "We Hold These Truths", dedicated to the 150th anniversary of the Bill of Rights." In early 1942, Stewart was asked to appear in a film to help recruit the anticipated 100,000 airmen that the USAAF would need to win the war. The USAAF's First Motion Picture Unit shot scenes of Lieutenant Stewart in his pilot's flight jacket and recorded his voice for narration. The short propaganda film, "Winning Your Wings", appeared nationwide beginning in late May and was very successful, resulting in 150,000 new recruits.
Stewart was concerned that his expertise and celebrity status would relegate him to instructor duties "behind the lines." His fears were confirmed when after his promotion to first lieutenant on July 7, 1942, he was stationed from August to December 1942 at Kirtland Army Airfield in Albuquerque, New Mexico, piloting AT-11 Kansans used in training bombardiers. He was transferred to Hobbs Army Airfield, New Mexico, for three months of transition training in the four-engine B-17 Flying Fortress, then sent to the Combat Crew Processing Center in Salt Lake City, where he expected to be assigned to a combat unit. Instead he was assigned in early 1943 to an operational training unit, the 29th Bombardment Group at Gowen Field, Boise, Idaho, as an instructor. He was promoted to captain on July 9, 1943, and appointed a squadron commander. For Stewart, now 35, combat duty seemed far away and unreachable and he had no clear plans for the future. However, a rumor that Stewart would be taken off flying status and assigned to making training films or selling bonds called for immediate action, because what he dreaded most was "the hope-shattering spectre of a dead end." Stewart appealed to his commander, 30-year-old Lt. Col. Walter E. Arnold Jr., who understood his situation and recommended Stewart to the commander of the 445th Bombardment Group, a B-24 Liberator unit that had just completed initial training at Gowen Field and gone on to final training at Sioux City Army Air Base, Iowa.
In August 1943, Stewart was assigned to the 445th Bomb Group as operations officer of the 703d Bombardment Squadron, but after three weeks became its commander. On October 12, 1943, judged ready for overseas movement, the 445th Bomb Group staged to Lincoln Army Airfield, Nebraska. Flying individually, the aircraft first flew to Morrison Army Airfield, Florida, and then on the circuitous Southern Route along the coasts of South America and Africa to RAF Tibenham, Norfolk, England. After several weeks of training missions, in which Stewart flew with most of his combat crews, the group flew its first combat mission on December 13, 1943, to bomb the U-boat facilities at Kiel, Germany, followed three days later by a mission to Bremen. Stewart led the high squadron of the group formation on the first mission, and the entire group on the second. Following a mission to Ludwigshafen, Germany, on January 7, 1944, Stewart was promoted to major. Stewart was awarded the Distinguished Flying Cross for actions as deputy commander of the 2nd Combat Bombardment Wing on the first day of "Big Week" operations in February and flew two other missions that week.
On March 22, 1944, Stewart flew his 12th combat mission, leading the 2nd Bomb Wing in an attack on Berlin. On March 30, 1944, he was sent to RAF Old Buckenham to become group operations officer of the 453rd Bombardment Group, a new B-24 unit that had just lost both its commander and operations officer on missions. As a means to inspire the unit, Stewart flew as command pilot in the lead B-24 on several missions deep into Nazi-occupied Europe. As a staff officer, Stewart was assigned to the 453rd "for the duration" and thus not subject to a quota of missions of a combat tour. He nevertheless assigned himself as a combat crewman on the group's missions until his promotion to lieutenant colonel on June 3 and reassignment on July 1, 1944, to the 2nd Bomb Wing, assigned as executive officer to Brigadier General Edward J. Timberlake. His official tally of mission credits while assigned to the 445th and 453rd Bomb Groups totaled 20 sorties.
Stewart continued to make missions, uncredited, flying with the pathfinder squadron of the 389th Bombardment Group, with his two former groups, and with groups of the 20th Combat Bomb Wing. He received a second award of the Distinguished Flying Cross for actions in combat and was awarded the Croix de Guerre. He also received the Air Medal with three oak leaf clusters. Stewart served in a number of staff positions in the 2nd and 20th Bomb Wings between July 1944 and the end of the war in Europe, and was promoted to full colonel on March 29, 1945. On May 10, 1945, he succeeded to command of the 2nd Bomb Wing, a position he held until June 15. Stewart was one of the few Americans to rise from private to colonel in four years.
At the beginning of June 1945, Stewart was the presiding officer of the court-martial of a pilot and navigator who were charged with dereliction of duty for having accidentally bombed the Swiss city of Zurich the previous March—the first instance of U.S. personnel being tried for an attack on a neutral country. The Court acquitted the defendants.
Stewart continued to play a role in the United States Air Force Reserve after the war. Stewart received permanent promotion to colonel in 1953 and served as Air Force Reserve commander of Dobbins Air Reserve Base. He was also one of the 12 founders and a charter member of the Air Force Association in October 1945. Stewart rarely spoke about his wartime service but did appear in January 1974 in an episode of the TV series "The World At War", "Whirlwind: Bombing Germany (September 1939 – April 1944)", commenting on the disastrous mission of October 14, 1943, against Schweinfurt, Germany. At his request, he was identified only as "James Stewart, Squadron Commander" in the documentary.
On July 23, 1959, Stewart was promoted to Brigadier General. During his active duty periods, he remained current as a pilot of Convair B-36 Peacemaker, Boeing B-47 Stratojet and B-52 Stratofortress intercontinental bombers of the Strategic Air Command. On February 20, 1966, Brigadier General Stewart flew as a non-duty observer in a B-52 on an Arc Light bombing mission during the Vietnam War. At the time of his B-52 flight, he refused the release of any publicity regarding his participation, as he did not want it treated as a stunt, but as part of his job as an officer in the Air Force Reserve. After 27 years of service, Stewart retired from the Air Force on May 31, 1968. He was promoted to major general on the retired list by President Ronald Reagan.
Military honors.
Stewart received a number of awards during his military service.
Postwar career.
After the war, Stewart took time off to reassess his career. He was an early investor in Southwest Airways, founded by Leland Hayward, and considered going into the aviation industry if his restarted film career did not prosper. Upon Stewart's return to Hollywood in fall 1945, he decided not to renew his MGM contract. He signed with the MCA talent agency. His former agent Leland Hayward got out of the talent business in 1944 after selling his A-list of stars, including Stewart, to MCA.
For his first film in five years, Stewart appeared in his third and final Frank Capra production, "It's a Wonderful Life". Capra paid RKO for the rights to the story and formed his own production company, Liberty Films. The female lead went to Donna Reed, after Capra's perennial first choice, Jean Arthur, was unavailable, and after Ginger Rogers, Olivia de Havilland, Ann Dvorak and Martha Scott had all turned down the role. Stewart appeared as George Bailey, an upstanding small-town man who becomes increasingly frustrated by his ordinary existence and financial troubles. Driven to suicide on Christmas Eve, he is led to reassess his life by Clarence Odbody AS2, an "angel, second class", played by Henry Travers.
Although "It's a Wonderful Life" was nominated for five Academy Awards, including Stewart's third Best Actor nomination, it received mixed reviews and only disappointingly moderate success at the box office. However, in the decades since the film's release, it grew to define Stewart's film persona and is widely considered as a sentimental Christmas film classic and, according to the American Film Institute, one of the best movies ever made. In an interview with Michael Parkinson in 1973, Stewart declared that out of all the movies he had made, "It's a Wonderful Life" was his favorite. After viewing the film, President Harry S Truman concluded, "If Bess and I had a son we'd want him to be just like Jimmy Stewart." In the aftermath of the film, Capra's production company went into bankruptcy, while Stewart started to have doubts about his ability to act after his military hiatus. His father kept insisting he come home and marry a local girl. Meanwhile in Hollywood, his generation of actors were fading and a new wave of actors would soon remake the town, including Marlon Brando, Montgomery Clift and James Dean.
"Magic Town" (1947), a comedy film directed by William A. Wellman, starring James Stewart and Jane Wyman, was one of the first films about the then-new science of public opinion polling. It was poorly received. He completed "Rope" (1948) directed by Alfred Hitchcock and "Call Northside 777" (1948), and weathered two box-office disappointments with "On Our Merry Way" (1948), a comedic musical ensemble in which Stewart and Henry Fonda were paired as two jazz musicians named "Slim" and "Lank," and "You Gotta Stay Happy" (1949), for which the posters depicted Stewart being kissed on one cheek by Joan Fontaine and on the other by a chimpanzee. In the documentary film "James Stewart: A Wonderful Life" (1987), hosted by Johnny Carson, Stewart said that he went back to Westerns in 1950 in part because of a string of films that were flops.
He returned to the stage to star in Mary Coyle Chase's "Harvey", which had opened to nearly universal praise in November 1944, as Elwood P. Dowd, a wealthy eccentric living with his sister and niece, and whose best friend is an invisible rabbit as large as a man. Dowd's eccentricity, especially the friendship with the rabbit, is ruining the niece's hopes of finding a husband. While trying to have Dowd committed to a sanatorium, his sister is committed herself while the play follows Dowd on an ordinary day in his not-so-ordinary life. Stewart took over the role from Frank Fay and gained an increased Broadway following in the unconventional play. The play, which ran for nearly three years with Stewart as its star, was successfully adapted into a 1950 film, directed by Henry Koster, with Stewart as Dowd and Josephine Hull as his sister, Veta. Bing Crosby was the first choice but he declined. Stewart reprised the role on the London stage in 1975.
Stewart received his fourth Best Actor nomination for his performance in the film. After "Harvey", the comedic adventure film "Malaya" (1949) with Spencer Tracy and the conventional but highly successful biographical film "The Stratton Story" in 1949, Stewart's first pairing with "on-screen wife" June Allyson, his career took another turn. During the 1950s, he expanded into the western and suspense genres, thanks to collaborations with Alfred Hitchcock and Anthony Mann.
Other notable performances by Stewart during this time include the critically acclaimed 1950 Delmer Daves western "Broken Arrow", which featured Stewart as an ex-soldier and Indian agent making peace with the Apache; a troubled clown in the 1952 Best Picture "The Greatest Show on Earth"; and Stewart's role as Charles Lindbergh in Billy Wilder's 1957 film "The Spirit of St. Louis". He also starred in the western radio show "The Six Shooter" for its one-season run from 1953 to 1954. During this time Stewart wore the same cowboy hat and rode the same horse, named "Pie", in most of his Westerns.
Collaborations with Hitchcock and Mann.
Stewart's collaborations with director Anthony Mann increased Stewart's popularity and sent his career into the realm of the western. Stewart's first appearance in a film directed by Mann came with the 1950 western, "Winchester '73". In choosing Mann (after first choice Fritz Lang declined), Stewart cemented a powerful partnership. The film, which became a massive box-office hit upon its release, set the pattern for their future collaborations. In it, Stewart is a tough, revengeful sharpshooter, the winner of a prized rifle which is stolen and then passes through many hands, until the showdown between Stewart and his brother (Stephen McNally).
Other Stewart–Mann westerns, such as "Bend of the River" (1952), "The Naked Spur" (1953), "The Far Country" (1954) and "The Man from Laramie" (1955), were perennial favorites among young audiences entranced by the American West. Frequently, the films featured Stewart as a troubled cowboy seeking redemption, while facing corrupt cattlemen, ranchers and outlaws—a man who knows violence first hand and struggles to control it. The Stewart–Mann collaborations laid the foundation for many of the westerns of the 1950s and remain popular today for their grittier, more realistic depiction of the classic movie genre. Audiences saw Stewart's screen persona evolve into a more mature, more ambiguous, and edgier presence.
Stewart and Mann also collaborated on other films outside the western genre. 1954's "The Glenn Miller Story" was critically acclaimed, garnering Stewart a BAFTA Award nomination, and (together with "The Spirit of St. Louis") cemented the popularity of Stewart's portrayals of 'American heroes'. "Thunder Bay", released the same year, transplanted the plot arc of their western collaborations to a more contemporary setting, with Stewart as a Louisiana oil driller facing corruption. "Strategic Air Command", released in 1955, allowed Stewart to use his experiences in the United States Air Force on film.
Stewart's starring role in "Winchester '73" was also a turning point in Hollywood. Universal Studios, who wanted Stewart to appear in both that film and "Harvey," balked at his $200,000 asking price. His agent, Lew Wasserman, brokered an alternate deal, in which Stewart would appear in both films for no pay, in exchange for a percentage of the profits as well as cast and director approval. Stewart ended up earning about $600,000 for "Winchester '73" alone. Hollywood's other stars quickly capitalized on this new way of doing business, which further undermined the decaying "studio system".
The second collaboration to define Stewart's career in the 1950s was with acclaimed mystery and suspense director Alfred Hitchcock. Like Mann, Hitchcock uncovered new depths to Stewart's acting, showing a protagonist confronting his fears and his repressed desires. Stewart's first movie with Hitchcock was the technologically innovative 1948 film "Rope", shot in long "real time" takes.
The two collaborated for the second of four times on the 1954 hit "Rear Window", widely considered one of Hitchcock's masterpieces. Stewart portrays photographer L.B. "Jeff" Jeffries, loosely based on "Life" photographer Robert Capa, who projects his fantasies and fears onto the people he observes out his apartment window while on hiatus due to a broken leg. Jeffries gets into more than he can handle, however, when he believes he has witnessed a salesman (Raymond Burr) commit a murder, and when his glamorous girlfriend (Grace Kelly), at first disdainful of his voyeurism and skeptical about any crime, eventually is drawn in and tries to help solve the mystery. Limited by his wheelchair, Stewart is led by Hitchcock to react to what his character sees with mostly facial responses. It was a landmark year for Stewart, becoming the highest grossing actor of 1954 and the most popular Hollywood star in the world, displacing John Wayne. Hitchcock and Stewart formed a corporation, Patron Inc., to produce the film, which later became the subject of a Supreme Court case "Stewart v. Abend" (1990).
After starring in Hitchcock's remake of the director's earlier production, "The Man Who Knew Too Much" (1956), with Doris Day, Stewart starred, with Kim Novak, in what many consider Hitchcock's most personal film, "Vertigo" (1958). The movie starred Stewart as John "Scottie" Ferguson, a former police investigator suffering from acrophobia, who develops an obsession with a woman he is shadowing. Scottie's obsession inevitably leads to the destruction of everything he once had and believed in. Though the film is widely considered a classic today, "Vertigo" met with very mixed reviews and poor box-office receipts upon its release, and marked the last collaboration between Stewart and Hitchcock. The director reportedly blamed the film's failure on Stewart looking too old to be Kim Novak's love interest, and cast Cary Grant as Roger Thornhill in "North by Northwest" (1959), a role Stewart had very much wanted. (Grant was actually four years older than Stewart but photographed much younger.) Today, "Vertigo" is ranked highest in the 2012 "Sight & Sound" critics poll for the greatest films ever made, taking the title from veteran favorite "Citizen Kane".
Career in the 1960s and 1970s.
In 1960, Stewart was awarded the New York Film Critics Circle Award for Best Actor and received his fifth and final Academy Award for Best Actor nomination, for his role in the 1959 Otto Preminger film "Anatomy of a Murder". The early courtroom drama stars Stewart as Paul Biegler, the lawyer of a hot-tempered soldier (played by Ben Gazzara) who claims temporary insanity after murdering a tavern owner who raped his wife. The film featured a career-making performance by George C. Scott as the prosecutor. The film was considered quite explicit for its time, and it was a box-office success. Stewart's nomination was one of seven for the film (Charlton Heston was the winner with "Ben-Hur"), and saw his transition into the final decades of his career. It is ranked as one of the best trial films of all time.
On January 1, 1960, Stewart received news of the death of Margaret Sullavan (her death was later identified as apparent suicide; the county coroner, however, officially ruled the death an accident). As a friend, mentor, and focus of his early romantic feelings, she had a unique influence on Stewart's life. On April 17, 1961, longtime friend Gary Cooper was too ill to attend the 33rd Academy Awards ceremony, so Stewart accepted the honorary Oscar on his behalf. Stewart's emotional speech hinted that something was seriously wrong, and the next day newspapers ran the headline, "Gary Cooper has cancer." One month later, on May 13, 1961, six days after his 60th birthday, Cooper died.
In the early 1960s, Stewart took leading roles in three John Ford films, his first work with the acclaimed director. The first, "Two Rode Together", paired him with Richard Widmark in a Western with thematic echoes of Ford's "The Searchers". The next, 1962's "The Man Who Shot Liberty Valance", Stewart's first picture with John Wayne, is a classic "psychological" western, shot in black and white film noir style featuring powerful use of shadows in the climactic sequence, with Stewart as an Eastern attorney who goes against his non-violent principles when he is forced to confront a psychopathic outlaw (played by Lee Marvin) in a small frontier town. At story's end, Stewart's character — now a rising political figure — faces a difficult ethical choice as he attempts to reconcile his actions with his personal integrity. The film's billing is unusual in that Stewart was given top billing over Wayne in the trailers and on the posters but Wayne was listed above Stewart in the film itself. The complex picture garnered mixed reviews but was an instant hit at the box office and became a critical favorite over the ensuing decades.
"How the West Was Won" (which Ford co-directed, though without directing Stewart's scenes) and "Cheyenne Autumn" were western epics released in 1962 and 1964 respectively. One of only a handful of movies filmed in true Cinerama, shot with three cameras and exhibited with three simultaneous projectors in theatres, "How the West Was Won" went on to win three Oscars and reap massive box-office figures. "Cheyenne Autumn", in which a white-suited Stewart played Wyatt Earp in a long semi-comedic sequence in the middle of the movie, failed domestically and was quickly forgotten. The historical drama was Ford's final Western and Stewart's last feature film with Ford. Stewart's entertainingly memorable middle sequence is not directly connected with the rest of the film and was often excised from the lengthy film in later theatrical exhibition prints and some television broadcasts.
Having played his last romantic lead in "Bell, Book and Candle" (1958), and silver-haired (although not all was his—he wore a partial hairpiece starting with "It's a Wonderful Life" and in every film thereafter), Stewart transitioned into more family-related films in the 1960s when he signed a multi-movie deal with 20th Century Fox. These included the successful Henry Koster outing "Mr. Hobbs Takes a Vacation" (1962), and the less memorable films "Take Her, She's Mine" (1963) and "Dear Brigitte" (1965), which featured French model Brigitte Bardot as the object of Stewart's son's mash notes. The Civil War period film "Shenandoah" (1965) and the western family film "The Rare Breed" fared better at the box office; the Civil War movie, with strong antiwar and humanitarian themes, was a smash hit in the South.
As an aviator, Stewart was particularly interested in aviation films and had pushed to appear in several in the 1950s, including "No Highway in the Sky" (1951), and most notably "Strategic Air Command" (1955) and "The Spirit of St. Louis (1957). " He continued in this vein in the 1960s, most notably in a role as a hard-bitten pilot in "The Flight of the Phoenix" (1965). Subbing for Stewart, famed stunt pilot and air racer Paul Mantz was killed when he crashed the "Tallmantz Phoenix P-1", the specially made, single-engined movie airplane, in an abortive "touch-and-go". Stewart also narrated the film "X-15" in 1961. In 1964, he and several other military aviators, including Curtis LeMay, Paul Tibbets, and Bruce Sundlun were founding directors of the board of Tibbets' Executive Jet Aviation Corporation.
After a progression of lesser western films in the late 1960s and early 1970s, Stewart transitioned from cinema to television. In the 1950s he had made guest appearances on the "Jack Benny Program". Stewart first starred in the NBC comedy "The Jimmy Stewart Show", on which he played a college professor, and was the only time in his career in which he was formally billed in the credits as "Jimmy" instead of "James". He followed it with the CBS mystery "Hawkins", in which he played a small town lawyer investigating cases, similar to his character in "Anatomy of a Murder." The series garnered Stewart a Golden Globe for Best Actor in a Dramatic TV Series, but failed to gain a wide audience, possibly because it rotated with "Shaft", another high-quality series but with a starkly conflicting demographic, and was cancelled after one season. During this time, Stewart periodically appeared on Johnny Carson's "The Tonight Show", sharing poems he had written at different times in his life. His poems were later compiled into a short collection, "Jimmy Stewart and His Poems" (1989).
Stewart returned to films after an absence of five years with a major supporting role in John Wayne's final film, "The Shootist" (1976) where Stewart played a doctor giving Wayne's gunfighter a terminal cancer diagnosis. At one point, both Wayne and Stewart were flubbing their lines repeatedly and Stewart turned to director Don Siegel and said, "You'd better get two better actors." Stewart also appeared in supporting roles in "Airport '77", the 1978 remake of "The Big Sleep" starring Robert Mitchum as Raymond Chandler's Philip Marlowe, and "The Magic of Lassie" (1978). All three movies received poor reviews, and "The Magic of Lassie" flopped at the box office.
Last years and later career.
Following the failure of "The Magic of Lassie", Stewart went into semi-retirement from acting. He donated his papers, films, and other records to Brigham Young University's Harold B. Lee Library in 1983. Stewart had diversified investments including real estate, oil wells, a charter-plane company and membership on major corporate boards, and he became a multimillionaire. In the 1980s/90s, he did voiceover work for commercials for Campbell's Soups.
Stewart's longtime friend Henry Fonda died in 1982, and former co-star and friend Grace Kelly was killed in a car crash shortly afterwards. A few months later, Stewart starred with Bette Davis in "Right of Way". He filmed several television movies in the 1980s, including "Mr. Krueger's Christmas", which allowed him to fulfill a lifelong dream to conduct the Mormon Tabernacle Choir. He made frequent visits to the Reagan White House and traveled on the lecture circuit. The re-release of his Hitchcock films gained Stewart renewed recognition. "Rear Window" and "Vertigo" were particularly praised by film critics, which helped bring these pictures to the attention of younger movie-goers. He was presented an Academy Honorary Award by Cary Grant in 1985, "for his 50 years of memorable performances, for his high ideals both on and off the screen, with respect and affection of his colleagues."
In 1988, Stewart made an impassioned plea in Congressional hearings, along with, among many others, Burt Lancaster, Katharine Hepburn, Ginger Rogers, and film director Martin Scorsese, against Ted Turner's decision to 'colorize' classic black and white films, including "It's a Wonderful Life". Stewart stated, "the coloring of black-and-white films is wrong. It's morally and artistically wrong and these profiteers should leave our film industry alone".
In 1989, Stewart founded the American Spirit Foundation to apply entertainment industry resources to developing innovative approaches to public education and to assist the emerging democracy movements in the former Iron Curtain countries. Peter F. Paul arranged for Stewart, through the offices of President Boris Yeltsin, to send a special print of "It's a Wonderful Life", translated by Lomonosov Moscow State University, to Russia as the first American program ever to be broadcast on Russian television. On January 5, 1992, coinciding with the first day of the existence of the democratic Commonwealth of Independent States and Russia, and the first free Russian Orthodox Christmas Day, Russian TV Channel 2 broadcast "It's a Wonderful Life" to 200 million Russians who celebrated an American holiday tradition with the American people for the first time in Russian history.
In association with politicians and celebrities such as President Ronald Reagan, Supreme Court Chief Justice Warren Burger, California Governor George Deukmejian, Bob Hope and Charlton Heston, Stewart worked from 1987 to 1993 on projects that enhanced the public appreciation and understanding of the U.S. Constitution and Bill of Rights.
In 1991, James Stewart voiced the character of Sheriff Wylie Burp in the movie "", which was his last film role. Shortly before his 80th birthday, he was asked how he wanted to be remembered. "As someone who 'believed in hard work and love of country, love of family and love of community.'"
Stewart was ordained as a minister by the Universal Life Church.
Personal life.
Stewart was almost universally described by his collaborators as a kind, soft-spoken man and a true professional. Joan Crawford praised the actor as an "endearing perfectionist" with "a droll sense of humor and a shy way of watching you to see if you react to that humor."
When Henry Fonda moved to Hollywood in 1934, he was again a roommate with Stewart in an apartment in Brentwood, and the two gained reputations as playboys. Both men's children later noted that their favorite activity when not working seemed to be quietly sharing time together while building and painting model airplanes, a hobby they had taken up in New York, years earlier.
After World War II, Stewart settled down, at age 41, marrying former model Gloria Hatrick McLean (March 10, 1918 – February 16, 1994) on August 9, 1949. As Stewart loved to recount in self-mockery, "I, I, I pitched the big question to her last night and to my surprise she, she, she said yes!" Stewart adopted her two sons, Michael and Ronald, and with Gloria he had twin daughters, Judy and Kelly, on May 7, 1951. The couple remained married until her death from lung cancer on February 16, 1994, at the age of 75. Ronald McLean was killed in action in Vietnam on June 8, 1969, at the age of 24, while serving as a lieutenant in the Marine Corps. Daughter Kelly Stewart is an anthropologist.
Stewart was active in philanthropic affairs over the years. His signature charity event, "The Jimmy Stewart Relay Marathon Race," held each year since 1982, has raised millions of dollars for the Child and Family Development Center at St. John's Health Center in Santa Monica, California. He was a lifelong supporter of Scouting, having been a Second Class Scout when he was a youth, an adult Scout leader, and a recipient of the prestigious Silver Buffalo Award from the Boy Scouts of America (BSA). In later years, he made advertisements for the BSA, which led to his being sometimes incorrectly identified as an Eagle Scout. An award for Boy Scouts, "The James M. Stewart Good Citizenship Award" has been presented since May 17, 2003.
Stewart was a Life Member of the Sons of the Revolution in the State of California.
One of Stewart's lesser-known talents was his homespun poetry. He once read a poem that he had written about his dog, entitled "Beau," while on "The Tonight Show Starring Johnny Carson." By the end of this reading, Carson's eyes were welling with tears. This was later parodied on a late 1980s episode of the NBC sketch show "Saturday Night Live," with Dana Carvey as Stewart reciting the poem on "Weekend Update" and bringing anchor Dennis Miller to tears. He was also an avid gardener. Stewart purchased the house next door to his own home at 918 North Roxbury Drive, razed the house, and installed his garden in the lot.
Politics.
Stewart was a staunch supporter of the Republican Party and actively campaigned for Richard Nixon and Ronald Reagan. He was a hawk on the Vietnam War, and maintained that his son, Ronald, did not die in vain. Stewart actively supported Reagan's bid to win the Republican presidential nomination in 1976.
Following the assassination of Senator Robert F. Kennedy in 1968, Stewart, Charlton Heston, Kirk Douglas and Gregory Peck issued a statement calling for support of President Johnson's Gun Control Act of 1968.
One of his best friends was fellow actor Henry Fonda, despite the fact that the two had very different political ideologies. A political argument in 1947 resulted in a fistfight, but they maintained their friendship by never discussing politics again. This tale may be apocryphal as Jhan Robbins quotes Stewart as saying: "Our views never interfered with our feelings for each other, we just didn't talk about certain things. I can't remember ever having an argument with him—ever!" However, Jane Fonda told Donald Dewey for his 1996 biography of Stewart that her father did have a falling out with Stewart at that time, although she did not know whether it was because of their political differences. There is a brief reference to their political differences in character in their movie "The Cheyenne Social Club." In the last years of his life, he donated to the campaign of Bob Dole in the 1996 presidential election and to Democratic Florida governor Bob Graham in his successful run for the Senate.
Death.
In December 1996, Stewart was due to have the battery in his pacemaker changed, but opted not to, preferring to let things happen naturally. In February 1997, Stewart was hospitalized for an irregular heartbeat. On June 25, a thrombosis formed in his right leg, leading to a pulmonary embolism seven days later. Surrounded by his children on July 2, 1997, Stewart died at the age of 89 at his home in Beverly Hills, California, with his final words to his family being "I'm going to be with Gloria now!" President Bill Clinton commented that America had lost a "national treasure ... a great actor, a gentleman and a patriot." Over 3,000 mourners, mostly celebrities, attended Stewart's memorial service, which included a firing of three volleys for his service in the Army Air Forces. Stewart is interred at Forest Lawn Memorial Park Cemetery in Glendale, California.
Filmography.
From the beginning of Stewart's film career in 1935, through his final theatrical project in 1991, he appeared in more than 92 films, television programs and shorts. Five of his movies were included on the American Film Institute's list of the 100 greatest American films: "Mr. Smith Goes to Washington", "The Philadelphia Story", "It's a Wonderful Life", "Rear Window" and "Vertigo". His roles in "Mr. Smith Goes to Washington", "The Philadelphia Story", "It's a Wonderful Life", "Harvey", and "Anatomy of a Murder" earned him Academy Award nominations—with one win for "The Philadelphia Story".
Legacy.
Throughout his seven decades in Hollywood, Stewart cultivated a versatile career and recognized screen image in such classics as "Mr. Smith Goes to Washington", "The Mortal Storm", "The Philadelphia Story", "Harvey", "It's a Wonderful Life", "Shenandoah", "Rear Window", "Rope", "The Man Who Knew Too Much", "The Shop Around the Corner", "The Man Who Shot Liberty Valance" and "Vertigo". He is the most represented leading actor on the AFI's 100 Years…100 Movies (10th Anniversary Edition) and AFI's 10 Top 10 lists. He is the most represented leading actor on the 100 Greatest Movies of All Time list presented by "Entertainment Weekly". As of 2007, ten of his films have been inducted into the United States National Film Registry. As part of their 100 Years Series, Stewart was named the third greatest screen legend actor in American film history by the AFI in 1999.
Stewart left his mark on a wide range of film genres, including westerns, suspense thrillers, family films, biographies, and screwball comedies. He worked for many renowned directors during his career, most notably Frank Capra, George Cukor, Henry Hathaway, Cecil B. DeMille, Ernst Lubitsch, Frank Borzage, George Stevens, Alfred Hitchcock, John Ford, Billy Wilder, Don Siegel, and Anthony Mann. He won many of the industry's highest honors and earned Lifetime Achievement awards from every major film organization, and is often considered to be one of the most influential actors in the history of American cinema.
Honors and tributes.
Stewart was the recipient of many official accolades throughout his life, receiving film industry awards, military and civilian medals, honorary degrees, and memorials and tributes for his contribution to the performing arts, humanitarianism, and military service.
The library at Brigham Young University houses his personal papers and movie memorabilia including his Academy Award for Best Actor.
In February 1980, Stewart was honored by Freedoms Foundation of Valley Forge, Pennsylvania, along with U.S. Senator Jake Garn, U.S. Ambassador Shirley Temple Black, singer John Denver, and Tom Abraham, a businessman from Canadian, Texas, who worked with immigrants seeking to become U.S. citizens.
On May 20, 1995, The James M. Stewart Foundation was created to honor Jimmy Stewart. In concert with his family members, the foundation also established The Jimmy Stewart Museum in his hometown of Indiana, Pennsylvania. The foundation was created to "preserve, promote and enshrine the accomplishments of James M. Stewart, actor, soldier, civic leader, and world citizen..." The registered office is at 835 Philadelphia Street, Indiana, Pa, 15701 and is located within easy walking distance of his place of birth, the home in which he grew up, and the former location of his father's hardware store. A large statue of Jimmy Stewart graces the lawn of the Indiana County Courthouse, just feet away from the museum.
The Jimmy Stewart Museum houses movie posters and photos, awards, personal artifacts, a gift shop and an intimate 1930's era theatre in which his films are regularly shown.
References.
Bibliography.
</dl>

</doc>
<doc id="16295" url="http://en.wikipedia.org/wiki?curid=16295" title="Josiah Wedgwood">
Josiah Wedgwood

Josiah Wedgwood (12 July 1730—3 January 1795) was an English potter who founded the Wedgwood company and is credited with the industrialisation of the manufacture of pottery. A prominent abolitionist, Wedgwood is remembered for his "Am I Not a Man And a Brother?" anti-slavery medallion. He was a member of the Darwin–Wedgwood family. He was the grandfather of Charles Darwin and Emma Darwin.
Biography.
Early life.
Born in Burslem, Staffordshire, England, the eleventh and last child of Thomas Wedgwood and Mary Wedgwood ("née" Stringer; d. 1766), Josiah was raised within a family of English Dissenters. By the age of nine, he was proving himself to be a skilled potter. He survived a childhood bout of smallpox to serve as an apprentice potter under his eldest brother Thomas Wedgwood IV. Smallpox left Josiah with a permanently weakened knee, which made him unable to work the foot pedal of a potter's wheel. As a result, he concentrated from an early age on designing pottery and then making it with the input of other potters.
In his early twenties, Wedgwood began working with the most renowned English pottery-maker of his day, Thomas Whieldon, who eventually became his business partner in 1754. He began experimenting with a wide variety of techniques, an experimentation that coincided with the burgeoning of the nearby industrial city of Manchester. Inspired, Wedgwood leased the Ivy Works in the town of Burslem. Over the course of the next decade, his experimentation (and a considerable injection of capital from his marriage to a richly-endowed distant cousin) transformed the sleepy artisan works into the first true pottery factory.
Marriage and children.
Wedgwood married Sarah Wedgwood (1734–1815), his third cousin, in January 1764. They had eight children:
Work.
Wedgwood was keenly interested in the scientific advances of his day and it was this interest that underpinned his adoption of its approach and methods to revolutionize the quality of his pottery. His unique glazes began to distinguish his wares from anything else on the market. He was perhaps the most famous potter of all time.
By 1763, he was receiving orders from the highest levels of the British nobility, including Queen Charlotte. Wedgwood convinced her to let him name the line of pottery she had purchased "Queen's Ware", and trumpeted the royal association in his paperwork and stationery. In 1773, Catherine the Great of Russia ordered the Green Frog Service from Wedgwood; it can still be seen in the Hermitage Museum. An even earlier commission from Catherine was the Husk Service (1770), now on exhibit in Petergof.
As a leading industrialist, Wedgwood was a major backer of the Trent and Mersey Canal dug between the River Trent and River Mersey, during which time he became friends with Erasmus Darwin. Later that decade, his burgeoning business caused him to move from the smaller Ivy Works to the newly built Etruria Works, which would run for 180 years. The factory was so-named after the Etruria district of Italy, where black porcelain dating to Etruscan times was being excavated. Wedgwood found this porcelain inspiring, and his first major commercial success was its duplication with what he called "Black Basalt". He combined experiments in his art and in the technique of mass production with an interest in improved roads, canals, schools and living conditions. At Etruria, he even built a village for his workers.
Not long after the new works opened, continuing trouble with his smallpox-afflicted knee made necessary the amputation of his right leg. In 1780, his long-time business partner Thomas Bentley died, and Wedgwood turned to Darwin for help in running the business. As a result of the close association that grew up between the Wedgwood and Darwin families, Josiah's eldest daughter would later marry Erasmus' son. One of the children of that marriage, Charles Darwin, would also marry a Wedgwood — Emma, Josiah's granddaughter. This double-barreled inheritance of Wedgwood's money gave Charles Darwin the leisure time to formulate his theory of evolution.
In the latter part of his life, Wedgwood's obsession was to duplicate the Portland Vase, a blue and white glass vase dating to the first century BC. For three years he worked on the project, eventually producing what he considered a satisfactory copy in 1789.
After passing on his company to his sons, Wedgwood died at home, probably of cancer of the jaw, in 1795. He was buried three days later in the parish church of Stoke-on-Trent. Seven years later a marble memorial tablet commissioned by his sons was installed there.
He belonged to the fourth generation of a family of potters whose traditional occupation continued through another five generations. Wedgwood's company is still a famous name in pottery today (as part of Waterford Wedgwood; see Waterford Crystal), and "Wedgwood China" is sometimes used as a term for his Jasperware, the coloured stoneware with applied relief decoration (usually white), still common throughout the world.
He was an active member of the Lunar Society of Birmingham often held at Erasmus Darwin House and is remembered on the Moonstones in Birmingham. He was elected to the Royal Society in 1783 for the development of a pyrometer.
Wedgwood is credited as the inventor of modern marketing, specifically direct mail, money back guarantees, travelling salesmen, self-service, free delivery, buy one get one free, and illustrated catalogues. Wedgwood is also noted as an early adopter/founder of managerial accounting principals in Anthony Hopwood's "Archaeology of Accounting Systems."
"Am I Not a Man And a Brother?".
Wedgwood was a prominent slavery abolitionist. His friendship with Thomas Clarkson – abolitionist campaigner and the first historian of the British abolition movement – aroused his interest in slavery. Wedgwood mass-produced cameos depicting the seal for the Society for Effecting the Abolition of the Slave Trade and had them widely distributed, which thereby became a popular and celebrated image. The Wedgwood medallion was the most famous image of a black person in all of 18th-century art. The actual design of the cameo was probably done by either William Hackwood or Henry Webber who were modellers in his Stoke-on-Trent factory. From 1787 until his death in 1795, Wedgwood actively participated in the abolition of slavery cause, and his "Slave Medallion", which brought public attention to abolition. Wedgwood reproduced the design in a cameo with the black figure against a white background and donated hundreds of these to the society for distribution. Thomas Clarkson wrote; "ladies wore them in bracelets, and others had them fitted up in an ornamental manner as pins for their hair. At length the taste for wearing them became general, and thus fashion, which usually confines itself to worthless things, was seen for once in the honorable office of promoting the cause of justice, humanity and freedom".
The design on the medallion became popular and was used elsewhere: large-scale copies were painted to hang on walls and it was used on clay tobacco pipes.
Sydney Cove medallion.
Commemorating the landing of the First Fleet in Botany Bay, the Sydney Cove medallion was made by Josiah Wedgwood after he was given a sample of clay from Sydney Cove by Sir Joseph Banks, who had received the sample from Governor Arthur Phillip. Wedgwood made it into a commemorative medallion titled "Hope encouraging Art and Labour, under the influence of Peace, to pursue the employments necessary to give security and happiness to an infant settlement".
Legacy and influence.
A locomotive named after Wedgwood ran on the Churnet Valley Railway.
A plaque, in Wedgwood's blue pottery style, marking the site of his London showrooms between 1774 and 1795 in Wedgwood Mews, is located at 12, Greek Street, London, W1.
Inventions.
Josiah Wedgwood also invented the pyrometer, a device to measure the extremely high temperatures that are found in kilns during the firing of pottery. For this he was elected a member of the Royal Society.

</doc>
<doc id="16310" url="http://en.wikipedia.org/wiki?curid=16310" title="Jacopo Amigoni">
Jacopo Amigoni

Jacopo Amigoni (1682–1752), also named Giacomo Amiconi, was an Italian painter of the late-Baroque or Rococo period, who began his career in Venice, but traveled and was prolific throughout Europe, where his sumptuous portraits were much in demand.
Biography.
He was born in Naples or Venice. Amigoni initially painted both mythological and religious scenes; but as the panoply of his patrons expanded northward, he began producing many parlour works depicting gods in sensuous languor or games. His style influenced Giuseppe Nogari. Among his pupils were Charles Joseph Flipart, Michelangelo Morlaiter, Pietro Antonio Novelli, Joseph Wagner, and Antonio Zucchi.
Starting in 1717, he is documented as working in Bavaria in the Castle of Nymphenburg (1719); in the castle of Schleissheim (1725–1729); and in the Benedictine abbey of Ottobeuren. He returned to Venice in 1726. His "Arraignment of Paris" hangs in the Villa Pisani at Stra. From 1730 to 1739 he worked in England, in Pown House, Moor Park Wolterton Hall and in the Theatre of Covent Garden. From there, he helped convince Canaletto to travel to England by telling him of the ample patronage available. 
From his travel to Paris in 1736, he met the celebrated castrato Farinelli. Later in Madrid, he was to paint a self-portrait with the singer and entourage. He also encountered the painting of François Lemoyne and Boucher. 
In 1739 he returned to Italy, perhaps to Naples and surely to Montecassino, in whose Abbey existed two canvases (destroyed during World War II). Until 1747, he travelled to Venice to paint for Sigismund Streit, for the Casa Savoia and other buildings of the city. In 1747 he left Italy and established himself in Madrid. There he became court painter to Ferdinand VI of Spain and director of the Royal Academy of Saint Fernando. He died in Madrid.

</doc>
<doc id="16313" url="http://en.wikipedia.org/wiki?curid=16313" title="Jacques Callot">
Jacques Callot

Jacques Callot (]; c.1592 – 14 March 1635) was a baroque printmaker and draftsman from the Duchy of Lorraine (an independent state on the north-eastern border of France, southwestern border of Germany and overlapping the southern Netherlands). He is an important person in the development of the old master print. He made more than 1,400 etchings that chronicled the life of his period, featuring soldiers, clowns, drunkards, Gypsies, beggars, as well as court life. He also etched many religious and military images, and many prints featured extensive landscapes in their background.
Life and training.
Callot was born and died in Nancy, the capital of Lorraine, now in France. He came from an important family (his father was master of ceremonies at the court of the Duke), and he often describes himself as having noble status in the inscriptions to his prints. At the age of fifteen he was apprenticed to a goldsmith, but soon afterward travelled to Rome where he learned engraving from an expatriate Frenchman, . He probably then studied etching with Antonio Tempesta in Florence, where he lived from 1612 to 1621. More than 2,000 preparatory drawings and studies for prints survive, but no paintings by him are known, and he probably never trained as a painter.
During his period in Florence he became an independent master, and worked often for the Medici court. After the death of Cosimo II de' Medici during 1621, he returned to Nancy where he lived for the rest of his life, visiting Paris and the Netherlands later during the decade. He was commissioned by the courts of Lorraine, France and Spain, and by publishers, mostly in Paris. Although he remained in Nancy, his prints were distributed widely through Europe; Rembrandt was a keen collector of them.
Callot's sketches of "Grotesque Dwarves" was to inspire Derby porcelain and other companies to create pottery figures known as "Mansion House Dwarves" or "Grotesque Dwarves". The former title comes from a father and son who were paid to wander around the Mansion House in London wearing oversized hats that contained advertisements.
Technical innovations: échoppe, new hard ground, stopping-out.
His technique was exceptional, and was helped by important technical advances he made. He developed the échoppe, a type of etching-needle with a slanting oval section at the end, which enabled etchers to create a swelling line, as engravers were able to do.
He also seems to have been responsible for an improved recipe for the etching ground that coated the plate and was removed to form the image, using lute-makers varnish rather than a wax-based formula. This enabled lines to be etched more deeply, prolonging the life of the plate in printing, and also greatly reducing the risk of "foul-biting", such that acid gets through the ground to the plate where it is not intended to, producing spots or blotches on the image. Previously the risk of foul-biting had always been present, preventing an engraver from investing too much time on a single plate that risked being ruined by foul-biting. Now etchers could do the very detailed work that was previously the monopoly of engravers, and Callot made good use of the new possibilities.
He also made more extensive and sophisticated use of multiple "stoppings-out" than previous etchers had done. This is the technique of letting the acid dissolve lightly over the whole plate, then stopping-out those parts of the work which the artist wishes to keep shallow by covering them with ground before bathing the plate in acid again. He achieved unprecedented subtlety in effects of distance and light and shade by careful control of this process. Most of his prints were relatively small – as much as about six inches or 15 cm on their longest dimension.
One of his devotees, the Parisian Abraham Bosse spread Callot's innovations all over Europe with the first published manual of etching, which was translated into Italian, Dutch, German and English.
"Miseries of War".
His most famous prints are his two series of prints each on "the Miseries and Misfortunes of War". These are known as "Les Grandes Misères de la guerre", consisting of 18 prints published during 1633, and the earlier and incomplete "Les Petites Misères" — referring to their sizes, large and small (though even the large set are only about 8 x 13 cm). These images show soldiers pillaging and burning their way through towns, country and convents, before being variously arrested and executed by their superiors, lynched by peasants, or surviving to live as crippled beggars. At the end the generals are rewarded by their monarch. During 1633, the year the larger set was published, Lorraine had been invaded by the French during the Thirty Years' War and Callot's artwork is still noted with Francisco Goya's "Los Desastres de la Guerra" ("The Disasters of War"), which was influenced by Callot, as among the most powerful artistic statements of the inhumanity of war.

</doc>
<doc id="16316" url="http://en.wikipedia.org/wiki?curid=16316" title="John Dowland">
John Dowland

John Dowland (1563 – buried 20 February 1626) was an English Renaissance composer, lutenist, and singer. He is best known today for his melancholy songs such as "Come, heavy sleep" (the basis for Benjamin Britten's "Nocturnal"), "Come again", "Flow my tears", "I saw my Lady weepe" and "In darkness let me dwell", but his instrumental music has undergone a major revival, and with the 20th century's Early music revival has been a continuing source of repertoire for lutenists and classical guitarists.
Career and compositions.
Very little is known of John Dowland's early life, but it is generally thought he was born in London. Irish historian W. H. Grattan Flood claimed that he was born in Dalkey, near Dublin, but no corroborating evidence has ever been found either for that statement or for Thomas Fuller's claim that he was born in Westminster. In 1580 Dowland went to Paris, where he was in service to Sir Henry Cobham, the ambassador to the French court, and his successor, Sir Edward Stafford. He became a Roman Catholic at this time. In 1584, Dowland moved back to England where he was married. In 1588 he was admitted Mus. Bac. from Christ Church, Oxford. In 1594 a vacancy for a lutenist came up at the English court, but Dowland's application was unsuccessful – he claimed his religion led to his not being offered a post at Elizabeth I's Protestant court. However, his conversion was not publicised, and being Catholic did not prevent some other important musicians (such as William Byrd) from having a court career in England.
From 1598 Dowland worked at the court of Christian IV of Denmark, though he continued to publish in London. King Christian was very interested in music and paid Dowland astronomical sums; his salary was 500 daler a year, making him one of the highest-paid servants of the Danish court. Though Dowland was highly regarded by King Christian, he was not the ideal servant, often overstaying his leave when he went to England on publishing business or for other reasons. Dowland was dismissed in 1606 and returned to England; in early 1612 he secured a post as one of James I's lutenists. There are few compositions dating from the moment of his royal appointment until his death in London in 1626. While the date of his death is not known, "Dowland's last payment from the court was on 20 January 1626, and he was buried at St Ann's, Blackfriars, London, on 20 February 1626."
Two major influences on Dowland's music were the popular consort songs, and the dance music of the day. Most of Dowland's music is for his own instrument, the lute. It includes several books of solo lute works, lute songs (for one voice and lute), part-songs with lute accompaniment, and several pieces for viol consort with lute. The poet Richard Barnfield wrote that Dowland's "heavenly touch upon the lute doth ravish human sense."
One of his better known works is the lute song "Flow my tears", the first verse of which runs:
Flow my tears, fall from your springs,
Exil'd for ever let me mourn;
Where night's black bird her sad infamy sings,
There let me live forlorn.—John Dowland, 
He later wrote what is probably his best known instrumental work, "Lachrimae, or Seaven Teares, Figured in Seaven Passionate Pavans", a set of seven pavanes for five viols and lute, each based on the theme derived from the lute song "Flow my tears". It became one of the best known collections of consort music in his time. His pavane, "Lachrymae antiquae", was also popular in the seventeenth century, and was arranged and used as a theme for variations by many composers.
Dowland's music often displays the melancholia that was so fashionable in music at that time. He wrote a consort piece with the punning title "Semper Dowland, semper dolens" (always Dowland, always doleful), which may be said to sum up much of his work.
Dowland's song, "Come Heavy Sleepe, the Image of True Death", was the inspiration for Benjamin Britten's "Nocturnal after John Dowland for guitar", written in 1964 for the guitarist Julian Bream. This work consists of eight variations, all based on musical themes drawn from the song or its lute accompaniment, finally resolving into a guitar setting of the song itself.
Richard Barnfield, Dowland's contemporary, refers to the lutenist in poem VIII of "The Passionate Pilgrim" (1598):
If music and sweet poetry agree,
As they must needs, the sister and the brother,
Then must the love be great 'twixt thee and me,
Because thou lovest the one, and I the other.
Dowland to thee is dear, whose heavenly touch
Upon the lute doth ravish human sense;
Spenser to me, whose deep conceit is such
As, passing all conceit, needs no defence.
Thou lovest to hear the sweet melodious sound
That Phoebus' lute, the queen of music, makes;
And I in deep delight am chiefly drown'd
When as himself to singing he betakes.
One god is god of both, as poets feign;
One knight loves both, and both in thee remain.—Richard Barnfield, The Passionate Pilgrim
Publications.
In 1597, Dowland published his "First Book of Songs" in London. It was one of the most influential and important musical publications of the history of the lute. This collection of lute-songs was set out in a way that allows performance by a soloist with lute accompaniment or various combinations of singers and instrumentalists.
Dowland published two books of songs after the "First Book of Songs", in 1600 and 1603, as well as the "Lachrymae" in 1604. He also published a translation of the "Micrologus" of Andreas Ornithoparcus in 1609, originally printed in Leipzig in 1517, a rather stiff and medieval treatise, but nonetheless occasionally entertaining.
Dowland's last, and in the opinion of most scholars, best work, "A Pilgrimes Solace", was published in 1612, and seems to have been conceived more as a collection of contrapuntal music than as solo works.
Suspicions of treason.
Dowland performed a number of espionage assignments for Sir Robert Cecil in France and Denmark; despite his high rate of pay, Dowland seems to have been only a court musician. However, we have in his own words the fact that he was for a time embroiled in treasonous Catholic intrigue in Italy,
whither he had travelled in the hopes of meeting and studying with Luca Marenzio, a famed madrigal composer. Whatever his religion, however, he was still intensely loyal to the Queen, though he seems to have had something of a grudge against her for her remark that he, Dowland, "was a man to serve any prince in the world, but [he] was an obstinate Papist." But in spite of this, and though the plotters offered him a large sum of money from the Pope, as well as safe passage for his wife and children to come to him from England, in the end he declined to have anything further to do with their plans and begged pardon from Sir Robert Cecil and from the Queen.
Private life.
John Dowland was married and had children, as referenced in his letter to Sir Robert Cecil. However, he had long periods of separation from his family, as his wife stayed in England while he worked on the Continent.
His son Robert Dowland was also a musician, working for some time in the service of the first Earl of Devonshire, and taking over his father's position of lutenist at court when John died.
Dowland's melancholic lyrics and music have often been described as his attempts to develop an "artistic persona" though he was actually a cheerful person, but many of his own personal complaints, and the tone of bitterness in many of his comments, suggest that much of his music and his melancholy truly did come from his own personality and frustration.
Modern interpretations.
One of the first 20th-century musicians who successfully helped reclaim Dowland from the history books was the singer-songwriter Frederick Keel. Keel included fifteen Dowland pieces in his two sets of "Elizabethan love songs" published in 1909 and 1913, which achieved popularity in their day. These free arrangements for piano and low or high voice were intended to fit the tastes and musical practices associated with art songs of the time.
In 1935, Australian-born composer Percy Grainger, who also had a deep interest in music made before Bach, arranged Dowland's "Now, O now I needs must part" for piano. Some years later, in 1953, Grainger wrote a work titled "Bell Piece (Ramble on John Dowland's 'Now, O now I needs must part')", which was a version scored for voice and wind band, based on his previously mentioned transcription.
In 1951 Alfred Deller, the famous counter-tenor (1912–1979), recorded songs by Dowland, Thomas Campion, and Philip Rosseter with the label HMV (His Master's Voice) HMV C.4178 and another HMV C.4236 of Dowland's "Flow my Tears". In 1977, Harmonia Mundi also published two records of Deller singing Dowland's Lute songs (HM 244&245-H244/246).
Dowland's music became part of the repertoire of the early music revival with lutenist Julian Bream and tenor Peter Pears, and later with Christopher Hogwood and David Munrow and the Early Music Consort in the late 1960s and later with the Academy of Ancient Music from the early 1970s.
Jan Akkerman, guitarist of the Dutch progressive rock band Focus, recorded "Tabernakel" in 1973 (though released in 1974), an album of John Dowland songs and some original material, performed on lute.
"The Collected Lute Music of John Dowland" with lute tablature and keyboard notation has been transcribed and edited by Diana Poulton and Basil Lam, Faber Music Limited, London 1974.
"John Dowland:Complete Solo Galliards for Renaissance Lute or Guitar" was transcribed and edited with performance suggestions, new divisions (or variations) and transposition for 6-stringed instruments by Ben Salfield, Peacock Press, UK, 2014.
The complete works of Dowland have been recorded in a boxed set by the Consort of Musicke.
The 1999 ECM New Series recording "In Darkness Let Me Dwell" features new interpretations of Dowland songs performed by tenor John Potter, lutenist Stephen Stubbs, and baroque violinist Maya Homburger in collaboration with English jazz musicians John Surman and Barry Guy.
Nigel North recorded Dowland's complete works for solo lute on four CDs between 2004 and 2007.
Elvis Costello included a recording (with Fretwork and the Composers Ensemble) of Dowland's "Can she excuse my wrongs" as a bonus track on the 2006 re-release of his "The Juliet Letters".
In October 2006, Sting, who says he has been fascinated by the music of John Dowland for 25 years, released an album featuring Dowland's songs titled "Songs from the Labyrinth", on Deutsche Grammophon, in collaboration with Edin Karamazov on lute and archlute. They described their treatment of Dowland's work in a "Great Performances" appearance. To give some idea of the tone and intrigues of life in late Elizabethan England, Sting also recites throughout the album portions of a 1593 letter written by Dowland to Sir Robert Cecil. The letter describes Dowland's travels to various points of Western Europe, then breaks into a detailed account of his activities in Italy, along with a heartfelt denial of the charges of treason whispered against him by unknown persons. Dowland most likely was suspected of this for travelling to the courts of various Catholic monarchs and accepting payment from them greater than what a musician of the time would normally have received for performing.
Other interpretations of Dowland's songs have been recorded by Windham Hill artist, Lisa Lynne, (for her CD, "Maiden's Prayer") and Lise Winne (for her "Wing'd With Hopes, New Interpretations of Renaissance Songs" CD). Several bands, such as Die Verbannten Kinder Evas, Aesma Daeva and Qntal, have recorded albums featuring lyrics by Dowland. The countertenor Andreas Scholl sings in "Crystal Tears: English consort songs" with Concerto Viole of Basel. A rendition of Dowland's "Come again" (sung by Sting) can also be found on Joshua Bell's 2009 album, "At home with Friends".
In 2012, mandolinists Joseph Brent and Alon Sariel recorded an album's worth of Dowland lute music arranged for mandolin. After a successful Kickstarter campaign, the duo's CD of the recordings, titled "An Englishman in New York," is due for release in 2013.
Also in 2012, Italian twelve-string guitar player Marco Poeta and vocalist Alessandra Losacco recorded an album called Reminiscence "of John Dowland" for Continuo Records.

</doc>
<doc id="16328" url="http://en.wikipedia.org/wiki?curid=16328" title="Jeepster Records">
Jeepster Records

Jeepster Records is a London, England-based independent record label, founded in 1995, and specialising in British indie and alternative bands, particularly Glasgow-based acts. It is most notable for its signing of Belle and Sebastian and Snow Patrol.
Early success.
Jeepster Records was founded in 1995 by Mark Jones and Stefano D’Andrea, through a mutual interest in the contemporary indie scene. Following their establishment and after extensive scouting, the label signed their first act, the newly formed Belle and Sebastian, in August 1996. In November of the same year, the band's first album with Jeepster, "If You're Feeling Sinister" was released. This put both Belle & Sebastian and Jeepster firmly on the map, and enabled them to release several EPs with Belle & Sebastian throughout 1997, as well as signing their second act, Snow Patrol, later in the year. 1998 then saw a further increase of activity, with the signing of Salako, and the release of albums for all three of their signed bands - most notably Belle & Sebastian's "The Boy With The Arab Strap". 
The label's strong relationship with Belle & Sebastian enabled them in 1999 to sign Stuart David's side-project Looper, and Isobel Campbell's solo project The Gentle Waves, releasing albums for each that same year, along with a string of EPs and singles for their entire roster. The label enjoyed further good publicity when Belle & Sebastian won Best Newcomer in the 1999 Brit Awards. Later that year, Jeepster reissued Belle & Sebastian's debut album "Tigermilk", which had previously been available only on limited issue vinyl.
2000 saw new albums released for Belle & Sebastian, Looper, and The Gentle Waves, as well as Belle & Sebastian's first appearance on "Top of the Pops". Towards the end of the year, the label released the "It's a Cool Cool Christmas" compilation in association with XFM, with proceeds going to The Big Issue charity. The album was only available during this Christmas period, and featured Belle & Sebastian and Snow Patrol, as well as numerous other bands such as The Flaming Lips and Teenage Fanclub. The following year finally marked the release of Snow Patrol's second album "When It's All Over We Still Have To Clear Up", which, despite slightly disappointing initial sales, would eventually go gold in the wake of the band's later fame, along with their debut "Songs For Polarbears".
Dormant period.
Despite critical acclaim for its acts, Jeepster nevertheless found itself financially troubled by 2002, largely due to increasing recording and marketing costs, and difficulty cultivating the images of their acts in the eye of the general public. This forced the label into a corner, and they had no choice but to decline renewal of the contracts of their most successful artists in order to continue producing the catalogue they had already built up. While they were neither able to retain their signed artists for this period, nor to consider signing new acts, the next few years nevertheless marked several additions to the catalogue. At this point, the running of the label came into the hands of Jo D'Andrea and Kay Heath.
The label released a Belle & Sebastian DVD in 2003, "Fans Only", and, in 2005, "Push Barman to Open Old Wounds", a compilation comprising all of Belle & Sebastian's single and EPs released under Jeepster. This was followed in 2006 by re-releases of "Songs for Polarbears" and "When It's All Over We Still Have to Clear Up", including as-before unseen bonus tracks.
Later signings.
Finding itself in a notably better state financially, in April 2006, Jeepster announced its first new signing in years, Reading-based act SixNationState. Following renewed scouting of the Glasgow underground scene, the label soon after announced the signing of another band, Parka, in November of the same year. 
Following several singles releases on behalf of both bands, the label released its first new album in six and a half years in late 2007 - SixNationState's self-titled debut album. In May 2008, Parka's own debut, "Attack of the Hundred Yard Hardman" was also released.
Before the end of 2008, Jeepster would also release another Belle & Sebastian compilation, "The BBC Sessions", collecting the tracks that the band had recorded for the BBC in 1996, which included rarities and unreleased songs, together with live recordings from Belfast.
Jeepster contributed six songs to the Polydor Records Snow Patrol compilation "Up To Now" in 2009.

</doc>
<doc id="16329" url="http://en.wikipedia.org/wiki?curid=16329" title="John Chrysostom">
John Chrysostom

John Chrysostom (; Greek: Ἰωάννης ὁ Χρυσόστομος), c. 349 – 407, Archbishop of Constantinople, was an important Early Church Father. He is known for his preaching and public speaking, his denunciation of abuse of authority by both ecclesiastical and political leaders, the "Divine Liturgy of St. John Chrysostom", and his ascetic sensibilities. The epithet "Χρυσόστομος" ("Chrysostomos," anglicized as Chrysostom) means "golden-mouthed" in Greek and was given because of his celebrated eloquence.
The Eastern Orthodox and Byzantine Catholic Churches honor him as a saint and count him among the Three Holy Hierarchs, together with Basil the Great and Gregory Nazianzus. He is recognized by the Eastern Orthodox Church and the Catholic Church as a saint and as a Doctor of the Church. Churches of the Western tradition, including the Roman Catholic Church, some Anglican provinces, and parts of the Lutheran Church, commemorate him on 13 September. Some Lutheran and many Anglican provinces commemorate him on the traditional Eastern feast day of 27 January. The Coptic Orthodox Church of Alexandria also recognizes John Chrysostom as a saint (with feast days on 16 Thout and 17 Hathor).
Chrysostom wrote of the Jews and of Judaizers in eight homilies "Adversus Judaeos" (against the Judaizers).Among his homilies, eight directed against Jews and Judaizing Christians are considered by some to have been an impetus to periodic bouts of Christian antisemitism.
Biography.
Early life and education.
John was born in Antioch in 349 to Greco-Syrian parents. Different scholars describe his mother Anthusa as a pagan or as a Christian, and his father was a high-ranking military officer. John's father died soon after his birth and he was raised by his mother.
He was baptised in 368 or 373 and tonsured as a reader (one of the minor orders of the Church). As a result of his mother's influential connections in the city, John began his education under the pagan teacher Libanius. From Libanius, John acquired the skills for a career in rhetoric, as well as a love of the Greek language and literature.
As he grew older, however, he became more deeply committed to Christianity and went on to study theology under Diodore of Tarsus, founder of the re-constituted School of Antioch. According to the Christian historian Sozomen, Libanius was supposed to have said on his deathbed that John would have been his successor "if the Christians had not taken him from us".
He lived in extreme asceticism and became a hermit in about 375; he spent the next two years continually standing, scarcely sleeping, and committing the Bible to memory. As a consequence of these practices, his stomach and kidneys were permanently damaged and poor health forced him to return to Antioch.
Diaconate and service in Antioch.
John was ordained as a deacon in 381 by Saint Meletius of Antioch who was not then in communion with Alexandria and Rome. After the death of Meletius, John separated himself from the followers of Meletius, without joining Paulinus, the rival of Meletius for the bishopric of Antioch. But after the death of Paulinus he was ordained a presbyter (that is, a priest) in 386 by Evagrius, the successor of Paulinus. He was destined later to bring about reconciliation between Flavian I of Antioch, the successor of Alexandria and Rome, thus bringing those three sees into communion for the first time in nearly seventy years.
In Antioch, over the course of twelve years (386–397), John gained popularity because of the eloquence of his public speaking at the Golden Church, Antioch's cathedral, especially his insightful expositions of Bible passages and moral teaching. The most valuable of his works from this period are his "Homilies" on various books of the Bible. He emphasised charitable giving and was concerned with the spiritual and temporal needs of the poor. He also spoke against abuse of wealth and personal property:
His straightforward understanding of the Scriptures – in contrast to the Alexandrian tendency towards allegorical interpretation – meant that the themes of his talks were practical, explaining the Bible's application to everyday life. Such straightforward preaching helped Chrysostom to garner popular support. He founded a series of hospitals in Constantinople to care for the poor.
One incident that happened during his service in Antioch illustrates the influence of his homilies. When Chrysostom arrived in Antioch, the bishop of the city had to intervene with Emperor Theodosius I on behalf of citizens who had gone on a rampage mutilating statues of the Emperor and his family. During the weeks of Lent in 387, John preached twenty-one homilies in which he entreated the people to see the error of their ways. These made a lasting impression on the general population of the city: many pagans converted to Christianity as a result of the homilies. As a result, Theodosius' vengeance was not as severe as it might have been.
Archbishop of Constantinople.
In the autumn of 397, John was appointed Archbishop of Constantinople, after having been nominated without his knowledge by the eunuch Eutropius. He had to leave Antioch in secret due to fears that the departure of such a popular figure would cause civil unrest.
During his time as Archbishop he adamantly refused to host lavish social gatherings, which made him popular with the common people, but unpopular with wealthy citizens and the clergy. His reforms of the clergy were also unpopular with these groups. He told visiting regional preachers to return to the churches they were meant to be serving—without any payout.
His time in Constantinople was more tumultuous than his time in Antioch. Theophilus, the Patriarch of Alexandria, wanted to bring Constantinople under his sway and opposed John's appointment to Constantinople. Theophilus had disciplined four Egyptian monks (known as "the Tall Brothers") over their support of Origen's teachings. They fled to John and were welcomed by him. Theophilus therefore accused John of being too partial to the teaching of Origen.
He made another enemy in Aelia Eudoxia, the wife of the eastern Emperor Arcadius, who assumed (perhaps with justification) that his denunciations of extravagance in feminine dress were aimed at herself. Eudoxia, Theophilus and other of his enemies held a synod in 403 (the Synod of the Oak) to charge John, in which his connection to Origen was used against him. It resulted in his deposition and banishment.
He was called back by Arcadius almost immediately, as the people became "tumultuous" over his departure, even threatening to burn the royal palace. There was also an earthquake the night of his arrest, which Eudoxia took for a sign of God's anger, prompting her to ask Arcadius for John's reinstatement.
Peace was short-lived. A silver statue of Eudoxia was erected in the Augustaion, near his cathedral. John denounced the pagan dedication ceremonies. He spoke against her in harsh terms: "Again Herodias raves; again she is troubled; she dances again; and again desires to receive John's head in a charger," an allusion to the events surrounding the death of John the Baptist. Once again he was banished, this time to the Caucasus in Abkhazia.
Around 405, Chrysostom began to lend moral and financial support to Christian monks who were enforcing the emperors' anti-Pagan laws, by destroying temples and shrines in Phoenicia and nearby regions.
Death and canonization.
Faced with exile, John Chrysostom wrote an appeal for help to three churchmen: Pope Innocent I, the Bishop of Milan, Venerius, and the third to the Bishop of Aquileia, Chromatius.
Pope Innocent I protested John's banishment out of Constantinople to the town of Cucusus in Cappadocia, but to no avail. Innocent sent a delegation to intercede on behalf of John in 405. It was led by Gaudentius of Brescia; Gaudentius and his companions, two bishops, encountered many difficulties and never reached their goal of entering Constantinople.
John wrote letters which still held great influence in Constantinople. As a result of this, he was further exiled from the Caucasus (where he stayed from 404–407) to Pitiunt (Pityus) (in modern Abkhazia) where his tomb is a shrine for pilgrims. He never reached this destination, though, as he died at Comana Pontica on 14 September 407 during the journey. His last words are said to have been, "δόξα τῷ θεῷ πάντων ἕνεκεν" (Glory be to God for all things).
John came to be venerated as a saint soon after his death. Three decades later, some of his adherents in Constantinople remained in schism. Saint Proclus, Patriarch of Constantinople (434–446), hoping to bring about the reconciliation of these Johannites, preached a homily praising his predecessor in the Church of Hagia Sophia. He said, "O John, your life was filled with sorrow, but your death was glorious. Your grave is blessed and reward is great, by the grace and mercy of our Lord Jesus Christ O graced one, having conquered the bounds of time and place! Love has conquered space, unforgetting memory has annihilated the limits, and place does not hinder the miracles of the saint."
These homilies helped to mobilize public opinion, and the patriarch received permission from the emperor to return Chrysostom's relics to Constantinople, where they were enshrined in the Church of the Holy Apostles on January 28, 438.
The Eastern Orthodox Church commemorates him as a "Great Ecumenical Teacher", together with Basil the Great and Gregory the Theologian. These three saints, in addition to having their own individual commemorations throughout the year, are commemorated together on 30 January, a feast known as the Synaxis of the Three Hierarchs.
There are several feast days dedicated to him:
Writings.
Homilies.
Paschal Homily.
The best known of his many homilies, his famous Paschal Homily ("Hieratikon"), is rather brief. In the Eastern Orthodox Church it is traditionally read in full each year at the Paschal Divine Liturgy (eucharistic) service following the midnight Orthros (or Matins).
General.
Known as "the greatest preacher in the early church", John's homilies have been one of his greatest lasting legacies. Chrysostom's extant homiletical works are vast, including many hundreds of exegetical homilies on both the New Testament (especially the works of Saint Paul) and the Old Testament (particularly on Genesis). Among his extant exegetical works are sixty-seven homilies on Genesis, fifty-nine on the Psalms, ninety on the Gospel of Matthew, eighty-eight on the Gospel of John, and fifty-five on the Acts of the Apostles.
The homilies were written down by stenographers and subsequently circulated, revealing a style that tended to be direct and greatly personal, but was also formed by the rhetorical conventions of his time and place. In general, his homiletical theology displays much characteristic of the Antiochian school (i.e., somewhat more literal in interpreting Biblical events), but he also uses a good deal of the allegorical interpretation more associated with the Alexandrian school.
John's social and religious world was formed by the continuing and pervasive presence of paganism in the life of the city. One of his regular topics was the paganism in the culture of Constantinople, and in his homilies he thunders against popular pagan amusements: the theatre, horseraces, and the revelry surrounding holidays. In particular, he criticized Christians for taking part in such activities:
"If you ask [Christians] who is Amos or Obadiah, how many apostles there were or prophets, they stand mute; but if you ask them about the horses or drivers, they answer with more solemnity than sophists or rhetors".
John's homilies on Saint Paul's Epistles proceed linearly, methodically treating the texts verse by verse, often going into great detail. He shows a concern to be understood by laypeople, sometimes offering colorful analogies and practical examples. At other times, he offers extended comments clearly intended to address the theological subtleties of a heretical misreading, or to demonstrate the presence of a deeper theme.
One of the recurring features of John's homilies is his emphasis on care for the needy. Echoing themes found in the Gospel of Matthew, he calls upon the rich to lay aside materialism in favor of helping the poor, often employing all of his rhetorical skills to shame wealthy people to abandon conspicuous consumption:
"Do you pay such honor to your excrements as to receive them into a silver chamber-pot when another man made in the image of God is perishing in the cold?"
Homilies on Jews and Judaizing Christians.
During his first two years as a presbyter in Antioch (386–387), John denounced Jews and Judaizing Christians in a series of eight homilies delivered to Christians in his congregation who were taking part in Jewish festivals and other Jewish observances. It is disputed whether the main target were specifically Judaizers or Jews in general. His homilies were expressed in the conventional manner, utilizing the uncompromising rhetorical form known as the "psogos" (Greek: blame, censure).
One of the purposes of these homilies was to prevent Christians from participating in Jewish customs, and thus prevent the perceived erosion of Chrysostom's flock. In his homilies, John criticized those "Judaizing Christians", who were participating in Jewish festivals and taking part in other Jewish observances, such as the shabbat, submitted to circumcision and made pilgrimage to Jewish holy places. John claimed that on the shabbats and Jewish festivals synagogues were full of Christians, especially women, who loved the solemnity of the Jewish liturgy, enjoyed listening to the shofar on Rosh Hashanah, and applauded famous preachers in accordance with the contemporary custom. A more recent theory is that he instead tried to persuade Jewish Christians, who for centuries had kept connections with Jews and Judaism, to choose between Judaism and Christianity.
In Greek the homilies are called "Kata Ioudaiōn" ("Κατὰ Ιουδαίων"), which is translated as "Adversus Judaeos" in Latin and "Against the Jews" in English. The original Benedictine editor of the homilies, Bernard de Montfaucon, gives the following footnote to the title: "A discourse against the Jews; but it was delivered against those who were Judaizing and keeping the fasts with them [the Jews]."
According to Patristics scholars, opposition to any particular view during the late 4th century was conventionally expressed in a manner, utilizing the rhetorical form known as the psogos, whose literary conventions were to vilify opponents in an uncompromising manner; thus, it has been argued that to call Chrysostom an "anti-Semite" is to employ anachronistic terminology in a way incongruous with historical context and record. That does not, however, prevent one from claiming that Chrysostom's theology was a form of Anti-Jewish supersessionism, or that his rhetoric was not Anti-Judaism.
Treatises.
Apart from his homilies, a number of John's other treatises have had a lasting influence. One such work is John's early treatise "Against Those Who Oppose the Monastic Life", written while he was a deacon (sometime before 386), which was directed to parents, pagan as well as Christian, whose sons were contemplating a monastic vocation. The book is a sharp attack on the values of Antiochene upper-class urban society written by someone who was a member of that class. Chrysostom also writes that, already in his day, it was customary for Antiochenes to send their sons to be educated by monks. Other important treatises written by John include "On the Priesthood" (written 390/1, it contains in Book 1 an account of his early years and a defence of his flight from ordination by Bishop Meletios of Antioch, and then proceeds in later books to expound on his exalted understanding of the priesthood), "Instructions to Catechumens", and "On the Incomprehensibility of the Divine Nature". In addition, he wrote a series of letters to the deaconess Olympias, of which seventeen are extant.
Liturgy.
Beyond his preaching, the other lasting legacy of John is his influence on Christian liturgy. Two of his writings are particularly notable. He harmonized the liturgical life of the Church by revising the prayers and rubrics of the Divine Liturgy, or celebration of the Holy Eucharist. To this day, Eastern Orthodox and Eastern Catholic Churches of the Byzantine Rite typically celebrate the Divine Liturgy of St. John Chrysostom as the normal Eucharistic liturgy, although his exact connection with it remains a matter of debate among experts.
Legacy and influence.
During a time when city clergy were subject to criticism for their high lifestyle, John was determined to reform his clergy in Constantinople. These efforts were met with resistance and limited success. He was an excellent preacher whose homilies and writings are still studied and quoted. As a theologian, he has been and continues to be very important in Eastern Christianity, and is generally considered the most prominent doctor of the Greek Church, but has been less important to Western Christianity. His writings have survived to the present day more so than any of the other Greek Fathers. He rejected the contemporary trend for allegory, instead speaking plainly and applying Bible passages and lessons to everyday life. His exile demonstrated the rivalry between Constantinople and Alexandria for recognition as the preeminent Eastern See, while in the west, the Pope's primacy remained unquestioned.
Influence on the Catechism of the Catholic Church and clergy.
John's influence on church teachings is interwoven throughout the current Catechism of the Catholic Church (revised 1992). The Catechism cites him in eighteen sections, particularly his reflections on the purpose of prayer and the meaning of the Lord's Prayer:
Consider how [Jesus Christ] teaches us to be humble, by making us see that our virtue does not depend on our work alone but on grace from on high. He commands each of the faithful who prays to do so universally, for the whole world. For he did not say "thy will be done in me or in us", but "on earth", the whole earth, so that error may be banished from it, truth take root in it, all vice be destroyed on it, virtue flourish on it, and earth no longer differ from heaven.
Christian clerics, such as R.S. Storr, refer to him as "one of the most eloquent preachers who ever since apostolic times have brought to men the divine tidings of truth and love", and the 19th-century John Henry Newman described John as a "bright, cheerful, gentle soul; a sensitive heart."
Music and literature.
John's liturgical legacy has inspired several musical compositions. Particularly noteworthy are Sergei Rachmaninoff's "Liturgy of St. John Chrysostom", Op. 31, composed in 1910, one of his two major unaccompanied choral works; Pyotr Tchaikovsky's "Liturgy of St. John Chrysostom", Op. 41; and Ukrainian composer Kyrylo Stetsenko's Liturgy of St. John Chrysostom.
Also noteworthy is the "Divine Liturgy of St. John Chrysostom" ("Božanstvena Liturgija Svetog Jovana Zlatoustog") by the Serbian composer Stevan Mokranjac.
Arvo Pärt's "Litany" sets Chrysostom's twenty-four prayers, one for each hour of the day, for soli, mixed choir and orchestra.
James Joyce's novel "Ulysses" includes a character named Mulligan who brings 'Chrysostomos' into another character (Stephen Dedalus)'s mind because Mulligan's gold-stopped teeth and his gift of the gab earn him the title which St. John Chrysostom's preaching earned him, 'golden-mouthed': "[Mulligan] peered sideways up and gave a long low whistle of call, then paused awhile in rapt attention, his even white teeth glistening here and there with gold points. Chrysostomos." 
The legend of the penance of St. John Chrysostom.
A late mediaeval legend (although not mentioned in the Golden Legend) relates that, when John Chrysostom was a hermit in the desert, he was approached by a royal princess in distress. The Saint, thinking she was a demon, at first refused to help her, but the princess convinced him that she was a Christian and would be devoured by wild beasts if she were not allowed to enter his cave. He therefore admitted her, carefully dividing the cave in two parts, one for each of them.
In spite of these precautions, the sin of fornication was committed, and in an attempt to hide it, the distraught saint took the princess and threw her over a precipice. He then went to Rome to beg absolution, which was refused. Realising the appalling nature of his crimes, Chrysostom made a vow that he would never rise from the ground until his sins were expiated, and for years he lived like a beast, crawling on all fours and feeding on wild grasses and roots. Subsequently the princess reappeared, alive, and suckling the saint's baby, who miraculously pronounced his sins forgiven.
This last scene was very popular from the late 15th century onwards as a subject for engravers and artists. The theme was depicted by Albrecht Dürer around 1496, Hans Sebald Beham and Lucas Cranach the Elder, among others. Martin Luther mocked this same legend in his Die Lügend von S. Johanne Chrysostomo (1537). The legend was also recorded in Croatia in the 16th century.
Relics.
John Chrysostom died in the city of Comana in the year 407 on his way to his place of exile. There his relics remained until 438 when, thirty years after his death, they were transferred to Constantinople during the reign of the Empress Eudoxia's son, the Emperor Theodosius II (408–450), under the guidance of John's disciple, St. Proclus, who by that time had become Archbishop of Constantinople (434–447).
Most of John's relics were looted from Constantinople by Crusaders in 1204 and taken to Rome, but some of his bones were returned to the Orthodox Church on 27 November 2004 by Pope John Paul II. They are now enshrined in the Church of St. George, Istanbul.
However, the skull of St John, having been kept at the Vatopedi Monastery on Mount Athos in northern Greece, was not among the relics that were taken by the crusaders in the 13th century. In 1655, at the request of Tsar Alexei Mikhailovich, the skull was taken to Russia, for which the monastery was compensated in the sum of 2000 rubles. In 1693, having received a request from the Vatopedi Monastery for the return of St John's skull, Tsar Peter the Great ordered that the skull remain in Russia but that the monastery was to be paid 500 rubles every four years. The Russian state archives document these payments up until 1735.
The skull was kept at the Moscow Kremlin, in the Cathedral of the Dormition of the Mother of God, until 1920, when it was confiscated by the Soviets and placed in the Museum of Silver Antiquities. In 1988, in connection with the 1000th Anniversary of the Baptism of Russia, the head, along with other important relics, was returned to the Russian Orthodox Church and kept at the Epiphany Cathedral, until being moved to the Cathedral of Christ the Saviour after its restoration.
However, today, the Vatopedi Monastery posits a rival claim to possession of the skull of St. John Chrysostom, and there a skull is venerated by pilgrims to the monastery as that of St John.
Two places in Italy also claim to have the skull of St. John Chrysostom: the Basilica di Santa Maria del Fiore in Florence and the "Dal Pozzo" chapel in Pisa.
The right hand of St. John is preserved on Mount Athos, and numerous smaller relics are scattered throughout the world.
References.
</dl>
Collected works.
Widely used editions of Chrysostom's works are available in Greek, Latin, English, and French.
The Greek edition is edited by Sir Henry Savile (eight volumes, Eton, 1613); the most complete Greek and Latin edition is edited by Bernard de Montfaucon (thirteen volumes, Paris, 1718–38, republished in 1834–40, and reprinted in Migne's "Patrologia Graeca", volumes 47–64). There is an English translation in the first series of the "Nicene and Post-Nicene Fathers" (London and New York, 1889–90). A selection of his writings has been published more recently in the original with facing French translation in "Sources Chrétiennes".

</doc>
<doc id="16371" url="http://en.wikipedia.org/wiki?curid=16371" title="Jacob Abbott">
Jacob Abbott

Jacob Abbott (November 14, 1803 – October 31, 1879) was an American writer of children's books.
Biography.
Abbott was born at Hallowell, Maine to Jacob and Betsey Abbott. He attended the Hallowell Academy, then he graduated from Bowdoin College in 1820; studied at Andover Theological Seminary in 1821, 1822, and 1824; was tutor in 1824–1825, and from 1825 to 1829 was professor of mathematics and natural philosophy at Amherst College; was licensed to preach by the Hampshire Association in 1826; founded the Mount Vernon School for Young Ladies in Boston in 1829, and was principal of it in 1829–1833; was pastor of Eliot Congregational Church (which he founded), at Roxbury, Massachusetts in 1834–1835; and was, with his brothers, a founder, and in 1843–1851 a principal of Abbott's Institute, and in 1845–1848 of the Mount Vernon School for Boys, in New York City.
He was a prolific author, writing juvenile fiction, brief histories, biographies, religious books for the general reader, and a few works in popular science. He wrote 180 books and was a coauthor or editor of 31 more. He died in Farmington, Maine, where he had spent part of his time after 1839, and where his brother, Samuel Phillips Abbott, founded the Abbott School.
His "Rollo Books", such as "Rollo at Work, Rollo at Play, Rollo in Europe", etc., are the best known of his writings, having as their chief characters a representative boy and his associates. In them Abbott did for one or two generations of young American readers a service not unlike that performed earlier, in England and America, by the authors of "Evenings at Home", "The History of Sandford and Merton", and the "The Parent's Assistant". To follow up his Rollo books, he wrote of "Uncle George", using him to teach the young readers about ethics, geography, history, and science. He also wrote 22 volumes of biographical histories and a 10 volume set titled the "Franconia Stories".
His brothers, John Stevens Cabot Abbott and Gorham Dummer Abbott, were also authors. His sons, Benjamin Vaughan Abbott, Austin Abbott, both eminent lawyers, Lyman Abbott, and Edward Abbott, a clergyman, were also well-known authors.
See his "Young Christian, Memorial Edition, with a Sketch of the Author" by Edward Abbott with a bibliography of his works.
Other works of note: "Lucy Books", "Jonas Books", "Harper's Story Books", "Marco Paul", "Gay Family", and "Juno Books".

</doc>
<doc id="16376" url="http://en.wikipedia.org/wiki?curid=16376" title="J. E. B. Stuart">
J. E. B. Stuart

James Ewell Brown "Jeb" Stuart (February 6, 1833 – May 12, 1864) was a United States Army officer from the U.S. state of Virginia who later became a Confederate States Army general during the American Civil War. He was known to his friends as "Jeb", from the initials of his given names. Stuart was a cavalry commander known for his mastery of reconnaissance and the use of cavalry in support of offensive operations. While he cultivated a cavalier image (red-lined gray cape, yellow sash, hat cocked to the side with an ostrich plume, red flower in his lapel, often sporting cologne), his serious work made him the trusted eyes and ears of Robert E. Lee's army and inspired Southern morale.
Stuart graduated from West Point in 1854 and served in Texas and Kansas with the U.S. Army, a veteran of the frontier conflicts with Native Americans and the violence of Bleeding Kansas. He participated in the capture of John Brown at Harpers Ferry. Resigning when his home state of Virginia seceded, he served first under Stonewall Jackson in the Shenandoah Valley, but then in increasingly important cavalry commands of the Army of Northern Virginia, playing a role in all of that army's campaigns until his death. He established a reputation as an audacious cavalry commander and on two occasions (during the Peninsula Campaign and the Maryland Campaign) circumnavigated the Union Army of the Potomac, bringing fame to himself and embarrassment to the North. At the Battle of Chancellorsville, he distinguished himself as a temporary commander of the wounded Stonewall Jackson's infantry corps.
Arguably Stuart's most famous campaign, Gettysburg, was marred when he was surprised by a Union cavalry attack at the Battle of Brandy Station and by his separation from Lee's army for an extended period, leaving Lee unaware of Union troop movements and contributing to the Confederate defeat at the Battle of Gettysburg. Stuart received significant criticism from the Southern press as well as the postbellum proponents of the Lost Cause movement, but historians have failed to agree on whether Stuart's exploit was entirely the fault of his judgment or simply bad luck and Lee's less-than-explicit orders.
During the 1864 Overland Campaign, Union Maj. Gen. Philip Sheridan's cavalry launched an offensive to defeat Stuart, who was mortally wounded at the Battle of Yellow Tavern. His widowed wife wore black for the rest of her life in remembrance of her deceased husband.
Early life.
Stuart was born at Laurel Hill Farm, a plantation in Patrick County, Virginia, near the border with North Carolina. He was of Scottish American and Scots-Irish background. He was the eighth of eleven children and the youngest of the five sons to survive past early age. His great-grandfather, Major Alexander Stuart, commanded a regiment at the Battle of Guilford Court House during the American Revolutionary War. His father, Archibald Stuart, was a War of 1812 veteran, slaveholder, attorney, and politician who represented Patrick County in both houses of the Virginia General Assembly, and also served one term in the United States House of Representatives. Archibald was a cousin of Alexander Hugh Holmes Stuart. Elizabeth Letcher Pannill Stuart, Jeb's mother, who was known as a strict religious woman with a good sense for business, ran the family farm.
Education.
Stuart was educated at home by his mother and tutors until the age of twelve, when he left Laurel Hill to be educated by various teachers in Wytheville, Virginia, and at the home of his aunt Anne (Archibald's sister) and her husband Judge James Ewell Brown (Stuart's namesake) at Danville. He entered Emory and Henry College when he was fifteen, and attended from 1848 to 1850.
During the summer of 1848, Stuart attempted to enlist in the U.S. Army, but was rejected as underaged. He obtained an appointment in 1850 to the United States Military Academy at West Point, New York, from Representative Thomas Hamlet Averett, the man who had defeated his father in the 1848 election. Stuart was a popular student and was happy at the Academy. Although not handsome in his teen years, his classmates called him by the nickname "Beauty", which they described as his "personal comeliness in inverse ratio to the term employed." He possessed a chin "so short and retiring as positively to disfigure his otherwise fine countenance." He quickly grew a beard after graduation and a fellow officer remarked that he was "the only man he ever saw that [a] beard improved."
Robert E. Lee was appointed superintendent of the Academy in 1852, and Stuart became a friend of the Lee family, seeing them socially on frequent occasions. Lee's nephew, Fitzhugh Lee, also arrived at the academy in 1852. In Stuart's final year, in addition to achieving the cadet rank of second captain of the corps, he was one of eight cadets designated as honorary "cavalry officers" for his skills in horsemanship. Stuart graduated 13th in his class of 46 in 1854. He ranked tenth in his class in cavalry tactics. Although he enjoyed the civil engineering curriculum at the academy and did well in mathematics, his poor drawing skills hampered his engineering studies, and he finished 29th in that discipline. A Stuart family tradition says he deliberately degraded his academic performance in his final year to avoid service in the elite, but dull, Corps of Engineers.
United States Army.
Stuart was commissioned a brevet second lieutenant and assigned to the U.S. Regiment of Mounted Riflemen in Texas. After an arduous journey, he reached Fort Davis on January 28, 1855, and was a leader for three months on scouting missions over the San Antonio to El Paso Road. He was soon transferred to the newly formed 1st Cavalry Regiment (1855) at Fort Leavenworth, Kansas Territory, where he became regimental quartermaster and commissary officer under the command of Col. Edwin V. Sumner. He was promoted to first lieutenant in 1855.
Also in 1855, Stuart met Flora Cooke, the daughter of the commander of the 2nd U.S. Dragoon Regiment, Lieutenant Colonel Philip St. George Cooke. Burke Davis described Flora as "an accomplished horsewoman, and though not pretty, an effective charmer," to whom "Stuart succumbed with hardly a struggle." They became engaged in September, less than two months after meeting. Stuart humorously wrote of his rapid courtship in Latin, "Veni, Vidi, Victus sum" (I came, I saw, I was conquered). Although a gala wedding was planned for Fort Riley, Kansas, the death of Stuart's father on September 20 caused a change of plans and the marriage on November 14 was small and limited to family witnesses. The couple owned two slaves until 1859, one inherited from his father's estate, the other purchased.
Stuart's leadership capabilities were soon recognized. He was a veteran of the frontier conflicts with Native Americans and the antebellum violence of Bleeding Kansas. He was wounded on July 29, 1857, while fighting at Solomon River, Kansas, against the Cheyenne. Col. Sumner ordered a charge with drawn sabers against a wave of Indian arrows. Scattering the warriors, Stuart and three other lieutenants chased one down, whom Stuart wounded in the thigh with his pistol. The Cheyenne turned and fired at Stuart with an old-fashioned pistol, striking him in the chest with a bullet, which did little more damage than to pierce the skin. Stuart returned in September to Fort Leavenworth and was reunited with his wife.
Their first child, a girl, had been born in 1856 but died the same day. On November 14, 1857, Flora gave birth to another daughter, whom the parents named Flora after her mother. The family relocated in early 1858 to Fort Riley, where they remained for three years.
In 1859, Stuart developed a new piece of cavalry equipment, for which he received patent number 25,684 on October 4—a saber hook, or an "improved method of attaching sabers to belts." The U.S. government paid Stuart $5,000 for a "right to use" license and Stuart contracted with Knorr, Nece and Co. of Philadelphia to manufacture his hook. While in Washington, D.C., to discuss government contracts, and in conjunction with his application for an appointment into the quartermaster department, Stuart heard about John Brown's raid on the U.S. Arsenal at Harpers Ferry. Stuart volunteered to be aide-de-camp to Col. Robert E. Lee and accompanied Lee with a company of U.S. Marines from the Washington Navy Yard and four companies of Maryland militia. While delivering Lee's written surrender ultimatum to the leader of the group, who had been calling himself Isaac Smith, Stuart recognized "Old Ossawatomie Brown" from his days in Kansas.
Stuart was promoted to captain on April 22, 1861, but resigned from the U.S. Army on May 3, 1861, to join the Confederate States Army, following the secession of Virginia. (His letter of resignation, sent from Cairo, Illinois, was accepted by the War Department on May 14.) Upon learning that his father-in-law, Col. Cooke, would remain in the U.S. Army during the coming war, Stuart wrote to his brother-in-law (future Confederate Brig. Gen. John Rogers Cooke), "He will regret it but once, and that will be continuously." On June 26, 1860, Flora gave birth to a son, Philip St. George Cooke Stuart, but his father changed the name to James Ewell Brown Stuart, Jr. ("Jimmie"), in late 1861 out of disgust with his father-in-law.
Confederate Army.
Early service.
Stuart was commissioned as a lieutenant colonel of Virginia Infantry in the Confederate Army on May 10, 1861. Maj. Gen. Robert E. Lee, now commanding the armed forces of Virginia, ordered him to report to Colonel Thomas J. Jackson at Harper's Ferry. Jackson chose to ignore Stuart's infantry designation and assigned him on July 4 to command all the cavalry companies of the Army of the Shenandoah, organized as the 1st Virginia Cavalry Regiment. He was promoted to colonel on July 16.
[Stuart] is a rare man, wonderfully endowed by nature with the qualities necessary for an officer of light cavalry. ... Calm, firm, acute, active, and enterprising, I know no one more competent than he to estimate the occurrences before him at their true value. If you add to this army a real brigade of cavalry, you can find no better brigadier-general to command it.
General Joseph E. Johnston, letter to Confederate President Jefferson Davis, August 1861
After early service in the Shenandoah Valley, Stuart led his regiment in the First Battle of Bull Run, and participated in the pursuit of the retreating Federals. He then commanded the Army's outposts along the upper Potomac River until given command of the cavalry brigade for the army then known as the Army of the Potomac (later named the Army of Northern Virginia). He was promoted to brigadier general on September 24, 1861.
Peninsula.
In 1862, the Union Army of the Potomac began its Peninsula Campaign against Richmond, Virginia, and Stuart's cavalry brigade assisted Gen. Joseph E. Johnston's army as it withdrew up the Virginia Peninsula in the face of superior numbers. Stuart fought at the Battle of Williamsburg, but in general the terrain and weather on the Peninsula did not lend themselves to cavalry operations. However, when Gen. Robert E. Lee became commander of the Army of Northern Virginia, he requested that Stuart perform reconnaissance to determine whether the right flank of the Union army was vulnerable. Stuart set out with 1,200 troopers on the morning of June 12 and, having determined that the flank was indeed vulnerable, took his men on a complete circumnavigation of the Union army, returning after 150 miles on July 15 with 165 captured Union soldiers, 260 horses and mules, and various quartermaster and ordnance supplies. His men met no serious opposition from the more decentralized Union cavalry, coincidentally commanded by his father-in-law, Col. Cooke. The maneuver was a public relations sensation and Stuart was greeted with flower petals thrown in his path at Richmond. He had become as famous as Stonewall Jackson in the eyes of the Confederacy.
Northern Virginia.
Early in the Northern Virginia Campaign, Stuart was promoted to major general on July 25, 1862, and his command was upgraded to the Cavalry Division. He was nearly captured and lost his signature plumed hat and cloak to pursuing Federals during a raid in August, but in a retaliatory raid at Catlett's Station the following day, managed to overrun Union army commander Maj. Gen. John Pope's headquarters, and not only captured Pope's full uniform, but also intercepted orders that provided Lee with valuable intelligence concerning reinforcements for Pope's army.
At the Second Battle of Bull Run (Second Manassas), Stuart's cavalry followed the massive assault by Longstreet's infantry against Pope's army, protecting its flank with artillery batteries. Stuart ordered Brig. Gen. Beverly Robertson's brigade to pursue the Federals and in a sharp fight against Brig. Gen. John Buford's brigade, Col. Thomas T. Munford's 2nd Virginia Cavalry was overwhelmed until Stuart sent in two more regiments as reinforcements. Buford's men, many of whom were new to combat, retreated across Lewis's Ford and Stuart's troopers captured over 300 of them. Stuart's men harassed the retreating Union columns until the campaign ended at the Battle of Chantilly.
Maryland.
During the Maryland Campaign of September 1862, Stuart's cavalry screened the army's movement north. He bears some responsibility for Robert E. Lee's lack of knowledge of the position and celerity of the pursuing Army of the Potomac under George B. McClellan. For a five-day period, Stuart rested his men and entertained local civilians at a gala ball at Urbana, Maryland. His reports make no reference to intelligence gathering by his scouts or patrols. As the Union Army drew near to Lee's divided army, Stuart's men skirmished at various points on the approach to Frederick and Stuart was not able to keep his brigades concentrated enough to resist the oncoming tide. He misjudged the Union routes of advance, ignorant of the Union force threatening Turner's Gap, and required assistance from the infantry of Maj. Gen. D.H. Hill to defend the South Mountain passes in the Battle of South Mountain. His horse artillery bombarded the flank of the Union army as it opened its attack in the Battle of Antietam. By mid-afternoon, Stonewall Jackson ordered Stuart to command a turning movement with his cavalry against the Union right flank and rear, which if successful would be followed up by an infantry attack from the West Woods. Stuart began probing the Union lines with more artillery barrages, which were answered with "murderous" counterbattery fire and the cavalry movement intended by Jackson was never launched.
Stuart and Jackson were an unlikely pair: one outgoing, the other introverted; one flashily uniformed, the other plainly dressed; one Prince Rupert and the other Cromwell. Yet Stuart's self-confidence, penchant for action, deep love of Virginia, and total abstinence from such vices as alcohol, tobacco, and pessimism endeared him to Jackson. ... Stuart was the only man in the Confederacy could make Jackson laugh—and who dared to do so.
James I. Robertson, Jr., "Stonewall Jackson"
Three weeks after Lee's army had withdrawn back to Virginia, on October 10–12, 1862, Stuart performed another of his audacious circumnavigations of the Army of the Potomac, his Chambersburg Raid—126 miles in under 60 hours, from Darkesville, West Virginia to as far north as Mercersburg, Pennsylvania and Chambersburg and around to the east through Emmitsburg, Maryland and south through Hyattstown, Maryland and White's Ford to Leesburg, Virginia—once again embarrassing his Union opponents and seizing horses and supplies, but at the expense of exhausted men and animals, without gaining much military advantage. Jubal Early referred to it as "the greatest horse stealing expedition" that only "annoyed" the enemy. Stuart gave his friend Jackson a fine, new officer's tunic, trimmed with gold lace, commissioned from a Richmond tailor, which he thought would give Jackson more of the appearance of a proper general (something to which Jackson was notoriously indifferent).
McClellan pushed his army slowly south, urged by President Lincoln to pursue Lee, crossing the Potomac starting on October 26. As Lee began moving to counter this, Stuart screened Longstreet's Corps and skirmished numerous times in early November against Union cavalry and infantry around Mountville, Aldie, and Upperville. On November 6, Stuart received sad news by telegram that his daughter Flora had died just before her fifth birthday of typhoid fever on November 3.
Fredericksburg and Chancellorsville.
In the December 1862 Battle of Fredericksburg, Stuart and his cavalry—most notably his horse artillery under Major John Pelham—protected Stonewall Jackson's flank at Hamilton's Crossing. General Lee commended his cavalry, which "effectually guarded our right, annoying the enemy and embarrassing his movements by hanging on his flank, and attacking when the opportunity occurred." Stuart reported to Flora the next day that he had been shot through his fur collar but was unhurt.
After Christmas, Lee ordered Stuart to conduct a raid north of the Rappahannock River to "penetrate the enemy's rear, ascertain if possible his position & movements, & inflict upon him such damage as circumstances will permit." Assigning 1,800 troopers and a horse artillery battery to the operation, Stuart's raid reached as far north as 4 miles south of Fairfax Court House, seizing 250 prisoners, horses, mules, and supplies. Tapping telegraph lines, his signalmen intercepted messages between Union commanders and Stuart sent a personal telegram to Union Quartermaster General Montgomery C. Meigs, "General Meigs will in the future please furnish better mules; those you have furnished recently are very inferior."
On March 17, 1863, Stuart's cavalry clashed with a Union raiding party at Kelly's Ford. The minor victory was marred by the death of Major Pelham, which caused Stuart profound grief, as he thought of him as close as a younger brother. He wrote to a Confederate Congressman, "The noble, the chivalric, the gallant Pelham is no more. ... Let the tears of agony we have shed, and the gloom of mourning throughout my command bear witness." Flora was pregnant at the time and Stuart told her that if it were a boy, he wanted him to be named John Pelham Stuart. (Virginia Pelham Stuart was born October 9.)
At the Battle of Chancellorsville, Stuart accompanied Stonewall Jackson on his famous flanking march of May 2, 1863, and started to pursue the retreating soldiers of the Union XI Corps when he received word that both Jackson and his senior division commander, Maj. Gen. A.P. Hill, had been wounded. Hill, bypassing the next most senior infantry general in the corps, Brig. Gen. Robert E. Rodes, sent a message ordering Stuart to take command of the Second Corps. Although the delays associated with this change of command effectively ended the flanking attack the night of May 2, Stuart performed credibly as an infantry corps commander the following day, launching a strong and well-coordinated attack against the Union right flank at Chancellorsville. When Union troops abandoned Hazel Grove, Stuart had the presence of mind to quickly occupy it and bombard the Union positions with artillery. Stuart relinquished his infantry command on May 6 when Hill returned to duty. Stephen W. Sears wrote:
... It is hard to see how Jeb Stuart, in a new command, a cavalryman commanding infantry and artillery for the first time, could have done a better job. The astute Porter Alexander believed all credit was due: "Altogether, I do not think there was a more brilliant thing done in the war than Stuart's extricating that command from the extremely critical position in which he found it."
Stonewall Jackson died on May 10 and Stuart was once again devastated by the loss of a close friend, telling his staff that the death was a "national calamity." Jackson's wife, Mary Anna, wrote to Stuart on August 1, thanking him for a note of sympathy: "I need not assure you of which you already know, that your friendship & admiration were cordially reciprocated by him. I have frequently heard him speak of Gen'l Stuart as one of his warm personal friends, & also express admiration for your Soldierly qualities."
Brandy Station.
The grand review of June 5 was surely the proudest day of Jeb Stuart's thirty years. As he led a cavalcade of resplendent staff officers to the reviewing stand, trumpeters heralded his coming and women and girls strewed his path with flowers. Before all of the spectators the assembled cavalry brigade stretched a mile and a half. After Stuart and his entourage galloped past the line in review, the troopers in their turn saluted the reviewing stand in columns of squadrons. In performing a second "march past," the squadrons started off at a trot, then spurred to a gallop. Drawing sabers and breaking into the Rebel yell, the troopers rush toward the horse artillery drawn up in battery. The gunners responded defiantly, firing blank charges. Amidst this tumult of cannon fire and thundering hooves, a number of ladies swooned in their escorts' arms.
Stephen W. Sears, "Gettysburg"
Returning to the cavalry for the Gettysburg Campaign, Stuart endured the two low points in his career, starting with the Battle of Brandy Station, the largest predominantly cavalry engagement of the war. By June 5, two of Lee's infantry corps were camped in and around Culpeper. Six miles northeast, holding the line of the Rappahannock River, Stuart bivouacked his cavalry troopers, mostly near Brandy Station, screening the Confederate Army against surprise by the enemy. Stuart requested a full field review of his troops by Gen. Lee. This grand review on June 5 included nearly 9,000 mounted troopers and 4 batteries of horse artillery, charging in simulated battle at Inlet Station, about two miles (3 km) southwest of Brandy Station.
Lee was not able to attend the review, however, so it was repeated in his presence on June 8, although the repeated performance was limited to a simple parade without battle simulations. Despite the lower level of activity, some of the cavalrymen and the newspaper reporters at the scene complained that all Stuart was doing was feeding his ego and exhausting the horses. Lee ordered Stuart to cross the Rappahannock the next day and raid Union forward positions, screening the Confederate Army from observation or interference as it moved north. Anticipating this imminent offensive action, Stuart ordered his tired troopers back into bivouac around Brandy Station.
Army of the Potomac commander Maj. Gen. Joseph Hooker interpreted Stuart's presence around Culpeper to be indicative of preparations for a raid on his army's supply lines. In reaction to this, he ordered his cavalry commander, Maj. Gen. Alfred Pleasonton, to take a combined arms force of 8,000 cavalrymen and 3,000 infantry on a "spoiling raid" to "disperse and destroy" the 9,500 Confederates. Pleasonton's force crossed the Rappahannock in two columns on June 9, 1863, the first crossing at Beverly's Ford (Brig. Gen. John Buford's division) catching Stuart by surprise, waking him and his staff to the sound of gunfire. The second crossing, at Kelly's Ford, surprised Stuart again, and the Confederates found themselves assaulted from front and rear in a spirited melee of mounted combat. A series of confusing charges and countercharges swept back and forth across Fleetwood Hill, which had been Stuart's headquarters the previous night. After 10 hours of fighting, Pleasonton ordered his men to withdraw across the Rappahannock.
If Gen. Stuart is to be the eyes and ears of the army we advise him to see more, and be seen less. ... Gen. Stuart has suffered no little in public estimation by the late enterprises of the enemy.
Richmond "Enquirer", June 12, 1863
Although Stuart claimed a victory because the Confederates held the field, Brandy Station is considered a tactical draw, and both sides came up short. Pleasonton was not able to disable Stuart's force at the start of an important campaign and he withdrew before finding the location of Lee's infantry nearby. However, the fact that the Southern cavalry had not detected the movement of two large columns of Union cavalry, and that they fell victim to a surprise attack, was an embarrassment that prompted serious criticism from fellow generals and the Southern press. The fight also revealed the increased competency of the Union cavalry, and foreshadowed the decline of the formerly invincible Southern mounted arm.
Stuart's ride in the Gettysburg Campaign.
Following a series of small cavalry battles in June as Lee's army began marching north through the Shenandoah Valley, Stuart may have had in mind the glory of circumnavigating the enemy army once again, desiring to erase the stain on his reputation of the surprise at Brandy Station. General Lee gave orders to Stuart on June 22 on how he was to participate in the march north, and the exact nature of those orders has been argued by the participants and historians ever since, but the essence was that he was instructed to guard the mountain passes with part of his force while the Army of Northern Virginia was still south of the Potomac and that he was to cross the river with the remainder of the army and screen the right flank of Ewell's Second Corps. Instead of taking a direct route north near the Blue Ridge Mountains, however, Stuart chose to reach Ewell's flank by taking his three best brigades (those of Brig. Gen. Wade Hampton, Brig. Gen. Fitzhugh Lee, and Col. John R. Chambliss, the latter replacing the wounded Brig. Gen. W.H.F. "Rooney" Lee) between the Union army and Washington, moving north through Rockville to Westminster and on into Pennsylvania, hoping to capture supplies along the way and cause havoc near the enemy capital. Stuart and his three brigades departed Salem Depot at 1 a.m. on June 25.
Unfortunately for Stuart's plan, the Union army's movement was underway and his proposed route was blocked by columns of Federal infantry, forcing him to veer farther to the east than either he or General Lee had anticipated. This prevented Stuart from linking up with Ewell as ordered and deprived Lee of the use of his prime cavalry force, the "eyes and ears" of the army, while advancing into unfamiliar enemy territory.
Stuart's command crossed the Potomac River at 3 a.m. on June 28. At Rockville they captured a wagon train of 140 brand-new, fully loaded wagons and mule teams. This wagon train would prove to be a logistical hindrance to Stuart's advance, but he interpreted Lee's orders as placing importance on gathering supplies. The proximity of the Confederate raiders provoked some consternation in the national capital and two Union cavalry brigades and an artillery battery were sent to pursue the Confederates. Stuart supposedly said that were it not for his fatigued horses "he would have marched down the 7th Street Road [and] took Abe & Cabinet prisoners."
In Westminster on June 29, his men clashed briefly with and overwhelmed two companies of Union cavalry, chasing them a long distance on the Baltimore road, which Stuart claimed caused a "great panic" in the city of Baltimore. The head of Stuart's column encountered Brig. Gen. Judson Kilpatrick's cavalry as it passed through Hanover and scattered it on June 30; the Battle of Hanover ended after Kilpatrick's men regrouped and drove the Confederates out of town. Stuart's brigades had been better positioned to guard their captured wagon train than to take advantage of the encounter with Kilpatrick. After a 20-mile trek in the dark, his exhausted men reached Dover on the morning of July 1, as the Battle of Gettysburg was commencing without them.
Stuart headed next for Carlisle, hoping to find Ewell. He lobbed a few shells into town during the early evening of July 1 and burned the Carlisle Barracks before withdrawing to the south towards Gettysburg. He and the bulk of his command reached Lee at Gettysburg the afternoon of July 2. He ordered Wade Hampton to cover the left rear of the Confederate battle lines, and Hampton fought with Brig. Gen. George Armstrong Custer at the Battle of Hunterstown before joining Stuart at Gettysburg.
Gettysburg and its aftermath.
When Stuart arrived at Gettysburg on the afternoon of July 2—bringing with him the caravan of captured Union supply wagons—he received a rare rebuke from Lee. (No one witnessed the private meeting between Lee and Stuart, but reports circulated at headquarters that Lee's greeting was "abrupt and frosty." Colonel Edward Porter Alexander wrote, "Although Lee said only, 'Well, General, you are here at last,' his manner implied rebuke, and it was so understood by Stuart.") On the final day of the battle, Stuart was ordered to get into the enemy's rear and disrupt its line of communications at the same time Pickett's Charge was sent against the Union positions on Cemetery Ridge, but his attack on East Cavalry Field was repulsed by Union cavalry under Brig. Gens. David Gregg and George Custer.
During the retreat from Gettysburg, Stuart devoted his full attention to supporting the army's movement, successfully screening against aggressive Union cavalry pursuit and escorting thousands of wagons with wounded men and captured supplies over difficult roads and through inclement weather. Numerous skirmishes and minor battles occurred during the screening and delaying actions of the retreat. Stuart's men were the final units to cross the Potomac River, returning to Virginia in "wretched condition—completely worn out and broken down."
The failure to crush the Federal army in Pennsylvania in 1863, in the opinion of almost all of the officers of the Army of Northern Virginia, can be expressed in five words—"the absence of the cavalry".
Confederate Maj. Gen. Henry Heth
The Gettysburg Campaign was the most controversial of Stuart's career. He became one of the scapegoats (along with James Longstreet) blamed for Lee's loss at Gettysburg by proponents of the postbellum Lost Cause movement, such as Jubal Early. This was fueled in part by opinions of less partisan writers, such as Stuart's subordinate, Thomas L. Rosser, who stated after the war that Stuart did, "on this campaign, "undoubtedly", make the fatal blunder which lost us the battle of Gettysburg." In General Lee's report on the campaign, he wrote
... the absence of the cavalry rendered it impossible to obtain accurate information. ... By the route [Stuart] pursued, the Federal Army was interposed between his command and our main body, preventing any communication with him until his arrival at Carlisle. The march toward Gettysburg was conducted more slowly than it would have been had the movements of the Federal Army been known.
One of the most forceful postbellum defenses of Stuart was by Col. John S. Mosby, who had served under him during the campaign and was fiercely loyal to the late general, writing, "He made me all that I was in the war. ... But for his friendship I would never have been heard of." He wrote numerous articles for popular publications and published a book length treatise in 1908, a work that relied on his skills as a lawyer to refute categorically all of the claims laid against Stuart.
Modern scholarship remains divided on Stuart's culpability. Edward G. Longacre argues that Lee deliberately gave Stuart wide discretion in his orders and had no complaints about Stuart's tardy arrival at Gettysburg because he established no date by which the cavalry was required to link up with Ewell. The 3½ brigades of cavalry left with the main army were adequate for Lee to negotiate enemy territory safely and that his choice not to use these brigades effectively cannot be blamed on Stuart. Edwin B. Coddington refers to the "tragedy" of Stuart in the Gettysburg Campaign and judges that when Fitzhugh Lee raised the question of "whether Stuart exercised the discretion "undoubtedly given to him, judiciously"," the answer is no. Nevertheless, replying to historians who maintain that Stuart's absence permitted Lee to be surprised at Gettysburg, Coddington points out that the Union commander, Maj. Gen. George Meade, was just as surprised, and the initial advantage lay with Lee. Eric J. Wittenberg and J. David Petruzzi have concluded that there was "plenty of blame to go around" and the fault should be divided between Stuart, the lack of specificity in Lee's orders, and Richard S. Ewell, who might have tried harder to link up with Stuart northeast of Gettysburg. Jeffry D. Wert acknowledges that Lee, his officers, and fighting by the Army of the Potomac bear the responsibility for the Confederate loss at Gettysburg, but states that "Stuart failed Lee and the army in the reckoning at Gettysburg. ... Lee trusted him and gave him discretion, but Stuart acted injudiciously."
Although Stuart was not reprimanded or disciplined in any official way for his role in the Gettysburg campaign, it is noteworthy that his appointment to corps command on September 9, 1863, did not carry with it a promotion to lieutenant general. Edward Bonekemper wrote that since all other corps commanders in the Army of Northern Virginia carried this rank, Lee's decision to keep Stuart at major general rank, while at the same time promoting Stuart's subordinates Wade Hampton and Fitzhugh Lee to major generals, could be considered an implied rebuke. Jeffry D. Wert wrote that there is no evidence Lee considered Stuart's performance during the Gettysburg Campaign and that it is "more likely that Lee thought the responsibilities in command of a cavalry corps did not equal those of an infantry corps."
Fall 1863 and the 1864 Overland Campaign.
[The cavalry's success in the Bristoe Campaign can be attributed] to the generalship, boldness, and untiring energy of Major-General Stuart, for it was he who directed every movement of importance, and his generalship, boldness, and energy won the unbounded confidence of officers and men, and gave the prestige of success.
Confederate Colonel Oliver Funsten
Lee reorganized his cavalry on September 9, creating a Cavalry Corps for Stuart with two divisions of three brigades each. In the Bristoe Campaign, Stuart was assigned to lead a broad turning movement in an attempt to get into the enemy's rear, but General Meade skillfully withdrew his army without leaving opportunities to take advantage. On October 13, Stuart blundered into the rear guard of the Union III Corps near Warrenton. Ewell's corps was sent to rescue him, but Stuart hid his troopers in a wooded ravine until the unsuspecting III Corps moved on, and the assistance was not necessary. As Meade withdrew towards Manassas Junction, brigades from the Union II Corps fought a rearguard action against Stuart's cavalry and the infantry of Brig. Gen. Harry Hays's division near Auburn on October 14. Stuart's cavalry boldly bluffed Warren's infantry and escaped disaster. After the Confederate repulse at Bristoe Station and an aborted advance on Centreville, Stuart's cavalry shielded the withdrawal of Lee's army from the vicinity of Manassas Junction. Judson Kilpatrick's Union cavalry pursued Stuart's cavalry along the Warrenton Turnpike but were lured into an ambush near Chestnut Hill and routed. The Federal troopers were scattered and chased five miles (8 km) in an affair that came to be known as the "Buckland Races". The Southern press began to mute its criticism of Stuart's following his successful performance during the fall campaign.
The Overland Campaign, Lt. Gen. Ulysses S. Grant's offensive against Lee in the spring of 1864, began at the Battle of the Wilderness, where Stuart aggressively pushed Thomas L. Rosser's Laurel Brigade into a fight against George Custer's better-armed Michigan Brigade, resulting in significant losses. General Lee sent a message to Stuart: "It is very important to save your Cavalry & not wear it out. ... You must use your good judgment to make any attack which may offer advantages." As the armies maneuvered toward their next confrontation at Spotsylvania Court House, Stuart's cavalry fought delaying actions against the Union cavalry. His defense at Laurel Hill, also directing the infantry of Brig. Gen. Joseph B. Kershaw, skillfully delayed the advance of the Federal army for nearly 5 critical hours.
Yellow Tavern and death.
The commander of the Army of the Potomac, Maj. Gen. George Meade, and his cavalry commander, Maj. Gen. Philip Sheridan, quarreled about the Union cavalry's performance in the first two engagements of the Overland Campaign. Sheridan heatedly asserted that he wanted to "concentrate all of cavalry, move out in force against Stuart's command, and whip it." Meade reported the comments to Grant, who replied "Did Sheridan say that? Well, he generally knows what he is talking about. Let him start right out and do it." Sheridan immediately organized a raid against Confederate supply and railroad lines close to Richmond, which he knew would bring Stuart to battle.
Sheridan moved aggressively to the southeast, crossing the North Anna River and seizing Beaver Dam Station on the Virginia Central Railroad, where his men liberated a train carrying 3,000 Union prisoners and destroyed more than one million rations and medical supplies destined for Lee's army. Stuart dispatched a force of about 3,000 cavalrymen to intercept Sheridan's cavalry, which was more than three times their numbers. As he rode in pursuit, accompanied by his aide, Maj. Andrew R. Venable, they were able to stop briefly along the way to be greeted by Stuart's wife, Flora, and his children, Jimmie and Virginia. Venable wrote of Stuart, "He told me he never expected to live through the war, and that if we were conquered, that he did not want to live."
The Battle of Yellow Tavern occurred May 11, at an abandoned inn located six miles (10 km) north of Richmond. The Confederate troopers tenaciously resisted from the low ridgeline bordering the road to Richmond, fighting for over three hours. A countercharge by the 1st Virginia Cavalry pushed the advancing Union troopers back from the hilltop as Stuart, on horseback, shouted encouragement while firing his revolver at the Union troopers. As the 5th Michigan Cavalry streamed in retreat past Stuart, a dismounted Union private, 44-year-old John A. Huff, turned and shot Stuart with his .44-caliber revolver from a distance of 10–30 yards.
Huff's bullet struck Stuart in the left side. It then sliced through his stomach and exited his back, one inch to the right of his spine. Stuart suffered great pain as an ambulance took him to Richmond to await his wife's arrival at the home of Dr. Charles Brewer, his brother-in-law. Stuart ordered his sword and spurs be given to his son. His last whispered words were: "I am resigned; God's will be done." He died at 7:38 p.m. on May 12, the following day, before Flora Stuart reached his side. He was 31 years old. Stuart was buried in Richmond's Hollywood Cemetery. Upon learning of Stuart's death, General Lee is reported to have said that he could hardly keep from weeping at the mere mention of Stuart's name and that Stuart had never given him a bad piece of information.
Flora wore the black of mourning for the remainder of her life, and never remarried. She lived in Saltville, Virginia, for 15 years after the war, where she opened and taught at a school in a log cabin. She worked from 1880 to 1898 as principal of the Virginia Female Institute in Staunton, Virginia, a position for which Robert E. Lee had recommended her before his death ten years earlier. In 1907, the Institute was renamed Stuart Hall School in her honor. Upon the death of her daughter Virginia, from complications in childbirth in 1898, Flora resigned from the Institute and moved to Norfolk, Virginia, where she helped Virginia's widower, Robert Page Waller, in raising her grandchildren. She died in Norfolk on May 10, 1923, after striking her head in a fall on a city sidewalk. She is buried alongside her husband and their daughter, Little Flora, in Hollywood Cemetery in Richmond.
Legacy and memorials.
Like his intimate friend, Stonewall Jackson, General J.E.B. Stuart was a legendary figure and is considered one of the greatest cavalry commanders in American history. His friend from his federal army days, Union Maj. Gen. John Sedgwick, said that Stuart was "the greatest cavalry officer ever foaled in America." Jackson and Stuart, both of whom were killed in battle, had colorful public images, although the latter seems to have been more deliberately crafted. Jeffry D. Wert wrote about Stuart:
Stuart had been the Confederacy's knight-errant, the bold and dashing cavalier, attired in a resplendent uniform, plumed hat, and cape. Amid a slaughterhouse, he had embodied chivalry, clinging to the pageantry of a long-gone warrior. He crafted the image carefully, and the image befitted him. He saw himself as the Southern people envisaged him. They needed a knight; he needed to be that knight.
A statue of General J.E.B. Stuart by sculptor Frederick Moynihan was dedicated on Richmond's famed Monument Avenue at Stuart Circle in 1907. Like General Stonewall Jackson, his equestrian statue faces north, indicating that he died in the war. In 1884 the town of Taylorsville, Virginia, was renamed Stuart. The British Army named two models of American-made World War II tanks, the M3 and M5, the Stuart tank in General Stuart's honor. A high school in Fairfax, Virginia and a middle school in Jacksonville, Florida are named for him.
In December 2006, a personal Confederate battle flag, sewn by Flora Stuart, was sold in a Heritage Auction for a world-record price for any Confederate flag, for $956,000 (including buyer's premium). The 34-inch by 34-inch flag was hand-sewn for Stuart by Flora in 1862 and Stuart carried it into some of his most famous battles. 
Stuart's birthplace, Laurel Hill, located in Patrick County, Virginia, was purchased by the J.E.B. Stuart Birthplace Preservation Trust, Inc., in 1992 to preserve and interpret it.
In popular media.
J.E.B. Stuart is a character in the historical adventure novel "Flashman and the Angel of the Lord" by George Macdonald Fraser featuring Stuart's early-career role in the US Army at abolitionist John Brown's raid on Harpers Ferry.
In the long-running comic book "G.I. Combat", featuring "The Haunted Tank", published by DC Comics from the 1960s through the late 1980s, the ghost of General Stuart guided a tank crew (the tank being, at first, a Stuart, later a Sherman) commanded by his namesake, Lt. Jeb Stuart.
Joseph Fuqua played Stuart in the films "Gettysburg" and "Gods and Generals".
Stuart, along with his warhorse Skylark, is featured prominently in the novel "Traveller" by Richard Adams.
Errol Flynn played Stuart in the movie "Santa Fe Trail", depicting his antebellum life, confronting John Brown in Kansas and at Harper's Ferry. The movie has become infamous for its many historical inaccuracies, one of which was that Stuart, George Armstrong Custer (portrayed by Ronald Reagan in the film), and Philip Sheridan were firm friends and all attended West Point together in 1854.
In the 1988 alternate history novel "Gray Victory", author Robert Skimin depicts Stuart surviving his wound from the battle of Yellow Tavern. After the war, in which the Confederacy emerges victorious, he faces a court of inquiry over his actions at the Battle of Gettysburg.
In the alternate-history novel "How Few Remain" by Harry Turtledove, Stuart is the commanding Confederate general in charge of the occupation and defense of the recently purchased Mexican provinces of Sonora and Chihuahua in 1881. This is the first volume of the Southern Victory series, where the USA and CSA fight each other repeatedly in the 19th and 20th centuries.
Several short stories in Barry Hannah's collection "Airships" feature Stuart as a character.
Stuart is also a character in L.M. Elliott's "Annie, Between the States".
"When I Was On Horseback," a song on the folk group Arborea's 2013 album "Fortress of the Sun", features lyrics that refer to Stuart's death near Richmond, Virginia.

</doc>
<doc id="16389" url="http://en.wikipedia.org/wiki?curid=16389" title="Java virtual machine">
Java virtual machine

A Java virtual machine (JVM) is an abstract computing machine. There are three notions of the JVM: specification, implementation, and instance. The specification is a book that formally describes what is required of a JVM implementation. Having a single specification ensures all implementations are interoperable. A JVM implementation is a computer program that meets the requirements of the JVM specification in a compliant and preferably performant manner. An instance of the JVM is a process that executes a computer program compiled into Java bytecode. 
The Oracle Corporation, which owns the Java trademark, distributes the Java Virtual Machine implementation HotSpot together with an implementation of the Java Class Library under the name Java Runtime Environment (JRE).
JVM specification.
The Java virtual machine is an abstract (virtual) computer defined by a specification. This specification omits implementation details that are not essential to ensure interoperability. For example, the memory layout of run-time data areas, the garbage-collection algorithm used, and any internal optimization of the Java virtual machine instructions (their translation into machine code). The main reason for this omission is to not unnecessarily constrain implementors. Any Java application can be run only inside some concrete implementation of the abstract specification of the Java virtual machine.
Starting with Java Platform, Standard Edition (J2SE) 5.0, changes to the JVM specification have been developed under the Java Community Process as JSR 924. s of 2006[ [update]], changes to specification to support changes proposed to the class file format (JSR 202) are being done as a maintenance release of JSR 924. The specification for the JVM is published in book form, known as "blue book". The preface states:
We intend that this specification should sufficiently document the Java Virtual Machine to make possible compatible clean-room implementations. Oracle provides tests that verify the proper operation of implementations of the Java Virtual Machine.
One of Oracle's JVMs is named HotSpot, the other, inherited from BEA Systems is JRockit. Clean-room Java implementations include Kaffe and IBM J9. Oracle owns the Java trademark, and may allow its use to certify implementation suites as fully compatible with Oracle's specification.
Class loader.
One of the organizational units of JVM bytecode is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides "class" files, but it must recognize "class" files.
The class loader performs three basic activities in this strict order:
In general, there are two types of class loader: bootstrap class loader and user defined class loader.
Every Java virtual machine implementation must have a bootstrap class loader, capable of loading trusted classes. The Java virtual machine specification doesn't specify how a class loader should locate classes.
Bytecode instructions.
The JVM has instructions for the following groups of tasks:
The aim is binary compatibility. Each particular host operating system needs its own implementation of the JVM and runtime. These JVMs interpret the bytecode semantically the same way, but the actual implementation may be different. More complex than just emulating bytecode is compatibly and efficiently implementing the Java core API that must be mapped to each host operating system.
JVM languages.
A JVM language is any language with functionality that can be expressed in terms of a valid class file which can be hosted by the Java Virtual Machine. A class file contains Java Virtual Machine instructions (or bytecode) and a symbol table, as well as other ancillary information. The class file format, the hardware- and operating system-independent binary format used to represent compiled classes and interfaces. Java 7 JVM implements on the Java Platform, a new feature which supports dynamically typed languages in the JVM. This feature is developed within the whose mission is to extend the JVM so that it supports languages other than Java.
Bytecode verifier.
A basic philosophy of Java is that it is inherently "safe" from the standpoint that no user program can "crash" the host machine or otherwise interfere inappropriately with other operations on the host machine, and that it is possible to protect certain methods and data structures belonging to "trusted" code from access or corruption by "untrusted" code executing within the same JVM. Furthermore, common programmer errors that often led to data corruption or unpredictable behavior such as accessing off the end of an array or using an uninitialized pointer are not allowed to occur. Several features of Java combine to provide this safety, including the class model, the garbage-collected heap, and the verifier.
The JVM "verifies" all bytecode before it is executed. This verification consists primarily of three types of checks:
The first two of these checks take place primarily during the "verification" step that occurs when a class is loaded and made eligible for use. The third is primarily performed dynamically, when data items or methods of a class are first accessed by another class.
The verifier permits only some bytecode sequences in valid programs, e.g. a jump (branch) instruction can only target an instruction within the same method. Furthermore, the verifier ensures that any given instruction operates on a fixed stack location, allowing the JIT compiler to transform stack accesses into fixed register accesses. Because of this, that the JVM is a stack architecture does not imply a speed penalty for emulation on register-based architectures when using a JIT compiler. In the face of the code-verified JVM architecture, it makes no difference to a JIT compiler whether it gets named imaginary registers or imaginary stack positions that must be allocated to the target architecture's registers. In fact, code verification makes the JVM different from a classic stack architecture, of which efficient emulation with a JIT compiler is more complicated and typically carried out by a slower interpreter.
The original specification for the bytecode verifier used natural language that was "incomplete or incorrect in some respects." A number of attempts have been made to specify the JVM as a formal system. By doing this, the security of current JVM implementations can more thoroughly be analyzed, and potential security exploits prevented. It will also be possible to optimize the JVM by skipping unnecessary safety checks, if the application being run is proven to be safe.
Secure execution of remote code.
A virtual machine architecture allows very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources, a model used by Java applets. Applets run within a VM incorporated into a user's browser, executing code downloaded from a remote HTTP server. The remote code runs in a restricted "sandbox", which is designed to protect the user from misbehaving or malicious code. Publishers can purchase a certificate with which to digitally sign applets as "safe", giving them permission to ask the user to break out of the sandbox and access the local file system, clipboard, execute external pieces of software, or network.
Bytecode interpreter and just-in-time compiler.
For each hardware architecture a different Java bytecode interpreter is needed. When a computer has a Java bytecode interpreter, it can run any Java bytecode program, and the same program can be run on any computer that has such an interpreter.
When Java bytecode is executed by an interpreter, the execution will be always slower than the execution of the same program compiled into native machine language. This problem is mitigated by just-in-time (JIT) compilers for executing Java bytecode. A just-in-time compiler may translate Java bytecode into native machine language while executing the program. The translated parts of the program can then be executed much more quickly than they could be interpreted. This technique gets applied to those parts of a program frequently executed. This way a just-in-time compiler can significantly speed up the overall execution time.
There is no necessary connection between Java and Java bytecode. A program written in Java can be compiled directly into the machine language of a real computer and programs written in other languages than Java can be compiled into Java bytecode.
Java bytecode is intended to be platform-independent and secure. Some JVM implementations do not include an interpreter, but consist only of a just-in-time compiler.
JVM in the web browser.
Since very early stages of the design process, Java (and JVM) has been marketed as a web technology for creating Rich Internet Applications.
Java applets.
On the client side, web browsers may be extended with a NPAPI Java plugin which executes so called Java applets embedded into HTML pages. The applet is allowed to draw into a rectangular region on the page assigned to it and use a restricted set of APIs that allow for example access to user's microphone or 3D acceleration. Java applets were superior to JavaScript both in performance and features until approximately 2011, when JavaScript engines in browsers were made significantly faster and the HTML 5 suite of web technologies started enhancing JavaScript with new APIs. Java applets are not able to modify the page outside its rectangular region which is not true about JavaScript. Adobe Flash Player, the main competing technology, works in the same way in this respect. Java applets are not restricted to Java and in general can be created in any JVM language.
According to W3Techs, Java applets are currently (February 2015) being used on 0.1% of all web sites. Flash is being used on 11.8% and Silverlight on 0.1% of web sites.
JavaScript JVMs and interpreters.
JVM implementations in JavaScript do exist, but are mostly limited to hobby projects unsuitable for production deployment or development tools to avoid having to recompile every time the developer wants to preview the changes just made.
Compilation to JavaScript.
With the continuing improvements in JavaScript execution speed, combined with the increased use of mobile devices whose web browsers do not implement support for plugins, there are efforts to target those users through compilation to JavaScript. It is possible to either compile the source code or JVM bytecode to JavaScript. Compiling the JVM bytecode which is universal across JVM languages allows building upon the existing compiler to bytecode.
Main JVM bytecode to JavaScript compilers are TeaVM, the compiler contained in Dragome Web SDK, Bck2Brwsr, and j2js-compiler.
Leading compilers from JVM languages to JavaScript include the Java to JavaScript compiler contained in Google Web Toolkit, Clojure script (Clojure), GrooScript (Groovy), Scala.js (Scala) and others.
Java Runtime Environment from Oracle.
The Java Runtime Environment (JRE) released by Oracle is a software distribution containing a stand-alone Java VM (HotSpot), browser plugin, Java standard libraries and a configuration tool. It is the most common Java environment installed on Windows computers. It is freely available for download at the website java.com.
Performance.
The JVM specification gives a lot of leeway to implementors regarding the implementation details. Since Java 1.3, JRE from Oracle contains a JVM called HotSpot. It has been designed to be a high-performance JVM.
To speed-up code execution, HotSpot relies on just-in-time compilation. To speed-up object allocation and garbage collection, HotSpot uses generational heap.
Generational heap.
The "Java virtual machine heap" is the area of memory used by the JVM for dynamic memory allocation.
In HotSpot the heap is divided into "generations":
The "permanent generation" (or "permgen") was used for class definitions and associated metadata prior to Java 8. Permanent generation was not part of the heap. The "permanent generation" was removed from Java 8.
Originally there was no permanent generation, and objects and classes were stored together in the same area. But as class unloading occurs much more rarely than objects are collected, moving class structures to a specific area allowed significant performance improvements.
Security.
Oracle's JRE is installed on a large number of computers. Since any web page the user visits may run Java applets, Java provides an easily accessible attack surface to malicious web sites that the user visits. Kaspersky Labs reports that the Java web browser plugin is the method of choice for computer criminals. Java exploits are included in many exploit packs that hackers deploy onto hacked web sites.
In the past, end users were often using an out-of-date version of JRE which was vulnerable to many known attacks. This lead to the widely shared belief between users that Java is inherently insecure. Since Java 1.7, Oracle's JRE for Windows includes automatic update functionality.
Toolbar controversy.
Beginning in 2005, Sun's (now Oracle's) JRE included unrelated software which was installed by default. In the beginning it was Google Toolbar, later MSN Toolbar, Yahoo Toolbar and finally the Ask Toolbar. The Ask Toolbar proved to be especially controversial. There has been a petition asking Oracle to remove it. The signers voiced their belief that "Oracle is violating the trust of the hundreds of millions of users who run Java on their machines. They are tarnishing the reputation of a once proud platform". Zdnet called their conduct deceptive, since after the user refuses the Toolbar the first time, the installer offers installing the toolbar during each update and relies on the user being too busy or distracted to install it by mistake. Ask Toolbar activates itself 10 minutes after the Java update is complete, presumably to further confuse unsophisticated users.
References.
</dl>

</doc>
<doc id="16402" url="http://en.wikipedia.org/wiki?curid=16402" title="Jan van Goyen">
Jan van Goyen

Jan Josephszoon van Goyen (]; 13 January 1596 – 27 April 1656) was a Dutch landscape painter. Van Goyen was an extremely prolific artist; approximately twelve hundred paintings and more than one thousand drawings by him are known.
Biography.
Jan van Goyen was the son of a shoemaker and started as an apprentice in Leiden, the town of his birth. Like many Dutch painters of his time, Jan van Goyen studied art in the town of Haarlem with Esaias van de Velde. At age 35, he established a permanent studio at Den Haag (The Hague). Crenshaw tells (and mentions the sources) that van Goyen's landscape paintings rarely fetched high prices, but he made up for the modest value of individual pieces by increasing his production, painting thinly and quickly with a limited palette of inexpensive pigments. Despite his market innovations, he always sought more income, not only through related work as an art dealer and auctioneer but also by speculating in tulips and real estate. Although the latter was usually a safe avenue of investing money, in van Goyen's experience it led to enormous debts. Paulus Potter rented one of his houses. Though he seems to have kept a workshop, his only registered pupils were Nicolaes van Berchem, Jan Steen, and Adriaen van der Kabel. The list of painters he influenced is much longer.
In 1652 and 1654 he was forced to sell his collection of paintings and graphic art, and he subsequently moved to a smaller house. He died in 1656 in The Hague, still unbelievably 18,000 guilders in debt, forcing his widow to sell their remaining furniture and paintings. Van Goyen's troubles also may have affected the early business prospects of his student and son-in-law Jan Steen, who left The Hague in 1654.
Dutch painting.
Typically, a Dutch painter of the 17th century (also known as the Dutch Golden Age) will fall into one of four categories, a painter of portraits, landscapes, still-lifes, or genre. Dutch painting was highly specialized and rarely could an artist hope to achieve greatness in more than one area in a lifetime of painting. Jan van Goyen would be classified primarily as a landscape artist with an eye for the genre subjects of everyday life. He painted many of the canals in and around Den Haag as well as the villages surrounding countryside of Delft, Rotterdam, Leiden, and Gouda. Other popular Dutch landscape painters of the sixteenth and seventeenth century were
Jacob van Ruisdael, Aelbert Cuyp, Hendrick Avercamp, Ludolf Backhuysen, Meindert Hobbema, Aert van der Neer.
Van Goyen's technique.
Jan van Goyen would begin a painting using a support primarily of thin oak wood. To this panel, he would scrub on several layers of a thin animal hide glue. With a blade, he would then scrape over the entire surface a thin layer of tinted white lead to act as a ground and to fill the low areas of the panel. The ground was tinted light brown, sometimes reddish, or ochre in colour.
Next, van Goyen would loosely and very rapidly sketch out the scene to be painted with pen and ink without going into the small details of his subject. This walnut ink drawing can be clearly seen in some of the thinly painted areas of his work. For a guide, he would have turned to a detailed drawing. The scene would have been drawn from life outdoors and then kept in the studio as reference material. Drawings by artists of the time were rarely works of art in their own right as they are viewed today.
On his palette he would grind out a colour collection of neutral grays, umbers, ochre and earthen greens that looked like they were pulled from the very soil he painted. A varnish oil medium was used as vehicle to grind his powered pigments into paint and then used to help apply thin layers of paint which he could easily blend.
The dark areas of the painting were kept very thin and transparent with generous amounts of the oil medium. The light striking the painting in these sections would be lost and absorbed into the painting ground. The lighter areas of the picture were treated heavier and opaque with a generous amount of white lead mixed into the paint. Light falling on the painting in a light section is reflected back at the viewer. The effect is a startling realism and three-dimensional quality. The surface of a finished painting resembles a fluid supple mousse, masterfully whipped and modeled with the brush. 
According to the art historian H.-U. Beck, "In his freely composed seascapes of the 1650s he reached the apex of his creative work, producing paintings of striking perfection."
Legacy.
Jan van Goyen was famously influential on the landscape painters of his century. His tonal quality was a feature that many imitated. According to the Netherlands Institute for Art History, he influenced Cornelis de Bie, Jan Coelenbier, Cornelis van Noorde, Abraham Susenier, Herman Saftleven, Pieter Jansz van Asch, and Abraham van Beijeren.

</doc>
<doc id="16413" url="http://en.wikipedia.org/wiki?curid=16413" title="James Tiptree, Jr. Award">
James Tiptree, Jr. Award

The James Tiptree, Jr. Award is an annual literary prize for works of science fiction or fantasy that expand or explore one's understanding of gender. It was initiated in February 1991 by science fiction authors Pat Murphy and Karen Joy Fowler, subsequent to a discussion at WisCon.
Background.
The award is named for Alice B. Sheldon, who wrote under the pseudonym James Tiptree, Jr. By choosing a masculine "nom de plume," having her stories accepted under that name and winning awards with them, Sheldon helped demonstrate that the division between male and female science fiction writing was illusory. Years after "Tiptree" first published science fiction, Sheldon wrote some work under the female pen name "Raccoona Sheldon"; later, the science fiction world discovered that "Tiptree" had been female all along. This discovery led to widespread discussion over which aspects of writing, if any, have an intrinsic gender. To remind audiences of the role gender plays in both reading and writing, the award was named in Sheldon's honor at the suggestion of Karen Joy Fowler.
Fundraising efforts for the Tiptree include publications (two cookbooks), "feminist bake sales", and auctions. (The Tiptree cookbook "The Bakery Men Don't See", edited by WisCon co-founder Jeanne Gomoll, was nominated for a 1992 Hugo Award.) Tiptree Award juries traditionally consist of four female jurors and one male juror (the "token man"). The funds are administrated by the "Tiptree Motherboard" (currently consisting of Fowler, Gomoll, Murphy, Ellen Klages, Debbie Notkin, and Jeffrey D. Smith ).
Award to the Tiptree Motherboard.
In 2011, the Science Fiction Research Association gave its 2011 "Thomas D. Clareson Award for Distinguished Service" to the Tiptree Motherboard. The Clareson Award was presented to the Tiptree Motherboard for “outstanding service activities – promotion of SF teaching and study, editing, reviewing, editorial writing, publishing, organizing meetings, mentoring, and leadership in SF/fantasy organizations.”
Anthologies.
Selections of the winners, various short listed fiction, and essays have appeared in four Tiptree-related collections, "Flying Cups and Saucers" (1999) and a series of annual anthologies published by Tachyon Publications of San Francisco. These include:

</doc>
<doc id="16415" url="http://en.wikipedia.org/wiki?curid=16415" title="Juventus F.C.">
Juventus F.C.

Juventus Football Club S.p.A. (from Latin "iuventus": youth, ]), commonly referred to as Juventus and colloquially as Juve ( ]), are a professional Italian association football club based in Turin, Piedmont. The club is the third oldest of its kind in the country and has spent the majority of its history, with the exception of the 2006–07 season, in the top flight First Division (known as Serie A since 1929).
Founded in 1897 as "Sport Club Juventus" by a group of young Torinese students, among them, who was their first president, Eugenio Canfari, and his brother Enrico, author of the company's historical memory; they have been managed by the industrial Agnelli family since 1923, which constitutes the oldest sporting partnership in Italy, thus making Juventus the first professional club in the country.
Over time, the club has become a symbol of the nation's "italianità" ("Italianness"), due to their tradition of success, some of which have had a significant impact in Italian society, especially in the 1930s and the first post-war decade; and the ideological politics and socio-economic origin of the club's sympathisers. This is reflected, among others, in the club's contribution to the national team, uninterrupted since the second half of the 1920s and recognised as one of the most influential in international football, having performed a decisive role in the World Cup triumphs of 1934, 1982 and 2006.
The club's fan base is larger than any other Italian football club and is one of the largest worldwide. Support for Juventus is widespread throughout the country and abroad, mainly in countries with a significant presence of Italian immigrants.
Juventus is historically the most successful club in Italian football and one of the most laureated and important globally. Overall, they have won fifty-eight official titles on the national and international stage, more than any other Italian club: a record thirty-one league titles, a record ten Italian cups, a record six national super cups, and, with eleven titles in confederation and inter-confederation competitions (two Intercontinental Cups, two European Champion Clubs' Cup/UEFA Champions Leagues, one European Cup Winners' Cup, three UEFA Cups, one UEFA Intertoto Cup and two UEFA Super Cups) the club ranks fourth in Europe and eighth in the world with the most trophies won.
In 1985, under the management of Giovanni Trapattoni, who led the Torinese team to thirteen official trophies in ten years until 1986, including six league titles and five international titles; Juventus became the first club in the history of European football to have won all three major competitions organised by the Union of European Football Associations: the European Champions' Cup, the (now-defunct) Cup Winners' Cup and the UEFA Cup (the first Italian and Southern European side to win the tournament). After their triumph in the Intercontinental Cup the same year, the club also became the first in football history—and remains the only one at present—to have won all possible official continental competitions and the world title.
According to the all-time ranking published in 2009 by the International Federation of Football History and Statistics, an organisation recognised by FIFA, based on clubs' performance in international competitions, Juventus were Italy's best club and second in Europe of the 20th century.
History.
Early years.
Juventus were founded as Sport Club Juventus in late 1897 by pupils from the Massimo D'Azeglio Lyceum school in Turin, but were renamed as Foot-Ball Club Juventus two years later. The club joined the Italian Football Championship during 1900. In 1904 the businessman Ajmone-Marsan revived the finances of the football club Juventus, making it also possible to transfer the training field from Piazza d'Armi to the more appropriate Velodrome Umberto I. During this period the team wore a pink and black kit. Juventus first won the league championship in 1905 while playing at their Velodrome Umberto I ground. By this time the club colours had changed to black and white stripes, inspired by English side Notts County.
There was a split at the club in 1906, after some of the staff considered moving Juve out of Turin. President Alfred Dick was unhappy with this and left with some prominent players to found FBC Torino which in turn spawned the Derby della Mole. Juventus spent much of this period steadily rebuilding after the split, surviving the First World War.
League dominance.
Fiat owner Edoardo Agnelli gained control of the club in 1923, and built a new stadium. This helped the club to its second "scudetto" (league championship) in the 1925–26 season beating Alba Roma with an aggregate score of 12–1, Antonio Vojak's goals were essential that season. The club established itself as a major force in Italian football since the 1930s, becoming the country's first professional club and the first with a decentralised fan base, which led it to win a record of five consecutive Italian championships the first four under the management of Carlo Carcano and form the core of the Italy national team during the Vittorio Pozzo's era, including the 1934 world champion squad. With star players such as Raimundo Orsi, Luigi Bertolini, Giovanni Ferrari and Luis Monti amongst others.
Juventus moved to the Stadio Comunale, but for the rest of the 1930s and the majority of the 1940s they were unable to recapture championship dominance. After the Second World War, Gianni Agnelli was appointed honorary president. The club added two more league championships to its name in the 1949–50 and 1951–52 seasons, the latter of which was under the management of Englishman Jesse Carver. Two new strikers were signed during 1957–58; Welshman John Charles and Italo-Argentine Omar Sivori, playing alongside longtime member Giampiero Boniperti. That season saw Juventus awarded with the Golden Star for Sport Excellence to wear on their shirts after becoming the first Italian side to win ten league titles. In the same season, Omar Sivori became the first ever player at the club to win the European Footballer of the Year. The following season they beat Fiorentina to complete their first league and cup double, winning Serie A and Coppa Italia. Boniperti retired in 1961 as the all-time top scorer at the club, with 182 goals in all competitions, a club record which stood for 45 years.
During the rest of the decade the club won the league just once more in 1966–67, However, the 1970s saw Juventus further solidify their strong position in Italian football. Under former player Čestmír Vycpálek they won the "scudetto" in 1971–72 and 1972–73, with players such as Roberto Bettega, Franco Causio and José Altafini breaking through. During the rest of the decade they won the league twice more, with defender Gaetano Scirea contributing significantly. The later win was under Giovanni Trapattoni, who helped the club's domination continue on into the early part of the 1980s and to form the backbone of the Italian national team during Enzo Bearzot's era, including the 1978 FIFA World Cup and 1982 world champion squads.
European stage.
The Trapattoni-era was highly successful in the 1980s; the club started the decade off well, winning the league title three more times by 1984. This meant Juventus had won 20 Italian league titles and were allowed to add a second golden star to their shirt, thus becoming the only Italian club to achieve this. Around this time the club's players were attracting considerable attention; Paolo Rossi was named European Footballer of the Year following his contribution to Italy's victory in the 1982 FIFA World Cup, where he was named player of the tournament.
Frenchman Michel Platini was also awarded the European Footballer of the Year title for three years in a row; 1983, 1984 and 1985, which is a record. Juventus are the only club to have players from their club winning the award in four consecutive years. Indeed it was Platini who scored the winning goal in the 1985 European Cup final against Liverpool, however this was marred by a tragedy which changed European football. That year, Juventus became the first club in the history of European football to have won all three major UEFA competitions and, after their triumph in the Intercontinental Cup, the club also became the first in association football history—and remain the world's only one at present—to have won all possible confederation and the club world title.
With the exception of winning the closely contested Italian Championship of 1985–86, the rest of the 1980s were not very successful for the club. As well as having to contend with Diego Maradona's Napoli, both of the Milanese clubs, Milan and Internazionale, won Italian championships. In 1990, Juventus moved into their new home, the Stadio delle Alpi, which was built for the 1990 World Cup.
Lippi era of success.
Marcello Lippi took over as Juventus manager at the start of the 1994–95 campaign. His first season at the helm of the club was a successful one, as Juventus recorded their first Serie A championship title since the mid-1980s. The crop of players during this period featured Ciro Ferrara, Roberto Baggio, Gianluca Vialli and a young Alessandro Del Piero. Lippi lead Juventus to the Champions League the following season, beating Ajax on penalties after a 1–1 draw in which Fabrizio Ravanelli scored for Juve.
The club did not rest long after winning the European Cup, more highly regarded players were brought into the fold in the form of Zinedine Zidane, Filippo Inzaghi and Edgar Davids. At home Juventus won Serie A in 1996–97 and 1997–98, as well as the 1996 UEFA Super Cup and the 1996 Intercontinental Cup. Juventus reached the 1997 and 1998 Champions League finals during this period, but lost out to Borussia Dortmund and Real Madrid respectively.
After a season's absence Lippi returned, signing big name players such as Gianluigi Buffon, David Trezeguet, Pavel Nedvěd and Lilian Thuram, helping the team to two more "scudetto" titles in the 2001–02 and 2002–03 seasons. Juventus were also part of an all Italian Champions League final in 2003 but lost out to Milan on penalties after the game ended in a 0–0 draw. The following year, Lippi was appointed as Italy's head coach, bringing an end to one of the most fruitful managerial spells in Juventus' history.
The "Calciopoli" scandal.
Fabio Capello became its coach in 2004, and led Juventus to two more Serie A titles. However, in May 2006, Juventus became one of the five clubs linked to a 2006 Italian football scandal, the result of which saw the club relegated to Serie B for the first time in its history. The club was also stripped of the two titles won under Capello in 2005 and 2006.
Many key players left following the demotion to Serie B, including Thuram, star striker Zlatan Ibrahimović and defensive stalwart Fabio Cannavaro. However, other big name players such as Buffon, Del Piero and Nedvěd remained to help the club return to Serie A while youngsters from the "Primavera" such as Sebastian Giovinco and Claudio Marchisio were integrated into the first team. The "bianconeri" were promoted straight back up as league winners after the 2006–07 season while captain Del Piero claimed the top scorer award with 21 goals.
As early as 2010, Juventus has considered challenging the stripping of their Scudetti from 2005 and 2006, dependent on the results of trials connected to the 2006 scandal. Subsequent investigations found in 2011 that Juventus' relegation in 2006 was without merit. When former general manager Luciano Moggi's conviction in criminal court in connection with the scandal was thrown out by an appeals court in 2015, the club sued the FIGC for €443 million for damages caused by their 2006 relegation. FIGC president Carlo Tavecchio offered to discuss reinstatement of the lost Scudetti in exchange for Juventus dropping the lawsuit.
Return to Serie A.
After returning to Serie A in the 2007–08 season, Juventus appointed Claudio Ranieri as manager. They finished in third place in their first season back in the top flight, and qualified for the 2008–09 Champions League third qualifying round in the preliminary stages. Juventus reached the group stages, where they beat Real Madrid in both home and away legs, before losing in the knockout round to Chelsea. Ranieri was sacked following a string of unsuccessful results, and Ciro Ferrara was appointed as manager on a temporary basis for the last two games of the 2008–09 season, before being subsequently appointed as the manager for the 2009–10 season.
However, Ferrara's stint as Juventus manager proved to be unsuccessful, with Juventus knocked out of Champions League and Coppa Italia, and just lying on the sixth place in the league table at the end of January 2010, leading to the dismissal of Ciro Ferrara and naming Alberto Zaccheroni as caretaker manager. Zaccheroni could not help the side improve, as Juventus finished the season in seventh place in Serie A. For the 2010–11 season, Jean-Claude Blanc was replaced by Andrea Agnelli as the club's president. Agnelli's first action was to replace Zaccheroni and Director of Sport Alessio Secco with Sampdoria manager Luigi Delneri and Director of Sport Giuseppe Marotta. However, Delneri failed to improve their fortunes and was dismissed. Former player and fan favourite Antonio Conte, fresh after winning promotion with Siena, was named as Delneri's replacement. In September 2011, Juve relocated to the new Juventus Stadium.
With Conte as manager, Juventus went unbeaten for the entire 2011–12 Serie A season. Towards the second half of the season, the team was mostly competing with northern rivals Milan for first place in a tight contest. Juventus won the title on the 37th matchday, after beating Cagliari 2–0, and Milan losing to Internazionale 4–2. After a 3–1 win in the final matchday against Atalanta, Juventus became the first team to go the season unbeaten in the current 38-game format. Other noteworthy achievements include the biggest away win (5–0 at Fiorentina), best defensive record (20 goals conceded, fewest ever in the current league format) in Serie A and second best in the top six European leagues that year.
In 2013–14, Juventus won a third consecutive "Scudetto" with a record 102 points. The title was the 30th official league championship in the club's history. They also achieved the semi-finals of 2013–14 UEFA Europa League being eliminated at home against 10-man Benfica's "catenaccio", missing the final at the Juventus Stadium.
In 2014–15, Juventus won a fourth straight scudetto. They also beat Real Madrid in the semi finals of the 2014–15 UEFA Champions League 3-2 on aggregate to face Barcelona in the final in Berlin for the first time since the 2002-03 UEFA Champions League.
Colours, badge, and nicknames.
Juventus' original home colours.
Juventus have played in black and white striped shirts, with white shorts, sometimes black shorts since 1903. Originally, they played in pink shirts with a black tie, but only because they had been sent the wrong shirts. The father of one of the players made the earliest shirts, but continual washing faded the colour so much that in 1903 the club sought to replace them.
Juventus asked one of their team members, Englishman John Savage, if he had any contacts in England who could supply new shirts in a colour that would better withstand the elements. He had a friend who lived in Nottingham, who being a Notts County supporter, shipped out the black and white striped shirts to Turin. Juve have worn the shirts ever since, considering the colours to be aggressive and powerful.
Juventus Football Club's official emblem has undergone different and small modifications since the 1920s. The last modification of the "Old Lady"'s badge took place before 2004–05 season. Since then, the emblem of the team is a black-and-white oval shield of a type used by Italian ecclesiastics. It is divided in five vertical stripes: two white stripes and three black stripes, inside which are the following elements; in its upper section, the name of the society superimposed on a white convex section, over golden curvature (gold for honour). The white silhouette of a charging bull is in the lower section of the oval shield, superimposed on a black old French shield; the charging bull is a symbol of the "Comune di Torino".
There is also a black silhouette of a mural crown above the black spherical triangle's base. This is a reminiscence to Augusta Tourinorum, the old city of the Roman era which the present capital of Piedmont region is its cultural heiress.
In the past, the convex section of the emblem had a blue colour (another symbol of Turin) and, furthermore, its shape was concave. The old French shield and the mural crown, also in the lower section of the emblem, had a considerably greater size with respect to the present. The two Golden Stars for Sport Excellence were located above the convex and concave section of Juventus' emblem. During the 1980s, the club emblem was the silhouette of a zebra, to both sides of the equide's head, the two golden stars and, above this badge, forming an arc, the club's name.
During its history, the club has acquired a number of nicknames, "la Vecchia Signora" (the Old Lady) being the best example. The "old" part of the nickname is a pun on Juventus which means "youth" in Latin. It was derived from the age of the Juventus star players towards the middle of the 1930s. The "lady" part of the nickname is how fans of the club affectionately referred to it before the 1930s. The club is also nicknamed "la Fidanzata d'Italia" (the Girlfriend of Italy), because over the years it has received a high level of support from Southern Italian immigrant workers (particularly from Naples and Palermo), who arrived in Turin to work for FIAT since the 1930s. Other nicknames include; "i bianconeri" (the black-and-whites), "le zebre" (the zebras) in reference to Juventus' colours. "I gobbi" (the hunchbacks) is the nickname that is used to define Juventus supporters, but is also used sometimes for team's players. The most widely accepted origin of "gobbi" dates to the fifties, when the "bianconeri" team was wearing a large jersey. When players ran on the field, the jersey, which had an opening on the chest with laces, generated a bulge on the back (a sort of parachute effect), giving the impression that the players have a hunchback.
Stadiums.
After the first two years (1897 and 1898), during which Juventus played in the Parco del Valentino and Parco Cittadella, their matches were held in the Piazza d'Armi Stadium until 1908, except in 1905, the first year of the "scudetto", and in 1906, years in which it played at the Corso Re Umberto.
From 1909 to 1922, Juventus played their internal competitions at Corso Sebastopoli Camp, and before moving the following year to Corso Marsiglia Camp where they remained until 1933, winning four league titles. At the end of 1933 they began to play at the new Stadio Mussolini stadium inaugurated for the 1934 World Championships. After the Second World War, the stadium was renamed as Stadio Comunale Vittorio Pozzo. Juventus played home matches at the ground for 57 years, a total of 890 league matches. The team continued to host training sessions at the stadium until July 2003.
From 1990 until the 2005–06 season, the Torinese side contested their home matches at Stadio delle Alpi, built for the 1990 FIFA World Cup, although in very rare circumstances, the club played some home games in other stadia such as Renzo Barbera at Palermo, Dino Manuzzi at Cesena and the Stadio Giuseppe Meazza at Milan.
In August 2006, the "bianconeri" returned to play in the Stadio Comunale, now known as Stadio Olimpico, after the restructuring of the stadium for the 2006 Winter Olympics onwards.
In November 2008, Juventus announced that they will invest around €120 million to build a new ground, the Juventus Stadium, on the site of Delle Alpi. Unlike the old ground, there will not be a running track; instead the pitch will be only 7.5 meters away from the stands. The planned capacity is 41,000. Work began during spring 2009 and the stadium was opened on 8 September 2011 for the start of the 2011–12 season.
Supporters.
Juventus are the best-supported football club in Italy, with over 12 million fans or "tifosi", which represent approximately 29% of the total Italian football fans according to a research published in September 2010 by Italian research agency Demos & Pi, and one of the most supported football clubs in the world, with 180 million supporters (43 million in Europe alone), particularly in the Mediterranean countries, to which a large number of Italian diaspora have emigrated. The Torinese side has fan clubs branches across the globe.
Demand for Juventus tickets in occasional home games held away from Turin is high; suggesting that Juventus have stronger support in other parts of the country. Juve is widely and especially popular throughout mainland Southern Italy, Sicily and Malta, leading the team to have one of the largest followings in its away matches, more than in Turin itself.
Rivalries.
Juventus have significant rivalries with two clubs. Their traditional rivals are fellow Turin club Torino F.C. and matches between the two side are known as the "Derby della Mole" (Derby of Turin). The rivalry dates back to 1906 as Torino was founded by break-away Juventus players and staff. Their most high-profile rivalry is with Internazionale, another big Serie A club located in Milan, the capital of the neighbouring region of Lombardy. Matches between these two clubs are referred to as the "Derby d'Italia" (Derby of Italy) and the two regularly challenge each other at the top of the league table, hence the intense rivalry. Up until the Calciopoli scandal which saw Juventus forcibly relegated, the two were the only Italian clubs to have never played below Serie A. Notably the two sides are the first and the second most supported clubs in Italy and the rivalry has intensified since the later part of the 1990s; reaching its highest levels ever post-Calciopoli, with the return of Juventus to Serie A.
They also have rivalries with Milan, Roma, Fiorentina, and Napoli.
Youth programme.
The Juventus youth set-up has been recognised as one of the best in Italy for producing young talents. While not all graduates made it to the first team, many have enjoyed successful careers in the Italian top flight. Under long-time coach Vincenzo Chiarenza, the "Primavera" (Under-20) squad enjoyed one of its successful periods, winning all age-group competitions from 2004 to 2006.
The youth system is also notable for its contribution to the Italian national senior and youth teams. 1934 World Cup winner Gianpiero Combi, 1936 Gold Medal and 1938 World Cup winner Pietro Rava, Giampiero Boniperti, Roberto Bettega, 1982 World Cup hero Paolo Rossi and more recently, Claudio Marchisio and Sebastian Giovinco are a number of former graduates who have gone on to make the first team and full Italy squad.
Like Dutch club Ajax and many Premier League clubs, Juventus operates several satellite clubs and football schools outside of the country (i.e. United States, Canada, Greece, Saudi Arabia, Australia and Switzerland) and numerous camps in the local region to expand talent scouting.
Presidential history.
Juventus have had numerous presidents over the course of their history, some of which have been the owners of the club, others have been honorary presidents, here is a complete list of them:
Managerial history.
Below is a list of Juventus managers from 1923 when the Agnelli family took over and the club became more structured and organised, until the present day.
Honours.
Italy's most successful club of the 20th century, and the most successful club in the history of Italian football, Juventus have won the Italian League Championship, the country's premier football club competition and organised by Lega Nazionale Professionisti Serie A (LNPA), a record 31 times and have the record of consecutive triumphs in that tournament (five, between 1930–31 and 1934–35). They have also won the Italian Cup, the country's primary cup competition, a record ten times, and becoming the first team to retain the trophy successfully with their triumph in the 1959–60 season. In addition, the club holds the joint-record for Italian Super Cup wins with six, along with AC Milan, the most recent coming in 2013.
Overall, Juventus have won 58 official competitions, more than any other team in the country: 47 domestic trophies, which is also a record, and 11 official international competitions, making them, in the latter case, the second most successful Italian club in European competition. The club is fourth in Europe and eighth in the world with the most international titles won officially recognised by their respective association football confederation and Fédération Internationale de Football Association (FIFA).
In 1977, the Torinese side become the first in Southern Europe to have won the UEFA Cup and the first—and only to date—in Italian football history to achieve an international title with a squad composed by national footballers. In 1993 the club won its third competition's trophy, an unprecedented feat in the continent until then and the most for an Italian club. Juventus was, also, the first Italian club to achieve the title in the European Super Cup, having won the competition in 1984, and the first European club to win the Intercontinental Cup, in 1985, since it was restructured by Union of European Football Associations (UEFA) and Confederación Sudamericana de Fútbol (CONMEBOL)'s organizing committee five years beforehand.
The club has earned the distinction of being allowed to wear three Golden Stars for Sport Excellence (it. "Stelle d'oro al Merito Sportivo") on its shirts representing its league victories, the tenth of which was achieved during the 1957–58 season, the twentieth in the 1981–82 season and the thirtieth in the 2013–14 season. Juventus were the first Italian team to have achieved "the national double" thrice (winning the Italian top tier division and the national cup competition in the same season), in the 1959–60; 1994–95 and 2014–15 seasons.
The club is unique in the world in having won all official international competitions, and they have received, in recognition to winning the three major UEFA competitions—first case in the history of the European football— The UEFA Plaque by the Union of European Football Associations on 12 July 1988.
The Torinese side was placed 7th—but the top Italian club—in the FIFA Clubs of the 20th Century selection of 23 December 2000.
Juventus have been proclaimed World's Club Team of the Year twice (1993 and 1996) and was ranked in 3rd place—the highest ranking of any Italian club—in the All-Time Club World Ranking (1991–2009 period) by the International Federation of Football History & Statistics.
Club statistics and records.
Alessandro Del Piero holds Juventus' official appearance record (646 as of 23 October 2010). He took over from Gaetano Scirea on 6 March 2008 against Palermo. He also holds the record for Serie A appearances with 467 (as of 21 December 2011).
Including all official competitions, Alessandro Del Piero is the all-time leading goalscorer for Juventus, with 277 goals—as of 23 October 2010—since joining the club in 1993. Giampiero Boniperti, who was the all-time topscorer since 1961 comes in second in all competitions with 182.
In the 1933–34 season, Felice Borel scored 31 goals in 34 appearances, setting the club record for Serie A goals in a single season. Ferenc Hirzer is the club's highest scorer in a single season with 35 goals in 26 appearances in the 1925–26 season (record of Italian football). The most goals scored by a player in a single match is 6, which is also an Italian record. This was achieved by Omar Enrique Sivori in a game against Internazionale in the 1960–61 season.
The first ever official game participated in by Juventus was in the Third Federal Football Championship, the predecessor of Serie A, against Torinese; Juve lost 0–1. The biggest ever victory recorded by Juventus was 15–0 against Cento, in the second round of the Coppa Italia in the 1926–27 season. In terms of the league; Fiorentina and Fiumana were famously on the end of the "Old Lady"'s biggest championship wins, both were beaten 11–0 and were recorded in the 1928–29 season. Juventus' heaviest championship defeats came during the 1911–12 and 1912–13 seasons; they were against Milan in 1912 (1–8) and Torino in 1913 (0–8).
The sale of Zinédine Zidane to Real Madrid of Spain from Juventus in 2001, was the world football transfer record at the time, costing the Spanish club around £46 million.
Contribution to the Italian national team.
Overall, Juventus are the club that has contributed the most players to the Italian national team in history, they are the only Italian club that has contributed players to every Italian national team since the 2nd FIFA World Cup. Juventus have contributed numerous players to Italy's World Cup campaigns, these successful periods principally have coincided with two golden ages of the Turin club's history, referred as "Quinquennio d'Oro" (The Golden Quinquennium), from 1931 until 1935, and "Ciclo Leggendario" (The Legendary Cycle), from 1972 to 1986.
Below are a list of Juventus players who represented the Italian national team during World Cup winning tournaments;
Two Juventus players have won the golden boot award at the World Cup with Italy; Paolo Rossi in 1982 and Salvatore Schillaci in 1990. As well as contributing to Italy's World Cup winning sides, two Juventus players Alfredo Foni and Pietro Rava, represented Italy in the gold medal winning squad at the 1936 Summer Olympics. Three "bianconeri" players represented their nation during the 1968 European Football Championship win for Italy; Sandro Salvadore, Ernesto Càstano and Giancarlo Bercellino.
The Torinese club has also contributed to a lesser degree to the national sides of other nations. Zinédine Zidane and captain Didier Deschamps were Juventus players when they won the 1998 World Cup with France, making it as the association football club which supplied the most globally (24) (three other players in the 1998 squad, Patrick Vieira, David Trézéguet and Lilian Thuram have all played for Juventus at one time or another). Three Juventus players have also won the "European Football Championship" with a nation other than Italy, Luis del Sol won it in 1964 with Spain, while the Frenchmen Michel Platini and Zidane won the competition in 1984 and 2000 respectively.
Economical information.
Since 27 June 1967 Juventus Football Club has been a Joint-stock company (it. "società per azioni") and since 3 December 2001 the torinese side is listed on the Borsa Italiana. As of 2011, the Juventus' shares are distributed between 63.77% to EXOR S.p.A, the Agnelli family's "holding" (a company of the "Giovanni Agnelli & C.S.a.p.a Group"), 7.5% to Libyan Arab Foreign Investment Co. and 32.5% to other shareholders. Since 2012, Jeep became the new sponsor of Juventus, a car brand acquired by FIAT after the 2000s Global Financial Crisis.
Along with Lazio and Roma, the "Old Lady" is one of only three Italian clubs quoted on Borsa Italiana (Italian stock exchange). Juventus was also the only association football club in the country member of "STAR" (Segment of Stocks conforming to High Requirements, it. "Segmento Titoli con Alti Requisiti"), one of the main market segment in the world. However due to 2011 financial result, Juventus had to move from "STAR" segment to "MTA" market.
The club's training ground was owned by Campi di Vinovo S.p.A, controlled by Juventus Football Club S.p.A. to 71.3%. In 2003 the club bought the lands from the subsidiary and later the company was dissolved. Since then Juventus FC did not had any subsidiary.
From 1 July 2008, the club has implemented a Safety Management System for employees and athletes in compliance with the requirements of international OHSAS 18001:2007 regulation and a Safety Management System in the medical sector according to the international ISO 9001:2000 resolution.
The club is one of the founders of the European Club Association (ECA), which was formed after the dissolution of the G-14, an international group of Europe's most elite clubs which Juventus were also a founding member.
According to the Deloitte Football Money League, a research published by consultants Deloitte Touche Tohmatsu in 17 January 2014, Juventus are the ninth highest earning football club in the world with an estimated revenue of €272.4 million, the most for an Italian club. The club is also ranked 9th on Forbes' list of the most valuable football clubs in the world with an estimate value of US$850 million (€654 million), making them the second richest association football club in Italy. The club was located in 2012 in top 50 sporting teams at worldwide level in terms of value.
Juventus re-capitalized on 28 June 2007, increased €104,807,731.60 shares capital. The team made an aggregate net loss in the following seasons (2006 to date): -€927,569 (2006–07), -€20,787,469 (2007–08), net income €6,582,489 (2008–09) and net loss €10,967,944 (2009–10). After an unaudited €43,411,481 net loss was recorded in the first 9 months of 2010–11 season, the BoD announced that a capital increase of €120 million was planned, scheduled to submit to the extraordinary shareholder's meeting in October. Eventually the 2010–11 season net loss was €95,414,019. In 2012–13 season Juventus continued the recover from recent seasons net losses thanks to the biggest payment in Uefa's Champions League 2012–13 revenue distribution, earning €65.3 million. Despite being knocked out in the quarterfinal stage, Juventus took the lion's share thanks to the largesse of Italian national TV market and the division of revenues with the only other Italian team attended at the competition's final phase, AC Milan. Confirming the trend of marked improvement in net result, the 2013–14, financial year closed with a loss of €6.7 million but the with first positive operating income since 2006.
Bibliography.
</dl>
</dl>
External links.
</noinclude>

</doc>
<doc id="16641" url="http://en.wikipedia.org/wiki?curid=16641" title="Kurtis Blow">
Kurtis Blow

Kurt Walker (born August 9, 1959), professionally known by his stage name Kurtis Blow, is an American rapper and record producer. He is the first commercially successful rapper and the first to sign with a major record label. "The Breaks", a single from his 1980 debut album, is the first certified gold record rap song. Throughout his career he has released 15 albums and is currently an ordained minister.
Life and career.
Originally from Harlem, Walker began DJ'ing under the name Kool DJ Kurt.
In 1979, aged twenty, Kurtis Blow became the first rapper to be signed by a major label, Mercury, which released "Christmas Rappin'". It sold over 400,000 copies. Its follow-up, "The Breaks", sold over half a million copies. He was also the first rapper to perform overseas. He released ten albums over the next eleven years. His first album was "Kurtis Blow", while his second was the Top 50 pop album "Deuce". "Party Time" featured a fusion of rap and go-go. "Ego Trip" included the hits: "8 Million Stories," "AJ Scratch," and "Basketball". His 1985 album, "America", garnered praise for its title track's music video. From this album, the song "If I Ruled the World" became a Top 5 hit on "Billboard'"s R&B chart. Towards the end of the 1980s, he recording career waned and he moved into production.
Besides his own work, Kurtis has been responsible for hits by The Fat Boys and Run DMC. Run began his career billed as 'The Son of Kurtis Blow'. Lovebug Starski, Dr. Jekyll and Mr. Hyde, Full Force, Russell Simmons and Wyclef Jean all have been produced by, or collaborated with, Walker. Former label mates René & Angela had their R&B chart topping debut "Save Your Love (For #1)" was produced by him. Along with Dexter Scott King, Walker co-ordinated "King Holiday," a song to celebrate Martin Luther King’s birthday, released in January 1986.
His acting performances and music coordination in several films includes Leon Kennedy’s "Knights of the City" and the hip hop film "Krush Groove". As host and co-producer for "Das Leben Amerikanischer Gangs", an international film production's focus on the West Coast gang scene, Kurt crossed international waters for inner city justice (1995). As host and associate producer for "Rhyme and Reason" Kurtis gave an informative account of the status of hip hop, while he participated in the three volume release "The History Of Rap" in 1998.
Kurt has spoken out emphatically against racism. He was an active participant in the Artists Against Apartheid record “Sun City”. Kurt has worked with Rev. Jesse Jackson's Operation Push and National Rainbow Coalition in Chicago. Kurt has also worked with Rev. Al Sharpton's Action Network in New York City. In 1995, he started working on-air in radio, Power 106, the #1 CHR radio station in Southern California. He hosted 'The Old School Show' on Sunday nights, featuring hits from the past. He also worked for Sirius Satellite Radio on the Classic Old School Hip Hop station Backspin on Channel 46.
Beginning in 1996, Kurt was featured in a hip hop display at the Rock and Roll Hall of Fame. While in the same year, rapper Nas debuted at #1 on the "Billboard" Hot 100 with his version of "If I Rule The World". In 1998, the group Next released "Too Close", in which the music of "Christmas Rappin'" was sampled. ASCAP honored Kurt and Next at a gala affair on May 26, 1999. In 2002, he traveled to the Middle East to tour the Armed Forces bases performing seventeen shows for the troops. He was a judge for the 8th annual Independent Music Awards.
Minister.
As the founder of The Hip Hop Church, Kurtis serves as rapper, DJ, worship leader and licensed minister. He became an ordained minister on August 16, 2009. Recently in December of 2014, Kurt was the Guest MC of the "Hip Hop Nutcracker", a well received rendition of Tchaikovski's holiday classic.

</doc>
<doc id="16644" url="http://en.wikipedia.org/wiki?curid=16644" title="Geography of Kazakhstan">
Geography of Kazakhstan

Kazakhstan is located in Central Asia and Eastern Europe at . With an area of about 2,724,800 square kilometers, Kazakhstan is more than twice the combined size of the other four Central Asian states, or about twice the size of Alaska. The country borders Turkmenistan, Uzbekistan, and Kyrgyzstan to the south; Russia to the north; Russia and the Caspian Sea to the west; and China's Xinjiang Uygur Autonomous Region to the east.
Topography and drainage.
There is considerable topographical variation within Kazakhstan. The highest point is the top of the mountain Khan Tengri, on the Kyrgyz border in the Tian Shan range, with an elevation of 7,010 meters, or 21,999 feet above sea level; the lowest point is the bottom of the Karagiye depression at 132 meters, or 433 feet below sea level, in the Mangystau province east of the Caspian Sea. Most of the country lies at between 200 and 300 meters above sea level, but Kazakhstan's Caspian shore includes some of the lowest elevations on Earth. The peak Khan Tengri in the Tian Shan Mountains (and on the border with Kyrgyzstan and China) is Kazakhstan highest elevation at 6995m (7010m with ice cap).
Many of the peaks of the Altay and Tien Shan ranges are snow-covered year-round, and their run-off is the source for most of Kazakhstan's rivers and streams.
Except for the Tobol, Ishim, and Irtysh rivers (the Kazak names for which are, respectively, Tobyl, Esil, and Ertis), portions of which flow through Kazakhstan, all of Kazakhstan's rivers and streams are part of landlocked systems. They either flow into isolated bodies of water such as the Caspian Sea or simply disappear into the steppes and deserts of central and southern Kazakhstan. Many rivers, streams, and lakes are seasonal, evaporating in summer. The three largest bodies of water are Lake Balkhash, a partially fresh, partially saline lake in the east, near Almaty, and the Caspian and Aral Seas, both of which lie partially within Kazakhstan.
Some 9.4% of Kazakhstan's land is mixed prairie and forest or treeless prairie, primarily in the north or in the basin of the Ural River in the west. More than three-quarters of the country, including the entire west and most of the south, is either semidesert (33.2%) or desert (44%). The terrain in these regions is bare, eroded, broken uplands, with sand dunes in the Qizilqum ("The Red Sands"; in the Russian form, Kyzylkum) and Moyunqum (in the Russian form, Muyunkum ()) deserts, which occupy south-central Kazakhstan.
Climate.
The Climate in Kazakhstan is continental. In summer the temperatures average more than 30 °C and in winter average -20 °C.
Environmental problems.
The environment of Kazakhstan has been badly damaged by human activity. Most of the water in Kazakhstan is polluted by industrial effluents, pesticide and fertilizer residue, and, in some places, radioactivity. The most visible damage has been to the Aral Sea, which as recently as the 1970s was larger than any of the Great Lakes of North America save Lake Superior. The sea began to shrink rapidly when sharply increased irrigation and other demands on the only significant tributaries, the Syr Darya and the Amu Darya (the latter reaching the Aral from neighboring Uzbekistan), all but eliminated inflow. During the Soviet Era, Kazakhstan received water from Tajikistan and Kyrgyzstan, and Kazakhstan, Turkmenistan, and Kazakhstan provided oil and gas for these two nations in return. However, after the collapse of the USSR this system had collapsed and no plan to replace this system has been put in place. According to research conducted by the International Crisis Group, there is little political will to solve this problem despite Central Asia's need for mutual resource-sharing. By 1993 the Aral Sea had lost an estimated 60% of its volume, in the process breaking into three unconnected segments. Increasing salinity and reduced habitat have killed the Aral Sea's fish, hence destroying its once-active fishing industry, and the receding shoreline has left the former port of Aral'sk more than seventy kilometers from the water's edge. The depletion of this large body of water has increased temperature variations in the region, which in turn have had an impact on agriculture. A much greater agricultural impact, however, has come from the salt- and pesticide-laden soil that the wind is known to carry as far away as the Himalaya Mountains and the Pacific Ocean. Deposition of this heavily saline soil on nearby fields effectively sterilizes them. Evidence suggests that salts, pesticides, and residues of chemical fertilizers are also adversely affecting human life around the former Aral Sea; infant mortality in the region approaches 10% compared with the 1991 national rate of 2.7%.
By contrast, the water level of the Caspian Sea has been rising steadily since 1978 for reasons that scientists have not been able to explain fully. At the northern end of the sea, more than 10,000 square kilometres of land in Atyrau Province have been flooded. Experts estimate that if current rates of increase persist, the coastal city of Atyrau, eighty-eight other population centers, and many of Kazakhstan's Caspian oil fields could be submerged by 2020.
Wind erosion has also had an impact in the northern and central parts of the republic because of the introduction of wide-scale dryland wheat farming. In the 1950s and 1960s, much soil was lost when vast tracts of Kazakhstan's prairies were plowed under as part of Khrushchev's Virgin Lands agricultural project. By the mid-1990s, an estimated 60% of the republic's pastureland was in various stages of desertification.
Industrial pollution is a bigger concern in Kazakhstan's manufacturing cities, where aging factories pump huge quantities of unfiltered pollutants into the air and groundwater. The former capital, Almaty, is particularly threatened, in part because of the postindependence boom in private automobile ownership.
The gravest environmental threat to Kazakhstan comes from radiation, especially in the Semey (Semipalatinsk) region of the northeast, where the Soviet Union tested almost 500 nuclear weapons, 116 of them above ground. Often, such tests were conducted without evacuating or even alerting the local population. Although nuclear testing was halted in 1990, radiation poisoning, birth defects, severe anemia, and leukemia are very common in the area.
With some conspicuous exceptions, lip service has been the primary official response to Kazakhstan's ecological problems. In February 1989, opposition to Soviet nuclear testing and its ill effects in Kazakhstan led to the creation of one of the republic's largest and most influential grass-roots movements, Nevada-Semipalatinsk, which was founded by Kazak poet and public figure Olzhas Suleymenov. In the first week of the movement's existence, Nevada-Semipalatinsk gathered more than 2 million signatures from Kazakhstanis of all ethnic groups on a petition to Mikhail Gorbachev demanding the end of nuclear testing in Kazakhstan. After a year of demonstrations and protests, the test ban took effect in 1990. It remained in force in 1996, although in 1995 at least one unexploded device reportedly was still in position near Semey.
Once its major ecological objective was achieved, Nevada-Semipalatinsk made various attempts to broaden into a more general political movement; it has not pursued a broad ecological or "green" agenda. A very small green party, Tabigat, made common cause with the political opposition in the parliament of 1994.
The government has established a Ministry of Ecology and Bioresources, with a separate administration for radioecology, but the ministry's programs are underfunded and given low priority. In 1994 only 23% of budgeted funds were actually allotted to environmental programs. Many official meetings and conferences are held (more than 300 have been devoted to the problem of the Aral Sea alone), but few practical programs have gone into operation. In 1994 the World Bank, the International Monetary Fund (IMF), and the United States Environmental Protection Agency agreed to give Kazakhstan US$62 million to help the country overcome ecological problems.

</doc>
<doc id="16743" url="http://en.wikipedia.org/wiki?curid=16743" title="Karl Marx">
Karl Marx

Karl Marx (; ]; 5 May 1818 – 14 March 1883) was a philosopher, economist, sociologist, journalist, and revolutionary socialist. Born in Germany, he later became stateless and spent much of his life in London in the United Kingdom. Marx's work in economics laid the basis for much of the current understanding of labour and its relation to capital, and subsequent economic thought. He published numerous books during his lifetime, the most notable being "The Communist Manifesto" (1848) and "Das Kapital" (1867–1894).
Born into a wealthy middle-class family in Trier in the Prussian Rhineland, Marx studied at the Universities of Bonn and Berlin where he became interested in the philosophical ideas of the Young Hegelians. After his studies he wrote for Rheinische Zeitung, a radical newspaper in Cologne, and began to work out the theory of the materialist conception of history. He moved to Paris in 1843, where he began writing for other radical newspapers and met Friedrich Engels, who would become his lifelong friend and collaborator. In 1849 he was exiled and moved to London together with his wife and children, where he continued writing and formulating his theories about social and economic activity. He also campaigned for socialism and became a significant figure in the International Workingmen's Association.
Marx's theories about society, economics and politics—the collective understanding of which is known as Marxism—hold that human societies progress through class struggle: a conflict between an ownership class that controls production and a dispossessed labouring class that provides the labour for production. States, Marx believed, were run on behalf of the ruling class and in their interest while representing it as the common interest of all; and he predicted that, like previous socioeconomic systems, capitalism produced internal tensions which would lead to its self-destruction and replacement by a new system: socialism. He argued that class antagonisms under capitalism between the bourgeoisie and proletariat would eventuate in the working class' conquest of political power and eventually establish a classless society, communism, a society governed by a free association of producers. Marx actively fought for its implementation, arguing that the working class should carry out organised revolutionary action to topple capitalism and bring about socio-economic change.
Both lauded and criticized, Marx has been described as one of the most influential figures in human history. Many intellectuals, labour unions and political parties worldwide have been influenced by Marx's ideas, with many variations on his groundwork. Marx is typically cited, with Émile Durkheim and Max Weber, as one of the three principal architects of modern social science. 
Early life.
Childhood and early education: 1818–1835.
Karl Marx was born on 5 May 1818 to Heinrich Marx and Henrietta Pressburg (1788-1863). He was born at 664 Brückergasse in Trier, a town then part of the Kingdom of Prussia's Province of the Lower Rhine. Ancestrally Jewish, his maternal grandfather was a Dutch rabbi, while his paternal line had supplied Trier's rabbis since 1723, a role taken by his grandfather Meier Halevi Marx. Karl's father, as a child known as Herschel, was the first in the line to receive a secular education; he became a lawyer and lived a relatively wealthy and middle-class existence, with his family owning a number of Moselle vineyards. Prior to his son's birth, and to escape the constraints of anti-semitic legislation, Herschel converted from Judaism to Lutheranism, the main Protestant denomination in Germany and Prussia at the time, taking on the German forename of Heinrich over the Yiddish Herschel.
Largely non-religious, Heinrich was a man of the Enlightenment, interested in the ideas of the philosophers Immanuel Kant and Voltaire. A classical liberal, he took part in agitation for a constitution and reforms in Prussia, then governed by an absolute monarchy. In 1815 Heinrich Marx began work as an attorney, in 1819 moving his family to a ten-room property near the Porta Nigra. His wife, a Dutch Jewish woman, Henrietta Pressburg, was semi-literate and was said to suffer from "excessive mother love", devoting much time to her family and insisting on cleanliness within her home. She was from a prosperous business family that later founded the company Philips Electronics: she was great-aunt to Anton and Gerard Philips, and great-great-aunt to Frits Philips. Her sister Sophie Presburg 1797-1854,was Marx's aunt and was married to Lion Philips 1794-1866 Marx's uncle through this marriage,and was the grandfather of both Gerald and Anton Philips. Lion Philips was a wealthy Dutch Tobacco manufacturer and industrialist, upon whom Karl and Jenny Marx would later often come to rely for loans while they were exiled in London. In contrast to her husband, Henrietta retained her Jewish faith.
Little is known of Karl Marx's childhood. The third of nine children, he became the oldest son when his brother Moritz died in 1819. Young Karl was baptised into the Lutheran Church in August 1824. His surviving siblings, Sophie, Hermann, Henriette, Louise, Emilie and Karoline, were also baptised as Lutherans. Young Karl was privately educated, by Heinrich Marx, until 1830, when he entered Trier High School, whose headmaster, Hugo Wyttenbach, was a friend of his father. By employing many liberal humanists as teachers, Wyttenbach incurred the anger of the local conservative government. Subsequently, police raided the school in 1832, and discovered that literature espousing political liberalism was being distributed among the students. Considering the distribution of such material a seditious act, the authorities instituted reforms and replaced several staff during Marx's attendance.
In October 1835 at the age of 17, Marx travelled to the University of Bonn wishing to study philosophy and literature: however, his father insisted on law as a more practical field. Due to a condition referred to as a "weak chest", Karl was excused from military duty when he turned 18. While at the University at Bonn, Marx joined the Poets' Club, a group containing political radicals that was being monitored by the police. Marx also joined the Trier Tavern Club drinking society ("Landsmannschaft der Treveraner"), at one point serving as club co-president. Additionally, Marx was involved in certain disputes, some of which became serious: in August 1836 he took part in a duel with a member of the university's Borussian Korps. Although his grades in the first term were good, they soon deteriorated, leading his father to force a transfer to the more serious and academic University of Berlin.
Hegelianism and early activism: 1836–1843.
Spending summer and autumn 1836 in Trier, Marx became more serious about his studies and his life. He became engaged to Jenny von Westphalen, an educated baroness of the Prussian ruling class who had known Marx since childhood. Having broken off her engagement with a young aristocrat to be with Marx, their relationship was socially controversial due to the differences between their ethnic and class origins, but Marx befriended her father, a liberal aristocrat, Ludwig von Westphalen, and later dedicated his doctoral thesis to him. Seven years after their engagement, on 19 June 1843, Marx married Jenny in a Protestant church in Kreuznach.
In October 1836 Marx arrived in Berlin, matriculating in the university's faculty of law and renting a room in the Mittelstrasse. Although studying law, he was fascinated by philosophy, and looked for a way to combine the two, believing that "without philosophy nothing could be accomplished". Marx became interested in the recently deceased German philosopher G. W. F. Hegel, whose ideas were then widely debated among European philosophical circles. During a convalescence in Stralau, he joined the Doctor's Club ("Doktorklub"), a student group which discussed Hegelian ideas, and through them became involved with a group of radical thinkers known as the Young Hegelians in 1837; they gathered around Ludwig Feuerbach and Bruno Bauer, with Marx developing a particularly close friendship with Adolf Rutenberg. Like Marx, the Young Hegelians were critical of Hegel's metaphysical assumptions, but adopted his dialectical method in order to criticise established society, politics, and religion from a leftist perspective. Marx's father died in May 1838, resulting in a diminished income for the family. Marx had been emotionally close to his father, and treasured his memory after his death.
By 1837, Marx was writing both fiction and non-fiction, having completed a short novel, "Scorpion and Felix", a drama, "Oulanem", and a number of love poems dedicated to Jenny von Westphalen, though none of this early work was published during his lifetime. Marx soon abandoned fiction for other pursuits, including the study of both English and Italian, art history and the translation of Latin classics. He began co-operating with Bruno Bauer on editing Hegel's "Philosophy of Religion" in 1840. Marx was also engaged in writing his doctoral thesis, "The Difference Between the Democritean and Epicurean Philosophy of Nature", which he completed in 1841. It was described as "a daring and original piece of work in which Marx set out to show that theology must yield to the superior wisdom of philosophy": the essay was controversial, particularly among the conservative professors at the University of Berlin. Marx decided, instead, to submit his thesis to the more liberal University of Jena, whose faculty awarded him his PhD in April 1841. As Marx and Bauer were both atheists, in March 1841 they began plans for a journal entitled "Archiv des Atheismus" ("Atheistic Archives"), but it never came to fruition. In July, Marx and Bauer took a trip to Bonn from Berlin. There they scandalised their class by getting drunk, laughing in church, and galloping through the streets on donkeys.
Marx was considering an academic career, but this path was barred by the government's growing opposition to classical liberalism and the Young Hegelians. Marx moved to Cologne in 1842, where he became a journalist, writing for the radical newspaper "Rheinische Zeitung" ("Rhineland News"), expressing his early views on socialism and his developing interest in economics. He criticised both right-wing European governments as well as figures in the liberal and socialist movements whom he thought ineffective or counter-productive. The newspaper attracted the attention of the Prussian government censors, who checked every issue for seditious material before printing; Marx lamented that "Our newspaper has to be presented to the police to be sniffed at, and if the police nose smells anything un-Christian or un-Prussian, the newspaper is not allowed to appear." After the "Rheinische Zeitung" published an article strongly criticising the Russian monarchy, Tsar Nicholas I requested it be banned; Prussia's government complied in 1843.
Communist agitation.
Paris: 1843–1845.
In 1843, Marx became co-editor of a new, radical leftist Parisian newspaper, the "Deutsch-Französische Jahrbücher" ("German-French Annals"), then being set up by the German socialist Arnold Ruge to bring together German and French radicals, and thus Marx and his wife moved to Paris in October 1843. Initially living with Ruge and his wife communally at 23 Rue Vaneau, they found the living conditions difficult, so moved out following the birth of their daughter Jenny in 1844. Although intended to attract writers from both France and the German states, the "Jahrbücher" was dominated by the latter; the only non-German writer was the exiled Russian anarcho-communist Mikhail Bakunin. Marx contributed two essays to the paper, "Introduction to a Contribution to the Critique of Hegel's Philosophy of Right" and "On the Jewish Question," the latter introducing his belief that the proletariat were a revolutionary force and marking his embrace of communism. Only one issue was published, but it was relatively successful, largely owing to the inclusion of Heinrich Heine's satirical odes on King Ludwig of Bavaria, leading the German states to ban it and seize imported copies; Ruge nevertheless refused to fund the publication of further issues, and his friendship with Marx broke down. After the paper's collapse, Marx began writing for the only uncensored German-language radical newspaper left, "Vorwärts!" ("Forward!"). Based in Paris, the paper was connected to the League of the Just, a utopian socialist secret society of workers and artisans. Marx attended some of their meetings, but did not join. In "Vorwärts!", Marx refined his views on socialism based upon Hegelian and Feuerbachian ideas of dialectical materialism, at the same time criticising liberals and other socialists operating in Europe.
On 28 August 1844, Marx met the German socialist Friedrich Engels at the Café de la Régence, beginning a lifelong friendship. Engels showed Marx his recently published "The Condition of the Working Class in England in 1844", convincing Marx that the working class would be the agent and instrument of the final revolution in history.
Soon Marx and Engels were collaborating on a criticism of the philosophical ideas of Marx's former friend, Bruno Bauer. This work was published in 1845 as "The Holy Family". Although critical of Bauer, Marx was increasingly influenced by the ideas of the Young Hegelians Max Stirner and Ludwig Feuerbach, but eventually Marx and Engels abandoned Feuerbachian materialism as well.
During the time that he lived at 38 Rue Vanneau in Paris (from October 1843 until January 1845), Marx engaged in an intensive study of "political economy" (Adam Smith, David Ricardo, James Mill "etc."), the French socialists (especially Claude Henri St. Simon and Charles Fourier) and the history of France." The study of political economy is a study that Marx would pursue for the rest of his life and would result in his major economic work—the three-volume series called "Capital." Marxism is based in large part on three influences: Hegel's dialectics, French utopian socialism and English economics. Together with his earlier study of Hegel's dialectics, the studying that Marx did during this time in Paris meant that all major components of "Marxism" (or political economy as Marx called it) were in place by the autumn of 1844. Although Marx was constantly being pulled away from his study of political economy by the usual daily demands on his time that everyone faces, and the additional special demands of editing a radical newspaper and later by the demands of organising and directing the efforts of a political party during years in which popular uprisings of the citizenry might at any moment become a revolution, Marx was always drawn back to his economic studies. Marx sought "to understand the inner workings of capitalism."
An outline of "Marxism" had definitely formed in the mind of Karl Marx by late 1844. Indeed, many features of the Marxist view of the world's political economy had been worked out in great detail. However, Marx needed to write down all of the details of his economic world view to further clarify the new economic theory in his own mind. Accordingly, Marx wrote "The Economic and Philosophical Manuscripts". These manuscripts covered numerous topics, detailing Marx's concept of alienated labour. However, by the spring of 1845 his continued study of political economy, capital and capitalism had led Marx to the belief that the new political economic theory that he was espousing—scientific socialism—needed to be built on the base of a thoroughly developed materialistic view of the world.
The "Economic and Philosophical Manuscripts of 1844" had been written between April and August 1844. Soon, though, Marx recognised that the "Manuscripts" had been influenced by some inconsistent ideas of Ludwig Feuerbach. Accordingly, Marx recognised the need to break with Feuerbach's philosophy in favour of historical materialism. Thus, a year later, in April 1845, after moving from Paris to Brussels, Marx wrote his eleven "Theses on Feuerbach", The "Theses on Feuerbach" are best known for Theses 11, which states that "philosophers have only interpreted the world in various ways, the point is to change it". This work contains Marx's criticism of materialism (for being contemplative), idealism (for reducing practice to theory) overall, criticising philosophy for putting abstract reality above the physical world. It thus introduced the first glimpse at Marx's historical materialism, an argument that the world is changed not by ideas but by actual, physical, material activity and practice. In 1845, after receiving a request from the Prussian king, the French government shut down "Vorwärts!", with the interior minister, François Guizot, expelling Marx from France. At this point, Marx moved from Paris to Brussels, where Marx hoped to, once again, continue his study of capitalism and political economy.
Brussels: 1845–1847.
Unable either to stay in France or to move to Germany, Marx decided to emigrate to Brussels in Belgium in February 1845. However, to stay in Belgium, Marx had to pledge not to publish anything on the subject of contemporary politics. In Brussels, he associated with other exiled socialists from across Europe, including Moses Hess, Karl Heinzen, and Joseph Weydemeyer, and soon, in April 1845, Engels moved from Barmen in Germany to Brussels to join Marx and the growing cadre of members of the League of the Just now seeking home in Brussels. Later, Mary Burns, Engels' long-time companion, left Manchester, England, to join Engels in Brussels.
In mid-July 1845, Marx and Engels left Brussels for England to visit the leaders of the Chartists, a socialist movement in Britain. This was Marx's first trip to England and Engels was an ideal guide for the trip. Engels had already spent two years living in Manchester, from November 1842 to August 1844. Not only did Engels already know the English language, he had developed a close relationship with many Chartist leaders. Indeed, Engels was serving as a reporter for many Chartist and socialist English newspapers. Marx used the trip as an opportunity to examine the economic resources available for study in various libraries in London and Manchester.
In collaboration with Engels, Marx also set about writing a book which is often seen as his best treatment of the concept of historical materialism, "The German Ideology". In this work, Marx broke with Feuerbach, Bruno Bauer, Max Stirner and the rest of the Young Hegelians, and also broke with Karl Grun and other "true socialists" whose philosophies were still based in part on "idealism." In "German Ideology" Marx and Engels finally completed their philosophy, which was based solely on materialism as the sole motor force in history.
"German Ideology" is written in a humorously satirical form. But even this satirical form did not save the work from censorship. Like so many other early writings of his, "German Ideology" would not be published in Marx's lifetime and would be published only in 1932.
After completing "German Ideology", Marx turned to a work that was intended to clarify his own position regarding "the theory and tactics" of a truly "revolutionary proletarian movement" operating from the standpoint of a truly "scientific materialist" philosophy. This work was intended to draw a distinction between the utopian socialists and Marx's own scientific socialist philosophy. Whereas the utopians believed that people must be persuaded one person at a time to join the socialist movement, the way a person must be persuaded to adopt any different belief, Marx knew that people would tend on most occasions to act in accordance with their own economic interests. Thus, appealing to an entire class (the working class in this case) with a broad appeal to the class's best material interest would be the best way to mobilise the broad mass of that class to make a revolution and change society. This was the intent of the new book that Marx was planning. However, to get the manuscript past the government censors, Marx called the book "The Poverty of Philosophy" (1847) and offered it as a response to the "petty bourgeois philosophy" of the French anarchist socialist Pierre-Joseph Proudhon as expressed in his book "The Philosophy of Poverty" (1840).
These books laid the foundation for Marx and Engels's most famous work, a political pamphlet that has since come to be commonly known as "The Communist Manifesto". While residing in Brussels in 1846, Marx continued his association with the secret radical organisation League of the Just. As noted above, Marx thought the League to be just the sort of radical organisation that was needed to spur the working class of Europe toward the mass movement that would bring about a working class revolution. However, to organise the working class into a mass movement, the League had to cease its "secret" or "underground" orientation and operate in the open as a political party. Members of the League eventually became persuaded in this regard. Accordingly, in June 1847 the League of the Just was reorganised by its membership into a new open "above ground" political society that appealed directly to the working classes. This new open political society was called the Communist League. Both Marx and Engels participated in drawing the programme and organisational principles of the new Communist League.
In late 1847, Marx and Engels began writing what was to become their most famous work — a programme of action for the Communist League. Written jointly by Marx and Engels from December 1847 to January 1848, "The Communist Manifesto" was first published on 21 February 1848. "The Communist Manifesto" laid out the beliefs of the new Communist League. No longer a secret society, the Communist League wanted to make aims and intentions clear to the general public rather than hiding its beliefs as the League of the Just had been doing. The opening lines of the pamphlet set forth the principal basis of Marxism, that "The history of all hitherto existing society is the history of class struggles." It goes on to examine the antagonisms that Marx claimed were arising in the clashes of interest between the bourgeoisie (the wealthy middle class) and the proletariat (the industrial working class). Proceeding on from this, the "Manifesto" presents the argument for why the Communist League, as opposed to other socialist and liberal political parties and groups at the time, was truly acting in the interests of the proletariat to overthrow capitalist society and to replace it with socialism.
Later that year, Europe experienced a series of protests, rebellions, and often violent upheavals that became known as the Revolution of 1848. In France, a revolution led to the overthrow of the monarchy and the establishment of the French Second Republic. Marx was supportive of such activity, and having recently received a substantial inheritance from his father of either 6,000 or 5,000 francs, allegedly used a third of it to arm Belgian workers who were planning revolutionary action. Although the veracity of these allegations is disputed, the Belgian Ministry of Justice accused him of it, subsequently arresting him, and he was forced to flee back to France, where, with a new republican government in power, he believed that he would be safe.
Cologne: 1848–1849.
Temporarily settling down in Paris, Marx transferred the Communist League executive headquarters to the city and also set up a German Workers' Club with various German socialists living there. Hoping to see the revolution spread to Germany, in 1848 Marx moved back to Cologne where he began issuing a handbill entitled the "Demands of the Communist Party in Germany", in which he argued for only four of the ten points of the "Communist Manifesto", believing that in Germany at that time, the bourgeoisie must overthrow the feudal monarchy and aristocracy before the proletariat could overthrow the bourgeoisie. On 1 June, Marx started publication of a daily newspaper, the "Neue Rheinische Zeitung", which he helped to finance through his recent inheritance from his father. Designed to put forward news from across Europe with his own Marxist interpretation of events, the newspaper featured Marx as a primary writer and the dominant editorial influence. Despite contributions by fellow members of the Communist League, it remained, according to Friedrich Engels, "a simple dictatorship by Marx".
Whilst editor of the paper, Marx and the other revolutionary socialists were regularly harassed by the police, and Marx was brought to trial on several occasions, facing various allegations including insulting the Chief Public Prosecutor, committing a press misdemeanor, and inciting armed rebellion through tax boycotting, although each time he was acquitted. Meanwhile, the democratic parliament in Prussia collapsed, and the king, Frederick William IV, introduced a new cabinet of his reactionary supporters, who implemented counter-revolutionary measures to expunge leftist and other revolutionary elements from the country. Consequently, the "Neue Rheinische Zeitung" was soon suppressed and Marx was ordered to leave the country on 16 May. Marx returned to Paris, which was then under the grip of both a reactionary counter-revolution and a cholera epidemic, and was soon expelled by the city authorities, who considered him a political threat. With his wife, Jenny, expecting their fourth child, and not able to move back to Germany or Belgium, in August 1849 he sought refuge in London.
Life in London.
Marx moved to London in early June 1849 and would remain based in the city for the rest of his life. The headquarters of the Communist League also moved to London. However, in the winter of 1849–1850, a split within the ranks of the Communist League occurred when a faction within it led by August Willich and Karl Schapper began agitating for an immediate uprising. Willich and Schapper believed that once the Communist League had initiated the uprising, the entire working class from across Europe would rise "spontaneously" to join it, thus, creating revolution across Europe. Marx and Engels protested that such an unplanned uprising on the part of the Communist League was "adventuristic" and would be suicide for the Communist League. Such an uprising, as that recommended by the Schapper/Willich group would easily be crushed by the police and the armed forces of the reactionary governments of Europe. This, Marx maintained, would spell doom for the Communist League itself. Changes in society, Marx argued, are not achieved overnight through the efforts and will power of "a handful of men." Instead, they are brought about through a scientific analysis of economic conditions of society and by moving toward revolution through different stages of social development. In the present stage of development (circa 1850), following the defeat of the uprisings across Europe in 1848, Marx felt that the Communist League should encourage the working class to unite with progressive elements of the rising bourgeoisie to defeat the feudal aristocracy on issues involving demands for governmental reforms, such as a constitutional republic with freely elected assemblies and universal (male) suffrage. In other words, the working class must join with bourgeois and democratic forces to bring about the successful conclusion of the bourgeois revolution before stressing the working class agenda and a working class revolution.
After a long struggle which threatened to ruin the Communist League, Marx's opinion prevailed and, eventually, the Willich/Schapper group left the Communist League. Meanwhile, Marx also became heavily involved with the socialist German Workers' Educational Society. The Society held their meetings in Great Windmill Street, Soho, central London's entertainment district. This organization was also racked by an internal struggle between its members, some of whom followed Marx while others followed the Schapper/Willich faction. The issues in this internal split were the same issues raised in the internal split within the Communist League. Marx, however, lost the fight with the Schapper/Willich faction within the German Workers' Educational Society and, on 17 September 1850, resigned from the Society.
Writing for the "New York Tribune".
While in London, Marx devoted himself to the task of revolutionary organising of the working class. For the first few years he and his family lived in extreme poverty. His main source of income was his colleague, Engels, who derived much of his income from his family's business. Later Marx and Engels both began writing for six different newspapers around the world, in England, the United States, Prussia, Austria and South Africa. Most of Marx's journalistic writing, however, was as a European correspondent for the "New York Daily Tribune". In earlier years, Marx had been able to communicate with the broad masses of the working class by editing his own newspaper or editing a newspaper financed by others sympathetic to his philosophy. Now, in London, Marx was unable to finance his own newspaper and unable to put together financing from others. Thus, Marx sought to communicate with the public by writing articles for the "New York Tribune" and other "bourgeois" newspapers. At first Marx's English-language articles were translated from German by Wilhelm Pieper; eventually, however, Marx learned English well enough to write without translation.
The "New York Daily Tribune" had been founded in New York City in the United States of America by Horace Greeley in April 1841. Marx's main contact on the "Tribune" was Charles Dana. Later, in 1868, Charles Dana would leave the "Tribune" to become the owner and editor-in-chief of the "New York Sun", a competing newspaper in New York City. However, at this time Charles Dana served on the editorial board of the "Tribune".
Several things about the "Tribune" made the newspaper an excellent vehicle for Marx to reach a sympathetic public across the Atlantic Ocean. Since its founding the "Tribune" had been an inexpensive newspaper—two cents per copy. Accordingly, it was popular with the broad masses of the working class of the United States. With a run of about 50,000 issues, the "Tribune" was the most widely circulated journal in the United States. Editorially, the "Tribune" reflected Greeley's anti-slavery opinions. Not only did the "Tribune" have wide readership with the United States and not only did that readership come from the working classes, but the readers seemed to be from the progressive wing of the working class. Marx's first article for the "New York Tribune" was on the British elections to Parliament and was published in the "Tribune" on 21 August 1852.
Marx was just one of the reporters in Europe that the "New York Tribune" employed. However, with the slavery crisis in the United States coming to a head in the late 1850s and with the outbreak of the American Civil War in 1861, the American public's interest in European affairs declined. Thus Marx very early began to write on issues affecting the United States — particularly the "slavery crisis" and the "War Between the States."
Marx continued to write articles for the "New York Daily Tribune" as long as he was sure that the "Tribune's" editorial policy was still progressive. However, the departure of Charles Dana from the paper in late 1861 and the resultant change in the editorial board brought about a new editorial policy. No longer was the "Tribune" to be a strong abolitionist paper dedicated to a complete Union victory. The new editorial board supported an immediate peace between the Union and the Confederacy in the Civil War in the United States with slavery left intact in the Confederacy. Marx strongly disagreed with this new political position and, in 1863, was forced to withdraw as a writer for the "Tribune."
From December 1851 to March 1852, Marx wrote "The Eighteenth Brumaire of Louis Napoleon", a work on the French Revolution of 1848, in which he expanded upon his concepts of historical materialism, class struggle and the dictatorship of the proletariat, advancing the argument that victorious proletariat has to smash the bourgeois state.
The 1850s and 1860s also mark the line between what some scholars see as the idealistic, Hegelian young Marx from the more scientifically minded mature Marx writings of the later period. This distinction is usually associated with the structural Marxism school, and not all scholars agree that it exists. The years of revolution from 1848 to 1849 had been a grand experience for both Marx and Engels. They both became sure that their economic view of the course of history was the only valid way that historic events like the revolutionary upsurge of 1848 could be adequately explained. For some time after 1848, Marx and Engels wondered if the entire revolutionary upsurge had completely played out. As time passed, they began to think that a new revolutionary upsurge would not occur until there was another economic downturn. The question of whether a recession would be necessary to create a new revolutionary situation in society became a point of contention between Marx and certain other revolutionaries. Marx accused these other revolutionaries of being "adventurists" because of their belief that a revolutionary situation could be created out of thin air by the sheer "will power" of the revolutionaries without regard to the economic realities of the current situation.
The downturn in the United States economy in 1852 led Marx and Engels to wonder if a revolutionary upsurge would soon occur. However, the United States' economy was too new to play host to a classical revolution. The western frontier in America always provided a relief valve for the pent-up forces that might in other countries cause social unrest. Any economic crisis which began in the United States would not lead to revolution unless one of the older economies of Europe "caught the contagion" from the United States. In other words, economies of the world were still seen as individual national systems which were contiguous with the national borders of each country. The Panic of 1857 broke the mould of all prior thinking on the world economy. Beginning in the United States, the Panic spread across the globe. Indeed, the Panic of 1857 was the first truly global economic crisis.
Marx longed to return to his economic studies. He had left these studies in 1844 and had been preoccupied with other projects over the last thirteen years. By returning to his study of economics, he felt he would be able to understand more thoroughly what was occurring in the world.
The First International.
In 1864, Marx became involved in the International Workingmen's Association (also known as "First International"), to whose General Council he was elected at its inception in 1864. In that organisation, Marx was involved in the struggle against the anarchist wing centred on Mikhail Bakunin (1814–1876). Although Marx won this contest, the transfer of the seat of the General Council from London to New York in 1872, which Marx supported, led to the decline of the International. The most important political event during the existence of the International was the Paris Commune of 1871 when the citizens of Paris rebelled against their government and held the city for two months. In response to the bloody suppression of this rebellion Marx wrote one of his most famous pamphlets, "The Civil War in France", a defence of the Commune.
Given the repeated failures and frustrations of workers' revolutions and movements, Marx also sought to understand capitalism, and spent a great deal of time in the reading room of the British Museum studying and reflecting on the works of political economists and on economic data. By 1857 he had accumulated over 800 pages of notes and short essays on capital, landed property, wage labour, the state, and foreign trade and the world market; this work did not appear in print until 1939, under the title 'Outlines of the Critique of Political Economy'.
Finally in 1859 Marx published "A Contribution to the Critique of Political Economy", his first serious economic work. This work was intended merely as a preview of his three-volume "Das Kapital" (English title: "Capital: Critique of Political Economy") on which he intended to publish at a later date. In "A Contribution to the Critique of Political Economy", Marx accepts the labour theory of value as advocated by David Ricardo, but whereas Ricardo drew a distinction between use value and value in commodities, Ricardo always had been unable to define the real relationship between use value and value. The reasoning Marx laid out in his book clearly delineated the true relationship between use value and value. He also produced a truly scientific theory of money and money circulation in the capitalist economy. Thus, "A Contribution to the Critique of Political Economy" created a storm of enthusiasm when it appeared in public. The entire edition of the book was sold out quickly.
The successful sales of "A Contribution to the Critique of Political Economy" stimulated Marx in the early 1860s to finish work on the three large volumes that would compose his major life's work—"Das Kapital" and the "Theories of Surplus Value", which discussed the theoreticians of political economy, particularly Adam Smith and David Ricardo. "Theories of Surplus Value" is often referred to as the fourth volume book of "Das Kapital" and constitutes one of the first comprehensive treatises on the history of economic thought. In 1867 the first volume of "Das Kapital" was published, a work which analysed the capitalist process of production. Here Marx elaborated his labour theory of value, which had been influenced by Thomas Hodgskin. Marx acknowledged Hodgskin's "admirable work" "Labour Defended against the Claims of Capital" at more than one point in "Capital." Indeed, Marx quoted Hodgskin as recognising the alienation of labour that occurred under modern capitalist production. No longer was there any "natural reward of individual labour. Each labourer produces only some part of a whole, and each part having no value or utility of itself, there is nothing on which the labourer can seize, and say: 'This is my product, this will I keep to myself.'" In this first volume of "Capital", Marx outlined his conception of surplus value and exploitation, which he argued would ultimately lead to a falling rate of profit and the collapse of industrial capitalism. Demand for a Russian language edition of "Capital" soon led to the printing of 3,000 copies of the book in the Russian language, which was published on 27 March 1872. By the autumn of 1871 the entire first edition of the German language edition of "Capital" had been sold out and a second edition was published.
Volumes II and III of "Capital" remained mere manuscripts upon which Marx continued to work for the rest of his life. Both volumes were published by Engels after Marx's death. Volume II of "Capital" was prepared and published by Engels in July 1893 under the name "Capital II: The Process of Circulation of Capital". Volume III of "Capital" was published a year later in October 1894 under the name "Capital III: The Process of Capitalist Production as a Whole". "Theories of Surplus Value" was developed from the "Economic Manuscripts of 1861–1863" which comprise Volumes 30, 31 32 and 33 of the "Collected Works of Marx and Engels" and from the "Economic Manuscripts of 1861–1864" which comprises Volume 34 of the "Collected Works of Marx and Engels." The exact part of the "Economic Manuscripts of 1861–1863" which makes up the "Theories of Surplus Value" are the last part of Volume 30 of the "Collected Works", the whole of Volume 31 of the "Collected Works", and the whole of Volume 32 of the "Collected Works". A German language abridged edition of "Theories of Surplus Value" was published in 1905 and in 1910. This abridged edition was translated into English and published in 1951 in London. However, the complete unabridged edition of "Theories of Surplus Value" was published as the "fourth volume" of "Capital" in 1963 and 1971 in Moscow.
During the last decade of his life, Marx's health declined and he became incapable of the sustained effort that had characterised his previous work. He did manage to comment substantially on contemporary politics, particularly in Germany and Russia. His "Critique of the Gotha Programme" opposed the tendency of his followers Wilhelm Liebknecht and August Bebel to compromise with the state socialism of Ferdinand Lassalle in the interests of a united socialist party. This work is also notable for another famous Marx's quote: "From each according to his ability, to each according to his need."
In a letter to Vera Zasulich dated 8 March 1881, Marx contemplated the possibility of Russia's bypassing the capitalist stage of development and building communism on the basis of the common ownership of land characteristic of the village "mir". While admitting that Russia's rural "commune is the fulcrum of social regeneration in Russia", Marx also warned that, in order for the mir to operate as a means for moving straight to the socialist stage without a preceding capitalist stage, it "would first be necessary to eliminate the deleterious influences which are assailing it (the rural commune) from all sides." Given the elimination of these pernicious influences, Marx allowed that "normal conditions of spontaneous development" of the rural commune could exist. However, in the same letter to Vera Zasulich, Marx points out that "at the core of the capitalist system ... lies the complete separation of the producer from the means of production." In one of the drafts of this letter, Marx reveals his growing passion for anthropology, motivated by his belief that future communism would be a return on a higher level to the communism of our prehistoric past. He wrote that "the historical trend of our age is the fatal crisis which capitalist production has undergone in the European and American countries where it has reached its highest peak, a crisis that will end in its destruction, in the return of modern society to a higher form of the most archaic type—collective production and appropriation". He added that "the vitality of primitive communities was incomparably greater than that of Semitic, Greek, Roman, etc. societies, and, a fortiori, that of modern capitalist societies". Before he died, Marx asked Engels to write up these ideas, which were published in 1884 under the title "The Origin of the Family, Private Property and the State".
Personal life.
Marx and von Westphalen had seven children together, but partly owing to the poor conditions in which they lived whilst in London, only three survived to adulthood. The children were: Jenny Caroline (m. Longuet; 1844–1883); Jenny Laura (m. Lafargue; 1845–1911); Edgar (1847–1855); Henry Edward Guy ("Guido"; 1849–1850); Jenny Eveline Frances ("Franziska"; 1851–1852); Jenny Julia Eleanor (1855–1898) and one more who died before being named (July 1857). There are allegations that Marx also fathered a son, Freddy, out of wedlock by his housekeeper, Helene Demuth.
Marx frequently used pseudonyms, often when renting a house or flat, apparently to make it harder for the authorities to track him down. While in Paris, he used that of "Monsieur Ramboz", whilst in London he signed off his letters as "A. Williams". His friends referred to him as "Moor", owing to his dark complexion and black curly hair, something which they believed made him resemble the historical Moors of North Africa, whilst he encouraged his children to call him "Old Nick" and "Charley". He also bestowed nicknames and pseudonyms on his friends and family as well, referring to Friedrich Engels as "General", his housekeeper Helene as "Lenchen" or "Nym", while one of his daughters, Jennychen, was referred to as "Qui Qui, Emperor of China" and another, Laura, was known as "Kakadou" or "the Hottentot".
According to the biographer Sylvia Nasar, Marx never learned to properly speak English and never visited an English factory despite living in England during his last thirty years.
Death.
Following the death of his wife, Jenny, in December 1881, Marx developed a catarrh that kept him in ill health for the last 15 months of his life. It eventually brought on the bronchitis and pleurisy that killed him in London on 14 March 1883 (age 64). He died a stateless person; family and friends in London buried his body in Highgate Cemetery, London, on 17 March 1883. There were between nine and eleven mourners at his funeral.
Several of his closest friends spoke at his funeral, including Wilhelm Liebknecht and Friedrich Engels. Engels' speech included the passage:
On the 14th of March, at a quarter to three in the afternoon, the greatest living thinker ceased to think. He had been left alone for scarcely two minutes, and when we came back we found him in his armchair, peacefully gone to sleep—but forever.
Marx's daughters Eleanor and Laura, as well as Charles Longuet and Paul Lafargue, Marx's two French socialist sons-in-law, were also in attendance. Liebknecht, a founder and leader of the German Social-Democratic Party, gave a speech in German, and Longuet, a prominent figure in the French working-class movement, made a short statement in French. Two telegrams from workers' parties in France and Spain were also read out. Together with Engels's speech, this constituted the entire programme of the funeral. Non-relatives attending the funeral included three communist associates of Marx: Friedrich Lessner, imprisoned for three years after the Cologne communist trial of 1852; G. Lochner, whom Engels described as "an old member of the Communist League"; and Carl Schorlemmer, a professor of chemistry in Manchester, a member of the Royal Society, and a communist activist involved in the 1848 Baden revolution. Another attendee of the funeral was Ray Lankester, a British zoologist who would later become a prominent academic.
Upon his own death in 1895, Engels left Marx's two surviving daughters a "significant portion" of his $4.8 million estate.
Marx's tombstone bears the carved message: "WORKERS OF ALL LANDS UNITE", the final line of "The Communist Manifesto," and from the 11th "Thesis on Feuerbach" (edited by Engels): "The philosophers have only interpreted the world in various ways—the point however is to change it". The Communist Party of Great Britain had the monumental tombstone built in 1954 with a portrait bust by Laurence Bradshaw; Marx's original tomb had only humble adornment. In 1970 there was an unsuccessful attempt to destroy the monument using a homemade bomb.
The late Marxist historian Eric Hobsbawm remarked that "One cannot say Marx died a failure" because, although he had not achieved a large following of disciples in Britain, his writings had already begun to make an impact on the leftist movements in Germany and Russia. Within 25 years of his death, the continental European socialist parties that acknowledged Marx's influence on their politics were each gaining between 15 and 47 per cent in those countries with representative democratic elections.
Thought.
Influences.
Marx's thought demonstrates influences from many thinkers, including but not limited to:
Marx's view of history, which came to be called historical materialism (controversially adapted as the philosophy of dialectical materialism by Engels and Lenin) certainly shows the influence of Hegel's claim that one should view reality (and history) dialectically. However, Hegel had thought in idealist terms, putting ideas in the forefront, whereas Marx sought to rewrite dialectics in materialist terms, arguing for the primacy of matter over idea. Where Hegel saw the "spirit" as driving history, Marx saw this as an unnecessary mystification, obscuring the reality of humanity and its physical actions shaping the world. He wrote that Hegelianism stood the movement of reality on its head, and that one needed to set it upon its feet. Despite his dislike of mystical terms Marx used Gothic language in several of his works. In Das Kapital he refers to capital as "necromancy that surrounds the products of labour".
Though inspired by French socialist and sociological thought, Marx criticised utopian socialists, arguing that their favoured small-scale socialistic communities would be bound to marginalisation and poverty, and that only a large-scale change in the economic system can bring about real change.
The other important contribution to Marx's revision of Hegelianism came from Engels's book, "The Condition of the Working Class in England in 1844", which led Marx to conceive of the historical dialectic in terms of class conflict and to see the modern working class as the most progressive force for revolution.
Marx believed that he could study history and society scientifically and discern tendencies of history and the resulting outcome of social conflicts. Some followers of Marx concluded, therefore, that a communist revolution would inevitably occur. However, Marx famously asserted in the eleventh of his "Theses on Feuerbach" that "philosophers have only interpreted the world, in various ways; the point however is to change it", and he clearly dedicated himself to trying to alter the world.
Philosophy and social thought.
Marx's polemic with other thinkers often occurred through critique, and thus he has been called "the first great user of critical method in social sciences." He criticised speculative philosophy, equating metaphysics with ideology. By adopting this approach, Marx attempted to separate key findings from ideological biases. This set him apart from many contemporary philosophers.
Human nature.
Like Tocqueville, who described a faceless and bureaucratic despotism with no identifiable despot, Marx also broke with classical thinkers who spoke of a single tyrant and with Montesquieu, who discussed the nature of the single despot. Instead, Marx set out to analyse "the despotism of capital". Fundamentally, Marx assumed that human history involves transforming human nature, which encompasses both human beings and material objects. Humans recognise that they possess both actual and potential selves. For both Marx and Hegel, self-development begins with an experience of internal alienation stemming from this recognition, followed by a realisation that the actual self, as a subjective agent, renders its potential counterpart an object to be apprehended. Marx further argues that, by moulding nature in desired ways, the subject takes the object as its own, and thus permits the individual to be actualised as fully human. For Marx, then, human nature—"Gattungswesen", or species-being—exists as a function of human labour. Fundamental to Marx's idea of meaningful labour is the proposition that, in order for a subject to come to terms with its alienated object, it must first exert influence upon literal, material objects in the subject's world. Marx acknowledges that Hegel "grasps the nature of "work" and comprehends objective man, authentic because actual, as the result of his "own work"", but characterises Hegelian self-development as unduly "spiritual" and abstract. Marx thus departs from Hegel by insisting that "the fact that man is a "corporeal", actual, sentient, objective being with natural capacities means that he has "actual, sensuous objects" for his nature as objects of his life-expression, or that he can only "express" his life in actual sensuous objects." Consequently, Marx revises Hegelian "work" into material "labour", and in the context of human capacity to transform nature the term "labour power".
Labour, class struggle, and false consciousness.
The history of all hitherto existing society is the history of class struggles.— Karl Marx, "The Communist Manifesto"
Marx had a special concern with how people relate to their own labour power. He wrote extensively about this in terms of the problem of alienation. As with the dialectic, Marx began with a Hegelian notion of alienation but developed a more materialist conception. Capitalism mediates social relationships of production (such as among workers or between workers and capitalists) through commodities, including labour, that are bought and sold on the market. For Marx, the possibility that one may give up ownership of one's own labour—one's capacity to transform the world—is tantamount to being alienated from one's own nature; it is a spiritual loss. Marx described this loss as commodity fetishism, in which the things that people produce, commodities, appear to have a life and movement of their own to which humans and their behaviour merely adapt.
Commodity fetishism provides an example of what Engels called "false consciousness", which relates closely to the understanding of ideology. By "ideology", Marx and Engels meant ideas that reflect the interests of a particular class at a particular time in history, but which contemporaries see as universal and eternal. Marx and Engels's point was not only that such beliefs are at best half-truths; they serve an important political function. Put another way, the control that one class exercises over the means of production includes not only the production of food or manufactured goods; it includes the production of ideas as well (this provides one possible explanation for why members of a subordinate class may hold ideas contrary to their own interests). An example of this sort of analysis is Marx's understanding of religion, summed up in a passage from the preface to his 1843 "Contribution to the Critique of Hegel's Philosophy of Right":
"Religious" suffering is, at one and the same time, the "expression" of real suffering and a "protest" against real suffering. Religion is the sigh of the oppressed creature, the heart of a heartless world, and the soul of soulless conditions. It is the "opium" of the people. The abolition of religion as the illusory happiness of the people is the demand for their real happiness. To call on them to give up their illusions about their condition is to call on them to give up a condition that requires illusions.
Whereas his Gymnasium senior thesis argued that religion had as its primary social aim the promotion of solidarity, here Marx sees the social function of religion in terms of highlighting/preserving political and economic status quo and inequality.
Economy, history and society.
Marx's thoughts on labour were related to the primacy he gave to the economic relation in determining the society's past, present and future (see also economic determinism). Accumulation of capital shapes the social system. Social change, for Marx, was about conflict between opposing interests, driven, in the background, by economic forces. This became the inspiration for the body of works known as the conflict theory. In his evolutionary model of history, he argued that human history began with free, productive and creative work that was over time coerced and dehumanised, a trend most apparent under capitalism. Marx noted that this was not an intentional process; rather, no individual or even state can go against the forces of economy.
The organisation of society depends on means of production. Literally those things, like land, natural resources, and technology, necessary for the production of material goods and the relations of production, in other words, the social relationships people enter into as they acquire and use the means of production. Together these compose the mode of production, and Marx distinguished historical eras in terms of distinct modes of production. Marx differentiated between base and superstructure, with the base (or substructure) referring to the economic system, and superstructure, to the cultural and political system. Marx regarded this mismatch between (economic) base and (social) superstructure as a major source of social disruption and conflict.
Despite Marx's stress on critique of capitalism and discussion of the new communist society that should replace it, his explicit critique of capitalism is guarded, as he saw it as an improved society compared to the past ones (slavery and feudal). Marx also never clearly discusses issues of morality and justice, although scholars agree that his work contained implicit discussion of those concepts.
Marx's view of capitalism was two-sided. On one hand, Marx, in the 19th century's deepest critique of the dehumanising aspects of this system, noted that defining features of capitalism include alienation, exploitation, and recurring, cyclical depressions leading to mass unemployment; on the other hand capitalism is also characterised by "revolutionising, industrialising and universalising qualities of development, growth and progressivity" (by which Marx meant industrialisation, urbanisation, technological progress, increased productivity and growth, rationality and scientific revolution), that are responsible for progress. Marx considered the capitalist class to be one of the most revolutionary in history, because it constantly improved the means of production, more so than any other class in history, and was responsible for the overthrow of feudalism and its transition to capitalism. Capitalism can stimulate considerable growth because the capitalist can, and has an incentive to, reinvest profits in new technologies and capital equipment.
According to Marx capitalists take advantage of the difference between the labour market and the market for whatever commodity the capitalist can produce. Marx observed that in practically every successful industry input unit-costs are lower than output unit-prices. Marx called the difference "surplus value" and argued that this surplus value had its source in surplus labour, the difference between what it costs to keep workers alive and what they can produce. Marx's dual view of capitalism can be seen in his description of the capitalists: he refers to them as to vampires sucking worker's blood, but at the same time, he notes that drawing profit is "by no means an injustice" and that capitalists simply cannot go against the system. The true problem lies with the "cancerous cell" of capital, understood not as property or equipment, but the relations between workers and owners—the economic system in general.
At the same time, Marx stressed that capitalism was unstable, and prone to periodic crises. He suggested that over time, capitalists would invest more and more in new technologies, and less and less in labour. Since Marx believed that surplus value appropriated from labour is the source of profits, he concluded that the rate of profit would fall even as the economy grew. Marx believed that increasingly severe crises would punctuate this cycle of growth, collapse, and more growth. Moreover, he believed that in the long-term this process would necessarily enrich and empower the capitalist class and impoverish the proletariat. In section one of "The Communist Manifesto" Marx describes feudalism, capitalism, and the role internal social contradictions play in the historical process:
We see then: the means of production and of exchange, on whose foundation the bourgeoisie built itself up, were generated in feudal society. At a certain stage in the development of these means of production and of exchange, the conditions under which feudal society produced and exchanged ... the feudal relations of property became no longer compatible with the already developed productive forces; they became so many fetters. They had to be burst asunder; they were burst asunder. Into their place stepped free competition, accompanied by a social and political constitution adapted in it, and the economic and political sway of the bourgeois class. A similar movement is going on before our own eyes ... The productive forces at the disposal of society no longer tend to further the development of the conditions of bourgeois property; on the contrary, they have become too powerful for these conditions, by which they are fettered, and so soon as they overcome these fetters, they bring order into the whole of bourgeois society, endanger the existence of bourgeois property.
Marx believed that those structural contradictions within capitalism necessitate its end, giving way to socialism, or a post-capitalistic, communist society:
The development of Modern Industry, therefore, cuts from under its feet the very foundation on which the bourgeoisie produces and appropriates products. What the bourgeoisie, therefore, produces, above all, are its own grave-diggers. Its fall and the victory of the proletariat are equally inevitable."
Thanks to various processes overseen by capitalism, such as urbanisation, the working class, the proletariat, should grow in numbers and develop class consciousness, in time realising that they have to and can change the system. Marx believed that if the proletariat were to seize the means of production, they would encourage social relations that would benefit everyone equally, abolishing exploiting class, and introduce a system of production less vulnerable to cyclical crises. Marx argued in "The German Ideology" that capitalism will end through the organised actions of an international working class:
Communism is for us not a state of affairs which is to be established, an ideal to which reality will have to adjust itself. We call communism the real movement which abolishes the present state of things. The conditions of this movement result from the premises now in existence."
In this new society the self-alienation would end, and humans would be free to act without being bound by the labour market. It would be a democratic society, enfranchising the entire population. In such a utopian world there would also be little if any need for a state, which goal was to enforce the alienation. He theorised that between capitalism and the establishment of a socialist/communist system, a dictatorship of the proletariat—a period where the working class holds political power and forcibly socialises the means of production—would exist. As he wrote in his "Critique of the Gotha Program", "between capitalist and communist society there lies the period of the revolutionary transformation of the one into the other. Corresponding to this is also a political transition period in which the state can be nothing but the revolutionary dictatorship of the proletariat." While he allowed for the possibility of peaceful transition in some countries with strong democratic institutional structures (such as Britain, the US and the Netherlands), he suggested that in other countries with strong centralised state-oriented traditions, like France and Germany, the "lever of our revolution must be force."
Legacy.
Marx's ideas have had a profound impact on world politics and intellectual thought. Followers of Marx have frequently debated amongst themselves over how to interpret Marx's writings and apply his concepts to the modern world. The legacy of Marx's thought has become contested between numerous tendencies, each of which sees itself as Marx's most accurate interpreter. In the political realm, these tendencies include Leninism, Marxism-Leninism, Trotskyism, Maoism, Luxemburgism, and libertarian Marxism. Various currents have also developed in academic Marxism, often under influence of other views, resulting in structuralist Marxism, historical Marxism, phenomenological Marxism, Analytical Marxism and Hegelian Marxism.
From an academic perspective, Marx's work contributed to the birth of modern sociology. He has been cited as one the nineteenth century's three masters of the "school of suspicion", alongside Friedrich Nietzsche and Sigmund Freud, and as one of the three principal architects of modern social science along with Émile Durkheim and Max Weber. In contrast to other philosophers, Marx offered theories that could often be tested with the scientific method. Both Marx and Auguste Comte set out to develop scientifically justified ideologies in the wake of European secularisation and new developments in the philosophies of history and science. Working in the Hegelian tradition, Marx rejected Comtean sociological positivism in attempt to develop a "science of society". Karl Löwith considered Marx and Søren Kierkegaard to be the two greatest Hegelian philosophical successors. In modern sociological theory, Marxist sociology is recognised as one of the main classical perspectives. Isaiah Berlin considers Marx the true founder of modern sociology, "in so far as anyone can claim the title." Beyond social science, he has also had a lasting legacy in philosophy, literature, the arts, and the humanities.
In social theory, twentieth- and twenty-first-century thinkers have pursued two main strategies in response to Marx. One move has been to reduce it to its analytical core, known as Analytical Marxism, which came at the cost of sacrificing its most interesting and perplexing ideas. Another, more common move has been to dilute the explanatory claims of Marx's social theory and to emphasise the "relative autonomy" of aspects of social and economic life not directly related to Marx's central narrative of interaction between the development of the "forces of production" and the succession of "modes of production." Such has been, for example, the neo-marxist theorising adopted by historians inspired by Marx's social theory, such as E. P. Thompson and Eric Hobsbawm. It has also been a line of thinking pursued by thinkers and activists like Antonio Gramsci who have sought to understand the opportunities and the difficulties of transformative political practice, seen in the light of Marxist social theory.
Politically, Marx's legacy is more complex. Throughout the twentieth century, revolutions in dozens of countries labelled themselves 'Marxist', most notably the Russian Revolution which lead to the founding of the USSR. Major world leaders including Vladimir Lenin, Mao Zedong, Fidel Castro, Salvador Allende Josip Tito and Kwame Nkrumah all cited Marx as an influence, and his ideas informed political parties worldwide beyond those where 'Marxist revolutions' took place. The brutal dictatorships associated with many Marxist nations have lead some to blame Marx for millions of deaths, but the fidelity of these varied revolutionaries, leaders and parties to Marx's work is highly contested, and rejected by many Marxists. It is now common to distinguish between the legacy and influence of Marx specifically, and the legacy and influence of those who shaped his ideas for political purposes.
Further reading.
Commentaries on Marx.
</dl>

</doc>
<doc id="16772" url="http://en.wikipedia.org/wiki?curid=16772" title="Korean War">
Korean War

<noinclude>
The Korean War (in South Korean Hangul: 한국전쟁, Hanja: 韓國戰爭, "Korean War"; in North Korean Chosungul: 조국해방전쟁, "Joguk Haebang Jeonjaeng", "Fatherland Liberation War"; 25 June 1950 – 27 July 1953) was a war between North and South Korea, in which a United Nations force led by the United States of America fought for the South, and China fought for the North, which was also assisted by the Soviet Union. The war arose from the division of Korea at the end of World War II and from the global tensions of the Cold War that developed immediately afterwards.
Korea was ruled by Japan from 1910 until the closing days of World War II. In August 1945, the Soviet Union declared war on Japan and—by agreement with the United States—occupied Korea north of the 38th parallel. U.S. forces subsequently occupied the south and Japan surrendered. By 1948, two separate governments had been set up. Both governments claimed to be the legitimate government of Korea, and neither side accepted the border as permanent. The conflict escalated into open warfare when North Korean forces—supported by the Soviet Union and China—invaded South Korea on 25 June 1950. On that day, the United Nations Security Council recognized this North Korean act as invasion and called for an immediate ceasefire. On 27 June, the Security Council adopted "S/RES/83 : Complaint of aggression upon the Republic of Korea" and decided the formation and dispatch of "the UN Forces in Korea". Twenty-one countries of the United Nations eventually contributed to the defense of South Korea, with the United States providing 88% of the UN's military personnel.
After the first two months of the conflict, South Korean forces were on the point of defeat, forced back to the Pusan Perimeter. In September 1950, an amphibious UN counter-offensive was launched at Inchon, and cut off many of the North Korean attackers. Those that escaped envelopment and capture were rapidly forced back north all the way to the border with China at the Yalu River, or into the mountainous interior. At this point, in October 1950, Chinese forces crossed the Yalu and entered the war. Chinese intervention triggered a retreat of UN forces which continued until mid-1951. After these dramatic reversals of fortune, which saw Seoul change hands four times, the last two years of conflict became a war of attrition, with the front line close to the 38th parallel. The war in the air, however, was never a stalemate. North Korea was subject to a massive bombing campaign. Jet aircraft were used in air-to-air combat for the first time in history, and Soviet pilots covertly flew in defense of their Communist allies.
The fighting ended on 27 July 1953, when the armistice was signed. The agreement created the Korean Demilitarized Zone to separate North and South Korea, and allowed the return of prisoners. Clashes have continued to the present.
Names.
In the U.S., the war was initially described by President Harry S. Truman as a "police action" as it was an undeclared military action, conducted under the auspices of the United Nations. It has been referred to in the Anglosphere as "The Forgotten War" or "The Unknown War" because of the lack of public attention it received both during and after the war, and in relation to the global scale of World War II, which preceded it, and the subsequent angst of the Vietnam War, which succeeded it.
In South Korea, the war is usually referred to as "625" or the "6–2–5 Upheaval" (6.25 동란(動亂), "yook-i-o dongnan"), reflecting the date of its commencement on 25 June.
In North Korea, the war is officially referred to as the "Fatherland Liberation War" ("Choguk haebang chǒnjaeng") or alternatively the "Chosǒn ["Korean"] War" (조선전쟁, "Chosǒn chǒnjaeng").
In China, the war is officially called the "War to Resist U.S. Aggression and Aid Korea" (), although the term "Chaoxian (Korean) War" () is also used in unofficial contexts, along with the term "Korean Conflict" () more commonly used in regions such as Hong Kong and Macau.
Background.
Imperial Japanese rule (1910–45).
Japan destroyed the influence of China over Korea in the First Sino-Japanese War (1894–95), ushering in the short-lived Korean Empire. A decade later, after defeating Imperial Russia in the Russo-Japanese War (1904–05), Japan made Korea its protectorate with the Eulsa Treaty in 1905, then annexed it with the Japan–Korea Annexation Treaty in 1910.
Many Korean nationalists fled the country. A Provisional Government of the Republic of Korea was founded in 1919 in Nationalist China. It failed to achieve international recognition, failed to unite nationalist groups, and had a fractious relationship with its American-based founding President, Syngman Rhee. From 1919 to 1925 and beyond, Korean Communists led internal and external warfare against the Japanese.
Korea was considered to be part of the Empire of Japan as an industrialized colony along with Taiwan, and both were part of the Greater East Asia Co-Prosperity Sphere. In 1937, the colonial Governor-General, General Jirō Minami, commanded the attempted cultural assimilation of Korea's 23.5 million people by banning the use and study of Korean language, literature, and culture, to be replaced with that of mandatory use and study of their Japanese counterparts. Starting in 1939, the populace was required to use Japanese names under the Sōshi-kaimei policy. Conscription of Koreans for labor in war industries began in 1939, with as many as 2 million Koreans conscripted into either the Japanese Army or into the Japanese labor force.
In China, the Nationalist National Revolutionary Army and the Communist People's Liberation Army helped organize refugee Korean patriots and independence fighters against the Japanese military, which had also occupied parts of China. The Nationalist-backed Koreans, led by Yi Pom-Sok, fought in the Burma Campaign (December 1941 – August 1945). The Communists, led by Kim Il-sung among others, fought the Japanese in Korea and Manchuria.
During World War II, Japan used Korea's food, livestock, and metals for their war effort. Japanese forces in Korea increased from 46,000 soldiers in 1941 to 300,000 in 1945. Japanese Korea conscripted 2.6 million forced laborers controlled with a collaborationist Korean police force; some 723,000 people were sent to work in the overseas empire and in metropolitan Japan. By 1942, Korean men were being conscripted into the Imperial Japanese Army. By January 1945, Koreans made up 32% of Japan's labor force. At the end of the war, other world powers did not recognize Japanese rule in Korea and Taiwan.
At the Cairo Conference in November 1943, China, the United Kingdom, and United States all decided "in due course Korea shall become free and independent".
Soviet-Japanese War (1945).
At the Tehran Conference in November 1943 and the Yalta Conference in February 1945, the Soviet Union promised to join its allies in the Pacific war within three months of the victory in Europe. Accordingly, it declared war on Japan on 9 August 1945. By 10 August, the Red Army had begun to occupy the northern part of the Korean peninsula.
On the night of 10 August in Washington, American Colonels Dean Rusk and Charles H. Bonesteel III were tasked with dividing the Korean Peninsula into Soviet and U.S. occupation zones and proposed the 38th parallel. This was incorporated into America's General Order No. 1 which responded to the Japanese surrender on 15 August. Explaining the choice of the 38th parallel, Rusk observed, "even though it was further north than could be realistically reached by U.S. forces, in the event of Soviet disagreement...we felt it important to include the capital of Korea in the area of responsibility of American troops". He noted that he was "faced with the scarcity of US forces immediately available, and time and space factors, which would make it difficult to reach very far north, before Soviet troops could enter the area". As Rusk's comments indicate, the Americans doubted whether the Soviet government would agree to this. Stalin, however, maintained his wartime policy of co-operation, and on 16 August the Red Army halted at the 38th parallel for three weeks to await the arrival of U.S. forces in the south.
Korea divided (1945–49).
On 8 September 1945, U.S. Lt. Gen. John R. Hodge arrived in Incheon to accept the Japanese surrender south of the 38th parallel. Appointed as military governor, General Hodge directly controlled South Korea as head of the United States Army Military Government in Korea (USAMGIK 1945–48). He established control by restoring to power the key Japanese colonial administrators, but in the face of Korean protests he quickly reversed this decision. The USAMGIK refused to recognize the provisional government of the short-lived People's Republic of Korea (PRK) because it suspected it was communist.
In December 1945, Korea was administered by a U.S.-Soviet Union Joint Commission, as agreed at the Moscow Conference (1945), with the aim of granting independence after a five-year trusteeship. The idea was not popular among Koreans and riots broke out. To contain them, the USAMGIK banned strikes on 8 December 1945 and outlawed the PRK Revolutionary Government and the PRK People's Committees on 12 December 1945.
The right-wing Representative Democratic Council, led by Syngman Rhee, who had arrived with the U.S. military, opposed the trusteeship, arguing that Korea had already suffered from foreign occupation far too long. General Hodge began to distance himself from the proposal, even though it had originated with his government.
On 23 September 1946, an 8,000-strong railroad worker strike began in Pusan. Civil disorder spread throughout the country in what became known as the Autumn uprising. On 1 October 1946, Korean police killed three students in the Daegu Uprising; protesters counter-attacked, killing 38 policemen. On 3 October, some 10,000 people attacked the Yeongcheon police station, killing three policemen and injuring some 40 more; elsewhere, some 20 landlords and pro-Japanese South Korean officials were killed. The USAMGIK declared martial law.
Citing the inability of the Joint Commission to make progress, the U.S. government decided to hold an election under United Nations auspices with the aim of creating an independent Korea. The Soviet authorities and the Korean Communists refused to co-operate on the grounds it would not be fair, and many South Korean politicians also boycotted it. A general election was held in the South on 10 May 1948. It was marred by terrorism and sabotage resulting in 600 deaths. North Korea held parliamentary elections three months later on 25 August.
The resultant South Korean government promulgated a national political constitution on 17 July 1948, and elected Syngman Rhee as President on 20 July 1948. The Republic of Korea (South Korea) was established on 15 August 1948. In the Russian Korean Zone of Occupation, the Soviet Union established a Communist North Korean government led by Kim Il-sung. President Rhee's régime excluded communists and leftists from southern politics. Disenfranchised, they headed for the hills, to prepare for guerrilla war against the US-sponsored ROK Government.
Meanwhile, on 3 April 1948, what began as a demonstration commemorating Korean resistance to Japanese rule ended with the Jeju Uprising where between 14,000 and 60,000 people died. South Korean soldiers carried out large scale atrocities during the suppression of the uprising. In October 1948, some South Korean soldiers mutinied against the clampdown in the Yeosu-Suncheon Rebellion.
The Soviet Union withdrew as agreed from Korea in 1948, and U.S. troops withdrew in 1949. On 24 December 1949, South Korean forces killed 86 to 88 people in the Mungyeong massacre and blamed the crime on communist marauding bands. By early 1950, Syngman Rhee had about 30,000 alleged communists in jails and about 300,000 suspected sympathizers enrolled in the Bodo League re-education movement.
Chinese Civil War (1945–1949).
With the end of the war with Japan, the Chinese Civil War resumed between the Chinese Communists and the Chinese Nationalists. While the Communists were struggling for supremacy in Manchuria, they were supported by the North Korean government with matériel and manpower. According to Chinese sources, the North Koreans donated 2,000 railway cars worth of matériel while thousands of Koreans served in the Chinese People's Liberation Army (PLA) during the war. North Korea also provided the Chinese Communists in Manchuria with a safe refuge for non-combatants and communications with the rest of China.
The North Korean contributions to the Chinese Communist victory were not forgotten after the creation of the People's Republic of China in 1949. As a token of gratitude, between 50,000 to 70,000 Korean veterans that served in the PLA were sent back along with their weapons, and they later played a significant role in the initial invasion of South Korea. China promised to support the North Koreans in the event of a war against South Korea. The Chinese support created a deep division between the Korean Communists, and Kim Il-sung's authority within the Communist party was challenged by the Chinese faction led by Pak Il-yu, who was later purged by Kim.
After the formation of the People's Republic of China in 1949, the Chinese government named the Western nations, led by the United States, as the biggest threat to its national security. Basing this judgment on China's century of humiliation beginning in the early 19th century, American support for the Nationalists during the Chinese Civil War, and the ideological struggles between revolutionaries and reactionaries, the Chinese leadership believed that China would become a critical battleground in the United States' crusade against Communism. As a countermeasure and to elevate China's standing among the worldwide Communist movements, the Chinese leadership adopted a foreign policy that actively promoted Communist revolutions throughout territories on China's periphery.
Course of the war.
Outbreak of war (1950).
By 1949, South Korean forces had reduced the active number of communist guerrillas in the South from 5,000 to 1,000. However, Kim Il-sung believed that the guerrillas had weakened the South Korean military and that a North Korean invasion would be welcomed by much of the South Korean population. Kim began seeking Stalin's support for an invasion in March 1949, travelling to Moscow to attempt to persuade Stalin.
Initially, Stalin did not think the time was right for a war in Korea. Chinese Communist forces were still fighting in China. American forces were still stationed in South Korea (they would complete their withdrawal in June 1949) and Stalin did not want the Soviet Union to become embroiled in a war with the United States.
By spring 1950, Stalin believed the strategic situation had changed. The Soviets had detonated their first nuclear bomb in September 1949; American soldiers had fully withdrawn from Korea; the Americans had not intervened to stop the communist victory in China, and Stalin calculated that the Americans would be even less willing to fight in Korea—which had seemingly much less strategic significance. The Soviets had also cracked the codes used by the US to communicate with the US embassy in Moscow, and reading these dispatches convinced Stalin that Korea did not have the importance to the US that would warrant a nuclear confrontation. Stalin began a more aggressive strategy in Asia based on these developments, including promising economic and military aid to China through the Sino-Soviet Friendship, Alliance, and Mutual Assistance Treaty.
Throughout 1949 and 1950 the Soviets continued to arm North Korea. After the Communist victory in the Chinese Civil War, ethnic Korean units in the Chinese People's Liberation Army (PLA) were released to North Korea. The combat veterans from China, the tanks, artillery and aircraft supplied by the Soviets, and rigorous training increased North Korea's military superiority over the South, which had been armed by the American military.
In April 1950, Stalin gave Kim permission to invade the South under the condition that Mao would agree to send reinforcements if they became needed. Stalin made it clear that Soviet forces would not openly engage in combat, to avoid a direct war with the Americans. Kim met with Mao in May 1950. Mao was concerned that the Americans would intervene but agreed to support the North Korean invasion. China desperately needed the economic and military aid promised by the Soviets. At that time, the Chinese were in the process of demobilizing half of the PLA's 5.6 million soldiers. However, Mao sent more ethnic Korean PLA veterans to Korea and promised to move an army closer to the Korean border. Once Mao's commitment was secured, preparations for war accelerated.
Soviet generals with extensive combat experience from the Second World War were sent to North Korea as the Soviet Advisory Group. These generals completed the plans for the attack by May. The original plans called for a skirmish to be initiated in the Ongjin Peninsula on the west coast of Korea. The North Koreans would then launch a "counterattack" that would capture Seoul and encircle and destroy the South Korean army. The final stage would involve destroying South Korean government remnants, capturing the rest of South Korea, including the ports.
On 7 June 1950, Kim Il-sung called for a Korea-wide election on 5–8 August 1950 and a consultative conference in Haeju on 15–17 June 1950. On 11 June, the North sent three diplomats to the South, as a peace overture that Rhee rejected. On 21 June, Kim Il-Sung revised his war plan to involve general attack across the 38th parallel, rather than a limited operation in the Ongjin peninsula. Kim was concerned that South Korean agents had learned about the plans and South Korean forces were strengthening their defenses. Stalin agreed to this change of plan.
While these preparations were underway in the North, there were frequent skirmishes along the 38th parallel, especially at Kaesong and Ongjin, many initiated by the South. The Republic of Korea Army (ROK Army) was being trained by the U.S. Korean Military Advisory Group (KMAG). On the eve of war, KMAG's commander General William Lynn Roberts voiced utmost confidence in the ROK Army and boasted that any North Korean invasion would merely provide "target practice". For his part, Syngman Rhee repeatedly expressed his desire to attack the North, including when American diplomat John Foster Dulles visited Korea on 18 June.
Although some South Korean and American intelligence officers were predicting an attack from the North, similar predictions had been made before and nothing had eventuated. The Central Intelligence Agency did note the southward movement by the Korean People's Army (KPA), but assessed this as a "defensive measure" and concluded an invasion was "unlikely". On 23 June, UN observers inspected the border and did not detect that war was imminent.
At dawn on Sunday, 25 June 1950, the Korean People's Army crossed the 38th parallel behind artillery fire. The KPA justified its assault with the claim that ROK troops had attacked first, and that they were aiming to arrest and execute the "bandit traitor Syngman Rhee". Fighting began on the strategic Ongjin peninsula in the west. There were initial South Korean claims that they had captured the city of Haeju, and this sequence of events has led some scholars to argue that the South Koreans actually fired first.
Whoever fired the first shots in Ongjin, within an hour, North Korean forces attacked all along the 38th parallel. The North Koreans had a combined arms force including tanks supported by heavy artillery. The South Koreans did not have any tanks, anti-tank weapons, nor heavy artillery, that could stop such an attack. In addition, South Koreans committed their forces in a piecemeal fashion and these were routed within a few days.
On 27 June, Rhee evacuated from Seoul with some of the government. On 28 June, at 2am, the South Korean Army blew up the highway bridge across the Han River in an attempt to stop the North Korean army. The bridge was detonated while 4,000 refugees were crossing the bridge, and hundreds were killed. Destroying the bridge also trapped many South Korean military units North of the Han River. In spite of such desperate measures, Seoul fell that same day. A number of South Korean National Assemblymen remained in Seoul when it fell, and forty-eight subsequently pledged allegiance to the North.
On 28 June, Rhee ordered the massacre of suspected political opponents in his own country.
In five days the South Korean forces, which had 95,000 men on 25 June, was down to less than 22,000 men. In early July, when U.S. forces arrived, what was left of the South Korean forces were placed under U.S. operational command of the United Nations Command.
Factors in US intervention.
The Truman administration was caught ill prepared and at a crossroads. Before the invasion, Korea was not included in the strategic Asian Defense Perimeter outlined by Secretary of State Dean Acheson. Military strategists were more concerned with the security of Europe against the Soviet Union than East Asia. At the same time, the Administration was worried that a war in Korea could quickly widen into another world war should the Chinese or Soviets decide to get involved as well.
One facet of the changing attitude toward Korea and whether to get involved was Japan. Especially after the fall of China to the Communists, U.S. East Asian experts saw Japan as the critical counterweight to the Soviet Union and China in the region. While there was no United States policy that dealt with South Korea directly as a national interest, its proximity to Japan increased the importance of South Korea. Said Kim: "The recognition that the security of Japan required a non-hostile Korea led directly to President Truman's decision to intervene... The essential point... is that the American response to the North Korean attack stemmed from considerations of US policy toward Japan."
A major consideration was the possible Soviet reaction in the event that the US intervened. The Truman administration was fretful that a war in Korea was a diversionary assault that would escalate to a general war in Europe once the United States committed in Korea. At the same time, "[t]here was no suggestion from anyone that the United Nations or the United States could back away from [the conflict]". Truman believed if aggression went unchecked a chain reaction would be initiated that would marginalize the United Nations and encourage Communist aggression elsewhere. The UN Security Council approved the use of force to help the South Koreans and the US immediately began using what air and naval forces that were in the area to that end. The Administration still refrained from committing on the ground because some advisers believed the North Koreans could be stopped by air and naval power alone.
The Truman administration was still uncertain if the attack was a ploy by the Soviet Union or just a test of U.S. resolve. The decision to commit ground troops became viable when a communiqué was received on 27 June indicating the Soviet Union would not move against U.S. forces in Korea. The Truman administration now believed it could intervene in Korea without undermining its commitments elsewhere.
United Nations Security Council Resolutions.
On 25 June 1950, the United Nations Security Council unanimously condemned the North Korean invasion of the Republic of Korea, with UN Security Council Resolution 82. The Soviet Union, a veto-wielding power, had boycotted the Council meetings since January 1950, protesting that the Republic of China (Taiwan), not the People's Republic of China, held a permanent seat in the UN Security Council. After debating the matter, the Security Council, on 27 June 1950, published Resolution 83 recommending member states provide military assistance to the Republic of Korea. On 27 June President Truman ordered U.S. air and sea forces to help the South Korean regime. On 4 July the Soviet Deputy Foreign Minister accused the United States of starting armed intervention on behalf of South Korea.
The Soviet Union challenged the legitimacy of the war for several reasons. The ROK Army intelligence upon which Resolution 83 was based came from U.S. Intelligence; North Korea was not invited as a sitting temporary member of the UN, which violated UN Charter Article 32; and the Korean conflict was beyond the UN Charter's scope, because the initial north–south border fighting was classed as a civil war. Because the Soviet Union was boycotting the Security Council at the time, legal scholars posited that deciding upon an action of this type required the unanimous vote of the five permanent members.
Comparison of military forces.
By mid-1950, North Korean forces numbered between 150,000 and 200,000 troops, organized into 10 infantry divisions, one tank division, and one air force division, with 210 fighter planes and 280 tanks, who captured scheduled objectives and territory, among them Kaesong, Chuncheon, Uijeongbu, and Ongjin. Their forces included 274 T-34-85 tanks, some 150 Yak fighters, 110 attack bombers, 200 artillery pieces, 78 Yak trainers, and 35 reconnaissance aircraft. In addition to the invasion force, the North KPA had 114 fighters, 78 bombers, 105 T-34-85 tanks, and some 30,000 soldiers stationed in reserve in North Korea. Although each navy consisted of only several small warships, the North and South Korean navies fought in the war as sea-borne artillery for their in-country armies.
In contrast, the ROK Army defenders were relatively unprepared and ill-equipped. In "South to the Naktong, North to the Yalu" (1961), R.E. Appleman reports the ROK forces' low combat readiness as of 25 June 1950. The ROK Army had 98,000 soldiers (65,000 combat, 33,000 support), no tanks (they had been requested from the U.S. military, but requests were denied), and a 22-piece air force comprising 12 liaison-type and 10 AT6 advanced-trainer airplanes. There were no large foreign military garrisons in Korea at the time of the invasion, but there were large U.S. garrisons and air forces in Japan.
Within days of the invasion, masses of ROK Army soldiers—of dubious loyalty to the Syngman Rhee regime—were either retreating southwards or were defecting en masse to the northern side, the KPA.
United Nations response (July – August 1950).
On Saturday, 24 June 1950, U.S. Secretary of State Dean Acheson informed President Truman that the North Koreans had invaded South Korea. Truman and Acheson discussed a U.S. invasion response and agreed that the United States was obligated to act, paralleling the North Korean invasion with Adolf Hitler's aggressions in the 1930s, with the conclusion being that the mistake of appeasement must not be repeated. Several U.S. industries were mobilized to supply materials, labor, capital, production facilities, and other services necessary to support the military objectives of the Korean War. However, President Truman later acknowledged that he believed fighting the invasion was essential to the American goal of the global containment of communism as outlined in the National Security Council Report 68 (NSC-68) (declassified in 1975):
Communism was acting in Korea, just as Hitler, Mussolini and the Japanese had ten, fifteen, and twenty years earlier. I felt certain that if South Korea was allowed to fall, Communist leaders would be emboldened to override nations closer to our own shores. If the Communists were permitted to force their way into the Republic of Korea without opposition from the free world, no small nation would have the courage to resist threat and aggression by stronger Communist neighbors.
In August 1950, the President and the Secretary of State obtained the consent of Congress to appropriate $12 billion for military action in Korea.
As an initial response, Truman called for a naval blockade of North Korea, and was shocked to learn that such a blockade could only be imposed 'on paper', since the U.S. Navy no longer had the warships with which to carry out his request. In fact, due to the extensive defense cuts and the emphasis placed on building a nuclear bomber force, none of the services were in a position to make a robust response with conventional military strength. General Omar Bradley, Chairman of the Joint Chiefs of Staff, was faced with re-organizing and deploying an American military force that was a shadow of its World War II counterpart. The impact of the Truman administration's defense budget cutbacks were now keenly felt, as American troops fought a series of costly rearguard actions. Lacking sufficient anti-tank weapons, artillery or armor, they were driven back down the Korean peninsula to Pusan. In a postwar analysis of the unpreparedness of U.S. Army forces deployed to Korea during the summer and fall of 1950, Army Major General Floyd L. Parks stated that "Many who never lived to tell the tale had to fight the full range of ground warfare from offensive to delaying action, unit by unit, man by man ... [T]hat we were able to snatch victory from the jaws of defeat ... does not relieve us from the blame of having placed our own flesh and blood in such a predicament."
Acting on State Secretary Acheson's recommendation, President Truman ordered General MacArthur to transfer matériel to the Army of the Republic of Korea while giving air cover to the evacuation of U.S. nationals. The President disagreed with advisers who recommended unilateral U.S. bombing of the North Korean forces, and ordered the US Seventh Fleet to protect the Republic of China (Taiwan), whose government asked to fight in Korea. The United States denied ROC's request for combat, lest it provoke a communist Chinese retaliation. Because the United States had sent the Seventh Fleet to "neutralize" the Taiwan Strait, Chinese premier Zhou Enlai criticized both the UN and U.S. initiatives as "armed aggression on Chinese territory."
The Battle of Osan, the first significant American engagement of the Korean War, involved the 540-soldier Task Force Smith, which was a small forward element of the 24th Infantry Division which had been flown in from Japan. On 5 July 1950, Task Force Smith attacked the North Koreans at Osan but without weapons capable of destroying the North Koreans' tanks. They were unsuccessful; the result was 180 dead, wounded, or taken prisoner. The KPA progressed southwards, pushing back the US force at Pyongtaek, Chonan, and Chochiwon, forcing the 24th Division's retreat to Taejeon, which the KPA captured in the Battle of Taejon; the 24th Division suffered 3,602 dead and wounded and 2,962 captured, including the Division's Commander, Major General William F. Dean.
By August, the KPA had pushed back the ROK Army and the Eighth United States Army to the vicinity of Pusan in southeast Korea. In their southward advance, the KPA purged the Republic of Korea's intelligentsia by killing civil servants and intellectuals. On 20 August, General MacArthur warned North Korean leader Kim Il-sung that he was responsible for the KPA's atrocities. By September, the UN Command controlled the Pusan perimeter, enclosing about 10% of Korea, in a line partially defined by the Nakdong River.
Although Kim's early successes had led him to predict that he would end the war by the end of August, Chinese leaders were more pessimistic. To counter a possible U.S. deployment, Zhou Enlai secured a Soviet commitment to have the Soviet Union support Chinese forces with air cover, and deployed 260,000 soldiers along the Korean border, under the command of Gao Gang. Zhou commanded Chai Chengwen to conduct a topographical survey of Korea, and directed Lei Yingfu, Zhou's military advisor in Korea, to analyze the military situation in Korea. Lei concluded that MacArthur would most likely attempt a landing at Incheon. After conferring with Mao that this would be MacArthur's most likely strategy, Zhou briefed Soviet and North Korean advisers of Lei's findings, and issued orders to Chinese army commanders deployed on the Korean border to prepare for American naval activity in the Korea Strait.
Escalation (August – September 1950).
In the resulting Battle of Pusan Perimeter (August–September 1950), the U.S. Army withstood KPA attacks meant to capture the city at the Naktong Bulge, P'ohang-dong, and Taegu. The United States Air Force (USAF) interrupted KPA logistics with 40 daily ground support sorties that destroyed 32 bridges, halting most daytime road and rail traffic. KPA forces were forced to hide in tunnels by day and move only at night. To deny matériel to the KPA, the USAF destroyed logistics depots, petroleum refineries, and harbors, while the U.S. Navy air forces attacked transport hubs. Consequently, the over-extended KPA could not be supplied throughout the south.
Meanwhile, U.S. garrisons in Japan continually dispatched soldiers and matériel to reinforce defenders in the Pusan Perimeter. Tank battalions deployed to Korea directly from the U.S. mainland from the port of San Francisco to the port of Pusan, the largest Korean port. By late August, the Pusan Perimeter had some 500 medium tanks battle-ready. In early September 1950, ROK Army and UN Command forces outnumbered the KPA 180,000 to 100,000 soldiers. The UN forces, once prepared, counterattacked and broke out of the Pusan Perimeter.
Battle of Inchon (September 1950).
Against the rested and re-armed Pusan Perimeter defenders and their reinforcements, the KPA were undermanned and poorly supplied; unlike the UN Command, they lacked naval and air support. To relieve the Pusan Perimeter, General MacArthur recommended an amphibious landing at Inchon (now known as Incheon), near Seoul and well over 100 mi behind the KPA lines. On 6 July, he ordered Major General Hobart R. Gay, Commander, 1st Cavalry Division, to plan the division's amphibious landing at Incheon; on 12–14 July, the 1st Cavalry Division embarked from Yokohama, Japan to reinforce the 24th Infantry Division inside the Pusan Perimeter.
Soon after the war began, General MacArthur had begun planning a landing at Incheon, but the Pentagon opposed him. When authorized, he activated a combined U.S. Army and Marine Corps, and ROK Army force. The X Corps, led by General Edward Almond, Commander, consisted of 40,000 men of the 1st Marine Division, the 7th Infantry Division and around 8,600 ROK Army soldiers. By the 15 September attack date, the amphibious assault force faced few KPA defenders at Incheon: military intelligence, psychological warfare, guerrilla reconnaissance, and protracted bombardment facilitated a relatively light battle. However, the bombardment destroyed most of the city of Incheon.
After the Incheon landing, the 1st Cavalry Division began its northward advance from the Pusan Perimeter. "Task Force Lynch" (after Lieutenant Colonel James H. Lynch), 3rd Battalion, 7th Cavalry Regiment, and two 70th Tank Battalion units (Charlie Company and the Intelligence–Reconnaissance Platoon) effected the "Pusan Perimeter Breakout" through 106.4 mi of enemy territory to join the 7th Infantry Division at Osan. The X Corps rapidly defeated the KPA defenders around Seoul, thus threatening to trap the main KPA force in Southern Korea.
On 18 September, Stalin dispatched General H. M. Zakharov to Korea to advise Kim Il-sung to halt his offensive around the Pusan perimeter and to redeploy his forces to defend Seoul. Chinese commanders were not briefed on North Korean troop numbers or operational plans. As the overall commander of Chinese forces, Zhou Enlai suggested that the North Koreans should attempt to eliminate the enemy forces at Inchon only if they had reserves of at least 100,000 men; otherwise, he advised the North Koreans to withdraw their forces north.
On 25 September, Seoul was recaptured by South Korean forces. American air raids caused heavy damage to the KPA, destroying most of its tanks and much of its artillery. North Korean troops in the south, instead of effectively withdrawing north, rapidly disintegrated, leaving Pyongyang vulnerable. During the general retreat only 25,000 to 30,000 soldiers managed to rejoin the Northern KPA lines. On 27 September, Stalin convened an emergency session of the Politburo, in which he condemned the incompetence of the KPA command and held Soviet military advisers responsible for the defeat.
UN forces cross partition line (September – October 1950).
On 27 September, MacArthur received the top secret National Security Council Memorandum 81/1 from Truman reminding him that operations north of the 38th parallel were authorized only if "at the time of such operation there was no entry into North Korea by major Soviet or Chinese Communist forces, no announcements of intended entry, nor a threat to counter our operations militarily..." On 29 September MacArthur restored the government of the Republic of Korea under Syngman Rhee. On 30 September, Defense Secretary George Marshall sent an eyes-only message to MacArthur: "We want you to feel unhampered tactically and strategically to proceed north of the 38th parallel." During October, the ROK police executed people who were suspected to be sympathetic to North Korea, and similar massacres were carried out until early 1951.
On 30 September, Zhou Enlai warned the United States that China was prepared to intervene in Korea if the United States crossed the 38th parallel. Zhou attempted to advise North Korean commanders on how to conduct a general withdrawal by using the same tactics which had allowed Chinese communist forces to successfully escape Chiang Kai-shek's Encirclement Campaigns in the 1930s, but by some accounts North Korean commanders did not utilize these tactics effectively. Historian Bruce Cumings argues, however, the KPA's rapid withdrawal was strategic, with troops melting into the mountains from where they could launch guerrilla raids on the UN forces spread out on the coasts.
By 1 October 1950, the UN Command repelled the KPA northwards past the 38th parallel; the ROK Army crossed after them, into North Korea. MacArthur made a statement demanding the KPA's unconditional surrender. Six days later, on 7 October, with UN authorization, the UN Command forces followed the ROK forces northwards. The X Corps landed at Wonsan (in southeastern North Korea) and Riwon (in northeastern North Korea), already captured by ROK forces. The Eighth U.S. Army and the ROK Army drove up western Korea and captured Pyongyang city, the North Korean capital, on 19 October 1950. The 187th Airborne Regimental Combat Team ("Rakkasans") made their first of two combat jumps during the Korean War on 20 October 1950 at Sunchon and Sukchon. The missions of the 187th were to cut the road north going to China, preventing North Korean leaders from escaping from Pyongyang; and to rescue American prisoners of war. At month's end, UN forces held 135,000 KPA prisoners of war. As they neared the Sino-Korean border, the UN forces in the west were divided from those in the east by 50–100 miles of mountainous terrain.
Taking advantage of the UN Command's strategic momentum against the communists, General MacArthur believed it necessary to extend the Korean War into China to destroy depots supplying the North Korean war effort. President Truman disagreed, and ordered caution at the Sino-Korean border.
China intervenes (October – December 1950).
On 27 June 1950, two days after the KPA invaded and three months before the Chinese entered the war, President Truman dispatched the United States Seventh Fleet to the Taiwan Strait, to prevent hostilities between the Nationalist Republic of China (Taiwan) and the People's Republic of China (PRC). On 4 August 1950, with the PRC invasion of Taiwan aborted, Mao Zedong reported to the Politburo that he would intervene in Korea when the People's Liberation Army's (PLA) Taiwan invasion force was reorganized into the PLA North East Frontier Force. China justified its entry into the war as a response to "American aggression in the guise of the UN".
On 20 August 1950, Premier Zhou Enlai informed the UN that "Korea is China's neighbor... The Chinese people cannot but be concerned about a solution of the Korean question". Thus, through neutral-country diplomats, China warned that in safeguarding Chinese national security, they would intervene against the UN Command in Korea. President Truman interpreted the communication as "a bald attempt to blackmail the UN", and dismissed it.
1 October 1950, the day that UN troops crossed the 38th parallel, was also the first anniversary of the founding of the People's Republic of China. On that day the Soviet ambassador forwarded a telegram from Stalin to Mao and Zhou requesting that China send five to six divisions into Korea, and Kim Il-sung sent frantic appeals to Mao for Chinese military intervention. At the same time, Stalin made it clear that Soviet forces themselves would not directly intervene.
In a series of emergency meetings that lasted from 2–5 October, Chinese leaders debated whether to send Chinese troops into Korea. There was considerable resistance among many leaders, including senior military leaders, to confronting the U.S. in Korea. Mao strongly supported intervention, and Zhou was one of the few Chinese leaders who firmly supported him. After Lin Biao politely refused Mao's offer to command Chinese forces in Korea (citing his upcoming medical treatment), Mao decided that Peng Dehuai would be the commander of the Chinese forces in Korea after Peng agreed to support Mao's position. Mao then asked Peng to speak in favor of intervention to the rest of the Chinese leaders. After Peng made the case that if U.S. troops conquered Korea and reached the Yalu they might cross it and invade China the Politburo agreed to intervene in Korea. Later, the Chinese claimed that US bombers had violated PRC national airspace on three separate occasions and attacked Chinese targets before China intervened. On 8 October 1950, Mao Zedong redesignated the PLA North East Frontier Force as the Chinese People's Volunteer Army (PVA).
In order to enlist Stalin's support, Zhou and a Chinese delegation left for Moscow on 8 October, arriving there on 10 October at which point they flew to Stalin's home at the Black Sea. There they conferred with the top Soviet leadership which included Joseph Stalin as well as Vyacheslav Molotov, Lavrentiy Beria and Georgi Malenkov. Stalin initially agreed to send military equipment and ammunition, but warned Zhou that the Soviet Union's air force would need two or three months to prepare any operations. In a subsequent meeting, Stalin told Zhou that he would only provide China with equipment on a credit basis, and that the Soviet air force would only operate over Chinese airspace, and only after an undisclosed period of time. Stalin did not agree to send either military equipment or air support until March 1951. Mao did not find Soviet air support especially useful, as the fighting was going to take place on the south side of the Yalu. Soviet shipments of matériel, when they did arrive, were limited to small quantities of trucks, grenades, machine guns, and the like.
Immediately on his return to Beijing on 18 October 1950, Zhou met with Mao Zedong, Peng Dehuai, and Gao Gang, and the group ordered two hundred thousand Chinese troops to enter North Korea, which they did on 25 October. After consulting with Stalin, on 13 November, Mao appointed Zhou the overall commander and coordinator of the war effort, with Peng as field commander. Orders given by Zhou were delivered in the name of the Central Military Commission.
UN aerial reconnaissance had difficulty sighting PVA units in daytime, because their march and bivouac discipline minimized aerial detection. The PVA marched "dark-to-dark" (19:00–03:00), and aerial camouflage (concealing soldiers, pack animals, and equipment) was deployed by 05:30. Meanwhile, daylight advance parties scouted for the next bivouac site. During daylight activity or marching, soldiers were to remain motionless if an aircraft appeared, until it flew away; PVA officers were under order to shoot security violators. Such battlefield discipline allowed a three-division army to march the 286 mi from An-tung, Manchuria, to the combat zone in some 19 days. Another division night-marched a circuitous mountain route, averaging 18 mi daily for 18 days.
Meanwhile, on 10 October 1950, the 89th Tank Battalion was attached to the 1st Cavalry Division, increasing the armor available for the Northern Offensive. On 15 October, after moderate KPA resistance, the 7th Cavalry Regiment and Charlie Company, 70th Tank Battalion captured Namchonjam city. On 17 October, they flanked rightwards, away from the principal road (to Pyongyang), to capture Hwangju. Two days later, the 1st Cavalry Division captured Pyongyang, the North's capital city, on 19 October 1950.
On 15 October 1950, President Truman and General MacArthur met at Wake Island in the mid-Pacific Ocean. This meeting was much publicized because of the General's discourteous refusal to meet the President on the continental United States. To President Truman, MacArthur speculated there was little risk of Chinese intervention in Korea, and that the PRC's opportunity for aiding the KPA had lapsed. He believed the PRC had some 300,000 soldiers in Manchuria, and some 100,000–125,000 soldiers at the Yalu River. He further concluded that, although half of those forces might cross south, "if the Chinese tried to get down to Pyongyang, there would be the greatest slaughter" without air force protection.
After secretly crossing the Yalu River on 19 October, the PVA 13th Army Group launched the First Phase Offensive on 25 October, attacking the advancing UN forces near the Sino-Korean border. This military decision made solely by China changed the attitude of the Soviet Union. Twelve days after Chinese troops entered the war, Stalin allowed the Soviet Air Force to provide air cover, and supported more aid to China. After decimating the ROK II Corps at the Battle of Onjong, the first confrontation between Chinese and U.S. military occurred on 1 November 1950; deep in North Korea, thousands of soldiers from the PVA 39th Army encircled and attacked the U.S. 8th Cavalry Regiment with three-prong assaults—from the north, northwest, and west—and overran the defensive position flanks in the Battle of Unsan. The surprise assault resulted in the UN forces retreating back to the Ch'ongch'on River, while the Chinese unexpectedly disappeared into mountain hideouts following victory. It is unclear why the Chinese did not press the attack and follow up their victory.
The UN Command, however, were unconvinced that the Chinese had openly intervened due to the sudden Chinese withdrawal. On 24 November, the Home-by-Christmas Offensive was launched with the U.S. Eighth Army advancing in northwest Korea, while the US X Corps were attacking along the Korean east coast. But the Chinese were waiting in ambush with their Second Phase Offensive.
On 25 November at the Korean western front, the PVA 13th Army Group attacked and overran the ROK II Corps at the Battle of the Ch'ongch'on River, and then decimated the US 2nd Infantry Division on the UN forces' right flank. The UN Command retreated; the U.S. Eighth Army's retreat (the longest in US Army history) was made possible because of the Turkish Brigade's successful, but very costly, rear-guard delaying action near Kunuri that slowed the PVA attack for two days (27–29 November). On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.
By 30 November, the PVA 13th Army Group managed to expel the U.S. Eighth Army from northwest Korea. Retreating from the north faster than they had counter-invaded, the Eighth Army crossed the 38th parallel border in mid December. UN morale hit rock bottom when commanding General Walton Walker of the U.S. Eighth Army was killed on 23 December 1950 in an automobile accident. In northeast Korea by 11 December, the U.S. X Corps managed to cripple the PVA 9th Army Group while establishing a defensive perimeter at the port city of Hungnam. The X Corps were forced to evacuate by 24 December in order to reinforce the badly depleted U.S. Eighth Army to the south.
During the Hungnam evacuation, about 193 shiploads of UN Command forces and matériel (approximately 105,000 soldiers, 98,000 civilians, 17,500 vehicles, and 350,000 tons of supplies) were evacuated to Pusan. The SS "Meredith Victory" was noted for evacuating 14,000 refugees, the largest rescue operation by a single ship, even though it was designed to hold 12 passengers. Before escaping, the UN Command forces razed most of Hungnam city, especially the port facilities; and on 16 December 1950, President Truman declared a national emergency with Presidential Proclamation No. 2914, 3 C.F.R. 99 (1953), which remained in force until 14 September 1978. The next day (17 December 1950) Kim Il-sung was deprived of the right of command of KPA by China. After that, the leading part of the war became the Chinese army. Following that, on 1 February 1951, United Nations General Assembly adopted a draft resolution condemning China as an aggressor in the Korean War.
Fighting around the 38th parallel (January – June 1951).
With Lieutenant-General Matthew Ridgway assuming the command of the U.S. Eighth Army on 26 December, the PVA and the KPA launched their Third Phase Offensive (also known as the "Chinese New Year's Offensive") on New Year's Eve of 1950. Utilizing night attacks in which UN Command fighting positions were encircled and then assaulted by numerically superior troops who had the element of surprise, the attacks were accompanied by loud trumpets and gongs, which fulfilled the double purpose of facilitating tactical communication and mentally disorienting the enemy. UN forces initially had no familiarity with this tactic, and as a result some soldiers panicked, abandoning their weapons and retreating to the south. The Chinese New Year's Offensive overwhelmed UN forces, allowing the PVA and KPA to conquer Seoul for the second time on 4 January 1951.
These setbacks prompted General MacArthur to consider using nuclear weapons against the Chinese or North Korean interiors, with the intention that radioactive fallout zones would interrupt the Chinese supply chains. However, upon the arrival of the charismatic General Ridgway, the "esprit de corps" of the bloodied Eighth Army immediately began to revive.
UN forces retreated to Suwon in the west, Wonju in the center, and the territory north of Samcheok in the east, where the battlefront stabilized and held. The PVA had outrun its logistics capability and thus were unable to press on beyond Seoul as food, ammunition, and matériel were carried nightly, on foot and bicycle, from the border at the Yalu River to the three battle lines. In late January, upon finding that the PVA had abandoned their battle lines, General Ridgway ordered a reconnaissance-in-force, which became "Operation Roundup" (5 February 1951). A full-scale X Corps advance proceeded, which fully exploited the UN Command's air superiority, concluding with the UN reaching the Han River and recapturing Wonju.
After cease-fire negotiations failed in January, the United Nations General Assembly passed Resolution 498 on 1 February, condemning PRC as an aggressor, and called upon its forces to withdraw from Korea.
In early February, the ran the operation to destroy the guerrillas and their sympathizer citizens in Southern Korea. During the operation, the division and police conducted the Geochang massacre and Sancheong-Hamyang massacre. In mid-February, the PVA counterattacked with the Fourth Phase Offensive and achieved initial victory at Hoengseong. But the offensive was soon blunted by the IX Corps positions at Chipyong-ni in the center. Units of the U.S. 2nd Infantry Division and the French Battalion fought a short but desperate battle that broke the attack's momentum. The battle is sometimes known as the Gettysburg of the Korean War. The battle saw 5,600 Korean, American and French troops defeat a numerically superior Chinese force. Surrounded on all sides, the U.S. 2nd Infantry Division Warrior Division's 23rd Regimental Combat Team with an attached French Battalion was hemmed in by more than 25,000 Chinese Communist forces. United Nations forces had previously retreated in the face of large Communist forces instead of getting cut off, but this time they stood and fought at odds of roughly 15 to 1.
In the last two weeks of February 1951, "Operation Roundup" was followed by "Operation Killer", carried out by the revitalized Eighth Army. It was a full-scale, battlefront-length attack staged for maximum exploitation of firepower to kill as many KPA and PVA troops as possible. "Operation Killer" concluded with I Corps re-occupying the territory south of the Han River, and IX Corps capturing Hoengseong. On 7 March 1951, the Eighth Army attacked with "Operation Ripper", expelling the PVA and the KPA from Seoul on 14 March 1951. This was the city's fourth conquest in a years' time, leaving it a ruin; the 1.5 million pre-war population was down to 200,000, and people were suffering from severe food shortages.
On 1 March 1951 Mao sent a cable to Stalin, in which he emphasized the difficulties faced by Chinese forces and the urgent need for air cover, especially over supply lines. Apparently impressed by the Chinese war effort, Stalin finally agreed to supply two air force divisions, three anti-aircraft divisions, and six thousand trucks. PVA troops in Korea continued to suffer severe logistical problems throughout the war. In late April Peng Dehuai sent his deputy, Hong Xuezhi, to brief Zhou Enlai in Beijing. What Chinese soldiers feared, Hong said, was not the enemy, but that they had nothing to eat, no bullets to shoot, and no trucks to transport them to the rear when they were wounded. Zhou attempted to respond to the PVA's logistical concerns by increasing Chinese production and improving methods of supply, but these efforts were never completely sufficient. At the same time, large-scale air defense training programs were carried out, and the Chinese Air Force began to participate in the war from September 1951 onward.
On 11 April 1951, Commander-in-Chief Truman relieved the controversial General MacArthur, the Supreme Commander in Korea. There were several reasons for the dismissal. MacArthur had crossed the 38th parallel in the mistaken belief that the Chinese would not enter the war, leading to major allied losses. He believed that whether or not to use nuclear weapons should be his own decision, not the President's. MacArthur threatened to destroy China unless it surrendered. While MacArthur felt total victory was the only honorable outcome, Truman was more pessimistic about his chances once involved in a land war in Asia, and felt a truce and orderly withdrawal from Korea could be a valid solution. MacArthur was the subject of congressional hearings in May and June 1951, which determined that he had defied the orders of the President and thus had violated the U.S. Constitution. A popular criticism of MacArthur was that he never spent a night in Korea, and directed the war from the safety of Tokyo.
General Ridgway was appointed Supreme Commander, Korea; he regrouped the UN forces for successful counterattacks, while General James Van Fleet assumed command of the U.S. Eighth Army. Further attacks slowly depleted the PVA and KPA forces; Operations "Courageous" (23–28 March 1951) and "Tomahawk" (23 March 1951) were a joint ground and airborne infilltration meant to trap Chinese forces between Kaesong and Seoul. UN forces advanced to "Line Kansas," north of the 38th parallel. The 187th Airborne Regimental Combat Team's ("Rakkasans") second of two combat jumps was on Easter Sunday, 1951, at Munsan-ni, South Korea, codenamed Operation Tomahawk. The mission was to get behind Chinese forces and block their movement north. The 60th Indian Parachute Field Ambulance provided the medical cover for the operations, dropping an ADS and a surgical team and treating over 400 battle casualties apart from the civilian casualties that formed the core of their objective as the unit was on a humanitarian mission.
The Chinese counterattacked in April 1951, with the Fifth Phase Offensive, also known as the Chinese Spring Offensive, with three field armies (approximately 700,000 men). The offensive's first thrust fell upon I Corps, which fiercely resisted in the Battle of the Imjin River (22–25 April 1951) and the Battle of Kapyong (22–25 April 1951), blunting the impetus of the offensive, which was halted at the "No-name Line" north of Seoul. On 15 May 1951, the Chinese commenced the second impulse of the Spring Offensive and attacked the ROK Army and the U.S. X Corps in the east at the Soyang River. After initial success, they were halted by 20 May. At month's end, the U.S. Eighth Army counterattacked and regained "Line Kansas", just north of the 38th parallel. The UN's "Line Kansas" halt and subsequent offensive action stand-down began the stalemate that lasted until the armistice of 1953.
Stalemate (July 1951 – July 1953).
For the remainder of the Korean War the UN Command and the PVA fought, but exchanged little territory; the stalemate held. Large-scale bombing of North Korea continued, and protracted armistice negotiations began 10 July 1951 at Kaesong. On the Chinese side, Zhou Enlai directed peace talks, and Li Kenong and Qiao Guanghua headed the negotiation team. Combat continued while the belligerents negotiated; the UN Command forces' goal was to recapture all of South Korea and to avoid losing territory. The PVA and the KPA attempted similar operations, and later effected military and psychological operations in order to test the UN Command's resolve to continue the war.
The principal battles of the stalemate include the Battle of Bloody Ridge (18 August–15 September 1951), the Battle of the Punchbowl (31 August-21 September 1951), the Battle of Heartbreak Ridge (13 September–15 October 1951), the Battle of Old Baldy (26 June–4 August 1952), the Battle of White Horse (6–15 October 1952), the Battle of Triangle Hill (14 October–25 November 1952), the Battle of Hill Eerie (21 March–21 June 1952), the sieges of Outpost Harry (10–18 June 1953), the Battle of the Hook (28–9 May 1953), the Battle of Pork Chop Hill (23 March–16 July 1953), and the Battle of Kumsong (13–27 July 1953).
Chinese troops suffered from deficient military equipment, serious logistical problems, overextended communication and supply lines, and the constant threat of UN bombers. All of these factors generally led to a rate of Chinese casualties that was far greater than the casualties suffered by UN troops. The situation became so serious that, on November 1951, Zhou Enlai called a conference in Shenyang to discuss the PVA's logistical problems. At the meeting it was decided to accelerate the construction of railways and airfields in the area, to increase the number of trucks available to the army, and to improve air defense by any means possible. These commitments did little to directly address the problems confronting PVA troops.
In the months after the Shenyang conference Peng Dehuai went to Beijing several times to brief Mao and Zhou about the heavy casualties suffered by Chinese troops and the increasing difficulty of keeping the front lines supplied with basic necessities. Peng was convinced that the war would be protracted, and that neither side would be able to achieve victory in the near future. On 24 February 1952, the Military Commission, presided over by Zhou, discussed the PVA's logistical problems with members of various government agencies involved in the war effort. After the government representatives emphasized their inability to meet the demands of the war, Peng, in an angry outburst, shouted: "You have this and that problem... You should go to the front and see with your own eyes what food and clothing the soldiers have! Not to speak of the casualties! For what are they giving their lives? We have no aircraft. We have only a few guns. Transports are not protected. More and more soldiers are dying of starvation. Can't you overcome some of your difficulties?" The atmosphere became so tense that Zhou was forced to adjourn the conference. Zhou subsequently called a series of meetings, where it was agreed that the PVA would be divided into three groups, to be dispatched to Korea in shifts; to accelerate the training of Chinese pilots; to provide more anti-aircraft guns to the front lines; to purchase more military equipment and ammunition from the Soviet Union; to provide the army with more food and clothing; and, to transfer the responsibility of logistics to the central government.
Armistice (July 1953 – November 1954).
The on-again, off-again armistice negotiations continued for two years, first at Kaesong, on the border between North and South Korea, and then at the neighbouring village of Panmunjom. A major, problematic negotiation point was prisoner of war (POW) repatriation. The PVA, KPA, and UN Command could not agree on a system of repatriation because many PVA and KPA soldiers refused to be repatriated back to the north, which was unacceptable to the Chinese and North Koreans. In the final armistice agreement, signed on 27 July 1953, a Neutral Nations Repatriation Commission, under the Chairman Indian General K. S. Thimayya, was set up to handle the matter.
In 1952, the United States elected a new president, and on 29 November 1952, the president-elect, Dwight D. Eisenhower, went to Korea to learn what might end the Korean War. With the United Nations' acceptance of India's proposed Korean War armistice, the KPA, the PVA, and the UN Command ceased fire with the battle line approximately at the 38th parallel. Upon agreeing to the armistice, the belligerents established the Korean Demilitarized Zone (DMZ), which has since been patrolled by the KPA and ROKA, United States, and Joint UN Commands.
The Demilitarized Zone runs northeast of the 38th parallel; to the south, it travels west. The old Korean capital city of Kaesong, site of the armistice negotiations, originally was in pre-war South Korea, but now is part of North Korea. The United Nations Command, supported by the United States, the North Korean People's Army, and the Chinese People's Volunteers, signed the Armistice Agreement on 27 July 1953 to end the fighting. The Armistice also called upon the governments of South Korea, North Korea, China and the United States to participate in continued peace talks. The war is considered to have ended at this point, even though there was no peace treaty. North Korea nevertheless claims that it won the Korean War.
After the war, Operation Glory was conducted from July to November 1954, to allow combatant countries to exchange their dead. The remains of 4,167 U.S. Army and U.S. Marine Corps dead were exchanged for 13,528 KPA and PVA dead, and 546 civilians dead in UN prisoner-of-war camps were delivered to the South Korean government. After Operation Glory, 416 Korean War unknown soldiers were buried in the National Memorial Cemetery of the Pacific (The Punchbowl), on the island of Oahu, Hawaii. Defense Prisoner of War/Missing Personnel Office (DPMO) records indicate that the PRC and the DPRK transmitted 1,394 names, of which 858 were correct. From 4,167 containers of returned remains, forensic examination identified 4,219 individuals. Of these, 2,944 were identified as American, and all but 416 were identified by name. From 1996 to 2006, the DPRK recovered 220 remains near the Sino-Korean border.
Division of Korea (1954–present).
The Korean Armistice Agreement provided for monitoring by an international commission. Since 1953, the Neutral Nations Supervisory Commission (NNSC), composed of members from the Swiss and Swedish Armed Forces, has been stationed near the DMZ.
In April 1975, South Vietnam's capital was captured by the North Vietnamese army. Encouraged by the success of Communist revolution in Indochina, Kim Il-sung saw it as an opportunity to invade the South. Kim visited China in April of that year, and met with Mao Zedong and Zhou Enlai to ask for military aid. Despite Pyongyang's expectations, however, Beijing refused to help North Korea for another war in Korea.
Since the armistice, there have been numerous incursions and acts of aggression by North Korea. In 1976, the axe murder incident was widely publicized. Since 1974, four incursion tunnels leading to Seoul have been uncovered. In 2010, a North Korean submarine torpedoed and sank the South Korean corvette ROKS "Cheonan", resulting in the deaths of 46 sailors. Again in 2010, North Korea fired artillery shells on Yeonpyeong island, killing two military personnel and two civilians.
After a new wave of UN sanctions, on 11 March 2013, North Korea claimed that it had invalidated the 1953 armistice. On 13 March 2013, North Korea confirmed it ended the 1953 Armistice and declared North Korea "is not restrained by the North-South declaration on non-aggression". On 30 March 2013, North Korea stated that it had entered a "state of war" with South Korea and declared that "The long-standing situation of the Korean peninsula being neither at peace nor at war is finally over". Speaking on 4 April 2013, the U.S. Secretary of Defense, Chuck Hagel, informed the press that Pyongyang had "formally informed" the Pentagon that it had "ratified" the potential usage of a nuclear weapon against South Korea, Japan and the United States of America, including Guam and Hawaii. Hagel also stated that the United States would deploy the Terminal High Altitude Area Defense anti-ballistic missile system to Guam, due to a credible and realistic nuclear threat from North Korea.
Characteristics.
Casualties.
According to the data from the U.S. Department of Defense, the United States suffered 33,686 battle deaths, along with 2,830 non-battle deaths, during the Korean War. U.S. battle deaths were 8,516 up to their first engagement with the Chinese on 1 November 1950. South Korea reported some 373,599 civilian and 137,899 military deaths. Western sources estimate the PVA suffered about 400,000 killed and 486,000 wounded, while the KPA suffered 215,000 killed and 303,000 wounded.
Data from official Chinese sources, on the other hand, reported that the PVA had suffered 114,000 battle deaths, 34,000 non-battle deaths, 340,000 wounded, 7,600 missing and 21,400 captured during the war. Among those captured, about 14,000 defected to Taiwan, while the other 7,110 were repatriated to China. Chinese sources also reported that North Korea had suffered 290,000 casualties, 90,000 captured and a "large" number of civilian deaths.
In return, the Chinese and North Koreans estimated that about 390,000 soldiers from the United States, 660,000 soldiers from South Korea and 29,000 other UN soldiers were "eliminated" from the battlefield.
Recent scholarship has put the full battle death toll on all sides at just over 1.2 million.
Armored warfare.
The initial assault by North Korean KPA forces were aided by the use of Soviet T-34-85 tanks. A North Korean tank corps equipped with about 120 T-34s spearheaded the invasion. These drove against a ROK Army with few anti-tank weapons adequate to deal with the Soviet T-34s. Additional Soviet armor was added as the offensive progressed. The North Korean tanks had a good deal of early successes against South Korean infantry, elements of the 24th Infantry Division, and those United States built M24 Chaffee light tanks that they encountered. Interdiction by ground attack aircraft was the only means of slowing the advancing Korean armor. The tide turned in favour of the United Nations forces in August 1950 when the North Koreans suffered major tank losses during a series of battles in which the UN forces brought heavier equipment to bear, including M4A3 Sherman medium tanks backed by U.S. M26 heavy tanks, along with the British Centurion, Churchill, and Cromwell tanks.
The U.S. landings at Inchon on 15 September cut off the North Korean supply lines, causing their armored forces and infantry to run out of fuel, ammunition, and other supplies. As a result, the North Koreans had to retreat, and many of the T-34s and heavy weapons had to be abandoned. By the time the North Koreans withdrew from the South, a total of 239 T-34s and 74 SU-76s had been lost. After November 1950, North Korean armor was rarely encountered.
Following the initial assault by the north, the Korean War saw limited use of the tank and featured no large-scale tank battles. The mountainous, forested terrain, especially in the Eastern Central Zone, was poor tank country, limiting their mobility. Through the last two years of the war in Korea, UN tanks served largely as infantry support and mobile artillery pieces.
Naval warfare.
"Further information: List of U.S. Navy ships sunk or damaged in action during the Korean conflict"
Because neither Korea had a significant navy, the Korean War featured few naval battles. A skirmish between North Korea and the UN Command occurred on 2 July 1950; the U.S. Navy cruiser USS "Juneau", the Royal Navy cruiser HMS "Jamaica", and the frigate HMS "Black Swan" fought four North Korean torpedo boats and two mortar gunboats, and sank them.
The USS "Juneau" later sank several ammunition ships that had been present. The last sea battle of the Korean War occurred at Inchon, days before the Battle of Incheon; the ROK ship "PC 703" sank a North Korean mine layer in the Battle of Haeju Island, near Inchon. Three other supply ships were sunk by "PC-703" two days later in the Yellow Sea. Thereafter, vessels from the UN nations held undisputed control of the sea about Korea. The gun ships were used in shore bombardment, while the carriers provided air support to the ground forces.
During most of the war, the UN navies patrolled the west and east coasts of North Korea, sinking supply and ammunition ships and denying the North Koreans the ability to resupply from the sea. Aside from very occasional gunfire from North Korean shore batteries, the main threat to United States and UN navy ships was from magnetic mines. During the war, five U.S. Navy ships were lost to mines: two minesweepers, two minesweeper escorts, and one ocean tug. Mines and gunfire from North Korean coastal artillery damaged another 87 U.S. warships, resulting in slight to moderate damage.
Aerial warfare.
For the initial months of the war, the P-80 Shooting Star, F9F Panther, and other jets under the UN flag dominated North Korea's prop-driven air force of Soviet Yakovlev Yak-9 and Lavochkin La-9s. The balance would shift with the arrival of the swept-wing Soviet MiG-15.
The Chinese intervention in late October 1950 bolstered the Korean People's Air Force (KPAF) of North Korea with the MiG-15, one of the world's most advanced jet fighters. The fast, heavily armed MiG outflew first-generation UN jets such as the F-80 (United States Air Force) and Gloster Meteors (Royal Australian Air Force) posing a real threat to B-29 Superfortress bombers even under fighter escort. Fearful of confronting the United States directly, the Soviet Union denied involvement of their personnel in anything other than an advisory role, but air combat quickly resulted in Soviet pilots dropping their code signals and speaking over the wireless in Russian. This known direct Soviet participation was a "casus belli" that the UN Command deliberately overlooked, lest the war for the Korean peninsula expand to include the Soviet Union, and potentially escalate into atomic warfare.
The USAF countered the MiG-15 by sending over three squadrons of its most capable fighter, the F-86 Sabre. These arrived in December 1950. The MiG was designed as a bomber interceptor. It had a very high service ceiling—50000 ft and carried very heavy weaponry: one 37 mm cannon and two 23 mm cannons. They were fast enough to dive past the fighter escort of P-80 Shooting Stars and F9F Panthers and could reach and destroy the U.S. heavy bombers. B-29 losses could not be avoided, and the Air Force was forced to switch from a daylight bombing campaign to the necessarily less accurate nighttime bombing of targets. The MiGs were countered by the F-86 Sabres. They had a ceiling of 42000 ft and were armed with six .50 caliber (12.7 mm) machine guns, which were range adjusted by radar gunsights. If coming in at higher altitude the advantage of engaging or not went to the MiG. Once in a level flight dogfight, both swept-wing designs attained comparable maximum speeds of around 660 mi/h. The MiG climbed faster, but the Sabre turned and dived better.
In summer and autumn 1951, the outnumbered Sabres of the USAF's 4th Fighter Interceptor Wing—only 44 at one point—continued seeking battle in MiG Alley, where the Yalu River marks the Chinese border, against Chinese and North Korean air forces capable of deploying some 500 aircraft. Following Colonel Harrison Thyng's communication with the Pentagon, the 51st Fighter-Interceptor Wing finally reinforced the beleaguered 4th Wing in December 1951; for the next year-and-a-half stretch of the war, aerial warfare continued. On the ground the battle lines had stabilized by early 1951 and a static front developed, which changed little till the armistice was signed in 1953.
UN forces held air superiority in the Korean theater from the outset, but this was challenged by the arrival of the Soviet MiGs. It was regained in 1951 and was maintained for the duration of the conflict. This was decisive for the UN: first, for attacking into the peninsular north, and second, for resisting the Chinese intervention. North Korea and China also had jet-powered air forces. Their limited training and experience made it strategically untenable to lose them against the better-trained UN air forces. Thus, the United States and the Soviet Union fed matériel to the war, battling by proxy and finding themselves virtually matched, technologically, when the USAF deployed the F-86F against the MiG-15 late in 1952.
Unlike the Vietnam War, in which the Soviet Union only officially sent "advisers," in the Korean aerial war Soviet forces participated via the 64th Airborne Corps. 1,106 enemy airplanes were officially downed by the Soviet pilots, 52 of whom got ace status. The Soviet system of confirming air kills erred on the conservative side that is the pilot's words had to be corroborated and enemy aircraft falling into the sea were not counted, the number might exceed 1,106.<ref name="airwar.ru/history"></ref>
After the war, and to the present day, the USAF reports an F-86 Sabre kill ratio in excess of 10:1, with 792 MiG-15s and 108 other aircraft shot down by Sabres, and 78 Sabres lost to enemy fire. The Soviet Air Force reported some 1,100 air-to-air victories and 335 MiG combat losses, while China's People's Liberation Army Air Force (PLAAF) reported 231 combat losses, mostly MiG-15s, and 168 other aircraft lost. The KPAF reported no data, but the UN Command estimates some 200 KPAF aircraft lost in the war's first stage, and 70 additional aircraft after the Chinese intervention. The USAF disputes Soviet and Chinese claims of 650 and 211 downed F-86s, respectively. However, one unconfirmed source claims that the U.S. Air Force has more recently cited 230 losses out of 674 F-86s deployed to Korea.
The Korean War was the first war in which jet aircraft played the central role in air combat. Once-formidable fighters such as the P-51 Mustang, F4U Corsair, and Hawker Sea Fury—all piston-engined, propeller-driven, and designed during World War II—relinquished their air-superiority roles to a new generation of faster, jet-powered fighters arriving in the theater.
The Korean War marked a major milestone not only for fixed-wing aircraft, but also for rotorcraft, featuring the first large-scale deployment of helicopters for medical evacuation (medevac). In 1944–1945, during the Second World War, the YR-4 helicopter saw limited ambulance duty, but in Korea, where rough terrain trumped the jeep as a speedy medevac vehicle, helicopters like the Sikorsky H-19 helped reduce fatal casualties to a dramatic degree when combined with complementary medical innovations such as Mobile Army Surgical Hospitals. The limitations of jet aircraft for close air support highlighted the helicopter's potential in the role, leading to development of the AH-1 Cobra and other helicopter gunships used in the Vietnam War (1965–75).
Bombing North Korea.
The first major U.S. strategic bombing campaign against North Korea, begun in late July 1950, was conceived much along the lines of the major offensives of World War II. On 12 August 1950, the U.S. Air Force dropped 625 tons of bombs on North Korea; two weeks later, the daily tonnage increased to some 800 tons. After the Chinese intervention in November, General MacArthur ordered the increased bombing campaign on North Korea, including incendiary attacks against their arsenals and communications centers and especially against the "Korean end" of all the bridges across the Yalu River. As with the aerial bombing campaigns over Germany and Japan in World War II, the nominal objectives of the U.S. Air Force was to destroy North Korea's war infrastructure and shatter their morale. After MacArthur was removed as Supreme Commander in Korea in April 1951, his successors continued this policy and eventually extended it to all of North Korea. U.S. warplanes dropped more napalm and bombs on North Korea than they did during the whole Pacific campaign of World War II.
As a result, almost every substantial building in North Korea was destroyed. The war's highest-ranking American POW, U.S. Major General William F. Dean, reported that most of the North Korean cities and villages he saw were either rubble or snow-covered wastelands. U.S. Air Force General Curtis LeMay commented, "we went over there and fought the war and eventually burned down every town in North Korea anyway, some way or another, and some in South Korea, too." The devastation of Pyongyang was so complete that bombing was halted as there were no longer any worthy targets.
As well as conventional bombing, the Communist side claimed that the U.S. had used biological weapons.
U.S. threat of atomic warfare.
On 5 November 1950, the Joint Chiefs of Staff (JCS) issued orders for the retaliatory atomic bombing of Manchurian PRC military bases, if either their armies crossed into Korea or if PRC or KPA bombers attacked Korea from there. The President ordered the transfer of nine Mark 4 nuclear bombs "to the Air Force's Ninth Bomb Group, the designated carrier of the weapons ... [and] signed an order to use them against Chinese and Korean targets", which he never transmitted.
Many American officials viewed the deployment of nuclear-capable (but not nuclear-armed) B-29 bombers to Britain as helping to resolve the Berlin Blockade of 1948–1949. Truman and Eisenhower both had military experience and viewed nuclear weapons as potentially usable components of their military. During Truman's first meeting to discuss the war on 25 June 1950, he ordered plans be prepared for attacking Soviet forces if they entered the war. By July, Truman approved another B-29 deployment to Britain, this time with bombs (but without their cores), to remind the Soviets of American offensive ability. Deployment of a similar fleet to Guam was leaked to "The New York Times". As United Nations forces retreated to Pusan, and the CIA reported that mainland China was building up forces for a possible invasion of Taiwan, the Pentagon believed that Congress and the public would demand using nuclear weapons if the situation in Korea required them.
As Chinese forces pushed back the United States forces from the Yalu River, Truman stated during a 30 November 1950 press conference that using nuclear weapons had "always been [under] active consideration", with control under the local military commander. The Indian ambassador, K. Madhava Panikkar, reports "that Truman announced that he was thinking of using the atom bomb in Korea. But the Chinese seemed totally unmoved by this threat ... The propaganda against American aggression was stepped up. The 'Aid Korea to resist America' campaign was made the slogan for increased production, greater national integration, and more rigid control over anti-national activities. One could not help feeling that Truman's threat came in very useful to the leaders of the Revolution, to enable them to keep up the tempo of their activities."
After his statement caused concern in Europe, Truman met on 4 December 1950 with UK prime minister and Commonwealth spokesman Clement Attlee, French Premier René Pleven, and Foreign Minister Robert Schuman to discuss their worries about atomic warfare and its likely continental expansion. The United States' forgoing atomic warfare was not because of "a disinclination by the Soviet Union and People's Republic of China to escalate" the Korean War, but because UN allies—notably from the UK, the Commonwealth, and France—were concerned about a geopolitical imbalance rendering NATO defenseless while the United States fought China, who then might persuade the Soviet Union to conquer Western Europe. The Joint Chiefs of Staff advised Truman to tell Attlee that the United States would use nuclear weapons only if necessary to protect an evacuation of UN troops, or to prevent a "major military disaster".
On 6 December 1950, after the Chinese intervention repelled the UN Command armies from northern North Korea, General J. Lawton Collins (Army Chief of Staff), General MacArthur, Admiral C. Turner Joy, General George E. Stratemeyer, and staff officers Major General Doyle Hickey, Major General Charles A. Willoughby, and Major General Edwin K. Wright met in Tokyo to plan strategy countering the Chinese intervention; they considered three potential atomic warfare scenarios encompassing the next weeks and months of warfare.
Both the Pentagon and the State Department were nonetheless cautious about using nuclear weapons due to the risk of general war with China and the diplomatic ramifications. Truman and his senior advisors agreed, and never seriously considered using them in early December 1950 despite the poor military situation in Korea.
In 1951, the U.S. escalated closest to atomic warfare in Korea. Because China had deployed new armies to the Sino-Korean frontier, pit crews at the Kadena Air Base, Okinawa, assembled atomic bombs for Korean warfare, "lacking only the essential pit nuclear cores". In October 1951, the United States effected Operation Hudson Harbor to establish nuclear weapons capability. USAF B-29 bombers practised individual bombing runs from Okinawa to North Korea (using dummy nuclear or conventional bombs), coordinated from Yokota Air Base in east-central Japan. "Hudson Harbor" tested "actual functioning of all activities which would be involved in an atomic strike, including weapons assembly and testing, leading, ground control of bomb aiming". The bombing run data indicated that atomic bombs would be tactically ineffective against massed infantry, because the "timely identification of large masses of enemy troops was extremely rare."
Ridgway was authorized to use nuclear weapons if a major air attack originated from outside Korea. An envoy was sent to Hong Kong to deliver a warning to China. The message likely caused Chinese leaders to be more cautious about potential American use of nuclear weapons, but whether they learned about the B-29 deployment is unclear and the failure of the two major Chinese offensives that month likely was what caused them to shift to a defensive strategy in Korea. The B-29s returned to the United States in June.
When Eisenhower succeeded Truman in early 1953 he was similarly cautious about using nuclear weapons in Korea, including for diplomatic purposes to encourage progress in the ongoing truce discussions. The administration prepared contingency plans for using them against China, but like Truman, the new president feared that doing so would result in Soviet attacks on Japan. The war ended as it had begun, without American nuclear weapons deployed near battle.
War crimes.
Civilian deaths and massacres.
There were numerous atrocities and massacres of civilians throughout the Korean war committed by both the North and South Koreans. Many of them started on the first days of the war. South Korean President Syngman Rhee ordered the Bodo League massacre on 28 June, beginning numerous killings of more than 100,000 suspected leftist sympathizers and their families by South Korean officials and right-wing groups. During the massacre, the British protested to their allies and saved some citizens.
In occupied areas, North Korean Army political officers purged South Korean society of its intelligentsia by executing every educated person—academic, governmental, religious—who might lead resistance against the North; the purges continued during the NPA retreat.
R. J. Rummel estimated that the North Korean Army executed at least 500,000 civilians during the Korean War with many dying in North Korea's drive to conscript South Koreans to their war effort. When the North Koreans retreated north in September 1950, they abducted tens of thousands of South Korean men. The reasons are not clear but many of the victims had skills, or had been arrested as right-wing activists.
In addition to conventional military operations, North Korean soldiers fought the UN forces by infiltrating guerrillas among refugees. These soldiers disguised as refugees would approach UN forces asking for food and help, then open fire and attack. U.S. troops acted under a "shoot-first-ask-questions-later" policy against any civilian refugee approaching U.S. battlefield positions, a policy that led U.S. soldiers to kill an estimated 400 civilians at No Gun Ri (26–29 July 1950) in central Korea because they believed some of the refugees killed to be North Korean soldiers in disguise. The South Korean Truth and Reconciliation Commission defended this policy as a "military necessity".
Beginning in 2005, the South Korean Truth and Reconciliation Commission has investigated numerous atrocities committed by the Japanese colonial government and the authoritarian South Korean governments that followed it. It has investigated atrocities before, during and after the Korean War.
The Commission has verified over 14,000 civilians were killed in the Jeju Uprising (1948–49) that involved South Korean military and paramilitary units against pro-North Korean guerrillas. Although most of the fighting had subsided by 1949, fighting continued until 1950. The Commission estimates 86% of the civilians were killed by South Korean forces. The Americans on the island documented the events, but never intervened.
Prisoners of war.
During the first days of the war North Korean soldiers committed the Seoul National University Hospital Massacre.
The United States reported that North Korea mistreated prisoners of war: soldiers were beaten, starved, put to forced labor, marched to death, and summarily executed. 
The KPA killed POWs at the battles for Hill 312, Hill 303, the Pusan Perimeter, and Daejeon—discovered during early after-battle mop-up actions by the UN forces. Later, a U.S. Congress war crimes investigation, the United States Senate Subcommittee on Korean War Atrocities of the Permanent Subcommittee of the Investigations of the Committee on Government Operations, reported that "two-thirds of all American prisoners of war in Korea died as a result of war crimes".
Although the Chinese rarely executed prisoners like their North Korean counterparts, mass starvation and diseases swept through the Chinese-run POW camps during the winter of 1950–51. About 43 percent of all U.S. POWs died during this period. The Chinese defended their actions by stating that all Chinese soldiers during this period were suffering mass starvation and diseases due to logistical difficulties. The UN POWs pointed out that most of the Chinese camps were located near the easily supplied Sino-Korean border, and that the Chinese withheld food to force the prisoners to accept the communism indoctrination programs. According to the reports of China, over a thousand U.S. POWs died by the end of June 1951, while only a dozen British POWs died, and all Turkish POW survived. The reason was, according to Hastings, that while the British POWs could help each other, the Americans thought sorghum, corn, and pickle, which were also the main food for Chinese soldiers, were livestock feed, and many refused to eat, partially due to their depression, called as "give-upitise" by British POWs. U.S. POWs also threw sick comrades out of their room to freezing outside. Turkish POWs felt most comfortable, as some of them even thought the food was better than what they ate at home.
Chinese claimed that UN soldiers helped anti-Communism POWs to torture Chinese POWs, such as to put anti-Communism tattoos on their body by force, so that they would have to refuse to be repatriated back to the north. They even killed Communist POWs in public, to frighten the others.
The unpreparedness of U.S. POWs to resist heavy communist indoctrination during the Korean War led to the Code of the United States Fighting Force which governs how U.S. military personnel in combat should act when they must "evade capture, resist while a prisoner or escape from the enemy".
North Korea may have detained up to 50,000 South Korean POWs after the ceasefire.:141 Over 88,000 South Korean soldiers were missing and the Communists' themselves had claimed that they had captured 70,000 South Koreans.:142 However, when ceasefire negotiations began in 1951, the Communists reported that they held only 8,000 South Koreans. The UN Command protested the discrepancies and alleged that the Communists were forcing South Korean POWs to join the KPA.
The Communist side denied such allegations. They claimed that their POW rosters were small because many POWs were killed in UN air raids and that they had released ROK soldiers at the front. They insisted that only volunteers were allowed to serve in the KPA.:143 By early 1952, UN negotiators gave up trying to get back the missing South Koreans. The POW exchange proceeded without access to South Korean POWs not on the Communist rosters.
North Korea continued to claim that any South Korean POW who stayed in the North did so voluntarily. However, since 1994, South Korean POWs have been escaping North Korea on their own after decades of captivity. As of 2010, the South Korean Ministry of Unification reported that 79 ROK POWs had escaped the North. The South Korean government estimates 500 South Korean POWs continue to be detained in North Korea.
The escaped POWs have testified about their treatment and written memoirs about their lives in North Korea. They report that they were not told about the POW exchange procedures, and were assigned to work in mines in the remote northeastern regions near the Chinese and Russian border.:31 Declassified Soviet Foreign Ministry documents corroborate such testimony.
In 1997 the Geoje POW Camp in South Korea was turned into a memorial.
Starvation.
In December 1950, National Defense Corps was founded; the soldiers were 406,000 drafted citizens.
In the winter of 1951, 50,000 to 90,000 South Korean National Defense Corps soldiers starved to death while marching southward under the Chinese offensive when their commanding officers embezzled funds earmarked for their food. This event is called the National Defense Corps Incident. There is no evidence that Syngman Rhee was personally involved in or benefited from the corruption.
Recreation.
In 1950, Secretary of Defense George C. Marshall and Secretary of the Navy Francis P. Matthews called on the USO which was disbanded by 1947 to provide support for U.S. servicemen. By the end of the war, more than 113,000 American USO volunteers were working at home front and abroad. Many stars came to Korea to give their performances. Throughout the Korean War, UN Comfort Stations were operated by South Korean officials for UN soldiers.
Aftermath.
Postwar recovery was different in the two Koreas. South Korea stagnated in the first postwar decade. In 1953, South Korea and the United States concluded a Mutual Defense Treaty. In 1960, the April Revolution occurred and students joined an anti-Syngman Rhee demonstration; 142 were killed by police; in consequence Syngman Rhee resigned and left for exile in the United States. Park Chung-hee's May 16 coup enabled social stability. In the 1960s, prostitution and related services earned 25 percent of South Korean GNP. From 1965 to 1973, South Korea dispatched troops to Vietnam and got $235,560,000 allowance and military procurement from the United States. GNP increased fivefold during the Vietnam War. South Korea industrialized and modernized. Contemporary North Korea remains underdeveloped. South Korea had one of the world's fastest-growing economies from the early 1960s to the late 1990s. In 1957 South Korea had a lower per capita GDP than Ghana, and by 2010 it was ranked thirteenth in the world (Ghana was 86th).
Postwar, about 100,000 North Koreans were executed in purges. According to Rummel, forced labor and concentration camps were responsible for over one million deaths in North Korea from 1945 to 1987; others have estimated 400,000 deaths in concentration camps alone. Estimates based on the most recent North Korean census suggest that 240,000 to 420,000 people died as a result of the 1990s North Korean famine and that there were 600,000 to 850,000 unnatural deaths in North Korea from 1993 to 2008. The North Korean government has been accused of "crimes against humanity" for its alleged culpability in creating and prolonging the 1990s famine. A study by South Korean anthropologists of North Korean children who had defected to China found that 18-year-old males were 5 inches shorter than South Koreans their age due to malnutrition.
Racial integration efforts in the U.S. military began during the Korean War, where African Americans fought in integrated units for the first time. Among the 1.8 million American soldiers who fought in the Korean War there were more than 100,000 African Americans.
South Korean anti-Americanism after the war was fueled by the presence and behavior of American military personnel (USFK) and U.S. support for the authoritarian regime, a fact still evident during the country's democratic transition in the 1980s. However, anti-Americanism has declined significantly in South Korea in recent years, from 46% favorable in 2003 to 74% favorable in 2011, making South Korea one of the most pro-American countries in the world.
In addition, a large number of mixed-race "G.I. babies" (offspring of American and other UN soldiers and Korean women) were filling up the country's orphanages. Korean traditional society places significant weight on paternal family ties, bloodlines, and purity of race. Children of mixed race or those without fathers are not easily accepted in South Korean society. International adoption of Korean children began in 1954. The U.S. Immigration Act of 1952 legalized the naturalization of non-whites as American citizens, and made possible the entry of military spouses and children from South Korea after the Korean War. With the passage of the Immigration Act of 1965, which substantially changed U.S. immigration policy toward non-Europeans, Koreans became one of the fastest-growing Asian groups in the United States.
Mao Zedong's decision to take on the United States in the Korean War was a direct attempt to confront what the Communist bloc viewed as the most powerful anti-Communist power in the world, undertaken at a time when the Chinese Communist regime was still consolidating its own power after winning the Chinese Civil War. Mao supported intervention not to save North Korea, but because he believed that a military conflict with the United States was inevitable after the United States entered the Korean War, and also to appease the Soviet Union in order to secure military dispensation and achieve Mao's goal of making China a major world military power. Mao was equally ambitious in improving his own prestige inside the communist international community by demonstrating that his Marxist concerns were international. In his later years Mao believed that Stalin only gained a positive opinion of him after China's entrance into the Korean War. Inside Mainland China, the war improved the long-term prestige of Mao, Zhou, and Peng, allowing the Chinese Communist Party to increase its legitimacy while weakening anti-Communist dissent.
The Chinese government have encouraged the point of view that the war was initiated by the United States and South Korea, though ComIntern documents have shown that Mao sought approval from Joseph Stalin to enter the war. In Chinese media, the Chinese war effort is considered as an example of China's engaging the strongest power in the world with an under-equipped army, forcing it to retreat, and fighting it to a military stalemate. These successes were contrasted with China's historical humiliations by Japan and by Western powers over the previous hundred years, highlighting the abilities of the People's Liberation Army and the Chinese Communist Party. The most significant negative long-term consequence of the war (for China) was that it led the United States to guarantee the safety of Chiang Kai-shek's regime in Taiwan, effectively ensuring that Taiwan would remain outside of PRC control until the present day. Mao had also discovered the usefulness of large-scale mass movements in the war while implementing them among most of his ruling measures over PRC. Finally, anti-American sentiments, which were already a significant factor during the Chinese Civil War, was ingrained into Chinese culture during the Communist propaganda campaigns of the Korean War.
The Korean War affected other participant combatants. Turkey, for example, entered NATO in 1952 and the foundation for bilateral diplomatic and trade relations was laid.
References.
</dl>

</doc>
<doc id="16869" url="http://en.wikipedia.org/wiki?curid=16869" title="Knights Templar">
Knights Templar

The Poor Fellow-Soldiers of Christ and of the Temple of Solomon (Latin: "Pauperes commilitones Christi Templique Salomonici"), commonly known as the Knights Templar, the Order of Solomon's Temple (French: "Ordre du Temple" or "Templiers") or simply as Templars, were among the most wealthy and powerful of the Western Christian military orders and were among the most prominent actors of the Christian finance. The organization existed for nearly two centuries during the Middle Ages.
Officially endorsed by the Roman Catholic Church around 1129, the Order became a favoured charity throughout Christendom and grew rapidly in membership and power. Templar knights, in their distinctive white mantles with a red cross, were among the most skilled fighting units of the Crusades. Non-combatant members of the Order managed a large economic infrastructure throughout Christendom, innovating financial techniques that were an early form of banking, and building fortifications across Europe and the Holy Land.
The Templars' existence was tied closely to the Crusades; when the Holy Land was lost, support for the Order faded. Rumours about the Templars' secret initiation ceremony created mistrust and King Philip IV of France, deeply in debt to the Order, took advantage of the situation. In 1307, many of the Order's members in France were arrested, tortured into giving false confessions, and then burned at the stake. Under pressure from King Philip, Pope Clement V disbanded the Order in 1312. The abrupt disappearance of a major part of the European infrastructure gave rise to speculation and legends, which have kept the "Templar" name alive into the modern day.
History.
Rise.
After the First Crusade recaptured Jerusalem in 1099, many Christians made pilgrimages to various Holy Places in the Holy Land. However, though the city of Jerusalem was under relatively secure control, the rest of Outremer was not. Bandits and marauding highwaymen preyed upon pilgrims who were routinely slaughtered, sometimes by the hundreds, as they attempted to make the journey from the coastline at Jaffa into the interior of the Holy Land.
In 1119, the French knight Hugues de Payens approached King Baldwin II of Jerusalem and Warmund, Patriarch of Jerusalem, and proposed creating a monastic order for the protection of these pilgrims. King Baldwin and Patriarch Warmund agreed to the request, probably at the Council of Nablus in January 1120, and the king granted the Templars a headquarters in a wing of the royal palace on the Temple Mount in the captured Al-Aqsa Mosque. The Temple Mount had a mystique because it was above what was believed to be the ruins of the Temple of Solomon. The Crusaders therefore referred to the Al Aqsa Mosque as Solomon's Temple, and from this location the new Order took the name of "Poor Knights of Christ and the Temple of Solomon", or "Templar" knights. The Order, with about nine knights including Godfrey de Saint-Omer and André de Montbard, had few financial resources and relied on donations to survive. Their emblem was of two knights riding on a single horse, emphasising the Order's poverty.
The impoverished status of the Templars did not last long. They had a powerful advocate in Saint Bernard of Clairvaux, a leading Church figure, French abbot primarily responsible for the founding of the Cistercian Order of monks and a nephew of André de Montbard, one of the founding knights. Bernard put his weight behind them and wrote persuasively on their behalf in the letter 'In Praise of the New Knighthood', and in 1129, at the Council of Troyes, he led a group of leading churchmen to officially approve and endorse the Order on behalf of the Church. With this formal blessing, the Templars became a favoured charity throughout Christendom, receiving money, land, businesses, and noble-born sons from families who were eager to help with the fight in the Holy Land. Another major benefit came in 1139, when Pope Innocent II's papal bull "Omne Datum Optimum" exempted the Order from obedience to local laws. This ruling meant that the Templars could pass freely through all borders, were not required to pay any taxes, and were exempt from all authority except that of the pope.
With its clear mission and ample resources, the Order grew rapidly. Templars were often the advance shock troops in key battles of the Crusades, as the heavily armoured knights on their warhorses would set out to charge at the enemy, ahead of the main army bodies, in an attempt to break opposition lines. One of their most famous victories was in 1177 during the Battle of Montgisard, where some 500 Templar knights helped several thousand infantry to defeat Saladin's army of more than 26,000 soldiers.
A Templar Knight is truly a fearless knight, and secure on every side, for his soul is protected by the armour of faith, just as his body is protected by the armour of steel. He is thus doubly armed, and need fear neither demons nor men."
Bernard de Clairvaux, c. 1135,"De Laude Novae Militae—In Praise of the New Knighthood"
Although the primary mission of the Order was military, relatively few members were combatants. The others acted in support positions to assist the knights and to manage the financial infrastructure. The Templar Order, though its members were sworn to individual poverty, was given control of wealth beyond direct donations. A nobleman who was interested in participating in the Crusades might place all his assets under Templar management while he was away. Accumulating wealth in this manner throughout Christendom and the Outremer, the Order in 1150 began generating letters of credit for pilgrims journeying to the Holy Land: pilgrims deposited their valuables with a local Templar preceptory before embarking, received a document indicating the value of their deposit, then used that document upon arrival in the Holy Land to retrieve their funds in an amount of treasure of equal value. This innovative arrangement was an early form of banking and may have been the first formal system to support the use of cheques; it improved the safety of pilgrims by making them less attractive targets for thieves, and also contributed to the Templar coffers.
Based on this mix of donations and business dealing, the Templars established financial networks across the whole of Christendom. They acquired large tracts of land, both in Europe and the Middle East; they bought and managed farms and vineyards; they built churches and castles; they were involved in manufacturing, import and export; they had their own fleet of ships; and at one point they even owned the entire island of Cyprus. The Order of the Knights Templar arguably qualifies as the world's first multinational corporation.
Decline.
In the mid-12th century, the tide began to turn in the Crusades. The Muslim world had become more united under effective leaders such as Saladin, and dissension arose amongst Christian factions in, and concerning, the Holy Land. The Knights Templar were occasionally at odds with the two other Christian military orders, the Knights Hospitaller and the Teutonic Knights, and decades of internecine feuds weakened Christian positions, both politically and militarily. After the Templars were involved in several unsuccessful campaigns, including the pivotal Battle of the Horns of Hattin, Jerusalem was recaptured by Muslim forces under Saladin in 1187. The Crusaders regained the city in 1229, without Templar aid, but held it only briefly. In 1244, the Khwarezmi Turks recaptured Jerusalem, and the city did not return to Western control until 1917 when the British captured it from the Ottoman Turks in World War I.
The Templars were forced to relocate their headquarters to other cities in the north, such as the seaport of Acre, which they held for the next century. It was lost in 1291, followed by their last mainland strongholds, Tortosa (Tartus in what is now Syria) and Atlit in present-day Israel. Their headquarters then moved to Limassol on the island of Cyprus, and they also attempted to maintain a garrison on tiny Arwad Island, just off the coast from Tortosa. In 1300, there was some attempt to engage in coordinated military efforts with the Mongols via a new invasion force at Arwad. In 1302 or 1303, however, the Templars lost the island to the Egyptian Mamluks in the Siege of Arwad. With the island gone, the Crusaders lost their last foothold in the Holy Land.
With the Order's military mission now less important, support for the organisation began to dwindle. The situation was complex, however, since during the two hundred years of their existence, the Templars had become a part of daily life throughout Christendom. The organisation's Templar Houses, hundreds of which were dotted throughout Europe and the Near East, gave them a widespread presence at the local level. The Templars still managed many businesses, and many Europeans had daily contact with the Templar network, such as by working at a Templar farm or vineyard, or using the Order as a bank in which to store personal valuables. The Order was still not subject to local government, making it everywhere a "state within a state"—its standing army, though it no longer had a well-defined mission, could pass freely through all borders. This situation heightened tensions with some European nobility, especially as the Templars were indicating an interest in founding their own monastic state, just as the Teutonic Knights had done in Prussia and the Knights Hospitaller were doing in Rhodes.
Arrests, charges and dissolution.
In 1305, the new Pope Clement V, based in Avignon, France, sent letters to both the Templar Grand Master Jacques de Molay and the Hospitaller Grand Master Fulk de Villaret to discuss the possibility of merging the two Orders. Neither was amenable to the idea, but Pope Clement persisted, and in 1306 he invited both Grand Masters to France to discuss the matter. De Molay arrived first in early 1307, but de Villaret was delayed for several months. While waiting, De Molay and Clement discussed criminal charges that had been made two years earlier by an ousted Templar and were being discussed by King Philip IV of France and his ministers. It was generally agreed that the charges were false, but Clement sent the king a written request for assistance in the investigation. According to some historians, King Philip, who was already deeply in debt to the Templars from his war with the English, decided to seize upon the rumors for his own purposes. He began pressuring the Church to take action against the Order, as a way of freeing himself from his debts. The French king's motivations went beyond merely financial though. By charging the Templars with heresy, the monarchy was also claiming for itself a charism proper to the papacy. The Templar case was another step in a process of appropriating these foundations, which had begun with the Franco-papal rift at the time of Boniface VIII. 
At dawn on Friday, 13 October 1307 (a date sometimes spuriously linked with the origin of the Friday the 13th superstition) King Philip IV ordered de Molay and scores of other French Templars to be simultaneously arrested. The arrest warrant started with the phrase: "Dieu n'est pas content, nous avons des ennemis de la foi dans le Royaume" ["God is not pleased. We have enemies of the faith in the kingdom"]. Claims were made that during Templar admissions ceremonies, recruits were forced to spit on the cross, deny Christ, and engage in indecent kissing; brethren were also accused of worshiping idols, and the order was said to have encouraged homosexual practices. The Templars were charged with numerous other offences such as financial corruption, fraud, and secrecy. Many of the accused confessed to these charges under torture, and these confessions, even though obtained under duress, caused a scandal in Paris. The prisoners were coerced to confess that they had spat on the Cross: "Moi, Raymond de La Fère, 21 ans, reconnais que [j'ai] craché trois fois sur la Croix, mais de bouche et pas de cœur" (free translation: "I, Raymond de La Fère, 21 years old, admit that I have spat three times on the Cross, but only from my mouth and not from my heart"). The Templars were accused of idolatry and were suspected of worshipping either a figure known as Baphomet or a mummified severed head they recovered, amongst other artifacts, at their original headquarters on the Temple Mount that many scholars theorize might have been that of John the Baptist, among other things.
Relenting to Phillip's demands, Pope Clement then issued the papal bull "Pastoralis Praeeminentiae" on 22 November 1307, which instructed all Christian monarchs in Europe to arrest all Templars and seize their assets. Pope Clement called for papal hearings to determine the Templars' guilt or innocence, and once freed of the Inquisitors' torture, many Templars recanted their confessions. Some had sufficient legal experience to defend themselves in the trials, but in 1310, having appointed the archbishop of Sens, Philippe de Marigny, to lead the investigation, Philip blocked this attempt, using the previously forced confessions to have dozens of Templars burned at the stake in Paris.
With Philip threatening military action unless the pope complied with his wishes, Pope Clement finally agreed to disband the Order, citing the public scandal that had been generated by the confessions. At the Council of Vienne in 1312, he issued a series of papal bulls, including "Vox in excelso", which officially dissolved the Order, and "Ad providam", which turned over most Templar assets to the Hospitallers.
As for the leaders of the Order, the elderly Grand Master Jacques de Molay, who had confessed under torture, retracted his confession. Geoffroi de Charney, Preceptor of Normandy, also retracted his confession and insisted on his innocence. Both men were declared guilty of being relapsed heretics, and they were sentenced to burn alive at the stake in Paris on 18 March 1314. De Molay reportedly remained defiant to the end, asking to be tied in such a way that he could face the Notre Dame Cathedral and hold his hands together in prayer. According to legend, he called out from the flames that both Pope Clement and King Philip would soon meet him before God. His actual words were recorded on the parchment as follows : "Dieu sait qui a tort et a péché. Il va bientot arriver malheur à ceux qui nous ont condamnés à mort" (free translation : "God knows who is wrong and has sinned. Soon a calamity will occur to those who have condemned us to death"). Pope Clement died only a month later, and King Philip died in a hunting accident before the end of the year.
With the last of the Order's leaders gone, the remaining Templars around Europe were either arrested and tried under the Papal investigation (with virtually none convicted), absorbed into other military orders such as the Knights Hospitaller, or pensioned off and allowed to live out their days peacefully. By papal decree, the property of the Templars was transferred to the Order of Hospitallers, which also absorbed many of the Templars' members. In effect, the dissolution of the Templars could be seen as the merger of the two rival orders. Some may have fled to other territories outside Papal control, such as excommunicated Scotland or to Switzerland. Templar organisations in Portugal simply changed their name, from Knights Templar to "Knights of Christ".
Chinon Parchment.
In September 2001, a document known as the "Chinon Parchment" dated 17–20 August 1308 was discovered in the Vatican Secret Archives by Barbara Frale, apparently after having been filed in the wrong place in 1628. It is a record of the trial of the Templars and shows that Clement absolved the Templars of all heresies in 1308 before formally disbanding the Order in 1312, as did another Chinon Parchment dated 20 August 1308 addressed to Philip IV of France, also mentioning that all Templars that had confessed to heresy were "restored to the Sacraments and to the unity of the Church". This other Chinon Parchment has been well-known to historians, having been published by Étienne Baluze in 1693 and by Pierre Dupuy in 1751.
The current position of the Roman Catholic Church is that the medieval persecution of the Knights Templar was unjust, that nothing was inherently wrong with the Order or its Rule, and that Pope Clement was pressed into his actions by the magnitude of the public scandal and by the dominating influence of King Philip IV, who was Clement's relative.
Organization.
The Templars were organised as a monastic order similar to Bernard's Cistercian Order, which was considered the first effective international organisation in Europe. The organisational structure had a strong chain of authority. Each country with a major Templar presence (France, England, Aragon, Portugal, Poitou, Apulia, Jerusalem, Tripoli, Antioch, Anjou, Hungary, and Croatia) had a Master of the Order for the Templars in that region.
All of them were subject to the Grand Master, appointed for life, who oversaw both the Order's military efforts in the East and their financial holdings in the West. The Grand Master exercised his authority via the visitors-general of the order, who were knights specially appointed by the Grand Master and convent of Jerusalem to visit the different provinces, correct malpractices, introduce new regulations, and resolve important disputes. The visitors-general had the power to remove knights from office and to suspend the Master of the province concerned.
No precise numbers exist, but it is estimated that at the Order's peak there were between 15,000 and 20,000 Templars, of whom about a tenth were actual knights.
Ranks within the order.
Three main ranks.
There was a threefold division of the ranks of the Templars: the noble knights, the non-noble sergeants, and the chaplains. The Templars did not perform knighting ceremonies, so any knight wishing to become a Knight Templar had to be a knight already. They were the most visible branch of the order, and wore the famous white mantles to symbolise their purity and chastity. They were equipped as heavy cavalry, with three or four horses and one or two squires. Squires were generally not members of the Order but were instead outsiders who were hired for a set period of time. Beneath the knights in the Order and drawn from non-noble families were the sergeants. They brought vital skills and trades such as blacksmithing and building, and administered many of the Order's European properties. In the Crusader States, they fought alongside the knights as light cavalry with a single horse. Several of the Order's most senior positions were reserved for sergeants, including the post of Commander of the Vault of Acre, who was the "de facto" Admiral of the Templar fleet. The sergeants wore black or brown. From 1139, chaplains constituted a third Templar class. They were ordained priests who cared for the Templar's' spiritual needs. All three classes of brother wore the Order's red cross patty.
Grand Masters.
Starting with founder Hugues de Payens in 1118–1119, the Order's highest office was that of Grand Master, a position which was held for life, though considering the martial nature of the Order, this could mean a very short tenure. All but two of the Grand Masters died in office, and several died during military campaigns. For example, during the Siege of Ascalon in 1153, Grand Master Bernard de Tremelay led a group of 40 Templars through a breach in the city walls. When the rest of the Crusader army did not follow, the Templars, including their Grand Master, were surrounded and beheaded. Grand Master Gérard de Ridefort was beheaded by Saladin in 1189 at the Siege of Acre.
The Grand Master oversaw all of the operations of the Order, including both the military operations in the Holy Land and Eastern Europe and the Templars' financial and business dealings in Western Europe. Some Grand Masters also served as battlefield commanders, though this was not always wise: several blunders in de Ridefort's combat leadership contributed to the devastating defeat at the Battle of Hattin. The last Grand Master was Jacques de Molay, burned at the stake in Paris in 1314 by order of King Philip IV.
Behaviour and clothing.
Bernard de Clairvaux and founder Hugues de Payens devised the specific code of behaviour for the Templar Order, known to modern historians as the Latin Rule. Its 72 clauses defined the ideal behaviour for the Knights, such as the types of garments they were to wear and how many horses they could have. Knights were to take their meals in silence, eat meat no more than three times per week, and not have physical contact of any kind with women, even members of their own family. A Master of the Order was assigned "4 horses, and one chaplain-brother and one clerk with three horses, and one sergeant brother with two horses, and one gentleman valet to carry his shield and lance, with one horse." As the Order grew, more guidelines were added, and the original list of 72 clauses was expanded to several hundred in its final form.
The knights wore a white surcoat with a red cross and a white mantle also with a red cross; the sergeants wore a black tunic with a red cross on the front and a black or brown mantle. The white mantle was assigned to the Templars at the Council of Troyes in 1129, and the cross was most probably added to their robes at the launch of the Second Crusade in 1147, when Pope Eugenius III, King Louis VII of France, and many other notables attended a meeting of the French Templars at their headquarters near Paris. According to their Rule, the knights were to wear the white mantle at all times, even being forbidden to eat or drink unless they were wearing it.
The red cross that the Templars wore on their robes was a symbol of martyrdom, and to die in combat was considered a great honour that assured a place in heaven. There was a cardinal rule that the warriors of the Order should never surrender unless the Templar flag had fallen, and even then they were first to try to regroup with another of the Christian orders, such as that of the Hospitallers. Only after all flags had fallen were they allowed to leave the battlefield. This uncompromising principle, along with their reputation for courage, excellent training, and heavy armament, made the Templars one of the most feared combat forces in medieval times.
Although not prescribed by the Templar Rule, it later became customary for members of the order to wear long and prominent beards. In about 1240, Alberic of Trois-Fontaines described the Templars as an "order of bearded brethren"; while during the interrogations by the papal commissioners in Paris in 1310–11, out of nearly 230 knights and brothers questioned, 76 are described as wearing a beard, in some cases specified as being "in the style of the Templars", and 133 are said to have shaved off their beards, either in renunciation of the order or because they had hoped to escape detection.
Initiation, known as Reception ("receptio") into the Order, was a profound commitment and involved a solemn ceremony. Outsiders were discouraged from attending the ceremony, which aroused the suspicions of medieval inquisitors during the later trials. New members had to willingly sign over all of their wealth and goods to the Order and take vows of poverty, chastity, piety, and obedience. Most brothers joined for life, although some were allowed to join for a set period. Sometimes a married man was allowed to join if he had his wife's permission, but he was not allowed to wear the white mantle.
Legacy.
With their military mission and extensive financial resources, the Knights Templar funded a large number of building projects around Europe and the Holy Land. Many of these structures are still standing. Many sites also maintain the name "Temple" because of centuries-old association with the Templars. For example, some of the Templars' lands in London were later rented to lawyers, which led to the names of the Temple Bar gateway and the Temple Underground station. Two of the four Inns of Court which may call members to act as barristers are the Inner Temple and Middle Temple.
Distinctive architectural elements of Templar buildings include the use of the image of "two knights on a single horse", representing the Knights' poverty, and round buildings designed to resemble the Church of the Holy Sepulchre in Jerusalem.
Modern organizations.
The story of the persecution and sudden dissolution of the secretive yet powerful medieval Templars has drawn many other groups to use alleged connections with the Templars as a way of enhancing their own image and mystery. There is no clear historical connection between the Knights Templar, which were dismantled in the Rolls of the Catholic Church in 1309 with the martyrdom of Jacques de Molay, and any of the modern organizations, of which, except for the Scottish Order, the earliest emerged publicly in the 18th century. There is often public confusion and many overlook the 400-year gap. However, in 1853, Napoleon III officially recognized the OSMTH. The Order operates on the basis of the traditions of the medieval Knights Templar, celebrating the spirit of, but not claiming direct descent from the ancient Order founded by Hugues de Payens in 1118 and dissolved by Pope Clement V in 1312.
Freemasonry.
Since at least the 18th century, Freemasonry has incorporated Templar symbols and rituals in a number of Masonic bodies, most notably, the "Order of the Temple" the final order joined in "The United Religious, Military and Masonic Orders of the Temple and of St John of Jerusalem, Palestine, Rhodes and Malta" commonly known as the Knights Templar. One theory of the origins of Freemasonry claims direct descent from the historical Knights Templar through its final fourteenth-century members who took refuge in Scotland whose King, Robert the Bruce was excommunicated by the Roman Catholic Church at the time, or in Portugal where the order changed its name to "Knights of Christ", other members having joined Knights of St. John. There have even been claims that some of the Templars who made it to Scotland contributed to the Scots' victory at Bannockburn. This theory is usually deprecated on grounds of lack of evidence, by both Masonic authorities and historians.
The Roman Catholic Church has historically opposed Freemasonry since it began to emerge, under the belief that the group is a "Secret Society" and has a deeply hidden agenda that opposes the church and its beliefs. Members of the Church found to be Freemasons were commonly excommunicated. This has often led to the misguided belief that the Church somehow also opposed the Knights Templar, however the Church makes distinction between the Templars, a public monastic order, and "Secret Societies".
The penalty of excommunication for joining the Masonic Lodge was explicit in the 1917 code of canon law (canon 2335), and it is implicit in the 1983 code (canon 1374).
Because the revised code of canon law is not explicit on this point, some drew the mistaken conclusion that the Church's prohibition of Freemasonry had been dropped. As a result of this confusion, shortly before the 1983 code was promulgated, the Sacred Congregation for the Doctrine of the Faith issued a statement indicating that the penalty was still in force. This statement was dated November 26, 1983 and may be found in Origins 13/27 (Nov. 15, 1983), 450.
Modern popular culture.
Based on Freemasonic speculation and popular literature since the 19th century, the Templars and associated "legends" or "mysteries" have become a common trope in modern pop culture.
Beginning in the 1960s, there have been speculative popular publications surrounding the Order's early occupation of the Temple Mount in Jerusalem and speculation about what relics the Templars may have found there, such as the Holy Grail or the Ark of the Covenant, or the historical accusation of idol worship (Baphomet) transformed into a context of "witchcraft".
The association of the Holy Grail with the Templars has precedents even in 12th century fiction; Wolfram von Eschenbach's "Parzival" calls the knights guarding the Grail Kingdom "templeisen", apparently a conscious fictionalization of the "templarii".
Modern fictionalization of the Templars begins with "Ivanhoe", the 1820 novel by Walter Scott, where the villain Sir Brian de Bois-Guilbert is a "Templar Knight".
The popular treatment of the Templars as a topic of esotericist "legend" and "mystery" begins in the later 20th century. "The Accursed Kings" (1973 et seq) by Maurice Druon depicts the death of the last Grand Master of the Order, and plays with the legend of the curse he laid on the pope, Philip the Fair and Guillaume de Nogaret. Esotericist treatments become common in the 1980s. Among them, the 1982 "The Holy Blood and the Holy Grail" would prove most influential.
The 1988 novel by Umberto Eco "Foucault's Pendulum" satirizes the presentation of the Templars in esotericist or pseudohistorical conspiracy theories. A revival of the 1980s themes took place in the 2000s due to the commercial success of "The Da Vinci Code", the 2003 novel by Dan Brown (adapted into a film version in 2006).
References.
Notes
Bibliography
Further reading
</dl>

</doc>
<doc id="16881" url="http://en.wikipedia.org/wiki?curid=16881" title="Kashrut">
Kashrut

Kashrut (also kashruth or kashrus, כַּשְׁרוּת) is the set of Jewish religious dietary laws. Food that may be consumed according to "halakha" (Jewish law) is termed kosher in English, from the Ashkenazi pronunciation of the Hebrew term "kashér" (כָּשֵׁר), meaning "fit" (in this context, fit for consumption).
Among the numerous laws that form part of "kashrut" are the prohibitions on the consumption of unclean animals (such as pork, shellfish (both Mollusca and Crustacea) and most insects, with the exception of certain species of kosher locusts), mixtures of meat and milk, and the commandment to slaughter mammals and birds according to a process known as "shechita". There are also laws regarding agricultural produce that might impact on the suitability of food for consumption.
Most of the basic laws of "kashrut" are derived from the Torah's Books of Leviticus and Deuteronomy. Their details and practical application, however, are set down in the oral law (eventually codified in the Mishnah and Talmud) and elaborated on in the later rabbinical literature. While the Torah does not state the rationale for most "kashrut" laws, many reasons have been suggested, including philosophical, practical and hygienic.
Over the past century, there have developed numerous rabbinical organizations that certify products, manufacturers, and restaurants as kosher, usually using a symbol (called a "hechsher") to indicate their support. Presently, about a sixth of American Jews or 0.3% of the American population fully keep kosher, and many more abstain from some non-kosher foods, especially pork.
Explanations.
Philosophical explanations.
Jewish philosophy divides the 613 "mitzvot" into three groups—laws that have a rational explanation and would probably be enacted by most orderly societies ("mishpatim"), laws that are understood after being explained but would not be legislated without the Torah's command ("edot"), and laws that do not have a rational explanation ("chukim"). Some Jewish scholars say that "kashrut" should be categorized as laws for which there is no particular explanation, since the human mind is not always capable of understanding divine intentions. In this line of thinking, the dietary laws were given as a demonstration of God's authority, and man must obey without asking why. However, Maimonides believed that Jews were permitted to seek out reasons for the laws of the Torah.
Some theologians have said that the laws of "kashrut" are symbolic in character: Kosher animals represent virtues, while non-kosher animals represent vices. The 1st century BCE Letter of Aristeas argues that the laws "have been given ... to awake pious thoughts and to form the character". This view reappears in the work of the 19th century Rabbi Samson Raphael Hirsch.
The Torah prohibits "seething the kid (goat, sheep, calf) in its mother's milk". While the Bible does not provide a reason, it has been suggested that the practice was perceived as cruel and insensitive.
Hasidism believes that everyday life is imbued with channels connecting with Divinity, the "activation" of which it sees as helping the Divine Presence to be drawn into the physical world; Hasidism argues that the food laws are related to the way such channels, termed "sparks of holiness", interact with various animals. These "sparks of Holiness" are released whenever a Jew manipulates any object for a "holy reason" (which includes eating); however, not all animal products are capable of releasing their "sparks of holiness". The Hasidic argument is that animals are imbued with signs that reveal the release of these sparks, and the signs are expressed in the biblical categorization of ritually "clean" and ritually "unclean".
According to Christian theologian Gordon J. Wenham, the purpose of "kashrut" was to help Jews maintain a distinct and separate existence from other peoples; he says that the effect of the laws was to prevent socialization and intermarriage with non-Jews, preventing Jewish identity from being diluted. Wenham argued that since the impact of the food laws was a public affair, this would have enhanced Jewish attachment to them as a reminder of their distinct status as Jews.
Health explanations.
There have been attempts to provide empirical support for the view that Jewish food laws have an overarching health benefit or purpose, one of the earliest being from Maimonides in his "Guide for the Perplexed". In 1953, David Macht, an Orthodox Jew and proponent of the theory of biblical scientific foresight, conducted toxicity experiments on many kinds of animals and fish. His experiment involved lupin seedlings being supplied with extracts from the meat of various animals; Macht reported that in 100% of cases, extracts from ritually "unclean" meat inhibited the seedling's growth more than that from ritually "clean" meats. At the same time, these explanations are controversial. Scholar Lester L. Grabbe, writing in the "Oxford Bible Commentary" on Leviticus, states that "[a]n explanation now almost universally rejected is that the laws in this section [] have hygiene as their basis. Although some of the laws of ritual purity roughly correspond to modern ideas of physical cleanliness, many of them have little to do with hygiene. For example, there is no evidence that the 'unclean' animals are intrinsically bad to eat or to be avoided in a Mediterranean climate, as is sometimes asserted."
Prohibited foods.
The laws of "kashrut" can be classified according to the origin of the prohibition (Biblical or rabbinical) and whether the prohibition concerns the food itself or a mixture of foods.
Biblically prohibited foods include:
Biblically prohibited mixtures include:
Rabbinically prohibited foods include:
Permitted and forbidden animals.
Only meat from particular species is permissible. Mammals that "both" chew their cud (ruminate) and have cloven hooves can be kosher. Animals with one characteristic but not the other (the camel, the hyrax, and the hare because they have no cloven hooves, and the pig because it does not ruminate) are specifically excluded (). In 2008, a rabbinical ruling determined that giraffes and their milk are eligible to be considered kosher. The giraffe has both split hooves and chews its cud, characteristics of animals considered kosher. Findings from 2008 show that giraffe milk curdles, meeting kosher standards. Although kosher, the giraffe is not slaughtered today because the process would be very costly. Giraffes are difficult to restrain, and their use for food could cause the species to become endangered.
Non-kosher birds are listed outright () but the exact "zoological" references are disputed and some references refer to families of birds (24 are mentioned). The Mishnah refers to four signs provided by the sages. First, a "dores" (predatory bird) is not kosher. Additionally, kosher birds possess three physical characteristics: an extra toe in the back (which does not join the other toes in supporting the leg), a "zefek" (crop), and a "korkoban" (gizzard) with a peelable lumen. However, individual Jews are barred from merely applying these regulations alone; an established tradition ("masorah") is necessary to allow birds to be consumed, even if it can be substantiated that they meet all four criteria. The only exception to this is turkey. There was a time when certain authorities considered the signs enough, so Jews started eating this bird without a masorah because it possesses all the signs ("simanim" in Hebrew).
Fish must have fins and scales to be kosher (). Shellfish and other non-fish water fauna are not kosher. Here is a list of kosher species of fish. Insects are not kosher except for certain species of kosher locust. Generally any animal that eats other animals, whether they kill their food or eat carrion (), is not kosher, as well as any animal that was partially eaten by other animals ().
Separation of meat and milk.
Meat and milk (or derivatives) cannot be mixed ( ) in the sense that meat and dairy products are not served at the same meal, served or cooked in the same utensils, or stored together. Observant Jews have separate sets of dishes, and sometimes different kitchens, for meat and milk, and wait anywhere between one and six hours after eating meat before consuming milk products. The "milchig" and "fleishig" utensils and dishes are the commonly referred to Yiddish delineations between "dairy" and "meat" (lit. "milky" and "meaty") utensils and dishes respectively.
Kosher slaughter.
Mammals and fowl must be slaughtered by a trained individual (a "shochet") using a special method of slaughter, "shechita" (). Among other features, "shechita" slaughter severs the jugular vein, carotid artery, esophagus, and trachea in a single continuous cutting movement with an unserrated, sharp knife. Failure of any of these criteria renders the meat of the animal unsuitable. The body must be checked after slaughter to confirm that the animal had no medical condition or defect that would have caused it to die of its own accord within a year, which would make the meat unsuitable. These conditions ("treifot") include 70 different categories of injuries, diseases, and abnormalities whose presence renders the animal non-kosher. It is forbidden to consume certain parts of the animal, such as certain fats ("chelev") and the sciatic nerves from the legs. As much blood as possible must be removed () through the "kashering" process; this is usually done through soaking and salting the meat, but the liver, as it is rich in blood, is grilled over an open flame. Fish (and kosher locusts, for those follow the traditions permitting them) must be killed before being eaten, but no particular method has been specified in Jewish law.
Kosher utensils.
Utensils used for non-kosher foods become non-kosher, and make even otherwise kosher food prepared with them non-kosher. Some such utensils, depending on the material they are made from, can be made suitable for preparing kosher food again by immersion in boiling water or by the application of a blowtorch. Food prepared in a manner that violates the "Shabbat" (Sabbath) may not be eaten; although in certain instances it is permitted after the Shabbat is over.
Passover laws.
Passover has special dietary rules, the most important of which is the prohibition on eating leavened bread or derivatives of this, which are known as "chametz". This prohibition is derived from . Utensils used in preparing and serving "chametz" are also forbidden on Passover unless they have been ritually cleansed ("kashered"). Observant Jews often keep separate sets of meat and dairy utensils for Passover use only. In addition, some groups follow various eating restrictions on Passover that go beyond the rules of "kashrut", such as not eating "gebrochts" or garlic.
Produce of the Land of Israel.
Biblical rules also control the use of agriculture produce. For produce grown in the Land of Israel a modified version of the biblical tithes must be applied, including "Terumat HaMaaser", "Maaser Rishon", "Maaser Sheni", and "Maaser Ani" (untithed produce is called "tevel"); the fruit of the first three years of a tree's growth or replanting are forbidden for eating or any other use as "orlah"; produce grown in the Land of Israel on the seventh year obtains "k'dushat shvi'it", and unless managed carefully is forbidden as a violation of the "Shmita" (Sabbatical Year). Some rules of "kashrut" are subject to different rabbinical opinions. For example, many hold that the rule against eating "chadash" (new grain) before the 16th of the month Nisan does not apply outside the Land of Israel.
Vegetables.
Many vegetarian restaurants and producers of vegetarian foods acquire a "hechsher", certifying that a Rabbinical organization has approved their products as being kosher. The "hechsher" usually certifies that certain vegetables have been checked for insect infestation and steps have been taken to ensure that cooked food meets the requirements of "bishul Yisrael". Vegetables such as spinach and cauliflower must be checked for insect infestation. The proper procedure for inspecting and cleaning varies by species, growing conditions, and views of individual rabbis.
Pareve foods.
Some processes convert a meat or dairy product into a "pareve" (neither meat nor dairy) one. For example, rennet is sometimes made from stomach linings, yet is acceptable for making kosher cheese, but such cheeses might not be acceptable to some vegetarians, who would eat only cheese made from a vegetarian rennet. The same applies to kosher gelatin, an animal product, derived from kosher animal sources. Other gelatin-like products from non-animal sources such as agar agar and carrageenan are "pareve" by nature. "Fish gelatin" is derived from fish and is therefore (like all kosher fish products) "pareve". Eggs are also considered "pareve" despite being an animal product.
"Kashrut" has procedures by which equipment can be cleaned of its previous non-kosher use, but that might be inadequate for those with allergies, vegetarians, or adherents to other religious statutes. For example, dairy manufacturing equipment can be cleaned well enough that the rabbis grant "pareve" status to products manufactured with it. Nevertheless, someone with a strong allergic sensitivity to dairy products might still react to the dairy residue, and that is why some products that are legitimately pareve carry "milk" warnings.
Supervision and marketing.
"Hashgacha".
Certain foods must be prepared in whole or in part by Jews. This includes grape wine, certain cooked foods ("bishul akum"), cheese ("g'vinat akum"), and according to some also butter ("chem'at akum"); dairy products (Hebrew: חלב ישראל chalav Yisrael "milk of Israel"); and bread (Pas Yisroel).
Product labeling standards.
Although reading the label of food products can identify obviously non-kosher ingredients, some countries allow manufacturers to omit identification of certain ingredients. Such "hidden" ingredients may include lubricants and flavorings, among other additives; in some cases, for instance, the use of "natural" flavorings, these ingredients are more likely to be derived from non-kosher substances. Furthermore, certain products, such as fish, have a high rate of mislabeling, which may result in a non-kosher fish being sold in a package labeled as a species of kosher fish.
Producers of foods and food additives can contact Jewish religious authorities to have their products certified as "kosher": this involves a visit to the manufacturing facilities by an individual rabbi or a committee from a rabbinic organization, who will inspect the production methods and contents, and if everything is sufficiently "kosher" a certificate would be issued.
Manufacturers sometimes identify the products that have received such certification by adding particular graphical symbols to the label. These symbols are known in Judaism as "hechsherim". Due to differences in "kashrut" standards held by different organizations, the "hechsheirim" of certain Jewish authorities may at times be considered invalid by other Jewish authorities. The certification marks of the various rabbis and organisations are too numerous to list, but one of the most commonly used in the United States of America is that of the Union of Orthodox Congregations, who use a "U" inside a circle ("O-U"), symbolising the initials of "Orthodox Union". In Britain, a commonly used symbol is the "KLBD" logo of the London Beth Din. A single "K" is sometimes used as a symbol for "kosher", but since many countries do not allow letters to be trademarked (the method by which other symbols are protected from misuse), it only indicates that the company producing the product claims that it is kosher.
Many of the certification symbols are accompanied by additional letters or words to indicate the category of the product, according to Jewish law; the categorisation may conflict with legal classifications, especially in the case of food that Jewish law regards as "dairy", but legal classification does not.
In many cases constant supervision is required because, for various reasons, such as changes in manufacturing processes, products that once were kosher may cease to be so. For example, a kosher lubricating oil may be replaced by one containing tallow, which many rabbinic authorities view as non-kosher. Such changes are often co-ordinated with the supervising rabbi, or supervising organisation, to ensure that new packaging does not suggest any "hechsher" or "kashrut". In some cases, however, existing stocks of pre-printed labels with the "hechsher" may continue to be used on the now non-kosher product. An active "grapevine" among the Jewish community discusses which products are now questionable, as well as products which have become kosher but whose labels have yet to carry the "hechsher". Some newspapers and periodicals also discuss "kashrut" products.
Products labeled kosher-style are non-kosher products that have characteristics of kosher foods, such as all-beef hot dogs, or are flavored or prepared in a manner consistent with Ashkenazi practices, like dill pickles. The designation usually refers to delicatessen items.
History of kosher supervision and marketing.
In 1911 Procter & Gamble became the first company to advertise one of their products, Crisco, as kosher. Over the next two decades, companies such as Lender's Bagels, Maxwell House, Manischewitz, and Empire evolved and gave the kosher market more shelf-space. In the 1960s, Hebrew National hotdogs launched a "we answer to a higher authority" campaign to appeal to Jews and non-Jews alike. From that point on, "kosher" became a symbol for both quality and value. The kosher market quickly expanded, and with it more opportunities for kosher products. Menachem Lubinsky, founder of the Kosherfest trade fair, estimates as many as 14 million kosher consumers and $40 billion in sales of kosher products in the USA.
In 2014 the Israeli Defense Forces decided to allow female kosher supervisors to work in its kitchens on military bases, and the first women kosher inspectors were 
certified in Israel.
Legal usage.
Advertising standards laws in many jurisdictions prohibit the use of the phrase "kosher" in a product's labelling unless the producer can show that the product conforms to Jewish dietary laws; however, different jurisdictions often define the legal qualifications for conforming to Jewish dietary laws differently. For example, in some places the law may require that a rabbi certify the "kashrut" nature, in others the rules of "kosher" are fully defined in law, and in others still it is sufficient that the manufacturer only believes that the product complies with Jewish dietary regulations. In several cases, laws restricting the use of the term "kosher" have later been determined to be illegal religious interference.
Costs.
The cost of certification for mass-produced items is typically minuscule, and is usually more than offset by the advantages of being certified. In 1975 the cost per item for obtaining kosher certification was estimated by "The New York Times" as being 6.5 millionths of a cent ($0.000000065) per item for a General Foods frozen-food item. According to a 2005 report by Burns & McDonnell, most US national certifying agencies are non-profit, only charging for supervision and on-site work, for which the on-site supervisor "typically makes less per visit than an auto mechanic does per hour." However, the costs of re-engineering an existing manufacturing process can be costly. Moreover, certification usually leads to increased revenues by opening up additional markets to Jews who keep kosher, Muslims who keep halal, Seventh-day Adventists, vegetarians, and the lactose intolerant who wish to avoid dairy products (products that are reliably certified as "pareve" meet this criterion). According to the Orthodox Union, one of the largest kashrut organization in the United States, "when positioned next to a competing non-kosher brand, a kosher product will do better by 20%."
In some European communities there is a special tax imposed on the purchase of kosher meat to help support the community’s educational institutions. In 2009, delegates at a meeting of the Rabbinical Council of Europe were in broad agreement that the tax which supports the rabbinate, mikvo’os and other communal facilities should be reduced. "While the supermarket Tesco sells a whole chicken for £2, its kosher counterpart of similar weight costs five to six times more."
Society and culture.
Adherence.
A 2013 survey found that 22% of American Jews kept kosher in the home. Many Jews observe "kashrut" partially, by abstaining from pork or shellfish, or not drinking milk with a meat dish. Some keep kosher at home but will eat in a non-kosher restaurant. In 2012, one analysis of the specialty food market in North America estimated that only 15% of Kosher consumers were Jewish. A sizable non-Jewish segment of the population views kosher certification as an indication of wholesomeness. Muslims, Hindus, and people with allergies to dairy foods often consider the "kosher-pareve" designation as an assurance that a food contains no animal-derived ingredients, including milk and all of its derivatives. However, since "kosher-pareve" foods may contain honey, eggs, or fish, strict vegetarians cannot rely on the certification.
Linguistics.
Kosher (Hebrew: כשר‎) in Ancient Hebrew means "be advantageous", "proper", "suitable", or "succeed" according to the Brown-Driver-Briggs Hebrew and English Lexicon. In Modern Hebrew, it generally refers to "kashrut" but can also sometimes mean "proper". For example, the Babylonian Talmud uses "kosher" in the sense of "virtuous", when referring to Darius I as a "kosher king"; Darius, a Persian King, assisted in building the Second Temple. In English, "kosher" often means "legitimate", "acceptable", "permissible", "genuine", or "authentic".
The word "kosher" is also part of some common product names. Sometimes it is used as an abbreviation of "koshering", meaning the process for making something "kosher"; for example, "kosher salt" is a form of salt with irregularly shaped crystals, making it particularly suitable for preparing meat according to the rules of "kashrut", because the increased surface area of the crystals absorbs blood more effectively. At other times it is used as a synonym for "Jewish tradition"; for example, a "kosher dill" pickle is simply a pickle made in the traditional manner of Jewish New York City pickle makers, using a generous addition of garlic to the brine, and is not necessarily compliant with the traditional Jewish food laws.
Further reading.
</dl>

</doc>
<doc id="16903" url="http://en.wikipedia.org/wiki?curid=16903" title="Kuomintang">
Kuomintang

The Kuomintang of China ( or ; KMT), or sometimes spelled as Guomindang by its Pinyin transliteration, is a political party in the Republic of China (ROC). It is the current ruling political party in Taiwan. The name is often translated as the Chinese Nationalist Party.
The predecessor of the KMT, the Revolutionary Alliance, was one of the major advocates of the overthrow of the Qing Dynasty and the establishment of a republic. The KMT was founded by Song Jiaoren and Sun Yat-sen shortly after the Xinhai Revolution of 1911. Sun was the provisional president but he did not have military power and ceded the first presidency to the military leader Yuan Shikai. After Yuan's death, China was divided by warlords, while the KMT was able to control only part of the south. Later led by Chiang Kai-shek, the KMT formed the National Revolutionary Army and succeeded in its Northern Expedition to unify much of China in 1928. It was the ruling party in mainland China from 1928 until its retreat to Taiwan in 1949 after being defeated by the Communist Party of China (CPC) during the Chinese Civil War. In Taiwan, the KMT continued as the single ruling party until reforms in the late 1970s through the 1990s loosened its grip on power. Since 1987, the Republic of China is no longer a single-party state; however, the KMT remains one of the main political parties, controlling the Legislative Yuan (Parliament) and most of the councils.
The guiding ideology is the Three Principles of the People, advocated by Sun Yat-sen. Its party headquarters are located in Taipei. It is currently the ruling party in Taiwan, and holds most seats in the Legislative Yuan. The KMT is a member of the International Democrat Union. Current president Ma Ying-jeou, elected in 2008 and re-elected in 2012, is the seventh KMT member to hold the office of the presidency.
Together with the People First Party and Chinese New Party, the KMT forms what is known as the Taiwanese Pan-Blue Coalition, which supports eventual unification with the mainland. However, the KMT has been forced to moderate its stance by advocating the political and legal status quo of modern Taiwan. The KMT accepts a "One China Principle" – it officially considers that there is only one China, but that the Republic of China rather than the People's Republic of China is its legitimate government under the 1992 Consensus. However, since 2008, in order to ease tensions with the PRC, the KMT endorses the "three noes" policy as defined by Ma Ying-jeou – no unification, no independence and no use of force.
History.
Early years, Sun Yat-sen era.
KMT traces its ideological and organizational roots to the work of Sun Yat-sen, a proponent of Chinese nationalism and democracy, who founded Revive China Society () in Honolulu, Hawaii, United States on 24 November 1894. In 1905, Sun joined forces with other anti-monarchist societies in Tokyo, Empire of Japan to form the Tongmenghui () on 20 August 1905, a group committed to the overthrow of the Qing Dynasty and the establishment of a republican government.
The group planned and supported the Xinhai Revolution of 1911 and the founding of the Republic of China on 1 January 1912. However, Sun did not have military power and ceded the provisional presidency of the republic to strongman Yuan Shikai, who arranged for the abdication of the Last Emperor on February 12.
On 25 August 1912, the Kuomintang (KMT; ) was established at the Huguang Guild Hall in Peking, where Tongmenghui and five smaller pro-revolution parties merged to contest the first national elections. Sun, the then Premier of the ROC, was chosen as the party chairman with Huang Xing as his deputy.
The most influential member of the party was the third ranking Song Jiaoren, who mobilized mass support from gentry and merchants for the KMT to advocate a constitutional parliamentary democracy. The party opposed to constitutional monarchists and sought to check the power of Yuan. KMT won an overwhelming majority of the first National Assembly election in December 1912.
But Yuan soon began to ignore the parliament in making presidential decisions. Song Jiaoren was assassinated in Shanghai in 1913. Members of the KMT led by Sun Yat-sen suspected that Yuan was behind the plot and thus staged the Second Revolution in July 1913, a poorly planned and ill-supported armed rising to overthrow Yuan, and failed. Yuan, claiming subversiveness and betrayal, expelled adherents of KMT from the parliament. Yuan dissolved KMT in November (whose members had largely fled into exile in Japan) and dismissed the parliament early in 1914.
Yuan Shikai proclaimed himself emperor in December 1915. While exiled in Japan in 1914, Sun established the Chinese Revolutionary Party () on 8 July 1914, but many of his old revolutionary comrades, including Huang Xing, Wang Jingwei, Hu Hanmin and Chen Jiongming, refused to join him or support his efforts in inciting armed uprising against Yuan. In order to join the Chinese Revolutionary Party, members must take an oath of personal loyalty to Sun, which many old revolutionaries regarded as undemocratic and contrary to the spirit of the revolution.
Thus, many old revolutionaries did not join Sun's new organisation, and he was largely sidelined within the Republican movement during this period. Sun returned to China in 1917 to establish a rival government at Canton, but was soon forced out of office and exiled to Shanghai. There, with renewed support, he resurrected the KMT on 10 October 1919, under the name Kuomintang of China () and established its headquarters in Kwangtung in 1920.
In 1923, the KMT and its government accepted aid from the Soviet Union after being denied recognition by the western powers. Soviet advisers - the most prominent of whom was Mikhail Borodin, an agent of the Comintern – arrived in China in 1923 to aid in the reorganization and consolidation of the KMT along the lines of the Communist Party of the Soviet Union, establishing a Leninist party structure that lasted into the 1990s. The CPC was under Comintern instructions to cooperate with the KMT, and its members were encouraged to join while maintaining their separate party identities, forming the First United Front between the two parties. Mao Zedong and early members of the CPC also joined KMT in 1923.
Soviet advisers also helped the KMT to set up a political institute to train propagandists in mass mobilization techniques, and in 1923 Chiang Kai-shek, one of Sun's lieutenants from the Tongmenghui days, was sent to Moscow for several months' military and political study. At the first party congress in 1924 in Kwangchow, Kwangtung, which included non-KMT delegates such as members of the CPC, they adopted Sun's political theory, which included the Three Principles of the People - nationalism, democracy and people's livelihood.
Chiang Kai-shek assumes leadership.
When Sun Yat-sen died in 1925, the political leadership of KMT fell to Wang Jingwei and Hu Hanmin, respectively the left wing and right wing leaders of the party. The real power, however, was in the hands of Chiang Kai-shek, who, as the superintendent of the Whampoa Military Academy, was in near complete control of the military.
With their military superiority, KMT confirmed their rule on Canton, the provincial capital of Kwangtung. The Guangxi warlords pledged loyalty to the KMT. The KMT now became a rival government in opposition to the warlord Beiyang government based in Peking.
Chiang assumed leadership of KMT on 6 July 1926. Unlike Sun Yat-sen, whom he admired greatly, Chiang had relatively less contact with the West. Sun had forged all his political, economic and revolutionary ideas primarily from what he had learned in Hawaii and indirectly through British Hong Kong and Empire of Japan under Meiji Restoration. Chiang, however, knew relatively little about the West. He also studied in Japan, but he was firmly rooted in his Chinese identity and was steeped in Chinese culture. As his life progressed, he became increasingly attached to Chinese culture and traditions. His few trips to the West confirmed his pro-Chinese outlook and he studied the Chinese classics and Chinese histories assiduously. In 1924, Sun Yat-sen sent Chiang to spend three months in Moscow studying the political and military system of the Soviet Union. Chiang met Leon Trotsky and other Soviet leaders, but quickly came to the conclusion that the Soviet model of government was not suitable for China. This laid the beginning of his lifelong antagonism against communism.
Chiang was also particularly committed to Sun's idea of "political tutelage". Sun believed that the only hope for a unified and better China lies in a military conquest, followed by a period of political tutelage that would culminate in the transition to democracy. Using this ideology, Chiang built himself into the dictator of the Republic of China, both in the Chinese mainland and when the national government was relocated to Taiwan.
Following the death of Sun Yat-sen, Chiang Kai-shek emerged as the KMT leader and launched the Northern Expedition to defeat the northern warlords and unite China under the party. With its power confirmed in the southeast, the Nationalist Government appointed Chiang Kai-shek commander-in-chief of the National Revolutionary Army (NRA), and the Northern Expedition to suppress the warlords began. Chiang had to defeat three separate warlords and two independent armies. Chiang, with Soviet supplies, conquered the southern half of China in nine months.
A split, however, erupted between the Chinese Communist Party and KMT, which threatened the Northern Expedition. Wang Jing Wei, who led the KMT leftist allies took the city of Wuhan in January 1927. With the support of the Soviet agent Mikhail Borodin, Wang declared the National Government as having moved to Wuhan. Having taken Nanking in March, Chiang halted his campaign and prepared a violent break with Wang and his communist allies. Chiang's expulsion of the CPC and their Soviet advisers led to the beginning of the Chinese Civil War. Wang finally surrendered his power to Chiang. Joseph Stalin ordered CPC to obey the KMT leadership. Once this split had been healed, Chiang resumed his Northern Expedition and managed to take Shanghai.
During the Nanking Incident in March 1927, the NRA stormed the consulates of the United States, United Kingdom (UK) and Empire of Japan, looted foreign properties and almost assassinated the Japanese consul. An American, two British, one French, an Italian and a Japanese were killed. These looters also stormed and seized millions of dollars worth of British concessions in Hankou, refusing to hand them back to the UK. Both Nationalists and Communist soldiers within the army participated in the rioting and looting of foreign residents in Nanking.
NRA took Peking in 1928. The city was the internationally recognized capital, though previously controlled by warlords. This event allowed KMT to receive widespread diplomatic recognition in the same year. The capital was moved from Peking to Nanking, the original capital of the Ming Dynasty, and thus a symbolic purge of the final Qing elements. This period of KMT rule in China between 1927 and 1937 was relatively stable and prosperous and is still known as the Nanjing decade.
After the Northern Expedition in 1928, the Nationalist government under KMT declared that China had been exploited for decades under unequal treaties signed between the foreign powers and the Qing Dynasty. The KMT government demanded that the foreign powers renegotiate the treaties on equal terms.
Before the Northern Expedition, the KMT began as a heterogeneous group advocating American-inspired federalism and provincial autonomy. However, the KMT under Chiang's leadership aimed at establishing a centralized one-party state with one ideology. This was even more evident following Sun's elevation into a cult figure after his death. The control by one single party began the period of "political tutelage," whereby the party was to lead the government while instructing the people on how to participate in a democratic system. The topic of reorganizing the army, brought up at a military conference in 1929, sparked the Central Plains War. The cliques, some of them former warlords, demanded to retain their army and political power within their own territories. Although Chiang finally won the war, the conflicts among the cliques would have a devastating effect on the survival of the KMT.
Muslim Generals in Kansu waged war against the Guominjun in favor of KMT during the conflict in Gansu in 1927-1930. 
Although the Second Sino-Japanese War officially broke out in 1937, Japanese aggression started in 1931 when they staged the Mukden Incident and occupied Manchuria. At the same time, the CPC had been secretly recruiting new members within the KMT government and military. Chiang was alarmed by the expansion of the communist influence. He believed that in order to fight against foreign aggression, the KMT must solve its internal conflicts first, so he started his second attempt to exterminate CPC members in 1934. With the advice from German military advisors, the KMT forces the Communists to withdraw from their bases in southern and central China into the mountains in a massive military retreat known as the Long March. Only less than 10% of the communist army survive the long retreat to Shaanxi province, but they reestablished their military base quickly with aid from the Soviet Union.
The KMT was also known to have used terror tactics against suspected communists, through the utilization of a secret police force, who were employed to maintain surveillance on suspected communists and political opponents. In "The Birth of Communist China", C.P. Fitzgerald describes China under the rule of KMT thus: "the Chinese people groaned under a regime Fascist in every quality except efficiency."
Zhang Xueliang, who believed that the Japanese invasion was a greater threat, was persuaded by the CPC to take Chiang hostage during the Xi'an Incident in 1937 and forced Chiang to agree to an alliance with them in the total war against the Japanese. However, in many situations the alliance was in name only; after a brief period of cooperation, the armies began to fight the Japanese separately, rather than as coordinated allies. Conflicts between KMT and CPC were still common during the war, and documented claims abound of CPC attacks upon the KMT forces and vice versa.
While the KMT army received heavy casualties fighting the Japanese, the CPC expanded its territory by guerrilla tactics within Japanese occupied regions, leading some claims that the CPC often refused to support the KMT troops, choosing to withdraw and let the KMT troops take the brunt of Japanese attacks.
After Japan surrendered in 1945, Taiwan was returned to the Republic of China on 25 October 1945. The brief period of celebration was soon shadowed by the possibility of a civil war between KMT and CPC. The Soviet Union declared war on Japan just before they surrendered and occupied Manchuria, the north eastern part of China. The Soviet Union denied the KMT army to enter the region and assisted CPC to take over the Japanese factories and supplies.
Full-scale civil war between CPC and KMT erupted in 1946. The Communist armies, People's Liberation Army (PLA), previously a minor faction, grew rapidly in influence and power due to several errors on the KMT's part. First, the KMT reduced troop levels precipitously after the Japanese surrender, leaving large numbers of able-bodied, trained fighting men who became unemployed and disgruntled with the KMT as prime recruits for PLA. Second, the KMT government proved thoroughly unable to manage the economy, allowing hyperinflation to result. Among the most despised and ineffective efforts it undertook to contain inflation was the conversion to the gold standard for the national treasury and the Gold Standard Scrip in August 1948, outlawing private ownership of gold, silver and foreign exchange, collecting all such precious metals and foreign exchange from the people and issuing the Gold Standard Scrip in exchange. As most farmland in the north were under CPC's control, the cities governed by the KMT lacked food supply and this added to the hyperinflation. The new scrip became worthless in only ten months and greatly reinforced the nationwide perception of KMT as a corrupt or at best inept entity. Third, Chiang Kai-shek ordered his forces to defend the urbanized cities. This decision gave CPC a chance to move freely through the countryside. At first, the KMT had the edge with the aid of weapons and ammunition from the United States (US). However, with the country suffering from hyperinflation, widespread corruption and other economic ills, the KMT continued to lose popular support. Some leading officials and military leaders of the KMT hoarded material, armament and military-aid funding provided by the US. This became an issue which proved to be a hindrance of its relationship with US government. US President Harry S. Truman wrote that "the Chiangs, the Kungs and the Soongs (were) all thieves", having taken $750 million in US aid.
At the same time, the suspension of American aid and tens of thousands of deserted or decommissioned soldiers being recruited to PLA cause tipped the balance of power quickly to CPC side, and the overwhelming popular support for the CPC in most of the country made it all but impossible for the KMT forces to carry out successful assaults against the Communist.
By the end of 1949, the CPC controlled almost all of mainland China, as the KMT retreated to Taiwan with a significant amount of China's national treasures and 2 million people, including military forces and refugees. Some party members stayed in the mainland and broke away from the main KMT to found the Revolutionary Committee of the Kuomintang, which still currently exists as one of the eight minor registered parties of the People's Republic of China.
KMT in Taiwan.
In 1895, Taiwan Island, including the Penghu islands, became a Japanese colony under Treaty of Shimonoseki, a concession by the Qing Dynasty after it lost the First Sino-Japanese War. After Japan's defeat at the end of World War II in 1945, General Order No. 1 instructed Japan, who surrendered to the US, to surrender its troops in Taiwan to the National Revolutionary Army.
Sovereignty over Taiwan was transferred to the ROC in 1945 on the basis of the Instrument of Surrender of Japan which recognized the Potsdam Declaration which referenced the Cairo Declaration, and the ROC put Taiwan under military rule. Tensions between the local Taiwanese and mainlanders from mainland China increased in the intervening years, culminating in a flashpoint on February 27, 1947 in Taipei when a dispute between a female cigarette vendor and an anti-smuggling officer in front of Tianma Tea House triggered civil disorder and protests that would last for days. The uprising turned bloody and was shortly put down by the ROC Army in the 228 Incident. As a result of the 228 Incident in 1947, Taiwanese people endured what is called the "White Terror", a KMT-led political repression that resulted in the death or disappearance of over 30,000 Taiwanese intellectuals, activists, and anyone suspected of opposition to the KMT.
Following the establishment of the People's Republic of China (PRC) on 1 October 1949, the commanders of the People's Liberation Army (PLA) believed that Kinmen and Matsu had to be taken before a final assault on Taiwan. KMT fought the Battle of Guningtou on 25-27 October 1949 and stopped the PLA invasion. The KMT headquarter was set up on 10 December 1949 at No. 11 Zhongshan South Road. In 1950, Chiang took office in Taipei under the Temporary Provisions Effective During the Period of Communist Rebellion. The provision declared martial law in Taiwan and halted some democratic processes, including presidential and parliamentary elections, until the mainland could be recovered from the CPC. KMT estimated it would take 3 years to defeat the Communists. The slogan was "prepare in the first year, start fighting in the second, and conquer in the third year." Chiang also initiated the Project National Glory to retake back the mainland in 1965, but was eventually dropped in July 1972 after many unsuccessful attempts.
However, various factors, including international pressure, are believed to have prevented the KMT from militarily engaging the CPC full-scale. The KMT backed Muslim insurgents formerly belonging to the NRA during the KMT Islamic insurgency in 1950–1958 in Mainland China. A cold war with a couple of minor military conflicts was resulted in the early years. The various government bodies previously in Nanjing, that were re-established in Taipei as the KMT-controlled government, actively claimed sovereignty over all China. The Republic of China in Taiwan retained China's seat in the United Nations until 1971.
Until the 1970s, KMT successfully pushed ahead with land reforms, developed the economy, implemented a democratic system in a lower level of the government, improved relations between Taiwan and the mainland and created the Taiwan economic miracle. However, the KMT controlled the government under a one-party authoritarian state until reforms in the late 1970s through the 1990s. The ROC in Taiwan was once referred to synonymously with the KMT and known simply as "Nationalist China" after its ruling party. In the 1970s, the KMT began to allow for "supplemental elections" in Taiwan to fill the seats of the aging representatives in the National Assembly.
Although opposition parties were not permitted, Tangwai (or, "outside the party") representatives were tolerated. In the 1980s, the KMT focused on transforming the government from a single-party system to a multi-party democratic one and embracing "Taiwanization". With the founding of the Democratic Progressive Party (DPP) on 28 September 1986, the KMT started competing against the DPP in Parliamentary elections.
In 1991, martial law ceased when President Lee Teng-Hui terminated the Temporary Provisions Effective During the Period of Communist Rebellion. All parties started to be allowed to compete at all levels of elections, including the presidential election. Lee Teng-hui, the ROC's first democratically elected President and the leader of the KMT during the 1990s, announced his advocacy of "special state-to-state relations" with the PRC. The PRC associated it with Taiwan independence.
The KMT faced a split in 1993 that led to the formation of the Chinese New Party in August 1993, alleged to be a result of Lee's "corruptive ruling style". The New Party has, since the purging of Lee, largely reintegrated into KMT. A much more serious split in the party occurred as a result of the 2000 Presidential election. Upset at the choice of Lien Chan as the party's presidential nominee, former party Secretary-General James Soong launched an independent bid, which resulted in the expulsion of Soong and his supporters and the formation of the People First Party (PFP) on 31 March 2000. The KMT candidate placed third behind Soong in the elections. After the election, Lee's strong relationship with the opponent became apparent. In order to prevent defections to the PFP, Lien moved the party away from Lee's pro-independence policies and became more favorable toward Chinese reunification. This shift led to Lee's expulsion from the party and the formation of the Taiwan Solidarity Union (TSU) on 24 July 2001.
In 2006, KMT sold its headquarters at No. 11 Zhongshan South Road to Evergreen Group for NT$2.3 billion (US$96 million). The KMT moved into a smaller building on Bade Road.
The deep-rooted hostility between Taiwanese aborigines and (Taiwanese) Hoklo, and the Aboriginal communities effective KMT networks contribute to Aboriginal skepticism against the DPP and the Aboriginals tendency to vote for the KMT.
Current issues and challenges.
As the ruling party on Taiwan, the KMT amassed a vast business empire of banks, investment companies, petrochemical firms, and television and radio stations, thought to have made it the world's richest political party, with assets once estimated to be around US$2–10 billion. Although this war chest appeared to help the KMT until the mid-1990s, it later led to accusations of corruption (often referred to as "black gold").
After 2000, the KMT's financial holdings appeared to be more of a liability than a benefit, and the KMT started to divest itself of its assets. However, the transactions were not disclosed and the whereabouts of the money earned from selling assets (if it has gone anywhere) is unknown. There were accusations in the 2004 presidential election that the KMT retained assets that were illegally acquired. Currently, there is a law proposed by the DPP in the Legislative Yuan to recover illegally acquired party assets and return them to the government; however, since the Pan-Blue Coalition, the KMT and its smaller partner PFP, control the legislature, it is very unlikely to be passed.
The KMT also acknowledged that part of its assets were acquired through extra-legal means and thus promised to "retro-endow" them to the government. However, the quantity of the assets which should be classified as illegal are still under heated debate. DPP, in its capacity as ruling party from 2000–2008, claimed that there is much more that the KMT has yet to acknowledge. Also, the KMT actively sold assets under its title in order to quench its recent financial difficulties, which the DPP argues is illegal. Former KMT Chairman Ma Ying-Jeou's position is that the KMT will sell some of its properties at below market rates rather than return them to the government and that the details of these transactions will not be publicly disclosed.
In December 2003, then-KMT chairman (present chairman emeritus) and presidential candidate Lien Chan initiated what appeared to some to be a major shift in the party's position on the linked questions of Chinese reunification and Taiwan independence. Speaking to foreign journalists, Lien said that while the KMT was opposed to "immediate independence," it did not wish to be classed as "pro-reunificationist" either.
At the same time, Wang Jin-pyng, speaker of the Legislative Yuan and the Pan-Blue Coalition's campaign manager in the 2004 presidential election, said that the party no longer opposed Taiwan's "eventual independence." This statement was later clarified as meaning that the KMT opposes any immediate decision on unification and independence and would like to have this issue resolved by future generations. The KMT's position on the cross-strait relations was redefined as hoping to remain in the current neither-independent-nor-united situation.
In 2005, then-party chairman Lien Chan announced that he was to leave his office. The two leading contenders for the position include Ma Ying-jeou and Wang Jin-pyng. On 5 April 2005, Taipei Mayor Ma Ying-jeou said he wished to lead the opposition KMT with Wang Jin-pyng. On 16 July 2005, Ma was elected as KMT Chairman in the first contested leadership in KMT's 93-year history. Some 54% of the party's 1.04 million members cast their ballots. Ma garnered 72.4% of vote share, or 375,056 votes, against Wang's 27.6%, or 143,268 votes. After failing to convince Wang to stay on as a vice chairman, Ma named holdovers Wu Po-hsiung, Chiang Pin-kung and Lin Cheng-chi (林澄枝), as well as long-time party administrator and strategist John Kuan as vice-chairmen. All appointments were approved by a hand count of party delegates.
There has been a recent warming of relations between the Pan-Blue Coalition and the PRC, with prominent members of both the KMT and PFP in active discussions with officials on the mainland. In February 2004, it appeared that KMT had opened a campaign office for the Lien-Soong ticket in Shanghai targeting Taiwanese businessmen. However, after an adverse reaction in Taiwan, the KMT quickly declared that the office was opened without official knowledge or authorization. In addition, the PRC issued a statement forbidding open campaigning in the mainland and formally stated that it had no preference as to which candidate won and cared only about the positions of the winning candidate.
On March 28, 2005, thirty members of KMT, led by Vice Chairman Chiang Pin-kung, arrived in mainland China. This marked the first official visit by the KMT to the mainland since it was defeated by communist forces in 1949 (although KMT members including Chiang had made individual visits in the past). The delegates began their itinerary by paying homage to the revolutionary martyrs of the Tenth Uprising at Huanghuagang. They subsequently flew to the former ROC capital of Nanjing to commemorate Sun Yat-sen. During the trip, KMT signed a 10-points agreement with the CPC. The opponents regarded this visit as the prelude of the third KMT-CPC cooperation, after the First and Second United Front. Weeks afterwards, in May 2005, Chairman Lien Chan visited the mainland and met with Hu Jintao. This marked the first meeting between leaders of KMT and CPC after the end of Chinese Civil War in 1949. No agreements were signed because incumbent Chen Shui-bian's government threatened to prosecute the KMT delegation for treason and violation of ROC laws prohibiting citizens from collaborating with CPC.
In 2005, Ma Ying-jeou became KMT chairman defeating Speaker Wang Jin-pyng in the first public election for KMT chairmanship. On 13 February 2007, Ma was indicted by the Taiwan High Prosecutors Office on charges of allegedly embezzling approximately NT$11 million (US$339,000), regarding the issue of "special expenses" while he was mayor of Taipei. Shortly after the indictment, he submitted his resignation as KMT chairman at the same press conference at which he formally announced his candidacy for ROC President. Ma argued that it was customary for officials to use the special expense fund for personal expenses undertaken in the course of their official duties. In December 2007, Ma was acquitted of all charges and immediately filed suit against the prosecutors. Despite having resigned the party chairmanship, Ma was the party's nominee in the 2008 presidential election which he won.
On 25 June 2009, President Ma launched his bid to regain KMT's leadership and registered as the sole candidate for the election of the KMT chairmanship. On July 26, Ma won 93.87% of the vote, becoming the new chairman of KMT,
taking office on 17 October 2009. This officially allows Ma to be able to meet with PRC President Xi Jinping (who is also the General Secretary of the Communist Party of China) and other PRC delegates, as he is able to represent the KMT as leader of a Chinese political party, rather than as head-of-state of a political entity unrecognized by the PRC.
In July 2014, KMT reported total assets of NT$26.8 billion (US$892.4 million) and interest earnings of NT$981.52 million for the year of 2013, making it one of the richest political parties in the world.
On 29 November 2014, KMT suffered a heavy loss in the local election to the DPP, winning only 6 municipalities and counties, down from 14 in the previous election in 2009 and 2010. Ma Ying-jeou subsequently resigned from the party chairmanship on 3 December and replaced by acting Chairman Wu Den-yih. Chairmanship election was held on 17 January 2015 and Eric Chu was elected to become the new chairman. He was inaugurated on 19 February.
Elections and results.
Prior to this, the party's voters had defected to both the PFP and TSU, and the KMT did poorly in the December 2001 legislative elections and lost its position as the largest party in the Legislative Yuan. However, the party did well in the 2002 local government mayoral and council election with Ma Ying-jeou, its candidate for Taipei mayor, winning reelection by a landslide and its candidate for Kaohsiung mayor narrowly losing but doing surprisingly well. Since 2002, the KMT and PFP have coordinated electoral strategies. In 2004, the KMT and PFP ran a joint presidential ticket, with Lien running for president and Soong running for vice-president.
The loss of the presidential election of 2004 to DPP President Chen Shui-bian by merely over 30,000 votes was a bitter disappointment to party members, leading to large scale rallies for several weeks protesting alleged electoral fraud and the "odd circumstances" of the shooting of President Chen. However, the fortunes of the party were greatly improved when the KMT did well in the legislative elections held in December 2004 by maintaining its support in southern Taiwan achieving a majority for the Pan-Blue Coalition.
Soon after the election, there appeared to be a falling out with the KMT's junior partner, the People First Party and talk of a merger seemed to have ended. This split appeared to widen in early 2005, as the leader of the PFP, James Soong appeared to be reconciling with President Chen Shui-Bian and the Democratic Progressive Party. Many PFP members including legislators and municipal leaders have defected to the KMT, and the PFP is seen as a fading party.
The KMT won a decisive victory in the 3-in-1 local elections of December 2005, replacing the DPP as the largest party at the local level. This was seen as a major victory for the party ahead of legislative elections in 2007. There were elections for the two municipalities of the ROC, Taipei and Kaohsiung on December 2006. The KMT won a clear victory in Taipei, but lost to the DPP in the southern city of Kaohsiung by the slim margin of 1,100 votes.
In 2008, the KMT won a landslide victory in the Republic of China Presidential Election on March 22, 2008. The KMT fielded former Taipei mayor and former KMT chairman Ma Ying-jeou to run against the DPP's Frank Hsieh. Ma won by a butt of 17% against Hsieh. Ma took office on May 20, 2008, with Vice-Presidential candidate Vincent Siew, and ended 8 years of the DPP presidency. The KMT also won a landslide victory in the 2008 legislative elections, winning 81 of 113 seats, or 71.7% of seats in the Legislative Yuan. These two elections gave the KMT firm control of both the executive and legislative yuans.
Supporter base.
Support for the KMT in the Republic of China encompasses a wide range of groups. KMT support tends to be higher in northern Taiwan and in urban areas, where it draws its backing from small to medium business owners, who make up the majority of commercial interests in Taiwan. Big businesses are also likely to support the KMT because of its policy of maintaining commercial links with mainland China.
The KMT also has strong support in the labor sector because of the many labor benefits and insurance implemented while the KMT was in power. The KMT traditionally has strong cooperation with labor unions, teachers, and government workers. Among the ethnic groups in Taiwan, the KMT has solid support among mainlanders and their descendants for ideological reasons and among Taiwanese aboriginals.
The deep-rooted hostility between Aboriginals and (Taiwanese) Hoklo, and the Aboriginal communities effective KMT networks, contribute to Aboriginal skepticism against the Democratic Progressive Party (DPP) and the Aboriginals tendency to vote for the KMT. Aboriginals have criticized politicians for abusing the "indigenization" movement for political gains, such as aboriginal opposition to the DPP's "rectification" by recognizing the Taroko for political reasons, with the majority of mountain townships voting for Ma Ying-jeou. In 2005 the Kuomintang displayed a massive photo of the anti-Japanese Aboriginal leader Mona Rudao at its headquarters in honor of the 60th anniversary of Taiwan's retrocession from Japan to the Republic of China. 
Opponents of the KMT include strong supporters of Taiwan independence, and rural residents particularly in southern Taiwan, though supporters of unification include Hoklo and supporters of independence include mainlanders. There is opposition due to an image of KMT both as a mainlanders' and a Chinese nationalist party out of touch with local values.
Organization.
Leadership history.
List of leaders of the Kuomintang (1912–1914).
President:
Premier:
List of leaders of the Kuomintang of China (1919–present).
Premier:
Chairman of Central Executive Committee:
Director-General:
Chairman:
List of Secretaries-General of the Kuomintang of China.
Secretaries-General of the Central Executive Committee:
Secretaries-General of the Central Reform Committee:
Secretaries-General of the Central Committee:
First Vice Chairman:
Ideology in mainland China (1920s–1950s).
Chinese nationalism.
KMT was a nationalist revolutionary party, which had been supported by the Soviet Union. It was organized on the Leninist principle of organisation, democratic centralism.
KMT had several influences upon its ideology by revolutionary thinking. KMT and Chiang Kai-shek used the words feudal and counterrevolutionary as synonyms for evil and backwardness, and proudly proclaimed themselves to be revolutionary. Chiang called the warlords feudalists, and called for feudalism and counterrevolutionaries to be stamped out by KMT. Chiang showed extreme rage when he was called a warlord, because of its negative, feudal connotations. Ma Bufang was forced to defend himself against the accusations, and stated to the news media that his army was a part of "National army, people's power".
Chiang Kai-shek, the head of KMT, warned the Soviet Union and other foreign countries about interfering in Chinese affairs. He was personally angry at the way China was treated by foreigners, mainly by the Soviet Union, Britain, and the United States. He and his New Life Movement called for the crushing of Soviet, Western, American and other foreign influences in China. Chen Lifu, a CC Clique member in the KMT, said "Communism originated from Soviet imperialism, which has encroached on our country." It was also noted that "the white bear of the North Pole is known for its viciousness and cruelty."
The Blue Shirts Society, a fascist paramilitary organization within KMT modeled after Mussolini's blackshirts, was anti-foreign and anticommunist, and stated that its agenda was to expel foreign (Japanese and Western) imperialists from China, crush Communism, and eliminate feudalism. In addition to being anticommunist, some KMT members, like Chiang Kai-shek's right-hand man Dai Li were anti-American, and wanted to expel American influence.
KMT leaders across China adopted nationalist rhetoric. The Chinese Muslim general Ma Bufang of Qinghai presented himself as a Chinese nationalist to the people of China, fighting against British imperialism, to deflect criticism by opponents that his government was feudal and oppressed minorities like Tibetans and Buddhist Mongols. He used his Chinese nationalist credentials to his advantage to keep himself in power.
KMT pursued a sinicization policy, it was stated that "the time had come to set about the business of making all natives either turn Chinese or get out" by foreign observers of KMT policy. It was noted that "Chinese colonization" of "Mongolia and Manchuria" led "to a conviction that the day of the barbarian was finally over."
New Guangxi Clique.
Dr. Sun Yat-sen, the founding father of the Republic of China and of KMT praised the Boxers in the Boxer Rebellion for fighting against Western Imperialism. He said the Boxers were courageous and fearless, fighting to the death against the Western armies, Dr. Sun specifically cited the Battle of Yangcun.
During the Northern Expedition, KMT incited anti-foreign, anti-western sentiment. Portraits of Sun Yatsen replaced the crucifix in several churches, KMT posters proclaimed- "Jesus Christ is dead. Why not worship something alive such as Nationalism?". Foreign missionaries were attacked and anti foreign riots broke out.
KMT branch in Guangxi province, led by the New Guangxi Clique implemented anti-imperialist, anti-religious, and anti-foreign policies.
During the Northern Expedition, in 1926 in Guangxi, Muslim General Bai Chongxi led his troops in destroying most of the Buddhist temples and smashing idols, turning the temples into schools and KMT headquarters. Bai led an anti-foreign wave in Guangxi, attacking American, European, and other foreigners and missionaries, and generally making the province unsafe for non-natives. Westerners fled from the province, and some Chinese Christians were also attacked as imperialist agents.
The three goals of his movement were anti-foreign, anti-imperialism, and anti-religion. Bai led the anti-religious movement, against superstition. Huang Shaoxiong, also a KMT member of the New Guangxi Clique, supported Bai's campaign, and Huang was not a Muslim, the anti religious campaign was agreed upon by all Guangxi KMT members.
The New Guangxi Clique clashed with Chiang Kai-shek, which led to the Central Plains War where Chiang defeated the clique.
Socialism and anti-capitalist agitation.
KMT had a left wing and a right wing, the left being more radical in its pro Soviet policies, but both wings equally persecuted merchants, accusing them of being counterrevolutionaries and reactionaries. The right wing under Chiang Kaishek prevailed, and continued radical policies against private merchants and industrialists, even as they denounced communism.
One of the Three Principles of the People of KMT, Mínshēng, was defined as socialism by Dr. Sun Yatsen. He defined this principle of saying in his last days "it's socialism and it's communism.". The concept may be understood as social welfare as well. Sun understood it as an industrial economy and equality of land holdings for the Chinese peasant farmers. Here he was influenced by the American thinker Henry George (see Georgism) and German thinker Karl Marx; the land value tax in Taiwan is a legacy thereof. He divided livelihood into four areas: food, clothing, housing, and transportation; and planned out how an ideal (Chinese) government can take care of these for its people.
KMT was referred to having a socialist ideology. "Equalization of land rights" was a clause included by Dr. Sun in the original Tongmenhui. KMT's revolutionary ideology in the 1920s incorporated unique Chinese Socialism as part of its ideology.
The Soviet Union trained KMT revolutionaries in the Moscow Sun Yat-sen University. In the West and in the Soviet Union, Chiang was known as the "Red General". Movie theaters in the Soviet Union showed newsreels and clips of Chiang, at Moscow Sun Yat-sen University Portraits of Chiang were hung on the walls, and in the Soviet May Day Parades that year, Chiang's portrait was to be carried along with the portraits of Karl Marx, Lenin, Stalin, and other socialist leaders.
KMT attempted to levy taxes upon merchants in Canton, and the merchants resisted by raising an army, the Merchant's volunteer corps. Dr. Sun initiated this anti merchant policy, and Chiang Kai-shek enforced it, Chiang led his army of Whampoa Military Academy graduates to defeat the merchant's army. Chiang was assisted by Soviet advisors, who supplied him with weapons, while the merchants were supplied with weapons from the Western countries.
KMT were accused of leading a "Red Revolution"in Canton. The merchants were conservative and reactionary, and their Volunteer Corp leader Chen Lianbao was a prominent comprador trader.
The merchants were supported by the foreign, western Imperialists such as the British, who led an international flotilla to support them against Dr. Sun. Chiang seized the western supplied weapons from the merchants, and battled against them. A KMT General executed several merchants, and KMT formed a Soviet inspired Revolutionary Committee. The British Communist party congratulated Dr. Sun for his war against foreign imperialists and capitalists.
In 1948, KMT again attacked the merchants of Shanghai, Chiang Kai-shek sent his son Chiang Ching-kuo to restore economic order. Ching-kuo copied Soviet methods, which he learned during his stay there, to start a social revolution by attacking middle-class merchants. He also enforced low prices on all goods to raise support from the Proletariat.
As riots broke out and savings were ruined, bankrupting shopowners, Ching-kuo began to attack the wealthy, seizing assets and placing them under arrest. The son of the gangster Du Yuesheng was arrested by him. Ching-kuo ordered KMT agents to raid the Yangtze Development Corporation's warehouses, which was privately owned by H.H. Kung and his family. H.H. Kung's wife was Soong Ai-ling, the sister of Soong May-ling who was Ching-kuo's stepmother. H.H. Kung's son David was arrested, the Kung's responded by blackmailing the Chiang's, threatening to release information about them, eventually he was freed after negotiations, and Ching-kuo resigned, ending the terror on the Shanghainese merchants.
KMT also promotes Government-owned corporations. KMT founder Sun Yat-sen, was heavily influenced by the economic ideas of Henry George, who believed that the rents extracted from natural monopolies or the usage of land belonged to the public. Dr. Sun argued for Georgism and emphasized the importance of a mixed economy, which he termed "The Principle of Minsheng" in his Three Principles of the People.
"The railroads, public utilities, canals, and forests should be nationalized, and all income from the land and mines should be in the hands of the State. With this money in hand, the State can therefore finance the social welfare programs."
KMT Muslim Governor of Ningxia, Ma Hongkui promoted state owned monopoly companies. His government had a company, Fu Ning Company, which had a monopoly over commercial and industry in Ningxia.
Corporations such as CSBC Corporation, Taiwan, CPC Corporation, Taiwan and Aerospace Industrial Development Corporation are owned by the state in the Republic of China.
Marxists also existed in KMT. They viewed the Chinese revolution in different terms than the CPC, claiming that China already went past its feudal stage and in a stagnation period rather than in another mode of production. These marxists in KMT opposed the CPC ideology.
Confucianism and religion in ideology.
KMT used traditional Chinese religious ceremonies, the souls of Party martyrs who died fighting for KMT and the revolution and the party founder Dr. Sun Yat-sen were sent to heaven according to KMT. Chiang Kai-shek believed that these martyrs witnessed events on earth from heaven.
The KMT backed the New Life Movement, which promoted Confucianism, and it was also against westernization. KMT leaders also opposed the May Fourth Movement. Chiang Kai-shek, as a nationalist, and Confucianist, was against the iconoclasm of the May Fourth Movement. He viewed some western ideas as foreign, as a Chinese nationalist, and that the introduction of western ideas and literature that the May Fourth Movement wanted was not welcome. He and Dr. Sun Yat-sen criticized these May Fourth intellectuals for corrupting morals of youth.
KMT also incorporated Confucianism in its jurisprudence. It pardoned Shi Jianqiao for murdering Sun Chuanfang, because she did it in revenge since Sun executed her father Shi Congbin, which was an example of Filial piety to one's parents in Confucianism. KMT encouraged filial revenge killings and extended pardons to those who performed them.
Education.
KMT purged China's education system of western ideas, introducing Confucianism into the curriculum. Education came under the total control of state, which meant, in effect, the KMT, via the Ministry of Education. Military and political classes on KMT's "Three Principles of the People" were added. Textbooks, exams, degrees and educational instructors were all controlled by the state, as were all universities.
Soviet-style military.
Chiang Ching-kuo, appointed as KMT director of Secret Police in 1950, was educated in the Soviet Union, and initiated Soviet style military organization in the Republic of China Military, reorganizing and Sovietizing the political officer corps, surveillance, and KMT activities were propagated throughout the military. Opposed to this was Sun Li-jen, who was educated at the American Virginia Military Institute. Chiang Ching-kuo then arrested Sun Li-jen, charging him of conspiring with the American CIA of plotting to overthrow Chiang Kai-shek and KMT, Sun was placed under house arrest in 1955.
Parties affiliated with the Kuomintang.
Malaysian Chinese Association.
Initially the Malaysian Chinese Association (MCA) party was pro-ROC and mainly consisted of KMT members who joined as an alternative and were also in opposition to the Malayan Communist Party, supporting the KMT in China by funding them with the intention of reclaiming the Chinese mainland from the communists.
Tibet Improvement Party.
The Tibet Improvement Party was founded by Pandatsang Rapga, a pro-ROC and pro-KMT Khampa revolutionary, who worked against the 14th Dalai Lama's Tibetan Government in Lhasa. Rapga borrowed Sun Yat-sen's Three Principles of the People doctrine and translated his political theories into the Tibetan language, hailing it as the best hope for Asian peoples against imperialism. Rapga stated that "the Sanmin Zhuyi was intended for all peoples under the domination of foreigners, for all those who had been deprived of the rights of man. But it was conceived especially for the Asians. It is for this reason that I translated it. At that time, a lot of new ideas were spreading in Tibet", during an interview in 1975 by Dr. Heather Stoddard. He wanted to destroy the feudal government in Lhasa, in addition to modernizing and secularizing Tibetan society. The ultimate goal of the party was the overthrow of the Dalai Lama's regime, and the creation of a Tibetan Republic which would be an autonomous Republic within the ROC. Chiang Kai-shek and the KMT funded the party and their efforts to build an army to battle the Dalai Lama's government. KMT was extensively involved in the Kham region, recruiting the Khampa people to both oppose the Dalai Lama's Tibetan government, fight the Communist Red Army, and crush the influence of local Chinese warlords who did not obey the central government.
Vietnamese Nationalist Party.
KMT assisted the Viet Nam Quoc Dan Dang party, which translates literally into Chinese as Yuenan Kuomintang (越南國民黨), meaning "Vietnamese Nationalist Party". When it was established, it was based on the Chinese KMT and was pro Chinese. The Chinese KMT helped the party, known as the VNQDD, set up headquarters in Canton and Yunnan, to aid their anti imperialist struggle against the French occupiers of Indo China and against the Vietnamese Communist Party. It was the first revolutionary nationalist party to be established in Vietnam, before the communist party. The KMT assisted VNQDD with funds and military training.
The VNQDD was founded with KMT aid in 1925, they were against Ho Chi Minh's Viet Nam Revolutionary Youth League. When the VNQDD fled to China after the failed uprising against the French, they settled in Yunnan and Canton, in two different branches. The VNQDD existed as a party in exile in China for 15 years, receiving help, militarily and financially, and organizationally from the Chinese KMT. The two VNQDD parties merged into a single organization, the Canton branch removed the word "revolutionary" from the party name. Lu Han, a KMT official in Nanjing, who was originally from Yunnan, was contacted by the VNQDD, and the KMT Central Executive Committee and Military made direct contact with VNQDD for the first time, the party was reestablished in Nanjing with KMT help.
The Chinese KMT used the VNQDD for its own interests in south China and Indo China. General Zhang Fakui (Chang Fa-kuei), who based himself in Guangxi, established the Viet Nam Cach Menh Dong Minh Hoi meaning "Viet Nam Revolutionary League" in 1942, which was assisted by the VNQDD to serve the KMT's aims. The Chinese Yunnan provincial army, under the KMT, occupied northern Vietnam after the Japanese surrender in 1945, the VNQDD tagging alone, opposing Ho Chi Minh's communist party. The Viet Nam Revolutionary League was a union of various Vietnamese nationalist groups, run by the pro Chinese VNQDD. Its stated goal was for unity with China under the Three Principles of the People, created by KMT founder Dr. Sun and opposition to Japanese and French Imperialists. The Revolutionary League was controlled by Nguyen Hai Than, who was born in China and could not speak Vietnamese. General Zhang shrewdly blocked the Communists of Vietnam, and Ho Chi Minh from entering the league, as his main goal was Chinese influence in Indo China. The KMT utilized these Vietnamese nationalists during World War II against Japanese forces.
A KMT left winger, General Chang Fa-kuei worked with Nguyen Hai Than, a VNQDD member, against French Imperialists and Communists in Indo China. General Chang Fa-kuei planned to lead a Chinese army invasion of Tonkin in Indochina to free Vietnam from French control, and to get Chiang Kai-shek's support. The VNQDD opposed the government of Ngo Dinh Diem during the Vietnam War.
Organizations sponsored by the Kuomintang.
Ma Fuxiang founded Islamic organizations sponsored by KMT, including the China Islamic Association (中國回教公會).
KMT Muslim General Bai Chongxi was Chairman of the Chinese Islamic National Salvation Federation. The Muslim Chengda school and Yuehua publication were supported by the Nationalist Government, and they supported KMT.
The Chinese Muslim Association was also sponsored by KMT, and it evacuated from the mainland to Taiwan with the party. The Chinese Muslim Association owns the Taipei Grand Mosque which was built with funds from KMT.
The Yihewani (Ikhwan al Muslimun a.k.a. Muslim brotherhood) was the predominant Muslim sect backed by KMT. Other Muslim sects, like the Xidaotang were also supported by the KMT. The Chinese Muslim brotherhood became a Chinese nationalist organization and supported KMT rule, Brotherhood Imams like Hu Songshan ordered Muslims to pray for the Nationalist Government, salute KMT flags during prayer, and listen to nationalist sermons.
Policy on ethnic minorities.
KMT considers all minorities to be members of the Chinese Nation, Chiang Kai-shek, the KMT leader, considered all the minority peoples of China, including the Hui, as descedants of Huangdi, the Yellow Emperor and semi mythical founder of the Chinese nation. Chiang considered all the minorities to belong to the Chinese Nation Zhonghua Minzu and he introduced this into KMT ideology, which was propagated into the educational system of the Republic of China, and the Constitution of the ROC considered Chiang's ideology to be true. In Taiwan, the President performs a ritual honoring Huangdi, while facing west, in the direction of the mainland China.
KMT kept the Mongolian and Tibetan Affairs Commission for dealing with Mongolian And Tibetan affairs. A Muslim, Ma Fuxiang, was appointed as its Chairman.
KMT was known for sponsoring Muslim students to study abroad at Muslim universities like Al Azhar and it established schools especially for Muslims, Muslim KMT warlords like Ma Fuxiang promoted education for Muslims. KMT Muslim Warlord Ma Bufang built a girl's school for Muslim girls in Linxia City which taught modern secular education.
Tibetans and Mongols refused to allow other ethnic groups like Kazakhs to participate in the Kokonur ceremony in Qinghai, until the KMT Muslim General Ma Bufang forced them to stop the racism and allowed them to participate.
Chinese Muslims were among the most hardline KMT members. Ma Chengxiang was a Muslim and a KMT member, and refused to surrender to the Communists.
KMT incited anti Yan Xishan and Feng Yuxiang sentiments among Chinese Muslims and Mongols, encouraging for them to topple their rule during the Central Plains War.
Masud Sabri, a Uyghur was appointed as Governor of Xinjiang by KMT, as was the Tatar Burhan Shahidi and the Uyghur Yulbars Khan.
The Muslim General Ma Bufang also put KMT symbols on his mansion, the Ma Bufang Mansion along with a portrait of party founder Dr. Sun Yatsen arranged with KMT flag and the Republic of China flag.
General Ma Bufang and other high ranking Muslim Generals attended the Kokonuur Lake Ceremony where the God of the Lake was worshipped, and during the ritual, the Chinese national anthem was sung, all participants bowed to a Portrait of KMT founder Dr. Sun Yat-sen, and the God of the Lake was also bowed to, and offerings were given to him by the participants, which included the Muslims. This cult of personality around KMT leader and KMT was standard in all meetings. Sun Yat-sen's portrait was bowed to three times by KMT party members. Dr. Sun's portrait was arranged with two flags crossed under, the KMT flag and the flag of the Republic of China.
KMT also hosted conferences of important Muslims like Bai Chongxi, Ma Fuxiang, and Ma Liang. Ma Bufang stressed "racial harmony" as a goal when he was Governor of Qinghai.
In 1939 Isa Yusuf Alptekin and Ma Fuliang were sent on a mission by KMT to the Middle eastern countries such as Egypt, Turkey and Syria to gain support for the Chinese War against Japan, they also visited Afghanistan in 1940 and contacted Muhammad Amin Bughra, they asked him to come to Chongqing, the capital of the Nationalist Government. Bughra was arrested by the British in 1942 for spying, and KMT arranged for Bughra's release. He and Isa Yusuf worked as editors of KMT Muslim publications. Ma Tianying (馬天英) (1900–1982) led the 1939 mission which had 5 other people including Isa and Fuliang.
Stance on separatism.
KMT is anti-separatist; during its rule on mainland China, it crushed Uyghur and Tibetan separatist uprisings. KMT claims sovereignty over Mongolia and Tuva as well as the territories of the modern People's Republic and Republic of China.
KMT Muslim General Ma Bufang waged war on the invading Tibetans during the Sino-Tibetan War with his Muslim army, and he repeatedly crushed Tibetan revolts during bloody battles in Qinghai provinces. Ma Bufang was fully supported by President Chiang Kai-shek, who ordered him to prepare his Muslim army to invade Tibet several times and threatened aerial bombardment on the Tibetans. With support from KMT, Ma Bufang repeatedly attacked the Tibetan area of Golog seven times during the KMT Pacification of Qinghai, eliminating thousands of Tibetans.
General Ma Fuxiang, the chairman of the Mongolian and Tibetan Affairs Commission stated that Mongolia and Tibet were an integral part of the Republic of China.
"Our party (KMT) takes the development of the weak and small and resistance to the strong and violent as our sole and most urgent task. This is even more true for those groups which are not of our kind [Ch. fei wo zulei zhe]. Now the people of Mongolia and Tibet are closely related to us, and we have great affection for one another: our common existence and common honor already have a history of over a thousand years... Mongolia and Tibet's life and death are China's life and death. China absolutely cannot cause Mongolia and Tibet to break away from China's territory, and Mongolia and Tibet cannot reject China to become independent. At this time, there is not a single nation on earth except China that will sincerely develop Mongolia and Tibet."
Under orders from Nationalist Government of Chiang Kai-shek, the Hui General Ma Bufang, Governor of Qinghai (1937–1949), repaired Yushu airport to prevent Tibetan separatists from seeking independence. Ma Bufang also crushed Mongol separatist movements, abducting the Genghis Khan Shrine and attacking Tibetan Buddhist Temples like Labrang, and keeping a tight control over them through the Kokonur God ceremony.
During the Kumul Rebellion, KMT 36th Division (National Revolutionary Army) crushed a separatist Uyghur First East Turkestan Republic, delivering it a fatal blow at the Battle of Kashgar (1934). The Muslim General Ma Hushan pledged allegiance to KMT and crushed another Uyghur revolt at Charkhlik Revolt.
KMT also fought against a Soviet and White Russian invasion during the Soviet Invasion of Xinjiang.
During the Ili Rebellion, KMT fought against Uyghur separatists and the Soviet Union, and against Mongolia.
See also.
Lists:

</doc>
<doc id="17076" url="http://en.wikipedia.org/wiki?curid=17076" title="Kenilworth Castle">
Kenilworth Castle

Kenilworth Castle is located in the town of the same name in Warwickshire, England. Constructed from Norman through to Tudor times, the castle has been described by architectural historian Anthony Emery as "the finest surviving example of a semi-royal palace of the later middle ages, significant for its scale, form and quality of workmanship". Kenilworth has also played an important historical role. The castle was the subject of the six-month-long Siege of Kenilworth in 1266, believed to be the longest siege in English history, and formed a base for Lancastrian operations in the Wars of the Roses. Kenilworth was also the scene of the removal of Edward II from the English throne, the French insult to Henry V in 1414 (said by John Strecche to have encouraged the Agincourt campaign), and the Earl of Leicester's lavish reception of Elizabeth I in 1575.
The castle was built over several centuries. Founded in the 1120s around a powerful Norman great tower, the castle was significantly enlarged by King John at the beginning of the 13th century. Huge water defences were created by damming the local streams and the resulting fortifications proved able to withstand assaults by land and water in 1266. John of Gaunt spent lavishly in the late 14th century, turning the medieval castle into a palace fortress designed in the latest perpendicular style. The Earl of Leicester then expanded the castle once again, constructing new Tudor buildings and exploiting the medieval heritage of Kenilworth to produce a fashionable Renaissance palace.
Kenilworth was partly destroyed by Parliamentary forces in 1649 to prevent it being used as a military stronghold. Ruined, only two of its buildings remain habitable today. The castle became a tourist destination from the 18th century onwards, becoming famous in the Victorian period following the publishing of Sir Walter Scott's novel "Kenilworth" in 1826. English Heritage has managed the castle since 1984. The castle is classed as a Grade I listed building and as a Scheduled Monument, and is open to the public.
Architecture and landscape.
Although now ruined as a result of the slighting, or deliberate partial destruction, of the castle after the English Civil War, Kenilworth illustrates five centuries of English military and civil architecture. The castle is built almost entirely from local new red sandstone.
Entrance and outer bailey wall.
To the south-east of the main castle lie the Brays, a corruption of the French word "braie", meaning an external fortification with palisades. Only earthworks and fragments of masonry remain of what was an extensive 13th-century barbican structure including a stone wall and an external gatehouse guarding the main approach to the castle. The area now forms part of the car park for the castle. Beyond the Brays are the ruins of the Gallery Tower, a second gatehouse remodelled in the 15th century. The Gallery Tower originally guarded the 152-metre (500-ft) long, narrow walled-causeway that still runs from the Brays to the main castle. This causeway was called the Tiltyard, as it was used for tilting, or jousting, in medieval times. The Tiltyard causeway acted both as a dam and as part of the barbican defences. To the east of the Tiltyard is a lower area of marshy ground, originally flooded and called the Lower Pool, and to the west an area once called the Great Mere. The Great Mere is now drained and forms a meadow, but would originally have been a large lake covering around 100 acre, dammed by the Tiltyard causeway.
The outer bailey of Kenilworth Castle is usually entered through Mortimer's Tower, today a modest ruin but originally a Norman stone gatehouse, extended in the late 13th and 16th centuries. The outer bailey wall, long and relatively low, was mainly built by King John; it has numerous buttresses but only a few towers, being designed to be primarily defended by the water system of the Great Mere and Lower Pool. The north side of the outer bailey wall was almost entirely destroyed during the slighting. Moving clockwise around the outer bailey from Mortimer's Tower, the defences include a west-facing watergate, which would originally have led onto the Great Mere; the King's gate, a late 17th-century agricultural addition; the Swan Tower, a late 13th-century tower with 16th century additions named after the swans that lived on the Great Mere; the early 13th-century Lunn's Tower; and the 14th-century Water Tower, so named because it overlooked the Lower Pool.
Inner court.
Kenilworth's inner court consists of a number of buildings set against a bailey wall, originally of Norman origin, exploiting the defensive value of a natural knoll that rises up steeply from the surrounding area. The 12th-century great tower occupies the knoll itself and forms the north-east corner of the bailey. Ruined during the slighting, the great tower is notable for its huge corner turrets, essentially hugely exaggerated Norman pilaster buttresses. Its walls are 5 metres (17 ft) thick, and the towers 30 metres (100 ft) high. Although Kenilworth's great tower is larger, it is similar to that of Brandon Castle near Coventry; both were built by the local Clinton family in the 1120s. The tower can be termed a hall keep, as it is longer than it is wide. The lowest floor is filled with earth, possibly taken from the earlier motte that may have been present on the site, and is further protected by a sloping stone plinth around the base. The tall Tudor windows at the top of the tower date from the 1570s.
Much of the northern part of the inner bailey was built by the 14th-century noble John of Gaunt between 1372 and 1380. This part of the castle is considered by historian Anthony Emery to be "the finest surviving example of a semi-royal palace of the later middle ages, significant for its scale, form and quality of workmanship". Gaunt's architectural style emphasised rectangular design, the separation of ground floor service areas from the upper stories and a contrast of austere exteriors with lavish interiors, especially on the 1st floor of the inner bailey buildings. The result is considered "an early example of the perpendicular style".
The most significant of Gaunt's buildings is his great hall. The great hall replaced an earlier sequence of great halls on the same site, and was heavily influenced by Edward III's design at Windsor Castle. The hall consists of a "ceremonial sequence of rooms", approached by a particularly grand staircase, now lost. From the great hall, visitors could look out to admire the Great Mere or the inner court through huge windows. The undercroft to the hall, used by the service staff, was lit with slits, similar to design at the contemporary Wingfield Manor. The roof was built in 1376 by William Wintringham, producing the widest hall, unsupported by pillars, existing in England at the time. There is some debate amongst historians as to whether this roof was a hammerbeam design, a collar and truss-brace design, or a combination of the two.
There was an early attempt at symmetry in the external appearance of the great hall – the Strong and Saintlowe Towers architecturally act as near symmetrical "wings" to the hall itself, while the plinth of the hall is designed to mirror that of the great tower opposite it. An unusual multi-sided tower, the Oriel, provides a counterpoint to the main doorway of the hall and was intended for private entertainment by Gaunt away from the main festivities on major occasions. The Oriel tower is based on Edward III's "La Rose" Tower at Windsor, which had a similar function. Gaunt's Strong Tower is so named for being entirely vaulted in stone across all its floors, an unusual and robust design. The great hall influenced the design of Bolton and Raby castles, while the hall's roof design became famous and was copied at Arundel Castle and Westminster Hall.
Other parts of the castle built by Gaunt include the southern range of state apartments, Gaunt's Tower and the main kitchen. Although now extensively damaged, these share the same style as the great hall; this would have unified the appearance of Gaunt's palace in a distinct break from the more eclectic medieval tradition of design. Gaunt's kitchen replaced the original 12th-century kitchens, built alongside the great tower in a similar fashion to the arrangement at Conisbrough. Gaunt's new kitchen was twice the size of that in equivalent castles, measuring 19 metres (66 ft) by eight metres (28 ft).
The remainder of the inner court was built by Robert Dudley, the Earl of Leicester, in the 1570s. He built a tower now known as Leicester's building on the south edge of the court as a guest wing, extending out beyond the inner bailey wall for extra space. Leicester's building was four floors high and built in a fashionable contemporary Tudor style with "brittle, thin walls and grids of windows". The building was intended to appear well-proportioned alongside the ancient great tower, one of the reasons for its considerable height. Leicester's building set the style for later Elizabethan country house design, especially in the Midlands, with Hardwick Hall being a classic example. Modern viewing platforms, installed in 2014, provide views from Elizabeth I's's former bedroom.
Leicester also built a loggia, or open gallery, beside the great keep to lead to the new formal gardens. The loggia was designed to elegantly frame the view as the observer slowly admired the gardens, and was a new design in the 16th century, only recently imported from Italy.
Base, left-hand and right-hand courts.
The rest of Kenilworth Castle's interior is divided into three areas: the base court, stretching between Mortimer's Tower and Leicester's gatehouse; the left-hand court, stretching south-west around the outside of the inner court; and the right-hand court, to the north-west of the inner court. The line of trees that cuts across the base court today is a relatively modern mid-19th century addition, and originally this court would have been more open, save for the collegiate chapel that once stood in front of the stables. Destroyed in 1524, only the chapel's foundations remain. Each of the courts was designed to be used for different purposes: the base court was considered a relatively public area, with the left and right courts used for more private occasions.
Leicester's gatehouse was built on the north side of the base court, replacing an older gatehouse to provide a fashionable entrance from the direction of Coventry. The external design, with its symbolic towers and, originally, battlements, echoes a style popular a century or more before, closely resembling Kirby Muxloe and the Beauchamp gatehouse at Warwick Castle. By contrast the interior, with its contemporary wood panelling, is in the same, highly contemporary Elizabethan fashion of Leicester's building in the inner court. Leicester's gatehouse is one of the few parts of the castle to remain intact. The stables built by John Dudley in the 1550s also survive and lie along the east side of the base court. The stable block is a large building built mostly in stone, but with a timber-framed, decoratively panelled first storey designed in an anachronistic, vernacular style. Both buildings could have easily been seen from Leicester's building and were therefore on permanent display to visitors. Leciester's intent may have been to create a deliberately anachronistic view across the base court, echoing the older ideals of chivalry and romance alongside the more modern aspects of the redesign of the castle.
Garden and landscape.
Much of the right-hand court of Kenilworth Castle is occupied by the castle garden. For most of Kenilworth's history the role of the castle garden, used for entertainment, would have been very distinct from that of the surrounding chase, used primarily for hunting. From the 16th century onwards there were elaborate knot gardens in the base court. The gardens today are designed to reproduce as closely as possible the primarily historical record of their original appearance in 1575, with a steep terrace along the south side of the gardens and steps leading down to eight square knot gardens. In Elizabethan gardens "the plants were almost incidental", and instead the design focus was on sculptures, including four wooden obelisks painted to resemble porphyry and a marble fountain with a statue of two Greek mythological figures. A timber aviary contains a range of birds. The original garden was heavily influenced by the Italian Renaissance garden at Villa d'Este.
To the north-west of the castle are earthworks marking the spot of the "Pleasance", created in 1414 by Henry V. The Pleasance was a banqueting house built in the style of a miniature castle. Surrounded by two diamond-shaped moats with its own dock, the Pleasance was positioned on the far side of the Great Mere and had to be reached by boat. It resembled Richard II's retreat at Sheen from the 1380s, and was later copied by his younger brother, Duke Humphrey of Gloucester, at Greenwich in the 1430s, as well by his son, John of Lancaster at Fulbrook. The Pleasance was eventually dismantled by Henry VIII and partially moved into the left-hand court inside the castle itself, possibly to add to the anachronistic appearance. These elements were finally destroyed in the 1650s.
The inner court as seen from the base court; left to right are the 16th-century Leicester's building; Gaunt's 14th-century Oriel tower and great hall; and Clinton's 12th-century great keep.
History.
12th century.
Kenilworth Castle was founded in the early 1120s by Geoffrey de Clinton, Lord Chamberlain to Henry I. The castle's original form is uncertain. It has been suggested that it consisted of a motte, an earthen mound surmounted by wooden buildings; however, the stone great tower may have been part of the original design. Clinton was a local rival to Roger de Beaumont, the Earl of Warwick and owner of the neighbouring Warwick Castle, and the king made Clinton the sheriff in Warwickshire to act as a counterbalance to Beaumont's power. Clinton had begun to lose the king's favour after 1130, and when he died in 1133 his son, also called Geoffrey, was only a minor. Geoffrey and his uncle William de Clinton were forced to come to terms with Beaumont; this set-back, and the difficult years of the Anarchy (1135–54), delayed any further development of the castle.
Henry II succeeded to the throne at the end of the Anarchy but during the revolt of 1173–74 he faced a significant uprising led by his son, Henry, backed by the French crown. The conflict spread across England and Kenilworth was garrisoned by Henry II's forces; Geoffrey II de Clinton died in this period and the castle was taken fully into royal possession, a sign of its military importance. The Clintons themselves moved on to Buckinghamshire. By this point Kenilworth Castle consisted of the great keep, the inner bailey wall, a basic causeway across the smaller lake that preceded the creation of the Great Mere, and the local chase for hunting.
13th century.
Henry's successor, Richard I, paid relatively little attention to Kenilworth but under King John significant building resumed at the castle. When John was excommunicated in 1208, he embarked on a programme of rebuilding and enhancing several major royal castles. These included Corfe, Odiham, Dover, Scarborough as well as Kenilworth. John spent £1,115 on Kenilworth Castle between 1210 and 1216, building the outer bailey wall in stone and improving the other defences, including creating Mortimer's and Lunn's Towers. He also significantly improved the castle's water defences by damming the Finham and Inchford Brooks, creating the Great Mere. The result was to turn Kenilworth into one of the largest English castles of the time, with one of the largest artificial lake defences in England. John was forced to cede the castle to the baronial opposition as part of the guarantee of the Magna Carta, before it reverted to royal control early in the reign of his son, Henry III.
Henry III granted Kenilworth in 1244 to Simon de Montfort, Earl of Leicester, who later became a leader in the Second Barons' War (1263–67) against the king, using Kenilworth as the centre of his operations. Initially the conflict went badly for King Henry, and after the Battle of Lewes in 1264 he was forced to sign the Mise of Lewes, under which his son, Prince Edward, was given over to the rebels as a hostage. Edward was taken back to Kenilworth, where chroniclers considered he was held in unduly harsh conditions. Released in early 1265, Edward then defeated Montfort at the Battle of Evesham; the surviving rebels under the leadership of Henry de Hastings, Montfort's constable at Kenilworth, regrouped at the castle the following spring. Edward's forces proceeded to lay siege to the rebels.
The Siege of Kenilworth Castle in 1266 was "probably the longest in English history" according to historian Norman Pounds, and at the time was also the largest siege to have occurred in England in terms of the number of soldiers involved. Simon de Monfort's son, Simon VI de Montfort, promised in January 1266 to hand over the castle to the king. Five months later this had not happened, and Henry III laid siege to Kenilworth Castle on 21 June. Protected by the extensive water defences, the castle withstood the attack, despite Edward targeting the weaker north wall, employing huge siege towers and even attempting a night attack using barges brought from Chester. The distance between the royal trebuchets and the walls severely reduced their effectiveness and heavier trebuchets had to be sent for from London. Papal intervention through the legate Ottobuono finally resulted in the compromise of the Dictum of Kenilworth, under which the rebels were allowed to re-purchase their confiscated lands provided they surrendered the castle; the siege ended on 14 December 1266. The water defences at Kenilworth influenced the construction of later castles in Wales, most notably Caerphilly.
Henry granted Kenilworth to his son, Edmund Crouchback, in 1267. Edmund held many tournaments at Kenilworth in the late 13th century, including a huge event in 1279, presided over by the royal favourite Roger de Mortimer, in which a hundred knights competed for three days in the tiltyard in an event called "the Round Table", in imitation of the popular Arthurian legends.
14th century.
Edmund Crouchback passed on the castle to his eldest son, Thomas, Earl of Lancaster, in 1298. Lancaster married Alice de Lacy, which made him the richest nobleman in England. Kenilworth became the primary castle of the Lancaster estates, replacing Bolingbroke, and acted as both a social and a financial centre for Thomas. Thomas built the first great hall at the castle from 1314 to 1317 and constructed the Water Tower along the outer bailey, as well as increasing the size of the chase. Lancaster, with support from many of the other English barons, found himself in increasing opposition to Edward II. War broke out in 1322, and Lancaster was captured at the Battle of Boroughbridge and executed. His estates, including Kenilworth, were confiscated by the crown. Edward and his wife, Isabella of France, spent Christmas 1323 at Kenilworth, amidst major celebrations.
In 1326, however, Edward was deposed by an alliance of Isabella and her lover, Roger Mortimer. Edward was eventually captured by Isabella's forces and the custody of the king was assigned to Henry, Earl of Lancaster, who had backed Isabella's invasion. Henry, reoccupying most of the Lancaster lands, was made constable of Kenilworth and Edward was transported there in late 1326; Henry's legal title to the castle was finally confirmed the following year. Kenilworth was chosen for this purpose by Isabella probably both because it was a major fortification, and also because of the symbolism of its former owners' links to popular ideals of freedom and good government. Royal writs were issued in Edward's name by Isabella from Kenilworth until the next year. A deputation of leading barons led by Bishop Orleton was then sent to Kenilworth to first persuade Edward to resign and, when that failed, to inform him that he had been deposed as king. Edward formally resigned as king in the great hall of the castle on 21 January 1326. As the months went by, however, it became clear that Kenilworth was proving a less than ideal location to imprison Edward. The castle was in a prominent part of the Midlands, in an area that held several nobles who still supported Edward and were believed to be trying to rescue him. Henry's loyalty was also coming under question. In due course, Isabella and Mortimer had Edward moved by night to Berkeley Castle, where he died shortly afterwards. Isabella continued to use Kenilworth as a royal castle until her fall from power in 1330.
Henry of Grosmont, the Duke of Lancaster, inherited the castle from his father in 1345 and remodelled the great hall with a grander interior and roof. On his death Blanche of Lancaster inherited the castle. Blanche married John of Gaunt, the third son of Edward III; their union, and combined resources, made John the second richest man in England next to the king himself. After Blanche's death, John married Constance, who had a claim to the kingdom of Castile, and John styled himself the king of Castile and León. Kenilworth was one of the most important of his thirty or more castles in England. John began building at Kenilworth between 1373 and 1380 in a style designed to reinforce his royal claims in Iberia. John constructed a grander great hall, the Strong Tower, Saintlowe Tower, the state apartments and the new kitchen complex. When not campaigning abroad, John spent much of his time at Kenilworth and Leicester, and used Kenilworth even more after 1395 when his health began to decline. In his final years, John made extensive repairs to the whole of the castle complex.
15th century.
Many castles, especially royal castles were left to decay in the 15th century; Kenilworth, however, continued to be used as a centre of choice, forming a late medieval "palace fortress". Henry IV, John of Gaunt's son, returned Kenilworth to royal ownership when he took the throne in 1399 and made extensive use of the castle. Henry V also used Kenilworth extensively, but preferred to stay in the Pleasance, the mock castle he had built on the other side of the Great Mere. According to the contemporary chronicler John Strecche, who lived at the neighbouring Kenilworth Priory, the French openly mocked Henry in 1414 by sending him a gift of tennis balls at Kenilworth. The French aim was to imply a lack of martial prowess; according to Strecche, the gift spurred Henry's decision to fight the Agincourt campaign. The account was used by Shakespeare as the basis for a scene in his play "Henry V".
English castles, including Kenilworth, did not play a decisive role during the Wars of the Roses (1455–85), which were fought primarily in the form of pitched battles between the rival factions of the Lancastrians and the Yorkists. With the mental collapse of King Henry VI, Queen Margaret used the Duchy of Lancaster lands in the Midlands, including Kenilworth, as one of her key bases of military support. Margaret removed Henry from London in 1456 for his own safety and until 1461, Henry's court divided almost all its time among Kenilworth, Leicester and Tutbury Castle for the purposes of protection. Kenilworth remained an important Lancastrian stronghold for the rest of the war, often acting as a military balance to the nearby castle of Warwick. With the victory of Henry VII at Bosworth, Kenilworth again received royal attention; Henry visited frequently and had a tennis court constructed at the castle for his use. His son, Henry VIII, decided that Kenilworth should be maintained as a royal castle. He abandoned the Pleasance and had part of the timber construction moved into the base court of the castle.
16th century.
The castle remained in royal hands until it was given to John Dudley in 1553. Dudley came to prominence under Henry VIII and became the leading political figure under Edward VI. Dudley was a patron of John Shute, an early exponent of classical architecture in England, and began the process of modernising Kenilworth. Before his execution in 1553 by Queen Mary for attempting to place Lady Jane Grey on the throne, Dudley had built the new stable block and widened the tiltyard to its current form.
Kenilworth was restored to Dudley's son, Robert, Earl of Leicester, in 1563, four years after the succession of Elizabeth I to the throne. Leicester's lands in Warwickshire were worth between £500–£700 but Leicester's power and wealth, including monopolies and grants of new lands, depended ultimately on his remaining a favourite of the queen.
Leicester continued his father's modernisation of Kenilworth, attempting to ensure that Kenilworth would attract the interest of Elizabeth during her regular tours around the country. Elizabeth visited in 1566 and 1568, by which time Leicester had commissioned the royal architect Henry Hawthorne to produce plans for a dramatic, classical extension of the south side of the inner court. In the event this proved unachievable and instead Leicester employed William Spicer to rebuild and extend the castle so as to provide modern accommodation for the royal court and symbolically boost his own claims to noble heritage. After negotiation with his tenants, Leicester also increased the size of the chase once again. The result has been termed an English "Renaissance palace".
Elizabeth viewed the partially finished results at Kenilworth in 1572, but the complete effect of Leicester's work was only apparent during the queen's last visit in 1575. Leicester was keen to impress Elizabeth in a final attempt to convince her to marry him, and no expense was spared. Elizabeth brought an entourage of thirty-one barons and four hundred staff for the royal visit that lasted an exceptional nineteen days; twenty horsemen a day arrived at the castle to communicate royal messages. Leicester entertained the Queen and much of the neighbouring region with pageants, fireworks, bear baiting, mystery plays, hunting and lavish banquets. The cost was reputed to have amounted to many thousand pounds, almost bankrupting Leicester, though it probably did not exceed £1,700 in reality. The event was considered a huge success and formed the longest stay at such a property during any of Elizabeth's tours, yet the queen did not decide to marry Leicester.
Kenilworth Castle was valued at £10,401 in 1588, when Leicester died without legitimate issue and heavily in debt. In accordance with his will, the castle passed first to his brother Ambrose, Earl of Warwick, and after the latter's death in 1590, to his illegitimate son, Sir Robert Dudley.
17th century.
Sir Robert Dudley, having tried and failed to establish his legitimacy in front of the Court of the Star Chamber, went to Italy in 1605. In the same year Sir Thomas Chaloner, governor (and from 1610 chamberlain) to James I's eldest son Prince Henry, was commissioned to oversee repairs to the castle and its grounds, including the planting of gardens, the restoration of fish-ponds and improvement to the game park. During 1611–12 Dudley arranged to sell Kenilworth Castle to Henry, by then Prince of Wales. Henry died before completing the full purchase, which was finalised by his brother, Charles, who bought out the interest of Dudley's abandoned wife, Alice Dudley. When Charles became king, he gave the castle to his wife, Henrietta Maria; he bestowed the stewardship on Robert Carey, Earl of Monmouth, and after his death gave it to Carey's sons Henry and Thomas. Kenilworth remained a popular location for both King James I and his son Charles, and accordingly was well maintained. The most famous royal visit occurred in 1624, when Ben Jonson's "The Masque of Owls at Kenilworth" was performed for Charles.
The First English Civil War broke out in 1642. During its early campaigns, Kenilworth formed a useful counterbalance to the Parliamentary stronghold of Warwick. Kenilworth was used by Charles on his advance to Edgehill in October 1642 as a base for raids on Parliamentary strongholds in the Midlands. After the battle, however, the royalist garrison was withdrawn on the approach of Lord Brooke, and the castle was then garrisoned by parliamentary forces. In April 1643 the new governor of the castle, Hastings Ingram, was arrested as a suspected Royalist double agent. By January 1645 the Parliamentary forces in Coventry had strengthened their hold on the castle, and attempts by Royalist forces to dislodge them from Warwickshire failed. Security concerns continued after the end of the First Civil War in 1646, and in 1649 Parliament ordered the slighting of Kenilworth. One wall of the great tower, various parts of the outer bailey and the battlements were destroyed, but not before the building was surveyed by the antiquarian William Dugdale, who published his results in 1656.
Colonel Joseph Hawkesworth, responsible for the implementation of the slighting, acquired the estate for himself and converted Leicester's gatehouse into a house; part of the base court was turned into a farm, and many of the remaining buildings were stripped for their materials. In 1660 Charles II was restored to the throne, and Hawkesworth was promptly evicted from Kenilworth. The Queen Mother, Henrietta Maria, briefly regained the castle, with the Earls of Monmouth acting as stewards once again, but after her death King Charles II granted the castle to Sir Edward Hyde, whom he later created Baron Hyde of Hindon and Earl of Clarendon. The ruined castle continued to be used as a farm, with the gatehouse as the principal dwelling; the King's Gate was added to the outer bailey wall during this period for the use of farm workers.
Kenilworth Castle from the south in 1649, adapted from the engraving by Wenceslaus Hollar. From left to right, the watergate; the relocated Pleasurance; the Strong Tower, Gaunt's great hall and Saintlowe Tower; the state apartments and Gaunt's Tower; the top of the great tower; Leicester's building; Leicester's gatehouse; Mortimer's tower; the Tiltyard/causeway and the Gallery Tower. In the foreground is the Great Mere.
18th and 19th centuries.
Kenilworth remained a ruin during the 18th and 19th centuries, still used as a farm but increasingly also popular as a tourist attraction. The first guidebook to the castle, "A Concise history and description of Kenilworth Castle", was printed in 1777 with many later editions following in the coming decades.
The castle's cultural prominence increased after Sir Walter Scott wrote "Kenilworth" in 1821 describing the royal visit of Queen Elizabeth. Very loosely based on the events of 1575, Scott's story reinvented aspects of the castle and its history to tell the story of "the pathetic, beautiful, undisciplined heroine Amy Robsart and the steely Elizabeth I". Although considered today as a less successful literary novel than some of his other historical works, it popularised Kenilworth Castle in the Victorian imagination as a romantic Elizabethan location. "Kenilworth" spawned "numerous stage adaptations and burlesques, at least eleven operas, popular redactions, and even a scene in a set of dioramas for home display", including Sir Arthur Sullivan's 1865 cantata "The Masque at Kenilworth".
The number of visitors increased, including Queen Victoria and Charles Dickens. Work was undertaken during the 19th century to protect the stonework from further decline, with particular efforts to remove ivy from the castle in the 1860s.
Today.
The castle remained the property of the Clarendons until 1937, when Lord Clarendon found the maintenance of the castle too expensive and sold Kenilworth to the industrialist Sir John Siddeley. Siddeley, whose tax accounting in the 1930s had been at least questionable, was keen to improve his public image and gave over the running of the castle, complete with a charitable donation, to the Commissioner of Works. In 1958 his son gave the castle itself to the town of Kenilworth and English Heritage has managed the property since 1984. The castle is classed as a Grade I listed building and as a Scheduled Monument, and is open to the public.
Between 2005–09 English Heritage attempted to restore Kenilworth's garden more closely to its Elizabethan form, using as a basis the description in the Langham letter and details from recent archaeological investigations. The reconstruction cost more than £2m and was criticised by some archaeologists as being a "matter of simulation as much as reconstruction", due to the limited amount of factual information on the nature of the original gardens. In 2008 plans were put forward to re-create and flood the original Great Mere around the castle. As well as re-creating the look of the castle it was hoped that a new mere would be part of the ongoing flood alleviation plan for the area and that the lake could be used for boating and other waterside recreations.
Kenilworth Castle viewed from the south-west, where the Great Mere used to be.

</doc>
<doc id="17086" url="http://en.wikipedia.org/wiki?curid=17086" title="Knout">
Knout

A knout is a heavy scourge-like multiple whip, usually made of a bunch of rawhide thongs attached to a long handle, sometimes with metal wire or hooks incorporated.
The English word stems from a spelling-pronunciation of a French transliteration of the Russian word кнут ("knut"), which simply means "whip".
The original.
Some claim it was a Tatar invention and was introduced into Russia in the 15th century, perhaps by Grand Duke Ivan III the Great (1462–1505). Others trace the word to Varangians and derive it from the Swedish "knutpiska", a kind of whip with "knots". Still others maintain it is of generic Germanic origin, not necessarily Scandinavian, comparing it with the German "Knute", Dutch "knoet" (both meaning knout) and with Old Norse "knutr", Anglo-Saxon "cnotta" and English "knot".
The Russian knout had different forms. One was a lash of rawhide, 40 cm long, attached to a wooden handle, 22 cm long. The lash ended in a metal ring, to which was attached a second lash as long, ending also in a ring, to which in turn was attached a few inches of hard leather ending in a beak-like hook. Another kind consisted of many thongs of skin plaited and interwoven with wire, ending in loose wired ends, like the cat-o-nine tails. 
A variation, known as the "great knout", consisted of a handle about 60 cm long, to which was fastened a flat leather thong about twice the length of the handle, terminating with a large copper or brass ring to which was affixed a strip of hide about 5 cm broad at the ring, and terminating at the end of 60 cm in a point. This was soaked in milk and dried in the sun to make it harder.
Knouts were used in Russia for flogging as formal corporal punishment of criminals and political offenders. The victim was tied to a post or on a triangle of wood and stripped, receiving the specified number of strokes on the back. A sentence of 100 or 120 lashes was equivalent to a death sentence. Even twenty lashes could maim, and with the specially extended Great Knout, twenty blows could kill, with death sometimes being attributed to the breaking of the spine.
The executioner was usually a criminal who had to pass through a probation and regular training, being let off his own penalties in return for his services. Peter the Great is traditionally accused of knouting his son Alexis to death; whoever the executioner may have been, there is little doubt that he was beaten until he died. 
Emperor Nicholas I abolished the punishment by knout in 1845, and replaced it with the "pleti", lashes with three thongs which could end in lead balls. They were later abolished throughout Russia and reserved for the penal settlements, mainly in Siberia. Prisoners transported to Siberia in the late 19th century were sometimes branded on their foreheads with irons with the letters VRNK meaning V thief, R robber, and NK punished by the Knout. This branding led to the Siberian slang word Varnok to mean either a settler or deportee.
Elsewhere and metaphoric use.
The dreaded instrument became synonymous in Western European languages with what was seen as the tyrannical cruelty of the autocratic government of Russia, much as the "sjambok" brought to mind the Apartheid government of South Africa or the bullwhip was associated with the period of slavery and Jim Crow laws in America.
The expression "under the knout" is used to designate any harsh totalitarianism, and by extension its equivalent in a private context, e.g., a grim patriarch ruling his household 'with an iron rod'. In Dutch, the image is commonly used for strict party discipline, e.g., eliminating actual debate when passing a law (compare the Whip function in English).
In The Most Dangerous Game Ivan the servant of the cossacks general was a professional knouter under the czars.

</doc>
<doc id="17102" url="http://en.wikipedia.org/wiki?curid=17102" title="Kesgrave">
Kesgrave

Kesgrave is a suburb containing two large estates in the English county of Suffolk on the eastern edge of Ipswich.
History.
Early history.
The area was recorded as "Gressgrava" in the Domesday Book, by the late 15th century its name had become Kesgrave. Kesgrave remained a small agricultural settlement with just a church, inn and a few farmsteads for over 700 years.
In 1921 the population was only 103 housed in 20 dwellings. Since then great changes have taken place.
Recent history.
By 1988 Kesgrave covered an area of more than 800 acre. 
Kesgrave parish council officially adopted the title of a town in January 2000.
Schools.
Kesgrave High School is a large 11-18 comprehensive co-educational school with nearly 2000 pupils. A study for Sustrans noted that 61% of the pupils cycled to the school. This is largely due to the installation of a large cycle lane through the local housing development and along the main road. The school actively encourages walking or cycling and provides bicycle storage facilities.
The five primary schools in the immediate vicinity of Kesgrave are Beacon Hill Primary School, Birchwood Primary School, Cedarwood Primary School the building of which was awarded a Civic Trust Award in 2003, Gorseland Primary School and Heath Primary School.
Kesgrave was home to a number of private day and boarding schools based at Kesgrave Hall

</doc>
<doc id="17166" url="http://en.wikipedia.org/wiki?curid=17166" title="Katanga Province">
Katanga Province

Katanga is one of the provinces of the Democratic Republic of the Congo. Between 1971 and 1997 (during the rule of Mobutu Sese Seko when Congo was known as Zaire), its official name was Shaba Province. Many in Katanga want the province to become an independent state in its own right and have fought for secession from the Democratic Republic of Congo.
Katanga's area is 497,000 km2, 16 times larger than Belgium. Farming and ranching are carried out on the Katanga Plateau. The eastern part of the province is a rich mining region, which supplies cobalt, copper, tin, radium, uranium, and diamonds. The region's capital, Lubumbashi, is the second largest city in the Congo.
Moïse Katumbi Chapwe, a businessman, is the governor of Katanga province. He took office on 24 February 2007.
History.
Copper mining in Katanga dates back over 1,000 years and mines in the region were producing standard sized ingots of copper for international transport by the end of the 1st Millennium AD.
In 1960, after the Democratic Republic of the Congo (then called Republic of the Congo) had received independence from Belgium, Katanga attempted to secede from the country. This was supported by Belgium but opposed by the Congolese Prime Minister Lumumba. It lead to the Katanga Crisis (or "Congo Crisis") which lasted from 1960 to 1965. The breakaway State of Katanga existed from 1960 to 1963.
Economy.
Copper mining is an important part of the economy of Katanga province. Cobalt mining by individual contractors is also prevalent. There are a number of reasons cited for the discrepancy between the vast mineral wealth of the province and the failure of the wealth to increase the overall standard of living. The local provincial budget was $440 million in 2011.
Mining.
Lubumbashi, the mining capital of the Democratic Republic of Congo, is a hub for many of the country's biggest mining companies. The Democratic Republic of Congo produces "more than 3 percent of the world’s copper and half its cobalt, most of which comes from Katanga".
Major mining concessions include Tilwezembe, Kalukundi.
Geography.
The province forms the Congolese border with Angola and Zambia. The province also borders Tanzania - although Katanga province and Tanzania do not share a land border - but the border is within Lake Tanganyika.
Katanga has a wet and dry season. Rainfall is about 49 inches per year (120 cm).
Education and medical care.
The University of Lubumbashi, located in the northern part of Lubumbashi city, is the largest university in the province and one of the largest in the country. A number of other university-level institutions exist in Lubumbashi, some public, some private: Institut Supérieur de Statistique, Institut Supérieur Pédagogique, Institut Supérieur des Études Sociales, Institut Supérieur de Commerce, Institut Supérieur des Techniques Médicales (all state-run), Université Protestante de Lubumbashi (Korean Presbyterian), Institut Supérieur Maria Malkia (Catholic), Institut Supérieur de Développement Mgr Mulolwa (Catholique), Theologicum St François de Sales (Salesian seminary), Institut Supérieur de Théologie Évangélique de Lubumbashi (Pentecostal/Anglican/Brethren), etc. Université Méthodiste au Katanga, the oldest private university-level institution in the province, is located at Mulungwishi (between Likasi and Kolwezi) but organizes its Masters in Leadership courses in Lubumbashi. The University of Kamina, the University of Kolwezi and the University of Likasi are former branches of the University of Lubumbashi, which continues to have branches in some locations such as Kalemie.
TESOL, the English Language School of Lubumbashi, is a secondary school that serves the expatriate community. It was founded in 1987 on the grounds of the French School, Lycée Français Blaise Pascal, which suspended operations in 1991 with a new French School starting in 2009.
There are French, Belgian and Greek schools in Lubumbashi sponsored by the respective embassies.
The Jason Sendwe Hospital is the largest hospital in the province, located in Lubumbashi. The Afia (Don Bosco) and Vie & Santé hospitals are among the best-equipped and staffed. The University of Lubumbashi maintains a small teaching hospital in the center of Lubumbashi.
Katanga province has the highest rate of infant mortality in the world, with 184 of 1000 babies born expected to die before the age of five.
Transportation.
The Congo Railway provides Katanga Province with limited railway service centered around Lubumbashi. Reliability is limited. Lubumbashi International Airport is located northeast of Lubumbashi. In April 2014, a train derailment killed 63 people.
Media.
Katanga province is served by television broadcasts. Radio-Télévision Nationale Congolaise (RTNC) has a transmitter in Lubumbashi that re-transmits the signal from Kinshasa. In 2005, new television broadcasts by Radio Mwangaza began in Lubumbashi.

</doc>
<doc id="17216" url="http://en.wikipedia.org/wiki?curid=17216" title="Knowledge Query and Manipulation Language">
Knowledge Query and Manipulation Language

The Knowledge Query and Manipulation Language, or KQML, is a language
and protocol for communication among software agents and knowledge-based systems. It was
developed in the early 1990s part of the DARPA knowledge Sharing Effort, which was aimed at developing techniques for building large-scale knowledge bases which are
shareable and reusable. While originally conceived of as an interface to knowledge based systems, it was soon repurposed as an Agent communication language.
Work on KQML was led by Tim Finin of the University of Maryland, Baltimore County and Jay Weber of EITech and involved contributions from many researchers.
The KQML message format and protocol can be used to interact with an intelligent system, either by an application program, or by another intelligent system. KQML's "performatives" are operations that agents perform on each other's knowledge and goal stores. Higher-level interactions such as contract nets and negotiation are built using these. KQML's "communication facilitators" coordinate the interactions of other agents to support knowledge sharing. 
Experimental prototype systems support concurrent engineering, intelligent design, intelligent planning, and scheduling.
KQML is superseded by FIPA-ACL.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="17217" url="http://en.wikipedia.org/wiki?curid=17217" title="KR">
KR

KR or Kr may refer to:

</doc>
<doc id="17226" url="http://en.wikipedia.org/wiki?curid=17226" title="Kremvax">
Kremvax

Kremvax was originally a fictitious Usenet site at the Kremlin, named like the then large number of Usenet VAXen with names of the form foovax. Kremvax was announced on April 1, 1984 in a posting ostensibly originated there by Soviet leader Konstantin Chernenko. The posting was actually forged by Piet Beertema of CWI (in Amsterdam) as an April Fool's joke. Other fictitious sites mentioned in the hoax were moskvax and kgbvax. The actual origin of the email was mcvax, one of the first European sites on the internet.
Six years later Usenet was joined by demos.su, the first genuine site based in Moscow. Some readers needed convincing that the postings from it were not just another prank. Vadim Antonov, the senior programmer at Demos and the major poster from there until mid-1991, was quite aware of all this, and referred to it frequently in his own postings. Antonov later arranged to have the domain's gateway site named kremvax.demos.su, turning fiction into truth and, according to one account, "demonstrating that the hackish sense of humor transcends cultural barriers".
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="17231" url="http://en.wikipedia.org/wiki?curid=17231" title="KSL">
KSL

KSL may refer to:

</doc>
<doc id="17248" url="http://en.wikipedia.org/wiki?curid=17248" title="Keflavík">
Keflavík

Keflavík (pronounced ], meaning "Driftwood Bay") is a town in the Reykjanes region in southwest Iceland. In 2009 its population was of 8,169.
In 1995 it merged with Njarðvík and Hafnir to form a municipality called Reykjanesbær with a population of 13,971 (January 2011).
History.
Founded in the 16th century, Keflavík developed on account of its fishing and fish processing industry, founded by Scottish entrepreneurs and engineers. Later its growth continued from flight operations at the Keflavík International Airport which was built by the US during the 1940s. The airport used to hold a significant NATO military base and was a vital pre-jet refueling stop for trans-Atlantic commercial air traffic. It now serves as Iceland's main international hub.
During World War II the military airfield served as a refueling and transit depot. During the Cold War, Naval Air Station Keflavik played an important role in monitoring marine and submarine traffic from the Norwegian and Greenland Seas into the Atlantic Ocean. Forces from the United States Air Force were added to provide radar monitoring, fighter intercept, in-flight refueling, and aerial/marine rescue. After the collapse of the Soviet Union, however, the base's role was cast into doubt. The base officially closed on 30 September 2006, when the United States withdrew the remaining 30 military personnel.
In Iceland, Keflavík was renowned as a rich source of musicians during the 1960s and 70s, and is therefore also known as "bítlabærinn" or "The Beatle Town".
Geography.
The local geography is dominated by fields of basalt rubble, interspersed with a few hardy plants and mosses. On a clear day, one can see Snæfellsjökull across the bay, some 115 km away.
Climate.
The climate of Keflavík is subpolar oceanic with cool summers and moderately cold winters. The wettest month on average is October and the driest month is July. Winter high temperatures average above the freezing mark, and summer high temperatures are cool to mild. The warmest month on average is July with an average high of 13 C (55 F) and the coldest is January with an average high of 2 C (35 F).
Sport.
The town is represented in Icelandic football by Keflavík Football Club.
In popular culture.
The former NATO military base Naval Air Station Keflavik is used as a setting for an important story line in Tom Clancy's novel "Red Storm Rising". Clancy described the base, the geography, local flora, and the station equipment.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="17270" url="http://en.wikipedia.org/wiki?curid=17270" title="Kanem-Bornu Empire">
Kanem-Bornu Empire

The Kanem-Bornu Empire was an empire that existed in modern Chad and Nigeria. It was known to the Arabian geographers as the Kanem Empire from the 9th century AD onward and lasted as the independent kingdom of Bornu until 1900. At its height it encompassed an area covering not only much of Chad, but also parts of modern southern Libya, eastern Niger, northeastern Nigeria and northern Cameroon. The history of the Empire in the "longue durée" is mainly known from the Royal Chronicle or "Girgam" discovered in 1851 by the German traveller Heinrich Barth.
Theories on the origin of Kanem.
Kanem was located at the southern end of the trans-Saharan trade route between Tripoli and the region of Lake Chad. Besides its urban elite it included a confederation of nomadic peoples who spoke languages of the Teda–Daza (Toubou) group.
Founding by immigrants c. 600 BC.
The origins of Kanem Empire are very unclear. Until today, historiographical debates oppose the followers of a foundation of Kanem-Bornu by populations from the ancient Near East and followers of a more local development. Some researches try to connect the creation of Kanem-Bornu with exodus from the collapsed Assyrian Empire c. 600 BC to the northeast of Lake Chad. The intensity of scholar discussions around this theory proves that the question hasn't been solved yet, and we must be very cautious concerning early formation of Kanem-Bornu.
Founding by local Kanembu (Dugua) c. 700 AD.
According to a more accepted theory, the empire of Kanem began forming around 700 AD under the nomadic Tebu-speaking Kanembu. The Kanembu were supposedly forced southwest towards the fertile lands around Lake Chad by political pressure and desiccation in their former range. The area already possessed independent, walled city-states belonging to the Sao culture. Under the leadership of the Duguwa dynasty, the Kanembu would eventually dominate the Sao, but not before adopting many of their customs. War between the two continued up to the late 16th century.
Islamization c. 1068 AD.
Rise of the Sayfawa dynasty.
The major factor that influenced the later history of the state of Kanem was the early penetration of Islam. North African traders, Berbers and Arabs, brought the new religion. Towards 1068, Hummay, a member of the Sayfawa establishment, who was already a Muslim, discarded the last Duguwa King Selma from power and thus established the new dynasty of the Sayfawa. Islam offered the Sayfawa rulers the advantage of new ideas from Arabia and the Mediterranean world, as well as literacy in administration. But many people resisted the new religion favouring traditional beliefs and practices. When Hummay had assumed power on the basis of his strong Islamic following, for example, it is believed that the Duguwa/Kanembu began some kind of internal opposition. This pattern of conflict and compromise with Islam occurs repeatedly in Chadian history.
Foundation of the new capital Njimi.
When the ruling dynasty changed, the royal establishment abandoned its capital of Manan and settled in the new capital Njimi further south of Kanem (the word for "south" in the Teda language). By the 13th century, Kanem's rule expanded. At the same time, the Kanembu people drew closer to the new rulers and increased the growing population in the new capital of Njimi. Even though the Kanembu became the main power-base of the Sayfuwa, Kanem's rulers continued to travel frequently throughout the kingdom and especially towards Bornu, west of lake Chad. Herders and farmers alike recognized the government's power and acknowledged their allegiance by paying tribute.
Expansion to Bornu.
"Mai" Dunama Dabbalemi.
Kanem's expansion peaked during the long and energetic reign of "Mai" Dunama Dabbalemi (ca. 1203–1242), also of the Sayfawa dynasty. Dabbalemi initiated diplomatic exchanges with sultans in North Africa and apparently arranged for the establishment of a special hostel in Cairo to facilitate pilgrimages to Mecca. During his reign, he declared "jihad" against the surrounding tribes and initiated an extended period of conquest. After consolidating their territory around Lake Chad the Fezzan region (in present-day Libya) fell under Kanem's authority, and the empire's influence extended westward to Kano (in present-day Nigeria) and thus included Bornu, eastward to Ouaddaï, and southward to the Adamawa grasslands (in present-day Cameroon). Portraying these boundaries on maps can be misleading, however, because the degree of control extended in ever-weakening gradations from the core of the empire around Njimi to remote peripheries, from which allegiance and tribute were usually only symbolic. Moreover, cartographic lines are static and misrepresent the mobility inherent in nomadism and migration, which were common. The loyalty of peoples and their leaders was more important in governance than the physical control of territory.
Dabbalemi devised a system to reward military commanders with authority over the people they conquered. This system, however, tempted military officers to pass their positions to their sons, thus transforming the office from one based on achievement and loyalty to the "mai" into one based on hereditary nobility. Dabbalemi was able to suppress this tendency, but after his death, dissension among his sons weakened the Sayfawa Dynasty. Dynastic feuds degenerated into civil war, and Kanem's outlying peoples soon ceased paying tribute.
Shift of the Sayfuwa court from Kanem to Bornu.
By the end of the 14th century, internal struggles and external attacks had torn Kanem apart. Between 1359 and 1383, seven "mais" reigned, but Bulala invaders (from the area around Lake Fitri to the east) killed five of them. This proliferation of "mais" resulted in numerous claimants to the throne and led to a series of internecine wars. Finally, around 1380 the Bulala forced "Mai" Umar Idrismi to abandon Njimi and move the Kanembu people to Bornu on the western edge of Lake Chad. Over time, the intermarriage of the Kanembu and Bornu peoples created a new people and language, the Kanuri.
But even in Bornu, the Sayfawa Dynasty's troubles persisted. During the first three-quarters of the 15th century, for example, fifteen "mais" occupied the throne. Then, around 1460 "Mai" Ali Dunamami defeated his rivals and began the consolidation of Bornu. He built a fortified capital at Ngazargamu, to the west of Lake Chad (in present-day Niger), the first permanent home a Sayfawa "mai" had enjoyed in a century. So successful was the Sayfawa rejuvenation that by the early 16th century "Mai" Idris Katakarmabe (1487–1509) was able to defeat the Bulala and retake Njimi, the former capital. The empire's leaders, however, remained at Ngazargamu because its lands were more productive agriculturally and better suited to the raising of cattle.
Kanem-Bornu peaked during the reign of the outstanding statesman "Mai" Idris Aluma (c. 1564–1596). Aluma is remembered for his military skills, administrative reforms, and Islamic piety. His main adversaries were the Hausa to the west, the Tuareg and Toubou to the north, and the Bulala to the east. One epic poem extols his victories in 330 wars and more than 1,000 battles. His innovations included the employment of fixed military camps (with walls); permanent sieges and "scorched earth" tactics, where soldiers burned everything in their path; armored horses and riders; and the use of Berber camelry, Kotoko boatmen, and iron-helmeted musketeers trained by Turkish military advisers. His active diplomacy featured relations with Tripoli, Egypt, and the Ottoman Empire, which sent a 200-member ambassadorial party across the desert to Aluma's court at Ngazargamu. Aluma also signed what was probably the first written treaty or cease-fire in Chadian history.
Aluma introduced a number of legal and administrative reforms based on his religious beliefs and Islamic law (sharia). He sponsored the construction of numerous mosques and made a pilgrimage to Mecca, where he arranged for the establishment of a hostel to be used by pilgrims from his empire. As with other dynamic politicians, Aluma's reformist goals led him to seek loyal and competent advisers and allies, and he frequently relied on slaves who had been educated in noble homes. Aluma regularly sought advice from a council composed of heads of the most important clans. He required major political figures to live at the court, and he reinforced political alliances through appropriate marriages (Aluma himself was the son of a Kanuri father and a Bulala mother).
Kanem-Bornu under Aluma was strong and wealthy. Government revenue came from tribute (or booty, if the recalcitrant people had to be conquered), sales of slaves, and duties on and participation in trans-Saharan trade. Unlike West Africa, the Chadian region did not have gold. Still, it was central to one of the most convenient trans-Saharan routes. Between Lake Chad and Fezzan lay a sequence of well-spaced wells and oases, and from Fezzan there were easy connections to North Africa and the Mediterranean Sea. Many products were sent north, including natron (sodium carbonate), cotton, kola nuts, ivory, ostrich feathers, perfume, wax, and hides, but the most important of all were slaves. Imports included salt, horses, silks, glass, muskets, and copper.
Aluma took a keen interest in trade and other economic matters. He is credited with having the roads cleared, designing better boats for Lake Chad, introducing standard units of measure for grain, and moving farmers into new lands. In addition, he improved the ease and security of transit through the empire with the goal of making it so safe that "a lone woman clad in gold might walk with none to fear but God."
Decline and fall of the Bornu Empire.
The administrative reforms and military brilliance of Aluma sustained the empire until the mid-17th century, when its power began to fade. By the late 18th century, Bornu rule extended only westward, into the land of the Hausa. Around that time, Fulani people, invading from the west, were able to make major inroads into Bornu. By the early 19th century, Kanem-Bornu was clearly an empire in decline, and in 1808 Fulani warriors conquered Ngazargamu. Usman dan Fodio led the Fulani thrust and proclaimed a jihad (holy war) on the irreligious Muslims of the area. His campaign eventually affected Kanem-Bornu and inspired a trend toward Islamic orthodoxy. But Muhammad al-Kanem contested the Fulani advance. Kanem was a Muslim scholar and non-Sayfawa warlord who had put together an alliance of Shuwa Arabs, Kanembu, and other seminomadic peoples. He eventually built in 1814 a capital at Kukawa (in present-day Nigeria). Sayfawa "mais" remained titular monarchs until 1846. In that year, the last "mai", in league with Ouaddai tribesmen, precipitated a civil war. It was at that point that Kanem's son, Umar, became king, thus ending one of the longest dynastic reigns in regional history.
Although the dynasty ended, the kingdom of Kanem-Bornu survived. But Umar, who eschewed the title "mai" for the simpler designation "shehu" (from the Arabic "shaykh"), could not match his father's vitality and gradually allowed the kingdom to be ruled by advisers ("wazirs"). Bornu began to decline, as a result of administrative disorganization, regional particularism, and attacks by the militant Ouaddai Empire to the east. The decline continued under Umar's sons, and in 1893 Rabih az-Zubayr, leading an invading army from eastern Sudan, conquered Bornu. He was defeated by French soldiers in 1900.

</doc>
<doc id="17274" url="http://en.wikipedia.org/wiki?curid=17274" title="Keeshond">
Keeshond

The Keeshond ( ; plural: Keeshonden) is a medium-sized dog with a plush two-layer coat of silver and black fur with a ruff and a curled tail. It originated in Germany, and its closest relatives are the German spitzes such as the Großspitz, Mittelspitz, and Kleinspitz or Pomeranian. Originally called the German Spitz, more specifically the Wolfspitz, the name was officially changed to Keeshond, in 1926 in England, where it had been known as the Dutch Barge Dog.
Description.
Appearance.
A member of the spitz group of dogs, the Keeshond in American Kennel Club (AKC) standard is 17 in to 18 in tall and 19.25 in ± 2.4 in in the Fédération Cynologique Internationale (FCI) standard and weighs 30 lb to 40 lb. Sturdily built, they have a typical spitz appearance, neither coarse nor refined. They have a wedge-shaped head, a medium-length muzzle with a definite stop, small pointed ears, and an expressive face. The tail is tightly curled and, in profile, should be carried such that it is indistinguishable from the compact body of the dog.
Coat.
Like all spitz-type dogs, the Keeshond has a dense double coat, with a thick ruff around the neck. Typically, the males of this breed will have a thicker, more pronounced ruff than the females. The body should be abundantly covered with long, straight, harsh hair standing well out from a thick, downy undercoat. The hair on the legs should be smooth and short, except for a feathering on the front legs and "trousers", as previously described, on the hind legs. The hair on the tail should be profuse, forming a rich plume. Head, including muzzle, skull, and ears, should be covered with smooth, soft, short hair—velvety in texture on the ears. Coat must not part down the back.
Coat care requires line brushing on a fairly regular basis. The Keeshond typically 'blows' its undercoat once a year for males, twice a year for females. During this time, the loss of coat is excessive and their guard hairs will lie flat to their back. It usually takes 2 weeks for the 'blow' to complete, in order for new undercoat to begin growing back in. A Keeshond should never be shaved, as their undercoat provides a natural barrier against heat and cold. Keeping their coat in good condition will allow efficient insulation in both hot and cold weather.
Color.
The colour should be a mixture of grey and black. The undercoat should be very pale grey or cream (not tawny). The hair of the outer coat is black tipped, the length of the black tips producing the characteristic shading of colour. The colour may vary from light to dark, but any pronounced deviation from the grey colour is not permissible. The plume of the tail should be very light grey when curled on back and the tip of the tail should be black. Legs and feet should be cream. Ears should be very dark - almost black.
Shoulder line markings (light grey) should be well defined. The colour of the ruff and "trousers" is generally lighter than that of the body. "Spectacles" and shadings, as later described, are characteristic of the breed and must be present to some degree. There should be no pronounced white markings.
According to the American Kennel Club breed standard, the legs and feet are to be cream; feet that are totally black or white are severe faults. Black markings more than halfway down the foreleg, except for pencilling, are faulted.
The other important marking is the "spectacles," a delicate dark line running from the outer corner of each eye toward the lower corner of each ear, which, coupled with markings forming short eyebrows, is necessary for the distinct expressive look of the breed. All markings should be clear, not muddled or broken. Absence of the spectacles is considered a serious fault. The eyes should be dark brown, almond-shaped with black eye rims.
Ears should be small, dark, triangular, and erect.
Temperament.
Keeshonden tend to be very playful, with quick reflexes and strong jumping ability. They are quick learners and eager to please. Because Keeshonden are quick learners, they also very quickly learn things their humans did not intend to teach them. However, Keeshonden make excellent agility and obedience dogs. So amenable to proper training is this bright, sturdy dog that Keeshonden have been successfully trained to serve as guide dogs for the blind; only their lack of size has prevented them from being more widely used in this role.
They love children and are excellent family dogs, preferring to be close to their humans whenever possible. They generally get along with other dogs as well and will enjoy a good chase around the yard. Keeshonden are very intuitive and empathetic and are often used as comfort dogs. Most notably, at least one Keeshond, Tikva, was at Ground Zero on 9/11 to help comfort the rescue workers. The breed has a tendency to become especially clingy towards their owners, more so than most other breeds. If their owner is out, or in another room behind a closed door, they may sit, waiting for their owner to reappear, even if there are other people nearby. Many have been referred to as their "owner's shadow," or "velcro dogs" .
They are known by their loud, distinctive bark. Throughout the centuries, the Keeshond has been very popular as a watch dog on barges on canals in the Netherlands and middle Europe. This trait is evident to this day, and they are alert dogs that warn their owners of any new visitors. Although loud and alert, Keeshonden are not aggressive towards visitors. They generally welcome visitors affectionately once their family has accepted them. Unfortunately, barking may become a problem if not properly handled. Keeshonden that are kept in a yard, and not allowed to be with their humans, are unhappy and often become nuisance barkers.
Training.
The Keeshond is very bright in work and obedience. The Keeshond ranks 16th in Stanley Coren's The Intelligence of Dogs, being of excellent working/obedience intelligence. This intelligence makes a Keeshond a good choice for the dog owner who is willing to help a dog learn the right lessons, but also entails added responsibility.
While affectionate, Keeshonden are good for the inexperienced trainer. Consistency and fairness are needed; and, while most dogs need a structured environment, it is especially necessary with a Keeshond. Like most of the independent-minded spitz breeds, Keeshonden respond poorly to heavy-handed or forceful training methods.
Many behavioral problems with Keeshonden stem from these intelligent dogs inventing their own activities (often destructive ones, like digging and chewing) out of boredom. They need daily contact with their owners and lots of activity to remain happy. The Keeshonden does not live happily alone in a kennel or backyard.
Keeshonden can also be timid dogs. It is important to train them to respect, but not fear, their owners and family. Keeshonden want to please, so harsh punishment is not necessary when the dog does not obey as quickly as desired. They like to spend time with their owners and love attention.
Health.
Keeshonden are generally a very healthy breed. Though congenital health issues are not common, the conditions which have been known to sometimes occur in Keeshonden are hip dysplasia, luxating patellas (trick knee), epilepsy, Cushing's disease, diabetes, primary hyperparathyroidism, and hypothyroidism. Von Willebrand's disease has been known in Keeshonden but is very rare. An accurate (or PHPT) has recently been developed at Cornell University. As with any breed, it is important when buying a puppy to make sure that both parents have been tested and certified free from inherited problems. Test results may be obtained from the breeder, and directly from the .
Keeshonds in a UK Kennel Club survey had a median lifespan of 12 years 2 months. 1 in 4 died of old age, at an average of 14–15 years.
Grooming.
Because of their double coat, Keeshonden need regular brushing; an hour a week will keep the dog comfortable and handsome. The Keeshond's coat sheds dirt when dry, and the breed is not prone to doggy odor, so frequent bathing is unnecessary and undesirable. The coat acts as insulation and protects the dog from sunburn and insects, so shaving or clipping is not desirable. The coat also loses its distinct color as the black tipping on the hairs will be shorn off. If frequent brushing is too much effort, it is better to choose another breed rather than clip the Keeshond short.
History.
The Keeshond was named after the 18th-century Dutch Patriot, Cornelis (Kees) de Gyselaer (spelled 'Gijzelaar' in Modern Dutch), leader of the rebellion against the House of Orange. The dog became the rebels' symbol; and, when the House of Orange returned to power, this breed almost disappeared. The word 'keeshond' is a compound word: 'Kees' is a nickname for Cornelius (de Gyselaer), and 'hond' is the Dutch word for dog. In the Netherlands, "keeshond" is the term for German Spitzes that encompass them all from the toy or dwarf (Pomeranian) to the Wolfspitz (Keeshond). The sole difference among the German Spitzes is their coloring and size guidelines. Although many American references point to the Keeshond as we know it originating in the Netherlands, the breed is cited as being part of the German Spitz family, originating in Germany along with the Pomeranian (toy or dwarf German Spitz) and American Eskimo dog (small or standard German Spitz) according to the FCI.
The first standard for "Wolfspitz" was posted at the Dog Show of 1880 in Berlin. The Club for German Spitzes was founded in 1899. The German standard was revised in 1901 to specify the characteristic color that we know today, "silver grey tipped with black". In the late 19th century the "Overweight Pomeranian", a white German Spitz and most likely a Standard German Spitz, was shown in the British Kennel Club. The "Overweight Pomeranian" was no longer recognized by the British Kennel Club in 1915. In the 1920s, Baroness van Hardenbroeck took an interest in the breed and began to build it up again. The Nederlandse Keeshond Club was formed in 1924. The Dutch Barge Dog Club of England was formed in 1925 by Mrs. Wingfield-Digby and accepted into the British Kennel Club in 1926, when the breed and the club were renamed to Keeshond.
Carl Hinderer is credited with bringing his Schloss Adelsburg Kennel, which he founded in 1922 in Germany, with him to America in 1923. His German Champion Wolfspitz followed him two by two in 1926. At that time, less than ten years after World War I, Germany was not regarded fondly in England and America; and the Wolfspitz/Keeshond was not recognized by the AKC. Consequently, Carl had to register each puppy with his club in Germany. Despite this, Carl joined the Maryland KC and attended local shows.
Carl regularly wrote to the AKC, including the New York headquarters, to promote the Wolfspitz. While going through New York on his way to Germany in 1930, Carl visited the AKC offices and presented Wachter, his Germany champion, to AKC President, Dr. DeMond, who promptly agreed to start the recognition process, with some caveats including changing the name to Keeshond, and asked Carl to bring back all the relevant data from Germany. Carl also translated the German standard to English for the AKC. The Keeshond was accepted for AKC registration in 1930.
Despite intense lobbying the FCI would not accept the Keeshond as a separate breed since it viewed the Wolfspitz and Keeshond as identical. In 1997, the German Spitz Club updated its standard so that the typically smaller Keeshond preferred in America and other English-speaking countries could be included. This greatly expanded the gene pool and unified the standard internationally for the first time. Now bred for many generations as a companion dog, the Keeshond easily becomes a loving family member.
As a result of the breed's history and friendly disposition, Keeshonden are sometimes referred to as "The Smiling Dutchman".
Miscellaneous.
Pronunciation.
The proper pronunciation is ("case-hawnd") or ("kayz-hawnd"). The plural keeshonden is pronounced or .
Colored Keeshonden.
Historically, Keeshonden being part of the German Spitz family had been interbred with their smaller brethren (small, standard, and dwarf German spitzes) and came in several colors—white, black, red, orange, orange-shaded white (also called orange and cream), and silver gray. Originally, like the other German spitzes, many colors, including piebalds, were allowed, but as time progressed, only the silver-grey and cream (wolf-gray) color was finally established into the Wolfspitz type. 
While other-colored Keeshonden can have terrific conformation, they are not allowed to be shown in the show ring. Colored Keeshonden are considered "pet quality".
The appearance of oddly-colored Kees in otherwise wolf-gray litters has caused research into the early history of Keeshond coat colors. Because of this, some breeders wonder whether the Keeshond should be bred for colors other than grey. There are many bloodlines carrying the colored gene, and rather than examples of mixed breeding, colors are legitimate throwbacks to an earlier era of the breed.
No one knows the exact number of colored Keeshonden born in the United States. Incorrect or incomplete documentation make it impossible to determine how many colored Keeshonden, and of which colors, have been born in the United States.

</doc>
<doc id="17442" url="http://en.wikipedia.org/wiki?curid=17442" title="Katsuhiro Otomo">
Katsuhiro Otomo

Katsuhiro Otomo (大友 克洋, Ōtomo Katsuhiro, born April 14, 1954) is a Japanese manga artist, screenwriter and film director. He is best known as the creator of the manga "Akira" and its animated film adaptation. He was decorated a "Chevalier" of the French Ordre des Arts et des Lettres in 2005, promoted to "Officier" of the order in 2014, became the fourth manga artist ever inducted into the American Eisner Award Hall of Fame in 2012, and was awarded the Purple Medal of Honor from the Japanese government in 2013. Otomo later received the Winsor McCay Award at the 41st Annie Awards in 2014 and the 2016 Grand Prix de la ville d'Angoulême, the first manga artist to receive the award.
Early life.
Katsuhiro Otomo was born in Tome, Miyagi Prefecture and grew up in Tome-gun. While he was in high school he was fascinated with movies, often taking a three-hour train ride during school holidays just to see them. In 1973 he graduated high school and left Miyagi, heading to Tokyo with the hopes of becoming a manga artist. On October 4, 1973, he published his first work, a manga adaptation of Prosper Merimee's short novel "Mateo Falcone", titled "A Gun Report".
Career.
In 1979, after writing multiple short-stories for the magazine "Action", Otomo created his first science-fiction work, titled "Fireball". Although the manga was never completed, it is regarded as a milestone in Otomo's career as it contained many of the same themes he would explore in his later, more successful manga such as "". "Dōmu" began serialization in January 1980 and ran for two years until completed. In 1983, it was published in book form and would win the Nihon SF Taisho Award, the Japanese equivalent to the Nebula Award.
In 1982, Otomo made his anime debut, working as character designer for the animated film "Harmagedon". The next year, Otomo began work on a manga which would become his most acclaimed and famous work: "Akira". It took eight years to complete and would eventually culminate in 2000 pages of artwork. In 1987, Otomo continued working in anime, directing an animated work for the first time: a segment, which he also wrote the screenplay and drew animation for, in the anthology feature "Neo Tokyo". He followed this up with two segments in another anthology, "Robot Carnival".
While the serialization of Akira was taking place, Otomo decided to animate it into a feature film, although the comic was yet to be finished. In 1988, the animated film "Akira" was released. In 1990, Otomo did a brief interview with MTV for a general segment on the Japanese manga scene at the time.
Otomo has recently worked extensively with noted studio Sunrise. The studio has animated and produced his recent projects, including the 2004 feature film "Steamboy", 2006's "Freedom Project" and his latest project, "", released in 2007.
Reports have suggested that Otomo will be the executive producer of the live action adaptation of his manga series "Akira".
In a 2012 interview, Otomo said he will start a new manga series, set during Japan's Meiji period (late 1800s early 1900s). It will be his first long-form work since "Akira".
In 2013, Otomo released his newest film in over 9 years since "Steamboy", called "Short Peace", an anthology consisting on 4 shorts: His own short based on one of his stories called "Combustible", a tragic love story set in the Edo period, "Tsukumo", directed by Shuhei Morita in which everyday tools metamorphose into supernatural things, "Gambo", directed by Hiroaki Ando, which features a battle between an oni goblin and a polar bear, and "Buki yo Saraba" directed by Hajime Katoki, depicting a battle in a ruined Tokyo. "Combustible" won the Grand Prize of the Cultural Affairs Agency's Japan Media Arts Festival Animation awards in 2012, and it was shortlisted for the 2013 Best Animated Short at the 85th Academy Awards, but it failed to get nominated. "Tsukumo", under the title "Possessions", would become nominated for the 2014 Best Animated Short at the 86th Academy Awards.
Filmography.
Additional work.
Besides his own animation, Otomo has contributed his art to anime as varied as the Genma Taisen movie, Harmagedon, the Crusher Joe movie, a special Gundam anniversary short film, and Space Dandy episode 22.
References.
</dl>

</doc>
<doc id="17450" url="http://en.wikipedia.org/wiki?curid=17450" title="Kirsten Dunst">
Kirsten Dunst

Kirsten Caroline Dunst (; born April 30, 1982) is a German-American actress, singer, model and director. She made her film debut in Woody Allen's short film "Oedipus Wrecks" for the anthology film "New York Stories" (1989). At the age of twelve, Dunst gained widespread recognition as vampire Claudia in "" (1994), a role for which she was nominated for a Golden Globe Award for Best Supporting Actress. She appeared in "Little Women" the same year and in "Jumanji" the following year. After a recurring role in the NBC medical drama "ER" (1996–97) as Charlie Chemingo and co-starring in films such as "Wag the Dog" (1997), "Small Soldiers" (1998) and "The Virgin Suicides" (1999), Dunst transitioned into romantic comedies and comedy-dramas, starring in "Drop Dead Gorgeous" (1999), "Bring It On" (2000), "Get Over It" and "Crazy/Beautiful" (both released in 2001).
Dunst achieved international fame for her portrayal of Mary Jane Watson in Sam Raimi's "Spider-Man" trilogy (2002–07). Since then, her films have included the romantic comedy "Wimbledon" (2004), the science fiction romantic comedy-drama "Eternal Sunshine of the Spotless Mind" (2004) and Cameron Crowe's romantic tragicomedy "Elizabethtown" (2005). She played the title role in Sofia Coppola's biographical film "Marie Antoinette" (2006) and starred in the comedy film "How to Lose Friends & Alienate People" (2008). She won the Best Actress Award at the Cannes Film Festival and the Saturn Award for Best Actress for her performance in Lars von Trier's "Melancholia" (2011).
In 2001, Dunst made her singing debut in the film "Get Over It", in which she performed two songs. She also sang the jazz song "After You've Gone" for the end credits of the film "The Cat's Meow" (2001).
Early life.
Dunst was born in Point Pleasant, New Jersey, to Inez (née Rupprecht) and Klaus Dunst. She has a younger brother, Christian. Her father worked as a medical services executive, and her mother worked for Lufthansa as a flight attendant and was an artist and one-time gallery owner. Dunst's father is German, originally from Hamburg, and Dunst's mother, who was born in New Jersey, is of German and Swedish descent.
Until the age of eleven, Dunst lived in Brick Township, New Jersey, where she attended Ranney School. In 1993, her parents separated, and she subsequently moved with her mother and brother to Los Angeles, California, where she attended Laurel Hall School in North Hollywood. In 1995, her mother filed for divorce. The following year Dunst began attending Notre Dame High School, a private Roman Catholic high school in Los Angeles.
After graduating from Notre Dame High School in 2000, Dunst continued the acting career that she had begun. As a teenager, she found it difficult to deal with her rising fame, and for a period she blamed her mother for pushing her into acting as a child. However, she later expressed that her mother "always had the best intentions". When asked if she had any regrets about the way she spent her childhood, Dunst said: "Well, it's not a natural way to grow up, but it's the way I grew up and I wouldn't change it. I have my stuff to work out ... I don't think anybody can sit around and say, 'My life is more screwed up than yours.' Everybody has their issues."
Career.
Early work.
Dunst began her career when she was three years old as a child fashion model in television commercials. She was signed with Ford Models and Elite Model Management. At the age of six, she made her film debut in a minor role in Woody Allen's short film "Oedipus Wrecks" that was released as one-third of the anthology film "New York Stories" (1989). Soon after, she made her feature film debut with Tom Hanks in the comedy-drama "The Bonfire of the Vanities" (1990) based on Tom Wolfe's novel of the same name, where she played the daughter of Hanks' character. In 1993, Dunst guest-starred on the science fiction drama "" in the season seven episode titled "Dark Page" as Hedril.
Critical success.
The breakthrough role in Dunst's career came in 1994 in the romantic horror film "" with Tom Cruise and Brad Pitt, based on Anne Rice's novel, in which she played Claudia, the child vampire who is a surrogate daughter to Cruise and Pitt's characters. The film received ambivalent reviews, but many film critics complimented Dunst's performance. Roger Ebert commented that Dunst's creation of the child vampire Claudia was one of the "creepier" aspects of the film, and mentioned her ability to convey the impression of great age inside apparent youth. Todd McCarthy in "Variety" noted that Dunst was "just right" for the family. The film featured a scene in which Dunst shared her first on-screen kiss with Pitt, who was eighteen years her senior. In an interview with "Interview" magazine, she revealed, while questioned about her kissing scene with Pitt, that kissing him had made her feel uncomfortable: "I thought it was gross, that Brad had cooties. I mean, I was 10." Her performance earned her the MTV Movie Award for Best Breakthrough Performance, the Saturn Award for Best Young Actress, and her first Golden Globe Award nomination.
Later in 1994, Dunst co-starred in the drama film "Little Women" opposite Winona Ryder and Claire Danes. The film received favorable reviews. Critic Janet Maslin of "The New York Times" wrote that the film was the greatest adaptation of the novel and remarked on Dunst's performance, "The perfect contrast to take-charge Jo comes from Kirsten Dunst's scene-stealing Amy, whose vanity and twinkling mischief make so much more sense coming from an 11-year-old vixen than they did from grown-up Joan Bennett in 1933. Ms. Dunst, also scarily effective as the baby bloodsucker of "Interview With the Vampire," is a little vamp with a big future."
In 1995, Dunst co-starred in the fantasy adventure film "Jumanji", loosely based on Chris Van Allsburg's 1981 book of the same name. The story is about a supernatural and ominous board game which makes animals and other jungle hazards appear upon each roll of the dice. She was part of an ensemble cast that included Robin Williams, Bonnie Hunt, and David Alan Grier. The movie grossed $262 million worldwide. That year, and again in 2002, she was named one of "People" magazine's 50 Most Beautiful People. From 1996 to 1997, Dunst had a recurring role in the third season of the NBC medical drama "ER". She played Charlie Chiemingo, a child prostitute taken under the guidance of the pediatrician Dr. Doug Ross, played by George Clooney. In 1997, she voiced Young Anastasia in the animated musical film "Anastasia". Also in 1997, Dunst co-starred in the black comedy film "Wag the Dog", opposite Robert De Niro and Dustin Hoffman. The following year she voiced the title character, Kiki, a thirteen-year-old apprentice witch who leaves her home village to spend a year on her own, in the anime movie "Kiki's Delivery Service" (1998).
Dunst was offered the role of Angela in the 1999 drama film "American Beauty", but turned it down because she did not want to appear in the film's suggestive sexual scenes or kiss the film's star Kevin Spacey. She later explained: "When I read it, I was 15 and I don't think I was mature enough to understand the script's material." That same year, she co-starred in the comedy film "Dick", opposite Michelle Williams. The film is a parody retelling the events of the Watergate scandal which led to the resignation of U.S. president Richard Nixon.
Dunst co-starred in Sofia Coppola's drama film "The Virgin Suicides" (1999) where she played Lux Lisbon, a troubled adolescent. The film was screened as a special presentation at the 43rd San Francisco International Film Festival in 2000. The movie received generally favorable reviews, and "San Francisco Chronicle" critic Peter Stack noted in his review that Dunst "beautifully balances innocence and wantonness."
In 2000, Dunst starred in the comedy "Bring It On" as Torrance Shipman, the captain of a cheerleading squad. The film generated mostly positive reviews, with many critics reserving praise for her performance. In his review, A. O. Scott called her "a terrific comic actress, largely because of her great expressive range, and the nimbleness with which she can shift from anxiety to aggression to genuine hurt." Charles Taylor of "Salon" noted that "among contemporary teenage actresses, Dunst has become the sunniest imaginable parodist", even though he thought the film had failed to provide her with as good a role as she had either in "Dick" or in "The Virgin Suicides." Jessica Winter from "The Village Voice" complimented Dunst, stating that her performance was "as sprightly and knowingly daft as her turn in "Dick"" and commenting that "[Dunst] provides the only major element of "Bring It On" that plays as tweaking parody rather than slick, strident, body-slam churlishness." Peter Stack of the "San Francisco Chronicle", despite giving the film an unfavorable review, commended Dunst for her willingness "to be as silly and cloyingly agreeable as it takes to get through a slapdash film."
The following year, Dunst starred in the comedy film "Get Over It" (2001). She later explained that one of the reasons for accepting the role was that it gave her the opportunity to sing. Also in 2001, she starred in the historical drama "The Cat's Meow", directed by Peter Bogdanovich, as the late American actress Marion Davies. Derek Elley of "Variety" described the film as "playful and sporty," saying that this was Dunst's best performance to date: "Believable as both a spoiled ingenue and a lover to two very different men, Dunst endows a potentially lightweight character with considerable depth and sympathy." In the "Esquire" review, Tom Carson called her performance "terrific." For her work, she won the Best Actress Silver Ombú category award at the 2002 Mar del Plata Film Festival.
"Spider-Man" and other film roles.
In 2002, Dunst co-starred opposite Tobey Maguire in the superhero film "Spider-Man", the most successful film of her career to date. She played Mary Jane Watson, the best friend and love interest of Peter Parker (Maguire). The film was directed by Sam Raimi. Owen Gleiberman of "Entertainment Weekly" remarked on Dunst's ability to "lend even the smallest line a tickle of flirtatious music." In the "Los Angeles Times" review, critic Kenneth Turan noted that Dunst and Maguire made a real connection on screen, concluding that their relationship involved audiences to an extent rarely seen in films. "Spider-Man" was a commercial and critical success. The movie grossed $114 million during its opening weekend in North America and went on to earn $822 million worldwide.
Following the success of "Spider-Man", Dunst co-starred in Ed Solomon's drama film "Levity" (2003). That same year she co-starred opposite Julia Roberts, Maggie Gyllenhaal, and Julia Stiles in the drama film "Mona Lisa Smile" (2003). The film generated mostly negative reviews, with Manohla Dargis of the "Los Angeles Times" describing it as "smug and reductive." She co-starred opposite Jim Carrey, Kate Winslet, and Tom Wilkinson in Michel Gondry's science fiction romantic drama "Eternal Sunshine of the Spotless Mind" (2004) as Mary Svevo. The latter film received very positive reviews, with "Entertainment Weekly" describing Dunst's subplot as "nifty and clever." The movie grossed $72 million worldwide.
The success of the first "Spider-Man" film led Dunst to reprise her role as Mary Jane Watson in 2004 in "Spider-Man 2". The movie was well received by critics and a financial success, setting a new opening weekend box office record for North America. With revenue of $783 million worldwide, it was the second highest grossing film in 2004. Also in 2004, Dunst co-starred opposite Paul Bettany in the romantic comedy "Wimbledon" where she portrayed a rising tennis player in the Wimbledon Championships, while Bettany portrayed a fading former tennis star. Reception for the film was mixed, but many critics enjoyed Dunst's performance. Claudia Puig of "USA Today" reported that the chemistry between Dunst and Bettany was potent, with Dunst doing a fine job as a sassy and self-assured player.
In 2005, she co-starred opposite Orlando Bloom in Cameron Crowe's romantic tragicomedy "Elizabethtown" as Claire Colburn, a flight attendant. The film premiered at the 2005 Toronto International Film Festival. Dunst revealed that working with Crowe was enjoyable, but more demanding than she had expected. The movie garnered mixed reviews, with the "Chicago Tribune" rating it one out of four stars and describing Dunst's portrayal of a flight attendant as "cloying." It was a box office disappointment.
In 2006, Dunst collaborated with Sofia Coppola again and starred as the title character in Coppola's biographical film "Marie Antoinette", adapted from Antonia Fraser's book "". The movie was screened at a special presentation at the 2006 Cannes Film Festival, and was reviewed favourably. International revenues were $45 million out of $60 million overall.
In 2007, Dunst reprised her role as Mary Jane Watson in "Spider-Man 3". In contrast to the previous two films' positive reviews, "Spider-Man 3" was met with a mixed reception by critics. Nonetheless, with a total worldwide gross of $891 million, it stands as the most commercially successful film in the series and Dunst's highest grossing film to the end of 2008. Having initially signed on for three "Spider-Man" films, she revealed that she would do a fourth, but only if Raimi and Maguire also returned. In January 2010, it was announced that the fourth film was cancelled and that the "Spider-Man" film series would be restarted, and therefore dropping Dunst, Maguire, and Raimi from the franchise.
In 2008, Dunst co-starred opposite Simon Pegg in the comedy film "How to Lose Friends & Alienate People", an adaptation of the memoir of the same name by former "Vanity Fair" contributing editor Toby Young.
Dunst made her directorial debut with the short film "Bastard" which premiered at the Tribeca Film Festival in 2010, and was later featured at the 2010 Cannes Film Festival. Dunst co-starred opposite Ryan Gosling in the mystery film "All Good Things" (2010), based on a true story as the wife of Gosling's character from a run-down neighborhood who goes missing. The feature received reasonable reviews, and earned $640,000 worldwide. Dunst co-starred with Brian Geraghty in Carlos Cuarón's short film "The Second Bakery Attack", an adaptation of Haruki Murakami's short story of the same name.
In 2011, Dunst co-starred opposite Charlotte Gainsbourg, Kiefer Sutherland and Charlotte Rampling in Lars von Trier's drama film "Melancholia" as a depressed woman at the end of the world. The film premiered at the 2011 Cannes Film Festival and received positive reviews and Dunst was singled out for praise. Steven Loeb of "Southampton Patch" wrote, "This film has brought the best out of von Trier, as well as his star. Dunst is so good in this film, playing a character unlike any other she has ever attempted... Even if the film itself were not the incredible work of art that it is, Dunst's performance alone would be incentive enough to recommend it." Sukhdev Sandhu wrote from Cannes in "The Daily Telegraph" that "Dunst is exceptional, so utterly convincing in the lead role – trouble, serene, a fierce savant – that it feels like a career breakthrough. Dunst won several awards for her performance, including the Best Actress Award at the Cannes Film Festival and the Best Actress Award from the US National Society of Film Critics
Dunst has signed to star in "Sweet Relief" as Marla Ruzicka, a peace activist and U.S. relief worker killed by a suicide bomb in Baghdad. She has expressed interest in playing the role of Blondie frontwoman Debbie Harry in Michel Gondry's upcoming biographical film about the band.
In 2012, Dunst co-starred in Juan Diego Solanas' science fiction romantic drama "Upside Down" opposite Jim Sturgess. She starred in Leslye Headland's comedy "Bachelorette", produced by Will Ferrell and Adam McKay. In 2012, she co-starred in the drama film "On the Road" as Camille Moriarty, based on Jack Kerouac's novel of the same name. She made a cameo appearance in the short feature "Fight For Your Right Revisited". It premiered at the 2011 Sundance Film Festival. Reports have stated that she would join Clive Owen and Orlando Bloom in the international thriller "Cities". In January 2014, Dunst began filming Jeff Nichols' science fiction drama "Midnight Special", with Michael Shannon and Joel Edgerton. In May 2015, it was announced that Dunst would star in the Rodarte label founders' feature directorial debut, "Woodshock". The film follows a woman who falls deeper into paranoia after taking a deadly drug. Filming will take place on June 29, 2015 in Eureka, California.
Music.
Dunst made her singing debut in the comedy film "Get Over It", performing two songs written by Marc Shaiman. She recorded Henry Creamer and Turner Layton's jazz standard "After You've Gone" that was used in the end credits of "The Cat's Meow". In "Spider-Man 3", she sang two songs as Mary Jane Watson, one during a Broadway performance, and one as a singing waitress in a jazz club. Dunst revealed that she recorded the songs earlier and later lip-sync to it when filming began. She also appeared in the music videos for Savage Garden's "I Knew I Loved You", Beastie Boys's "Make Some Noise" and R.E.M.'s "We All Go Back to Where We Belong" and she sang two tracks which were "This Old Machine" and "Summer Day" on Jason Schwartzman's 2007 solo album "Nighttiming". In an interview with "The Advertiser", Dunst explained that she has no plans to follow the steps of other actors who release albums, saying: "Definitely not. No way. It worked when Barbra Streisand was doing it, but now it's a little cheesy, I think. It works better when singers are in movies."
Dunst starred as the magical princess Majokko in the Takashi Murakami and McG directed short "Akihabara Majokko Princess" singing a cover of "Turning Japanese". This was shown at the "Pop Life" exhibition in London's Tate Modern museum from October 1, 2009, to January 17, 2010. It shows Dunst prancing around Akihabara, a crowded shopping district in Tokyo.
Personal life.
Dunst was treated for depression in early 2008 at the Cirque Lodge treatment center in Utah. She explained that she had been feeling low in the six months before her admission. In late March 2008, she checked out of the treatment center and began filming "All Good Things". In May 2008, she went public with this information in order to highlight the struggle faced by so many other successful women and to dispel rumors that had been very painful for her friends and family. She has also gone public detailing her "sedate lifestyle" and the fact that she has a single apartment in New York with one bedroom.
Dunst dated actor Jake Gyllenhaal from 2002 to 2004. In 2012, she began dating her "On the Road" co-star Garrett Hedlund.
On August 31, 2014, along with other female celebrities, Dunst had personal nude photos leaked online during the 2014 celebrity pictures hack.
Citizenship.
She gained German citizenship in 2011 and now holds dual citizenship of Germany and the United States.
Political activism.
Dunst supported Democratic candidate John Kerry for the 2004 U.S. presidential election. Four years later, she supported Democrat Barack Obama for the 2008 presidential election. Dunst revealed that she supported Obama "from the beginning" of the presidential campaign. In support of this, she directed and narrated a documentary entitled "Why Tuesday", explaining the United States tradition of voting on Tuesdays. Dunst explained that Tuesday is "not a holiday, and the United States is one of the lowest democratic countries in voter turnout." She felt it important to "influence people in a positive way" to vote on November 4, 2008.
Charity work.
Her charity work includes participation with the Elizabeth Glaser Pediatric AIDS Foundation, in which she helped design and promote a necklace, for which all proceeds from sales went to the Glaser foundation. She also has helped with breast cancer awareness, in September 2008, she participated in the Stand Up to Cancer telethon, to help raise funds to accelerate cancer research. On December 5, 2009, she participated in the Teletón in Mexico, to help raise funds to treat cancer and children rehabilitation.

</doc>
<doc id="17537" url="http://en.wikipedia.org/wiki?curid=17537" title="Lysergic acid diethylamide">
Lysergic acid diethylamide

Lysergic acid diethylamide (/daɪ eθəl ˈæmaɪd/ or /æmɪd/ or /eɪmaɪd/), abbreviated LSD or LSD-25, also known as lysergide (INN) and colloquially as acid, is a psychedelic drug of the ergoline family, well known for its psychological effects - which can include altered thinking processes, closed- and open-eye visuals, synesthesia, an altered sense of time, and spiritual experiences - as well as for its key role in 1960s counterculture. It is used mainly as an entheogen and recreational drug. LSD is non-addictive. However, acute adverse psychiatric reactions such as anxiety, paranoia, and delusions are possible.
LSD was first synthesized by Albert Hofmann in 1938 from ergotamine, a chemical derived by Arthur Stoll from ergot, a grain fungus that typically grows on rye. The short form "LSD" comes from its early code name "LSD-25", which is an abbreviation for the German "Lysergsäure-diethylamid" followed by a sequential number. LSD is sensitive to oxygen, ultraviolet light, and chlorine, especially in solution, though its potency may last for years if it is stored away from light and moisture at low temperature. In pure form it is a colorless, odorless, tasteless solid. LSD is typically either swallowed (oral) or held under the tongue (sublingual), usually on a substrate such as absorbent blotter paper, a sugar cube, or gelatin. In its liquid form, it can also be administered by intramuscular or intravenous injection. Interestingly, unlike most other classes of illicit drugs and other groups of psychedelic drugs such as tryptamines and phenethylamines, when LSD is administered via intravenous injection the onset is not immediate, instead taking approximately 30 minutes before the effects are realized. LSD is very potent, with 20–30 µg (micrograms) being the threshold dose.
Hofmann discovered the psychedelic properties of LSD in 1943. It was introduced commercially in 1947 by Sandoz Laboratories under the trade-name Delysid as a drug with various psychiatric uses. In the 1950s, officials at the U.S. Central Intelligence Agency (CIA) thought the drug might be applicable to mind control and chemical warfare; the agency's MKULTRA research program propagated the drug among young servicemen and students. The subsequent recreational use of the drug by youth culture in the Western world during the 1960s led to a political firestorm that resulted in its prohibition.
Effects.
Physical.
LSD can cause pupil dilation, reduced appetite, and wakefulness. Other physical reactions to LSD are highly variable and nonspecific, some of which may be secondary to the psychological effects of LSD. Among the reported symptoms are numbness, weakness, nausea, hypothermia or hyperthermia, elevated blood sugar, goose bumps, heart rate increase, jaw clenching, perspiration, saliva production, mucus production, sleeplessness, hyperreflexia, and tremors. Some users, including Albert Hofmann, report a strong metallic taste for the duration of the effects.
LSD is not considered addictive by the medical community. Tolerance to LSD-25 builds up over consistent use and cross-tolerance has been demonstrated between LSD, mescaline
and psilocybin. This tolerance diminishes a few days after cessation of use and is probably caused by downregulation of 5-HT2A receptors in the brain.
Psychological.
LSD's psychological effects (colloquially called a "trip") vary greatly between persons and places. If the user is in a hostile or otherwise unsettling environment, or is not mentally prepared for the powerful distortions in perception and thought that the drug causes, effects are more likely to be unpleasant than if he or she is in a comfortable environment and has a relaxed, balanced and open mindset.
An LSD trip can have long-term psychological or emotional effects; some users report the LSD experience as causing significant long term changes in their personality and life perspective. Widely different effects emerge based on what Timothy Leary called "set and setting"; the "set" being the general mindset of the user, and the "setting" being the physical and social environment in which the drug's effects are experienced. Long-term adverse effects observed in some users include recurrence of perceptual effects initially experienced while under the acute influence of the drug. These effects have been described as "distressing, spontaneous, recurrent, pervasive" and in some cases irreversible.
Some psychological effects may include an experience of radiant colors, objects and surfaces appearing to ripple or "breathe", colored patterns behind the closed eyelids (eidetic imagery), an altered sense of time (time seems to be stretching, repeating itself, changing speed or stopping), crawling geometric patterns overlaying walls and other objects, morphing objects, a sense that one's thoughts are spiraling into themselves, loss of a sense of identity or the ego (known as "ego death"), and other powerful psycho-physical reactions. Many users experience a dissolution between themselves and the "outside world". This unitive quality may play a role in the spiritual and religious aspects of LSD. The drug sometimes leads to disintegration or restructuring of the user's historical personality and creates a mental state that some users report allows them to have more choice regarding the nature of their own personality.
Sensory.
LSD causes an animated sensory experience of senses, emotions, memories, time, and awareness for 6 to 14 hours, depending on dosage and tolerance. Generally beginning within 30 to 90 minutes after ingestion, the user may experience anything from subtle changes in perception to overwhelming cognitive shifts. Changes in auditory and visual perception are typical. Visual effects include the illusion of movement of static surfaces ("walls breathing"), after image-like trails of moving objects ("tracers"), the appearance of moving colored geometric patterns (especially with closed eyes), an intensification of colors and brightness ("sparkling"), new textures on objects, blurred vision, and shape suggestibility. Users commonly report that the inanimate world appears to animate in an inexplicable way; for instance, objects that are static in three dimensions can seem to be moving relative to one or more additional spatial dimensions. Many of the basic visual effects resemble the phosphenes seen after applying pressure to the eye and have also been studied under the name "form constants". The auditory effects of LSD may include echo-like distortions of sounds, changes in ability to discern concurrent auditory stimuli, and a general intensification of the experience of music. Higher doses often cause intense and fundamental distortions of sensory perception such as synaesthesia, the experience of additional spatial or temporal dimensions, and temporary dissociation.
Uses.
Medical.
LSD currently has no recognized medical use.
Recreational and spiritual.
Despite its mostly illegal status worldwide, LSD is commonly used as a recreational drug for its psychedelic effects.
Entheogen.
LSD is considered an entheogen because it can catalyze intense spiritual experiences, during which users may feel they have come into contact with a greater spiritual or cosmic order. Users claim to experience lucid sensations where they have "out of body" experiences. In 1966, Timothy Leary established the League for Spiritual Discovery with LSD as its sacrament. Stanislav Grof has written that religious and mystical experiences observed during LSD sessions appear to be phenomenologically indistinguishable from similar descriptions in the sacred scriptures of the great religions of the world and the secret mystical texts of ancient civilizations.
Adverse effects.
Suicide and acts of violence have been associated with LSD use. Long-term effects include "flashbacks" and a syndrome of long-term perceptual changes that are experienced as distressing LSD can temporarily impair the ability to fully understand common dangers and therefore users may be more susceptible to personal injury and accidents. LSD may also cause temporary confusion, difficulty with abstract thinking, or signs of impaired memory and attention span.
Mental disorders.
LSD may trigger panic attacks or feelings of extreme anxiety, colloquially referred to as a "bad trip". There is evidence that people with severe mental illnesses like schizophrenia have a higher likelihood of experiencing adverse effects from taking LSD (see psychosis for further details). No real prolonged effects have been proven, although PTSD has been reported after some intensely distressful LSD experiences.
Suggestibility.
While publicly available documents indicate that the CIA and Department of Defense have discontinued research into the use of LSD as a means of mind control, research from the 1960s suggests there exists evidence that both mentally ill and healthy people are more suggestible while under its influence.
Psychosis.
Historical data suggests that there has been the occasional incidence of LSD induced psychosis in people who appeared to be healthy prior to taking the drug.
Flashbacks and HPPD.
"Flashbacks" are a reported psychological phenomenon in which an individual experiences an episode of some of LSD's subjective effects long after the drug has worn off, usually in the days after typical doses. Several studies have tried to determine how likely a user of LSD, not suffering from known psychiatric conditions, is to experience flashbacks. The larger studies include Blumenfeld's in 1971 and Naditch and Fenwick's in 1977, which arrived at figures of 20% and 28%, respectively.
Hallucinogen Persisting Perception Disorder (HPPD) describes a post-LSD exposure syndrome which LSD-like visual changes are not temporary and brief, as they are in flashbacks, but instead are persistent, and cause clinically significant impairment or distress. The syndrome is a DSM-IV diagnosis. Several scientific journal articles have described the disorder.
HPPD differs from flashbacks in that it is persistent and apparently entirely visual (although mood and anxiety disorders are sometimes diagnosed in the same individuals). A recent review suggests that HPPD (as defined in the DSM-IV) is uncommon and affects a distinctly vulnerable subpopulation of users.
Uterine contractions.
Early pharmacological testing by Sandoz in laboratory animals showed that LSD can stimulate uterine contractions, with efficacy comparable to ergobasine, the active uterotonic component of the ergot fungus.
Genetic.
The mutagenic potential of LSD is unclear. Overall, the evidence seems to point to limited or no effect at commonly used doses.
Overdose.
Reassurance in a calm, safe environment is beneficial. Agitation can be safely addressed with benzodiazepines such as diazepam. Neuroleptics such as haloperidol reduce hallucinations and paranoid delusions and can be used to terminate the effects of LSD. LSD is rapidly absorbed, so activated charcoal and emptying of the stomach will be of little benefit. Sedation or physical restraint is rarely required, and excessive restraint may cause complications such as hyperthermia (over-heating) or rhabdomyolysis.
Massive doses require supportive care, which may include endotracheal intubation or respiratory support. Overdose has been recorded at 1,000 to 7,000μg per 100ml & 2.1 to 26ng per ml in serum concentrations of the tartrate salt form. High blood pressure, tachycardia (rapid heart-beat) and hyperthermia, if present, should be treated symptomatically. Treat low blood pressure initially with fluids and then with pressors if necessary. Intravenous administration of anticoagulants, vasodilators, and sympatholytics may be useful with massive doses.
Pharmacology.
Pharmacokinetics.
LSD's effects normally last from 6–12 hours depending on dosage, tolerance, body weight and age. The Sandoz prospectus for "Delysid" warned: "intermittent disturbances of affect may occasionally persist for several days." Contrary to early reports and common belief, LSD effects do not last longer than the amount of time significant levels of the drug are present in the blood. Aghajanian and Bing (1964) found LSD had an elimination half-life of only 175 minutes. However, using more accurate techniques, Papac and Foltz (1990) reported that 1 µg/kg oral LSD given to a single male volunteer had an apparent plasma half-life of 5.1 hours, with a peak plasma concentration of 5 ng/mL at 3 hours post-dose.
Pharmacodynamics.
LSD affects a large number of the G protein-coupled receptors, including all dopamine receptor subtypes, and all adrenoreceptor subtypes, as well as many others. Most serotonergic psychedelics are not significantly dopaminergic, and LSD is therefore rather unique in this regard. LSD's agonism of D2 receptors contributes to its psychoactive effects. LSD binds to most serotonin receptor subtypes except for 5-HT3 and 5-HT4. However, most of these receptors are affected at too low affinity to be sufficiently activated by the brain concentration of approximately 10–20 nM. In humans, recreational doses of LSD can affect 5-HT1A (Ki=1.1nM), 5-HT2A (Ki=2.9nM), 5-HT2B (Ki=4.9nM), 5-HT2C (Ki=23nM), 5-HT5A (Ki=9nM [in cloned rat tissues]), and 5-HT6 receptors (Ki=2.3nM). 5-HT5B receptors, which are not present in humans, also have a high affinity for LSD. The psychedelic effects of LSD are attributed to cross-activation of 5-HT2A receptor heteromers. Many but not all 5-HT2A agonists are psychedelics and 5-HT2A antagonists block the psychedelic activity of LSD. LSD exhibits functional selectivity at the 5-HT2A and 5HT2C receptors in that it activates the signal transduction enzyme phospholipase A2 instead of activating the enzyme phospholipase C as the endogenous ligand serotonin does. Exactly how LSD produces its effects is unknown, but it is thought that it works by increasing glutamate release in the cerebral cortex and therefore excitation in this area, specifically in layers IV and V. LSD, like many other drugs, has been shown to activate DARPP-32-related pathways.
LSD enhances dopamine D2R protomer recognition and signaling of D2–5-HT2A receptor complexes. This mechanism may contribute to the psychotic actions of LSD.
Physical and chemical properties.
LSD is a chiral compound with two stereocenters at the carbon atoms C-5 and C-8, so that theoretically four different optical isomers of LSD could exist. LSD, also called (+)--LSD, has the absolute configuration (5"R",8"R"). The C-5 isomers of lysergamides do not exist in nature and are not formed during the synthesis from -lysergic acid. Retrosynthetically, the C-5 stereocenter could be analysed as having the same configuration of the alpha carbon of the naturally occurring amino acid L-tryptophan, the precursor to all biosynthetic ergoline compounds.
However, LSD and iso-LSD, the two C-8 isomers, rapidly interconvert in the presence of bases, as the alpha proton is acidic and can be deprotonated and reprotonated. Non-psychoactive iso-LSD which has formed during the synthesis can be separated by chromatography and can be isomerized to LSD.
Pure salts of LSD are triboluminescent, emitting small flashes of white light when shaken in the dark. LSD is strongly fluorescent and will glow bluish-white under UV light.
Synthesis.
LSD is an ergoline derivative. It is commonly synthesised by reacting diethylamine with an activated form of lysergic acid. Activating reagents include phosphoryl chloride and peptide coupling reagents. Lysergic acid is made by alkaline hydrolysis of lysergamides like ergotamine, a substance usually derived from the ergot fungus on agar plate, or theoretically possible, but impractical and uncommon from ergine (lysergic acid amide, LSA) extracted from morning glory seeds. Lysergic acid can also be produced synthetically, eliminating the need for ergotamines.
Dosage.
A single dose of LSD may be between 40 and 500 micrograms—an amount roughly equal to one-tenth the mass of a grain of sand.
Threshold effects can be felt with as little as 25 micrograms of LSD. Dosages of LSD are measured in micrograms (µg), or millionths of a gram. By comparison, dosages of most drugs, both recreational and medicinal, are measured in milligrams (mg), or thousandths of a gram. For example, an active dose of mescaline, roughly 0.2 to 0.5 g, has effects comparable to 100 µg or less of LSD.
In the mid-1960s, the most important black market LSD manufacturer (Owsley Stanley) distributed acid at a standard concentration of 270 µg,
while street samples of the 1970s contained 30 to 300 µg. By the 1980s, the amount had reduced to between 100 and 125 µg, dropping more in the 1990s to the 20–80 µg range, and even more in the 2000s (decade).
Reactivity and degradation.
"LSD," writes the chemist Alexander Shulgin, "is an unusually fragile molecule...As a salt, in water, cold, and free from air and light exposure, it is stable indefinitely."
LSD has two labile protons at the tertiary stereogenic C5 and C8 positions, rendering these centres prone to epimerisation. The C8 proton is more labile due to the electron-withdrawing carboxamide attachment, but removal of the chiral proton at the C5 position (which was once also an alpha proton of the parent molecule tryptophan) is assisted by the inductively withdrawing nitrogen and pi electron delocalisation with the indole ring.
LSD also has enamine-type reactivity because of the electron-donating effects of the indole ring. Because of this, chlorine destroys LSD molecules on contact; even though chlorinated tap water contains only a slight amount of chlorine, the small quantity of compound typical to an LSD solution will likely be eliminated when dissolved in tap water. The double bond between the 8-position and the aromatic ring, being conjugated with the indole ring, is susceptible to nucleophilic attacks by water or alcohol, especially in the presence of light. LSD often converts to "lumi-LSD", which is inactive in human beings.
A controlled study was undertaken to determine the stability of LSD in pooled urine samples.
The concentrations of LSD in urine samples were followed over time at various temperatures, in different types of storage containers, at various exposures to different wavelengths of light, and at varying pH values. These studies demonstrated no significant loss in LSD concentration at 25 °C for up to four weeks. After four weeks of incubation, a 30% loss in LSD concentration at 37 °C and up to a 40% at 45 °C were observed. Urine fortified with LSD and stored in amber glass or nontransparent polyethylene containers showed no change in concentration under any light conditions. Stability of LSD in transparent containers under light was dependent on the distance between the light source and the samples, the wavelength of light, exposure time, and the intensity of light. After prolonged exposure to heat in alkaline pH conditions, 10 to 15% of the parent LSD epimerized to iso-LSD. Under acidic conditions, less than 5% of the LSD was converted to iso-LSD. It was also demonstrated that trace amounts of metal ions in buffer or urine could catalyze the decomposition of LSD and that this process can be avoided by the addition of EDTA.
Detection in body fluids.
LSD may be quantified in urine as part of a drug abuse testing program, in plasma or serum to confirm a diagnosis of poisoning in hospitalized victims or in whole blood to assist in a forensic investigation of a traffic or other criminal violation or a case of sudden death. Both the parent drug and its major metabolite are unstable in biofluids when exposed to light, heat or alkaline conditions and therefore specimens are protected from light, stored at the lowest possible temperature and analyzed quickly to minimize losses.
History.
 "... affected by a remarkable restlessness, combined with a slight dizziness. At home I lay down and sank into a not unpleasant intoxicated-like condition, characterized by an extremely stimulated imagination. In a dreamlike state, with eyes closed (I found the daylight to be unpleasantly glaring), I perceived an uninterrupted stream of fantastic pictures, extraordinary shapes with intense, kaleidoscopic play of colors. After some two hours this condition faded away."
Albert Hofmann, on his first experience with LSD
LSD was first synthesized on November 16, 1938 by Swiss chemist Albert Hofmann at the Sandoz Laboratories in Basel, Switzerland as part of a large research program searching for medically useful ergot alkaloid derivatives. LSD's psychedelic properties were discovered 5 years later when Hofmann himself accidentally ingested an unknown quantity of the chemical. The first intentional ingestion of LSD occurred on April 19, 1943, when Hofmann ingested 250 µg of LSD. He said this would be a threshold dose based on the dosages of other ergot alkaloids. Hofmann found the effects to be much stronger than he anticipated. Sandoz Laboratories introduced LSD as a psychiatric drug in 1947.
Beginning in the 1950s, the US Central Intelligence Agency began a research program code named Project MKULTRA. Experiments included administering LSD to CIA employees, military personnel, doctors, other government agents, prostitutes, mentally ill patients, and members of the general public in order to study their reactions, usually without the subjects' knowledge. The project was revealed in the US congressional Rockefeller Commission report in 1975.
In 1963, the Sandoz patents expired on LSD. Several figures, including Aldous Huxley, Timothy Leary, and Al Hubbard, began to advocate the consumption of LSD. LSD became central to the counterculture of the 1960s. On October 24, 1968, possession of LSD was made illegal in the United States. The last FDA approved study of LSD in patients ended in 1980, while a study in healthy volunteers was made in the late 1980s. Legally approved and regulated psychiatric use of LSD continued in Switzerland until 1993.
Society and culture.
Legal status.
The United Nations Convention on Psychotropic Substances (adopted in 1971) requires its parties to prohibit LSD. Hence, it is illegal in all parties to the convention, which includes the United States, Australia, New Zealand, and most of Europe. However, enforcement of extant laws varies from country to country. Medical and scientific research with LSD in humans is permitted under the 1971 UN Convention.
Canada.
In Canada, LSD is a controlled substance under Schedule III of the Controlled Drugs and Substances Act. Every person who seeks to obtain the substance, without disclosing authorization to obtain such substances 30 days before obtaining another prescription from a practitioner, is guilty of an indictable offense and liable to imprisonment for a term not exceeding 3 years. Possession for purpose of trafficking is an indictable offense punishable by imprisonment for 10 years.
United Kingdom.
In the United Kingdom, LSD is a Schedule 1 Class 'A' drug. This means it has no recognised legitimate uses and possession of the drug without a license is punishable with 7 years' imprisonment and/or an unlimited fine, and trafficking is punishable with life imprisonment and an unlimited fine ("see main article on drug punishments Misuse of Drugs Act 1971)."
In 2000, after consultation with members of the Royal College of Psychiatrists' Faculty of Substance Misuse, the UK Police Foundation issued the Runciman Report which recommended "the transfer of LSD from Class A to Class B".
In November 2009, the UK Transform Drug Policy Foundation released in the House of Commons a guidebook to the legal regulation of drugs, "After the War on Drugs: Blueprint for Regulation", which details options for regulated distribution and sale of LSD and other psychedelics.
United States.
LSD is Schedule I in the United States, according to the Controlled Substances Act of 1970. This means LSD is illegal to manufacture, buy, possess, process, or distribute without a DEA license. By classifying LSD as a Schedule I substance, the Drug Enforcement Administration holds that LSD meets the following three criteria: it is deemed to have a high potential for abuse; it has no legitimate medical use in treatment; and there is a lack of accepted safety for its use under medical supervision. There are no documented deaths from chemical toxicity; most LSD deaths are a result of behavioral toxicity.
There can also be substantial discrepancies between the amount of chemical LSD that one possesses and the amount of possession with which one can be charged in the U.S. This is because LSD is almost always present in a medium (e.g. blotter or neutral liquid), and the amount that can be considered with respect to sentencing is the total mass of the drug and its medium. This discrepancy was the subject of 1995 United States Supreme Court case, "Neal v. U.S."
Lysergic acid and lysergic acid amide, LSD precursors, are both classified in Schedule III of the Controlled Substances Act. Ergotamine tartrate, a precursor to lysergic acid, is regulated under the Chemical Diversion and Trafficking Act.
Economics.
Production.
An active dose of LSD is very minute, allowing a large number of doses to be synthesized from a comparatively small amount of raw material. Twenty five kilograms of precursor ergotamine tartrate can produce 5–6 kg of pure crystalline LSD; this corresponds to 100 million doses. Because the masses involved are so small, concealing and transporting illicit LSD is much easier than smuggling other illegal drugs like cocaine or cannabis.
Manufacturing LSD requires laboratory equipment and experience in the field of organic chemistry. It takes two to three days to produce 30 to 100 grams of pure compound. It is believed that LSD is not usually produced in large quantities, but rather in a series of small batches. This technique minimizes the loss of precursor chemicals in case a step does not work as expected.
Forms.
LSD is produced in crystalline form and then mixed with excipients or redissolved for production in ingestible forms. Liquid solution is either distributed in small vials or, more commonly, sprayed onto or soaked into a distribution medium. Historically, LSD solutions were first sold on sugar cubes, but practical considerations forced a change to tablet form. Appearing in 1968 as an orange tablet measuring about 6 mm across, "Orange Sunshine" acid was the first largely available form of LSD after its possession was made illegal. Tim Scully, a prominent chemist, made some of these tablets, but said that most "Sunshine" in the USA came by way of Ronald Stark, who imported approximately thirty-five million doses from Europe.
Over a period of time, tablet dimensions, weight, shape and concentration of LSD evolved from large (4.5–8.1 mm diameter), heavyweight (≥150 mg), round, high concentration (90–350 µg/tab) dosage units to small (2.0–3.5 mm diameter) lightweight (as low as 4.7 mg/tab), variously shaped, lower concentration (12–85 µg/tab, average range 30–40 µg/tab) dosage units. LSD tablet shapes have included cylinders, cones, stars, spacecraft, and heart shapes. The smallest tablets became known as "Microdots".
After tablets came "computer acid" or "blotter paper LSD", typically made by dipping a preprinted sheet of blotting paper into an LSD/water/alcohol solution. More than 200 types of LSD tablets have been encountered since 1969 and more than 350 blotter paper designs have been observed since 1975. About the same time as blotter paper LSD came "Windowpane" (AKA "Clearlight"), which contained LSD inside a thin gelatin square a quarter of an inch (6 mm) across. LSD has been sold under a wide variety of often short-lived and regionally restricted street names including Acid, Trips, Uncle Sid, Blotter, Lucy, Alice and doses, as well as names that reflect the designs on the sheets of blotter paper. Authorities have encountered the drug in other forms—including powder or crystal, and capsule.
Modern distribution.
LSD manufacturers and traffickers in the United States can be categorized into two groups: A few large-scale producers, and an equally limited number of small, clandestine chemists, consisting of independent producers who, operating on a comparatively limited scale, can be found throughout the country. As a group, independent producers are of less concern to the Drug Enforcement Administration than the larger groups, as their product reaches only local markets.
Mimics.
Since 2005, law enforcement in the United States and elsewhere has seized several chemicals and combinations of chemicals in blotter paper which were sold as LSD mimics, including DOB, a mixture of DOC and DOI, 25I-NBOMe, and a mixture of DOC and DOB. Street users of LSD are often under the impression that blotter paper which is actively hallucinogenic can only be LSD because that is the only chemical with low enough doses to fit on a small square of blotter paper. While it is true that LSD requires lower doses than most other hallucinogens, blotter paper is capable of absorbing a much larger amount of material. The DEA performed a chromatographic analysis of blotter paper containing 2C-C which showed that the paper contained a much greater concentration of the active chemical than typical LSD doses, although the exact quantity was not determined. Blotter LSD mimics can have relatively small dose squares; a sample of blotter paper containing DOC seized by Concord, California police had dose markings approximately 6 mm apart.
Research.
Currently, a number of organizations—including the Beckley Foundation, MAPS, Heffter Research Institute and the Albert Hofmann Foundation—exist to fund, encourage and coordinate research into the medicinal and spiritual uses of LSD and related psychedelics. New clinical LSD experiments in humans started in 2009 for the first time in 35 years.
Experimental uses of LSD include the treatment of alcoholism and pain and cluster headache relief. It has also been used for spiritual purposes, and for its putative effects in increasing creativity. The United States Drug Enforcement Administration states that LSD "produces no aphrodisiac effects, does not increase creativity, has no lasting positive effect in treating alcoholics or criminals, does not produce a 'model psychosis', and does not generate immediate personality change."
Therapeutic use.
Psychedelic therapy.
In the 1950s and 1960s LSD was used in psychiatry to enhance psychotherapy known as psychedelic therapy. Some psychiatrists believed LSD was especially useful at helping patients to "unblock" repressed subconscious material through other psychotherapeutic methods, and also for treating alcoholism. One study concluded, "The root of the therapeutic value of the LSD experience is its potential for producing self-acceptance and self-surrender," presumably by forcing the user to face issues and problems in that individual's psyche.
Two recent reviews concluded that conclusions drawn from most of these early trials are unreliable due serious methodological flaws. These include the absence of adequate control groups, lack of followup, and vague criteria for therapeutic outcome. In many cases studies failed to convincingly demonstrate whether the drug or the therapeutic interaction was responsible for any beneficial effects.
End-of-life anxiety.
Since 2008 there has been ongoing research in Switzerland into using LSD to alleviate anxiety for terminally ill cancer patients coping with their impending deaths.
Alcoholism treatment.
A 2012 meta-analysis of six randomized controlled trials found evidence that a single dose of LSD in conjunction with various alcoholism treatment programs was associated with a decrease in alcohol abuse, lasting for several months, but no effect was seen at one year. Adverse events included seizure, moderate confusion and agitation, nausea, vomiting, and acting in a bizarre fashion.
Pain management.
LSD was studied in the 1960s by Eric Kast for pain management as an analgesic for serious and chronic suffer caused by cancer or other major trauma.
Cluster headaches.
LSD has been used as a treatment for cluster headaches with positive results in some small studies.
Creativity.
In the 1950s and 1960s, psychiatrists like Oscar Janiger explored the potential effect of LSD on creativity. Experimental studies attempted to measure the effect of LSD on creative activity and aesthetic appreciation. Seventy professional artists were asked to draw two pictures of a Hopi Indian kachina doll, one before ingesting LSD and one after.
Further reading.
</dl>
External links.
Documentaries

</doc>
<doc id="17748" url="http://en.wikipedia.org/wiki?curid=17748" title="Limestone">
Limestone

Limestone is a sedimentary rock composed largely of the minerals calcite and aragonite, which are different crystal forms of calcium carbonate (CaCO3). Most limestone is composed of skeletal fragments of marine organisms such as coral or foraminifera.
Limestone makes up about 10% of the total volume of all sedimentary rocks. The solubility of limestone in water and weak acid solutions leads to karst landscapes, in which water erodes the limestone over thousands to millions of years. Most cave systems are through limestone bedrock.
Limestone has numerous uses: as a building material, as aggregate for the base of roads, as white pigment or filler in products such as toothpaste or paints, and as a chemical feedstock.
The first geologist to distinguish limestone from dolomite was Belsazar Hacquet in 1778.
Description.
Like most other sedimentary rocks, most limestone is composed of grains. Most grains in limestone are skeletal fragments of marine organisms such as coral or foraminifera. Other carbonate grains comprising limestones are ooids, peloids, intraclasts, and extraclasts. These organisms secrete shells made of aragonite or calcite, and leave these shells behind after the organisms die.
Limestone often contains variable amounts of silica in the form of chert (chalcedony, flint, jasper, etc.) or siliceous skeletal fragment (sponge spicules, diatoms, radiolarians), and varying amounts of clay, silt and sand (terrestrial detritus) carried in by rivers.
Some limestones do not consist of grains at all, and are formed completely by the chemical precipitation of calcite or aragonite, i.e. travertine. Secondary calcite may be deposited by supersaturated meteoric waters (groundwater that precipitates the material in caves). This produces speleothems, such as stalagmites and stalactites. Another form taken by calcite is oolitic limestone, which can be recognized by its granular (oolite) appearance.
The primary source of the calcite in limestone is most commonly marine organisms. Some of these organisms can construct mounds of rock known as reefs, building upon past generations. Below about 3,000 meters, water pressure and temperature conditions cause the dissolution of calcite to increase nonlinearly, so limestone typically does not form in deeper waters (see lysocline). Limestones may also form in lacustrine and evaporite depositional environments.
Calcite can be dissolved or precipitated by groundwater, depending on several factors, including the water temperature, pH, and dissolved ion concentrations. Calcite exhibits an unusual characteristic called retrograde solubility, in which it becomes less soluble in water as the temperature increases.
Impurities (such as clay, sand, organic remains, iron oxide, and other materials) will cause limestones to exhibit different colors, especially with weathered surfaces.
Limestone may be crystalline, clastic, granular, or massive, depending on the method of formation. Crystals of calcite, quartz, dolomite or barite may line small cavities in the rock. When conditions are right for precipitation, calcite forms mineral coatings that cement the existing rock grains together, or it can fill fractures.
Travertine is a banded, compact variety of limestone formed along streams; particularly where there are waterfalls and around hot or cold springs. Calcium carbonate is deposited where evaporation of the water leaves a solution supersaturated with the chemical constituents of calcite. Tufa, a porous or cellular variety of travertine, is found near waterfalls. Coquina is a poorly consolidated limestone composed of pieces of coral or shells.
During regional metamorphism that occurs during the mountain building process (orogeny), limestone recrystallizes into marble.
Limestone is a parent material of Mollisol soil group.
Classification.
Two major classification schemes, the Folk and the Dunham, are used for identifying limestone and carbonate rocks.
Folk classification.
Robert L. Folk developed a classification system that places primary emphasis on the detailed composition of grains and interstitial material in carbonate rocks. Based on composition, there are three main components: allochems (grains), matrix (mostly micrite), and cement (sparite). The Folk system uses two-part names; the first refers to the grains and the second is the root. It is helpful to have a petrographic microscope when using the Folk scheme, because it is easier to determine the components present in each sample.
Dunham classification.
The Dunham scheme focuses on depositional textures. Each name is based upon the texture of the grains that make up the limestone. Robert J. Dunham published his system for limestone in 1962; it focuses on the depositional fabric of carbonate rocks. Dunham divides the rocks into four main groups based on relative proportions of coarser clastic particles. Dunham names are essentially for rock families. His efforts deal with the question of whether or not the grains were originally in mutual contact, and therefore self-supporting, or whether the rock is characterized by the presence of frame builders and algal mats. Unlike the Folk scheme, Dunham deals with the original porosity of the rock. The Dunham scheme is more useful for hand samples because it is based on texture, not the grains in the sample.
Limestone landscape.
Limestone makes up about 10% of the total volume of all sedimentary rocks.
Limestone is partially soluble, especially in acid, and therefore forms many erosional landforms. These include limestone pavements, pot holes, cenotes, caves and gorges. Such erosion landscapes are known as karsts. Limestone is less resistant than most igneous rocks, but more resistant than most other sedimentary rocks. It is therefore usually associated with hills and downland, and occurs in regions with other sedimentary rocks, typically clays.
Karst topography and caves develop in limestone rocks due to their solubility in dilute acidic groundwater. The solubility of limestone in water and weak acid solutions leads to karst landscapes. Regions overlying limestone bedrock tend to have fewer visible above-ground sources (ponds and streams), as surface water easily drains downward through joints in the limestone. While draining, water and organic acid from the soil slowly (over thousands or millions of years) enlarges these cracks, dissolving the calcium carbonate and carrying it away in solution. Most cave systems are through limestone bedrock. Cooling groundwater or mixing of different groundwaters will also create conditions suitable for cave formation.
Coastal limestones are often eroded by organisms which bore into the rock by various means. This process is known as bioerosion. It is most common in the tropics, and it is known throughout the fossil record (see Taylor and Wilson, 2003).
Bands of limestone emerge from the Earth's surface in often spectacular rocky outcrops and islands. Examples include the Burren in Co. Clare, Ireland; the Verdon Gorge in France; Malham Cove in North Yorkshire and the Isle of Wight, England; on Fårö near the Swedish island of Gotland, the Niagara Escarpment in Canada/United States, Notch Peak in Utah, the Ha Long Bay National Park in Vietnam and the hills around the Lijiang River and Guilin city in China.
The Florida Keys, islands off the south coast of Florida, are composed mainly of oolitic limestone (the Lower Keys) and the carbonate skeletons of coral reefs (the Upper Keys), which thrived in the area during interglacial periods when sea level was higher than at present.
Unique habitats are found on alvars, extremely level expanses of limestone with thin soil mantles. The largest such expanse in Europe is the Stora Alvaret on the island of Öland, Sweden. Another area with large quantities of limestone is the island of Gotland, Sweden. Huge quarries in northwestern Europe, such as those of Mount Saint Peter (Belgium/Netherlands), extend for more than a hundred kilometers.
The world's largest limestone quarry is at Michigan Limestone and Chemical Company in Rogers City, Michigan.
Uses.
Limestone is very common in architecture, especially in Europe and North America. Many landmarks across the world, including the Great Pyramid and its associated complex in Giza, Egypt, are made of limestone. So many buildings in Kingston, Ontario, Canada were constructed from it that it is nicknamed the 'Limestone City'. On the island of Malta, a variety of limestone called Globigerina limestone was, for a long time, the only building material available, and is still very frequently used on all types of buildings and sculptures. Limestone is readily available and relatively easy to cut into blocks or more elaborate carving. It is also long-lasting and stands up well to exposure. However, it is a very heavy material, making it impractical for tall buildings, and relatively expensive as a building material.
Limestone was most popular in the late 19th and early 20th centuries. Train stations, banks and other structures from that era are normally made of limestone. It is used as a facade on some skyscrapers, but only in thin plates for covering, rather than solid blocks. In the United States, Indiana, most notably the Bloomington area, has long been a source of high quality quarried limestone, called Indiana limestone. Many famous buildings in London are built from Portland limestone.
Limestone was also a very popular building block in the Middle Ages in the areas where it occurred, since it is hard, durable, and commonly occurs in easily accessible surface exposures. Many medieval churches and castles in Europe are made of limestone. Beer stone was a popular kind of limestone for medieval buildings in southern England.
Limestone and (to a lesser extent) marble are reactive to acid solutions, making acid rain a significant problem to the preservation of artifacts made from this stone. Many limestone statues and building surfaces have suffered severe damage due to acid rain. Acid-based cleaning chemicals can also etch limestone, which should only be cleaned with a neutral or mild alkaline-based cleaner.
Other uses include:
Degradation by organisms.
The cyanobacterium "Hyella balani" can bore through limestone; as can the green algae "Eugamantia sacculata" and the fungus "Ostracolaba implexa".

</doc>
<doc id="17753" url="http://en.wikipedia.org/wiki?curid=17753" title="History of Laos">
History of Laos

Laos emerged from the French Colonial Empire as an independent country in 1953. Laos exists in truncated form from the thirteenth century Lao kingdom of Lan Xang. Lan Xang existed as a unified kingdom from 1357-1707, divided into the three rival kingdoms of Luang Prabang, Vientiane, and Champasak from 1707-1779, fell to Siamese suzerainty from 1779-1893, and was reunified under the French Protectorate of Laos in 1893. The borders of the modern state of Laos were established by the French colonial government in the late 19th and early 20th centuries.
Prehistory in Laos.
The Mekong River valley region is one of the cradles of human civilization. Anatomically modern humans have inhabited the regions around modern Laos since the late Pleistocene to early Holocene eras. In 2009 an ancient skull was recovered from "Tam Pa Ling" cave in the Annamite Range of northern Laos which was dated between 46,000 and 63,000 years old, making it the oldest fully modern human remains found to date in Southeast Asia.[1] The findings are critical to understanding the migration patterns of early humans, who traveled in successive waves moving west to east following the coastlines, but also moving further inland and further north than previously theorized.
Archaeological exploration in Laos has been limited due to rugged and remote topography, a history of twentieth century conflicts which have left over two million tons of unexploded ordnance throughout the country, and local sensitivities to history which involve the Communist government of Laos, village authorities and rural poverty. The first archaeological explorations of Laos began with French explorers acting under the auspices of the École française d'Extrême-Orient. However, due to the Lao Civil War it is only since the 1990s that serious archaeological efforts have begun in Laos. Since 2005, one such effort, The Middle Mekong Archaeological Project (MMAP) has excavated and surveyed numerous sites along the Mekong and its tributaries around Luang Prabang in northern Laos, with the goal of investigating early human settlement of the Mekong River Valleys.
Archaeological evidence suggests that agriculture and later metallurgy developed in Laos during the Middle Holocene (6000-2000 BCE). During this period the first evidence of ceramics, and farming practices emerged. Hunting and gathering Hoabinhian societies began to settle and rice cultivation was introduced from southern China. The earliest inhabitants of Laos belonged to the Austro-Asiatic Language Family. These earliest societies are the ancestors of the upland Lao ethnicities known collectively as “Lao Theung,” with the largest ethnic groups being the Khamu of northern Laos, and the Brao and Katang in the south.
The Plain of Jars.
From the 8th century BCE to as late as the 2nd century CE an inland trading society emerged on the Xieng Khouang Plateau, near Lao’s most remarkable megalithic remains on a site called the Plain of Jars. The Plain of Jars was nominated to the tentative list as a UNESCO World Heritage Site in 1992, and unexploded ordnance has continued to be removed from the site since 1998. The jars are stone sarcophagi dating from the early Iron Age (500BCE to 800CE) and contained evidence of human remains, burial goods and ceramics. Some sites contain more than 250 individual jars. The tallest jars are more than 3 meters in height. Little is known about the megalithic culture which produced the jars, but the jars and prevalence of iron ore in the region suggest that people who created the site grew wealthy from overland trade routes.
Early Kingdoms.
The first recorded indigenous kingdom to emerge in Southeast Asia was recorded in Chinese histories as the Kingdom of Funan and was located in the area of modern Cambodia, and the coasts of southern Vietnam and southern Thailand during the 1st century CE. Funan was part of Greater India, and was heavily influenced by early Hindu civilization. By the 2nd century CE, Malayo-Polynesian settlers in what is today south Vietnam had established a rival indic kingdom known as Champa. The Cham people established the first settlements near modern Champasak, Laos. Funan forced the Cham people out of the Champasak region by the sixth century CE, where the Chenla a proto-Khmer people would establish the earliest kingdom in Laos. 
The capital of early Chenla was Shrestapura which was located in the vicinity of Champasak and the UNESCO World Heritage Site of Wat Phu. Wat Phu is a vast temple complex in southern Laos which combined natural surroundings with ornate sandstone structures, which were maintained and embellished by the Chenla peoples until 900 CE, and were subsequently rediscovered and embellished by the Khmer in the 10th century. By the 8th century CE Chenla had divided into “Land Chenla” located in Laos, and “Water Chenla” founded by Mahendravarman near Sambor Prei Kuk in Cambodia. Land Chenla was known to the Chinese as “Po Lou” or “Wen Dan” and dispatched a trade mission to the Tang Dynasty court in 717 CE. Water Chenla, would come under repeated attack from Champa, the Medang sea kingdoms in Indonesia based in Java, and finally pirates. From the instability the Khmer emerged, and under the king Jayavarman II the Khmer Empire began to take shape in the 9th century CE.
In what is modern northern and central Laos, and northeast Thailand the Mon people established their own kingdoms during the 8th century CE, outside the reach of the contracting Chenla kingdoms. By the 6th century in the Chao Phraya River Valley, Mon peoples had coalesced to create the Dvaravati kingdoms. In the north, Haripunjaya (Lamphun) emerged as a rival power to the Dvaravati. By the 8th century the Mon had pushed north to create city states, known as “muang,” in Fa Daet (northeast Thailand), Sri Gotapura (Sikhottabong) near modern Tha Khek, Laos, Muang Sua (Luang Prabang), and Chantaburi (Vientiane). In the 8th century CE, Sri Gotapura (Sikhottabong) was the strongest of these early city states, and controlled trade throughout the middle Mekong region. The city states were loosely bound politically, but were culturally similar and introduced Therevada Buddhism from Sri Lankan missionaries throughout the region.
The Tai Migrations.
The Chinese Han Dynasty chronicles of the southern military campaigns provide the first written accounts of Tai–Kadai speaking peoples who inhabited the areas of modern Yunnan China and Guangxi. The Lao, are the dominant ethnicity in modern Laos and are a subgroup within the Tai-Kadai family. The Tai peoples (which include Tai-Lao, Tai-Syam or Tai-Thai, Shan, Tai-Daeng, Tai-Dam, Tai-Yai, Tai-Leu, Tai-Phuan and others) began moving south and westward from their ancestral homelands in southern China and northwest Vietnam in the 8th century CE.
In the 750s CE the Kingdom of Nanzhao managed to defeat four invading Chinese armies, creating a buffer state from Chinese expansion into Southeast Asia for approximately 150 years. As a consequence the Tai were able to put pressure on the settled Mon areas, while the Khmer Empire expanded north and westward from Angkor to absorb most of the Indochinese peninsula from the 8th-12th centuries CE. By the 12th century CE the Khmer Empire had reached its zenith, moving as far north as Chandapuri (Vientiane) and had established trading outposts at Xay Fong on the Khorat Plateau.
The Mongol invasions of Yunnan China (1253-1256) led to an influx of Tai peoples into areas of northern Laos, where they had been slowly expanding since the 8th century. The Tai kingdom of Lanna was founded in 1259 (in the north of modern Thailand). The Sukhothai Kingdom was founded in 1279 (in modern Thailand) and expanded eastward to take the city of "Chantaburi" and renamed it to "Vieng Chan Vieng Kham" (modern Vientiane) and northward to the city of "Muang Sua" which was taken in 1271 and renamed the city to "Xieng Dong Xieng Thong" or “City of Flame Trees beside the River Dong,” (modern Luang Prabang, Laos). The Tai peoples had firmly established control in areas to the northeast of the declining Khmer Empire. Following the death of the Sukhothai king Ram Khamhaeng, and internal disputes within the kingdom of Lanna, both "Vieng Chan Vieng Kham" (Vientiane) and "Xieng Dong Xieng Thong" (Luang Prabang) were independent city-states until the founding of Lan Xang in 1354.
The Legend of Khun Borom.
The history of the Tai migrations into Laos were preserved in myth and legends. The "Nithan Khun Borom" or "Story of Khun Borom" recalls the origin myths of the Lao, and follows the exploits of his seven sons to found the Tai kingdoms of Southeast Asia. The myths also recorded the laws of Khun Borom, which set the basis of common law and identity among the Lao. Among the Khamu the exploits of their folk hero Thao Hung are recounted in the "Thao Hung Thao Cheuang" epic, which dramatizes the struggles of the indigenous peoples with the influx of Tai during the migration period. In later centuries the Lao themselves would preserve the legend in written form, becoming one of the great literary treasures of Laos and one of the few depictions of life in Southeast Asia prior to Therevada Buddhism and Tai cultural influence.
Lan Xang (1354-1707).
Lan Xang (1353-1707) was one of the largest kingdoms in Southeast Asia. Also known as the "Land of a million elephants under the white parasol" the kingdom's name alludes to the power of the kingship and formidable war machine of the early kingdom. The founding of Lan Xang was recorded in 1353, after a series of conquests by Fa Ngum. From 1353-1560 the capitol of Lan Xang was Luang Prabang (known alternately as "Muang Sua" and "Xieng Dong Xieng Thong"). Under successive kings the kingdom expanded its sphere of influence over an area that now incorporates all of modern Laos, the Sipsong Chu Tai of Vietnam, Sipsong Panna of Southern China, Khorat Plateau region of Thailand, and the Stung Treng region of Northern Cambodia.
Lan Xang existed as a sovereign kingdom for over 350 years. The first serious foreign invasion came from the Dai Viet in 1479, which was defeated, though leaving the capital of Luang Prabang largely destroyed. The first half of the sixteenth century allowed for the power, prestige and cultural influence of the kingdom to be restored under a series of strong kings (see Souvanna Balang, Vixun, Photisarath). In the 1540s a series of succession disputes in the neighboring Kingdom of Lanna, created a regional rivalry between Burma, Ayutthaya and Lan Xang. In 1540 Lan Xang defeated an incursion from Ayutthaya. By 1545 the Kingdom of Lanna was attacked by the Burmese and then Ayutthaya. Lan Xang entered into an alliance with Lanna, and aided in the defense of the kingdom. In 1547 the kingdoms of Lan Xang and Lanna were briefly unified under Photisarath of Lan Xang and his son Setthathirath in Lanna. Setthathirath would go on to become the king of Lan Xang on the death of his father, and become one of the greatest kings of Lan Xang.
The Burmese Toungoo Dynasty began a series of expansions during the late 1550s which culminated under King Bayinnaung. Setthathirath moved the capital of Lan Xang from Luang Prabang to Vientiane in 1560, to better defend against the threat of Burma and to more ably administer the central and southern provinces. Bayinnaung subjugated the Kingdom of Lanna and went on to destroy the kingdom and city of Ayutthaya in 1564. King Setthathirath fought two successful guerilla campaigns against the Burmese invasions, leaving Lan Xang the only independent Tai kingdom until his death in 1572, while on campaign against the Khmer. The Burmese succeeded with the third invasion of Lan Xang around 1573, and Lan Xang became a vassal state until 1591 when the son of Setthathirath, Nokeo Koumane, was able to successfully reassert independence.
Lan Xang recovered and reached the apex of its political and economic power during the seventeenth century under King Sourigna Vongsa, who became the longest reigning of Lan Xang’s monarchs (1637-1694). In the 1640s the first European explorers to leave a detailed account of the kingdom arrived looking to establish trade and secure Christian converts, both were ultimately largely unsuccessful. Upon the death of Sourigna Vongsa a succession dispute erupted and the kingdom of Lan Xang was ultimately divided into constituent kingdoms in 1707.
Regional Kingdoms (1707-1779).
Beginning in 1707 the Lao kingdom of Lan Xang was partitioned into regional kingdoms of Vientiane, Luang Prabang and later Champasak (1713). The Kingdom of Vientiane was the strongest of the three, with Vientiane extending influence across the Khorat Plateau (now part of modern Thailand) and conflicting with the Kingdom of Luang Prabang for control of the Xieng Khouang Plateau (on the boarder of modern Vietnam).
The Kingdom of Luang Prabang was the first of the regional kingdoms to emerge in 1707, when King Xai Ong Hue of Lan Xang was challenged by Kingkitsarat, the grandson of Sourigna Vongsa. Xai Ong Hue and his family had sought asylum in Vietnam when they were exiled during the reign of Sourigna Vongsa. Xai Ong Hue gained the support of the Vietnamese Emperor Le Duy Hiep in exchange for recognition of Vietnamese suzerainty over Lan Xang. At the head of a Vietnamese army Xai Ong Hue attacked Vientiane and executed King Nantharat another claimant to the throne. In response Sourigna Vongsa’s grandson Kingkitsarat rebelled and moved with his own army from the Sipsong Panna toward Luang Prabang. Kingkitsarat then moved south to challenge Xai Ong Hue in Vientiane. Xai Ong Hue then turned toward the Kingdom of Ayutthaya for support, and an army was dispatched which rather than supporting Xai Ong Hue arbitrated the division between Luang Prabang and Vientiane.
In 1713, the southern Lao nobility continued the rebellion against Xai Ong Hue under Nokasad, a nephew of Sourigna Vongsa, and the Kingdom of Champasak emerged. The Kingdom of Champasak comprised the area south of the Xe Bang River as far as Stung Treng together with the areas of the lower Mun and Chi rivers on the Khorat Plateau. Although less populous than either Luang Prabang or Vientiane, Champasak occupied an important position for regional power and international trade via the Mekong River.
Throughout the 1760s and 1770s the kingdoms of Siam and Burma competed against each other in a bitter armed rivalry, and sought out alliances with the Lao kingdoms to strengthen their relative positions by adding to their own forces and denying them to their enemy. As a result, the use of competing alliances would further militarize the conflict between the northerly Lao kingdoms of Luang Prabang and Vientiane. Between the two major Lao kingdoms if an alliance with one was sought by either Burma or Siam, the other would tend to support the remaining side. The network of alliances shifted with the political and military landscape throughout the latter half of the eighteenth century.
Siam and Suzerainty (1779-1893).
By 1779 General Taksin had driven the Burmese from Siam, had overrun the Lao Kingdoms of Champasak and Vientiane, and forced Luang Prabang to accept vassalage (Luang Prabang had aided Siam during the siege of Vientiane). Traditional power relationships in Southeast Asia followed the Mandala model, warfare was waged to secure population centers for corvee labor, control regional trade, and confirm religious and secular authority by controlling potent Buddhist symbols (white elephants, important stupas, temples, and Buddha images). To legitimize the Thonburi Dynasty, General Taksin seized the Emerald Buddha and Phra Bang images from Vientiane. Taksin also demanded that the ruling elites of the Lao kingdoms and their royal families pledge vassalage to Siam in order to retain their regional autonomy in accordance with the Mandala model. In the traditional Mandala model, vassal kings retained their power to raise tax, discipline their own vassals, inflict capital punishment, and appoint their own officials. Only matters of war, and succession required approval from the suzerain. Vassals were also expected to provide annual tribute of gold and silver (traditionally modeled into trees), provide tax and tax in-kind, raise support armies in time of war, and provide corvee labor for state projects.
However, by 1782 Taksin had been deposed and Rama I was king of Siam, and began a series of reforms which fundamentally altered the traditional Mandala. Many of the reforms took place to more closely administer and assimilate the Khorat Plateau(or Isan) which was traditionally and culturally part of the Lao kingdoms’ tributary networks. In 1778, only Nakhon Ratchasima was a tributary of Siam, yet by the end of the reign of Rama I Sisaket, Ubon, Roi Et, Yasothon, Khon Khaen, and Kalasin paid tribute directly to Bangkok. According to Thai records, by 1826 (less than fifty years) the number of towns and cities in Isan had grown from 13 to 35. Forced population transfers from Lao areas were further reinforced by corvee labor projects and increased taxes. Siam required labor to help rebuild from repeated Burmese invasions, and growing sea trade. Increasing the productivity and population living on the Khorat Plateau provided the labor and material access to strengthen Siam.
Siribunnyasan the last independent king of Vientiane had died by 1780, and his sons Nanthasen, Inthavong, and Anouvong had been taken to Bangkok as prisoners during the sack of Vientiane in 1779. The sons would become successive kings of Vientiane (under Siamese suzerainty), beginning with Nanthasen in 1781. Nanthasen was allowed to return to Vientiane with the Phra Bang, the palladium of Lan Xang, the Emerald Buddha remained in Bangkok and became an important symbol to the Lao of their captivity. One of Nanthasen’s first acts was to seize Chao Somphu a Phuan prince from Xieng Khouang who had entered into a tributary relationship with Vietnam, and released him only when it was agreed that Xieng Khouang would also acknowledge Vientiane as suzerain. In 1791, Anuruttha was confirmed by Rama I as king of Luang Prabang. By 1792 Nanthasen had convinced Rama I that Anuruttha was secretly dealing with the Burmese, and Siam allowed Nanthasen to lead an army and besiege and capture Luang Prabang. Anuruttha was sent to Bangkok as a prisoner, and only through diplomatic exchanges facilitated by China, was Anuruttha released in 1795. Soon after Anuruttha’s release it was alleged that Nanthasen had been plotting with the governor of Nakhon Phanom to rebel against Siam. Rama I ordered the immediate arrest of Nanthasen, and soon after he died in captivity. Inthavong (1795-1804) became the next king of Vientiane, and dispatched armies to aide Siam against Burmese invasions in 1797 and 1802, and to capture the Sipsong Chau Tai (with his brother Anouvong as general).
Anouvong’s Rebellion and Lao Nationalism.
Anouvong is a symbolic and controversial figure even today, his short lived rebellion against Siam from 1826-1829 ultimately proved futile and led to the total annihilation of Vientiane as a kingdom and a city, yet among the Lao he remains a potent symbol of unyielding defiance and national identity. Thai and Vietnamese histories record that Anouvong rebelled as the result of personal insult suffered at the funeral of Rama II in Bangkok. Yet, the Anouvong Rebellion lasted three years and engulfed the whole of the Khorat Plateau for more complex reasons.
The history of forced population transfers, corvee labor projects, loss of national symbols and prestige (most notably the Emerald Buddha) formed the backdrop to specific actions taken by Rama III to directly annex the Isan region. In 1812 Siam and Vietnam were at odds over the succession of the Cambodian king, the Vietnamese gained the upper hand with their chosen successor and Siam compensated itself by annexing territory on the Dangrek Mountains and along the Mekong River in Stung Treng. As a result Lao international trade along the Mekong was effectively blockaded, and heavy duties were imposed on Lao merchants who were viewed suspiciously by Siam for their trade with both the Cambodians and Vietnamese.
In 1819 a rebellion in Champasak provided Anouvong with opportunity, and he dispatched an army under his son Nyo who managed to suppress the conflict. In exchange Anouvong successfully made the case that his son be crowned as king in Champasak, which was confirmed by Bangkok. Anouvong had successfully expanded his influence throughout Vientiane, Isan, Xieng Khouang and now Champasak. Anouvong dispatched a number of diplomatic missions to Luang Prabang, which were viewed suspiciously in light of his growing regional influence.
By 1825 Rama II had died, and Rama III was consolidating his position against prince Mongkut (Rama IV). In the ensuing power struggle before the accession of Rama III one of Anouvong’s grandsons was killed. When Anouvong arrived for the funerary services, he made several requests of the king Rama III which were dismissed including the return of his sister who had been captured in 1779, and Lao families which had been relocated to Saraburi near Bangkok. Before returning to Vientiane, Anouvong’s son Ngau, the crown prince, was forced to perform manual labor during which he was beaten.
Early in his reign, Rama III ordered a census of all peoples on the Khorat Plateau, the census involved the forced tattooing of each villager’s census number and name of their village. The aim of the policy was to more tightly administer Lao territories from Bangkok and was facilitated by the nobility Siam had installed in the newly created cities throughout the region. Popular resentment against the forced tattooing and increased taxes became "casus belli" for rebellion.
Toward the end of 1826 Anouvong was making military preparations for armed rebellion. Anouvong’s strategy involved three objectives, first was to repatriate all ethnic Lao living in Siam to the right bank of the Mekong and execute any Siamese engaged in the tattooing of Lao, the second objective was to consolidate Lao power by forging an alliance with Chiang Mai and Luang Prabang, the third and final goal was to gain international support from either the Vietnamese, Chinese, Burmese or British. In January hostilities commenced, and the Lao armies were sent from Vientiane to capture Nakhon Ratchasima, Kalasin, and Lomsak. From Champasak forces rushed to take Ubon and Suvannaphum, while pursuing a scorched-earth policy ensuring the Lao time to retreat.
Anouvong’s forces pushed south eventually to Saraburi to free the Lao there, but the flood of refugees pushing north slowed the armies’ retreat. Anouvong also severely underestimated the Siamese arms stockpile, which under the terms of Burney Treaty had provided Siam with weaponry from the Napoleonic Wars in Europe. A Lao defense was staged at Nong Bua Lamphu the traditional Lao stronghold in the Isan, but the Siamese emerged victorious and leveled the city. The Siamese pushed north to take Vientiane and Anouvong fled southeast to the border with Vietnam. By 1828 Anouvong had been captured, tortured and sent to Bangkok with his family to die in a cage. Rama III ordered Chao Bodin to return and level the city of Vientiane, and forcibly move the entire population of the former Lao capital to the Isan region.
Aftermath and Vietnamese Intervention.
Following the Anouvong Rebellion Siam and Vietnam were increasingly at odds over control of the Indochinese Peninsula. In 1831 Emperor Minh Mang sent Vietnamese troops to seize Xieng Khouang and annexed the area as the province of Tran Ninh. Also in 1831 and again in 1833 King Mantha Tourath sent a tributary mission to the Vietnamese, which were quietly ignored so as not to antagonize the Siamese further. In 1893 these tributary missions from Luang Prabang were used by the French as part of a legal argument for all the territories on the east bank of the Mekong. In late 1831 Siam and Vietnam had a series of wars (Siamese-Vietnamese War 1831-1834, and Siamese-Vietnamese War 1841-1845) over control of Xieng Khouang and Cambodia.
In the aftermath of Vientiane's destruction the Siamese divided the Lao lands into three administrative regions. In the north, the king of Luang Prabang and a small Siamese garrison controlled Luang Prabang, the Sipsong Panna, and Sipsong Chao Tai. The central region was administered from Nong Khai and extended to the borders of Tran Ninh (Xieng Khouang) and south to Champasak. The southern regions were controlled from Champasak and extended to areas bordering Cochin China and Cambodia. From the 1830s through the 1860s small rebellions took place across Lao lands and the Khorat Plateau, but they lacked both the scale and coordination of the Anouvong Rebellion. Importantly, at the end of each rebellion Siamese troops would return to their administrative centers, and no Lao region was allowed to have a buildup of force which could have been used in rebellion.
Population Transfers and Slavery.
Population transfers of ethnic Lao to Siam began in 1779 with Siamese suzerainty. Artisans and members of the court were forcibly moved to Saraburi near Bangkok, and several thousand farmers and peasant who were transported throughout Siam to Phetchaburi, Ratchaburi, and Nakhon Chaisi in the southwest and to Prachinburi and Chanthaburi in the southeast. However, massive deportations estimated between 100,000-300,000 people began following the defeat of King Anouvong in 1828, and would continue until the 1870s. From 1828-1830 over 66,000 people were forcibly relocated from Vientiane. In 1834 the first of several relocations of the Phuan areas of Xieng Khouang began, transferring more than 6,000 people. Most of those relocated were settled in the Isan region and were considered that "cha loei" or “war slaves” who were to serve as serfs in underpopulated areas for the Thai elite. The result changed the demographics and cultural traditions of Thailand and Laos and continues today with a five-fold disparity between the ethnic Lao living on the West Bank of the Mekong and those left in the East in what is today Laos.
Although slavery existed in Lao areas before the rebellion in 1828, the defeat and subsequent removal of most ethnic Lao left a depopulated and vulnerable position for the remaining people of the East Bank of the Mekong. Lao Theung hill tribes which had little involvement in the 1828 rebellion bore the brunt of organized slave raids into Laos and became known collectively and pejoratively in Thai and Lao as "kha" or “slaves.” Lao Theung were hunted or sold into slavery frequent organized raiding parties from Vietnam, Cambodia, Siam, Laos and China. Larger tribes of Lao Theung, such as the Brao, would conduct slave raids against weaker tribes. The raids continued throughout the remainder of the nineteenth century, a Siamese military campaign in Laos in 1876 was described by a British observer as having been "transformed into slave-hunting raids on a large scale."
The population transfers and slave raids ameliorated toward the end of the nineteenth century when European observers and anti-slavery groups made their presence increasingly difficult for the Bangkok elite. In 1880 both slave raiding and trading became illegal, although debt slavery would persist until 1905 by decree of King Chulalongkorn. The French would use the existence of slavery in Siam as one of the major professed motivations for establishing a Protectorate of Laos during the 1880s and 1890s. 
The Haw Wars.
In the 1840s sporadic rebellions, slave raids, and movement of refugees throughout the areas that would become modern Laos left whole regions politically and militarily weak. In China the Qing Dynasty was pushing south to incorporate hill peoples into the central administration, at first floods of refugees and later bands of rebels from the Taiping Rebellion pushed into Lao lands. The rebel groups became known by their banners and included the Yellow (or Striped) Flags, Red Flags and the Black Flags. The bandit groups rampaged throughout the countryside, with little response from Siam.
During the early and mid-nineteenth century the first Lao Sung including the Hmong, Mien, Yao and other Sino-Tibetan groups began settling in the higher elevations of Phongsali province and northeast Laos. The influx of immigration was facilitated by the same political weakness which had given shelter to the Haw bandits and left large depopulated areas throughout Laos.
By the 1860s the first French explorers were pushing north charting the path of the Mekong River, with hope of a navigable waterway to southern China. Among the early French explorers was an expedition led by Francis Garnier, who was killed during an expedition by Haw rebels in northern Laos. The French would increasingly conduct military campaigns against the Haw in both Laos and Vietnam (Tonkin) until the 1880s.
Colonialism and the French Protectorate of Laos (1893-1953).
French colonial interests in Laos began with the exploratory missions of Doudart de Lagree and Francis Garnier during the 1860s in the hopes of utilizing the Mekong River as a passage to southern China. Although the Mekong is unnavigable due to a number of rapids, the hope was that the river might be tamed with the help of French engineering and a combination of railways. In 1886 Britain secured the right to appoint a representative in Chiang Mai, in northern Siam. To counter British control in Burma and growing influence in Siam, that same year France sought to establish representation in Luang Prabang, and dispatched Auguste Pavie to secure French interests.
Pavie and French auxiliaries arrived in Luang Prabang in 1887, in time to witness an attack on Luang Prabang by Chinese and Tai bandits, hoping to liberate the brothers of their leader Đèo Văn Trị, who were being held prisoner by the Siamese. Pavie prevented the capture of the ailing King Oun Kham by ferrying him away from the burning city to safety. The incident won the gratitude of the king, provided an opportunity for France to gain control of the Sipsong Chu Thai as part of Tonkin in French Indochina, and demonstrated the weakness of the Siamese in Laos. In 1892 Pavie became Resident Minister in Bangkok, where he encouraged a French policy which first sought to deny or ignore Siamese sovereignty over Lao territories on the east bank of the Mekong, and secondly to suppress the slavery of upland Lao Theung and population transfers of Lao Loum by the Siamese as a prelude to establishing a protectorate in Laos. Siam reacted by denying French trading interests, which by 1893 had increasingly involved military posturing and gunboat diplomacy. France and Siam would position troops to deny each other’s interests, resulting in a Siamese siege of Khong Island in the south and a series of attacks on French garrisons in the north. The result was the Paknam Incident, the Franco-Siamese War and the ultimate recognition of French territorial claims in Laos. 
The French were aware that the east-bank territories of the Mekong were “a depopulated, devastated country,” the Siamese forced population transfers following the Anouvong Rebellion left only a fifth of the original population on the east-bank, the majority of Lao Loum and Phuan peoples had been resettled to the areas around the Khorat Plateau. Territorial gains in 1893 were only a springboard to secure French control of the Mekong, deny Siam as much territorial control as possible by acquiring the Mekong’s west bank territories including the Khorat Plateau, and negotiating stable borders with British Burma along the former territories which paid tribute to the Kingdom of Luang Prabang. France settled a treaty with China in 1895, gaining control of Luang Namtha and Phongsali. British control of the Shan States and French control of the upper Mekong increased tensions between the colonial rivals. A joint commission completed its work in 1896 and the city of Muang Sing was gained by France, in exchange France recognized Siamese sovereignty over the areas of the Chaophraya River basin. However, the issue of Siamese control over the Khorat Plateau, which was ethnically and historically Lao, was left open for the French as was Siamese control over the Malay Peninsula which favored British interests. Political events in Europe would shape French Indochinese policy however, and between 1896 and 1904 a new political party took power which viewed Britain as much more of an ally than a colonial rival. In 1904 the Entente Cordiale was signed as part of the alliance against Germany and Austria-Hungary that fought the First World War. The agreement established respective spheres of influence in Southeast Asia, although French territorial claims would continue until 1907 in Cambodia.
The French Protectorate of Laos established two and at times three administrative regions governed from Vietnam in 1893. It was not until 1899 that Laos became centrally administered by a single Resident Superieur based in Savannakhet, and later in Vientiane. The French chose to establish Vientiane as the colonial capital for two reasons, firstly it was more centrally located between the central provinces and Luang Prabang, and secondly the French were aware of the symbolic importance of rebuilding the former capital of the Lan Xang Kingdom which the Siamese had destroyed.
As part of French Indochina both Laos and Cambodia were seen as a source of raw materials and labor for the more important holdings in Vietnam. French colonial presence in Laos was light; the Resident Superieur was responsible for all colonial administration from taxation to justice and public works. The French maintained a military presence in the colonial capital under the "Garde Indigene" made up of Vietnamese soldiers under a French commander. In important provincial cities like Luang Prabang, Savannakhet, and Pakse there would be an assistant resident, police, paymaster, postmaster, schoolteacher and a doctor. Vietnamese filled most upper level and mid-level positions within the bureaucracy, with Lao being employed as junior clerks, translators, kitchen staff and general laborers. Villages remained under the traditional authority of the local headmen or "chao muang". Throughout the colonial administration in Laos the French presence never amounted to more than a few thousand Europeans. The French concentrated on the development of infrastructure, the abolition of slavery and indentured servitude (although corvee labor was still in effect), trade including opium production, and most importantly the collection of taxes.
The Lao response to French colonialism was mixed, although the French were viewed as preferable to the Siamese by the nobility, the majority of Lao Loum, Lao Theung, and Lao Sung were burdened by regressive taxes and demands for corvee labor to establish colonial outposts. The first serious resistance to the French colonial presence began in southern Laos, as the "Holy Man’s Rebellion" led by Ong Keo, and would last until 1910. The rebellion began in 1901 when a French commissioner in Salavan was attempting to pacify Lao Theung tribes for taxation and corvee labor, Ong Keo provoked anti-French sentiment and in response the French burned a local temple. The commissioner and his troops were massacred and a general uprising began throughout the Bolaven Plateau. Ong Keo would be killed by French forces, but for several years his harassment and protests gained popularity in the southern Laos. It was not until the movement spread to the Khorat Plateau and threatened to become an international incident involving Siam that several French columns of the "Garde Indigene" converged to put down the rebellion. In the north Tai Lu groups from the areas around Phongsali and Muang Sing also began to rebel against French attempts at taxation and corvee labor. 
In 1914 the Tai Lu king had fled to the Chinese portions of the Sipsong Panna, where he began a two-year guerilla campaign against the French in northern Laos, which required three military expeditions to suppress and resulted in direct French control of Muang Sing. In northeast Laos, Chinese and Lao Theung rebelled against French attempts to tax the opium trade which resulted in another rebellion from 1914-1917. By 1915 most of northeast Laos was controlled by Chinese and Lao Theung rebels. The French dispatched the largest military presence yet to Laos which included 160 French officers and 2500 Vietnamese troops divided in two columns. The French drove the Chinese led rebels across the Chinese border and placed Phongsali under direct colonial control. Yet northeastern Laos was still not entirely pacified and a Hmong shaman named Pa Chay Vue attempted to establish a Hmong homeland through a rebellion (pejoratively termed the Madman’s War) which lasted from 1919-1921.
By 1920 the majority of French Laos was at peace and colonial order had been established. In 1928 the first school for the training of Lao civil servants was established, and allowed for the upward mobility of Lao to fill positions occupied by the Vietnamese. Throughout the 1920s and 1930s France attempted to implement Western, particularly French, education, modern healthcare and medicine, and public works with mixed success. The budget for colonial Laos was secondary to Hanoi, and the worldwide Great Depression further restricted funds. It was also in the 1920s and 1930s that the first stirings of Lao nationalist identity emerged due to the work of Prince Phetsarath Rattanavongsa and the French Ecole Francaise d’Extreme Orient to restore ancient monuments, temples, and conduct general research into Lao history, literature, art and architecture. French interest in indigenous history served a dual purpose in Laos it reinforced the image of the colonial mission as protection against Siamese domination, and was also a legitimate route for scholarship.
World War II.
Developing Lao national identity gained importance in 1938 with the rise of the ultranationalist prime minister Phibunsongkhram in Bangkok. Phibunsongkhram renamed Siam to Thailand, a name change which was part of a larger political movement to unify all Tai peoples under the central Thai of Bangkok. The French viewed these developments with alarm, but the Vichy Government was diverted by events in Europe and World War II. Despite a non-aggression treaty signed in June 1940, Thailand took advantage of the French position and initiated the Franco-Thai War. The war concluded unfavorably for Lao interests with the "Treaty of Tokyo", and the loss of trans-Mekong territories of Xainyaburi and part of Champasak. The result was Lao distrust of the French and the first overtly national cultural movement in Laos, which was in the odd position of having limited French support. Charles Rochet the French Director of Public Education in Vientiane, and Lao intellectuals led by Nyuy Aphai and Katay Don Sasorith began the Movement for National Renovation.
Yet the wider impact of World War II had little effect on Laos until February 1945, when a detachment from the Japanese Imperial Army moved into Xieng Khouang. The Japanese preempted that the Vichy administration of French Indochina under Admiral Decoux would be replaced by a representative of the Free French loyal to Charles DeGaulle and initiated Operation Meigo Sakusan. The Japanese succeeded in the internment of the French living in Vietnam and Cambodia, but in the remote areas of Laos the French were able with the help of the Lao and "Garde Indigene" to establish jungle bases which were supplied by British airdrops from Burma. However, French control in Laos had been sidelined.
Lao Issara and Independence.
1945 was a watershed year in the history of Laos, under Japanese pressure King Sisavangvong declared independence in April. The move allowed the various independence movements in Laos including the Lao Seri and Lao Pen Lao to coalesce into the Lao Issara or “Free Lao” movement which was led by Prince Phetsarath and opposed the return of Laos to the French. The Japanese surrender on 15 August 1945 emboldened pro-French factions and Prince Phetsarath was dismissed by King Sisavangvong. Undeterred Prince Phetsarath staged a coup in September and placed the royal family in Luang Prabang under house arrest. In 12 October 1945 the Lao Issara government was declared under the civil administration of Prince Phetsarath. In the next six months the French rallied against the Lao Issara and were able to reassert control over Indochina in April 1946. The Lao Issara government fled to Thailand, where they maintained opposition to the French until 1949, when the group split over questions regarding relations with the Vietminh and the communist Pathet Lao was formed. With the Lao Issara in exile, in August 1946 France instituted a constitutional monarchy in Laos headed by King Sisavangvong, and Thailand agreed to return territories seized during the Franco-Thai War in exchange for a representation at the United Nations. The Franco-Lao General Convention of 1949 provided most members of the Lao Issara with a negotiated amnesty and sought appeasement by establishing the Kingdom of Laos a quasi-independent constitutional monarchy within the French Union. In 1950 additional powers were granted to the Royal Lao Government including training and assistance for a national army. On October 22, 1953, the Franco–Lao Treaty of Amity and Association transferred remaining French powers to the independent Royal Lao Government. By 1954 the defeat at Dien Bien Phu brought eight years of fighting with the Vietminh, during the First Indochinese War, to an end and France abandoned all claims to the colonies of Indochina.
The Kingdom of Laos and the Lao Civil War (1953-1975).
Elections were held in 1955, and the first coalition government, led by Prince Souvanna Phouma, was formed in 1957. The coalition government collapsed in 1958. In 1960 Captain Kong Le staged a coup when the cabinet was away at the royal capital of Luang Prabang and demanded reformation of a neutralist government. The second coalition government, once again led by Souvanna Phouma, was not successful in holding power. Rightist forces under General Phoumi Nosavan drove out the neutralist government from power later that same year. The North Vietnamese invaded Laos between 1958–1959 to create the Ho Chi Minh Trail.
A second Geneva conference, held in 1961-62, provided for the independence and neutrality of Laos, but the agreement meant little in reality and the war soon resumed. Growing North Vietnamese military presence in the country increasingly drew Laos into the Second Indochina War (1954-1975). As a result for nearly a decade, eastern Laos was subjected to some of the heaviest bombing in the history of warfare , as the U.S. sought to destroy the Ho Chi Minh Trail that passed through Laos and defeat the Communist forces. The North Vietnamese also heavily backed the Pathet Lao and repeatedly invaded Laos. The government and army of Laos were backed by the USA during the conflict and the United States formed and trained irregular forces.
Shortly after the Paris Peace Accords led to the withdrawal of U.S. forces from Vietnam, a ceasefire between the Pathet Lao and the government led to a new coalition government. However, North Vietnam never withdrew from Laos and the Pathet Lao remained little more than a proxy army for Vietnamese interests. After the fall of South Vietnam to communist forces in April 1975, the Pathet Lao with the backing of North Vietnam were able to take total power with little resistance. On December 2, 1975, the king was forced to abdicate his throne and the Lao People's Democratic Republic was established.
The Lao People's Democratic Republic (1975-Present).
The new communist government led by Kaysone Phomvihane imposed centralized economic decision-making and incarcerated many members of the previous government and military in "re-education camps" which also included the Hmongs. While nominally independent, the communist government was for many years effectively little more than a puppet regime run from Vietnam.
The government's policies prompted about 10 percent of the Lao population to leave the country. Laos depended heavily on Soviet aid channeled through Vietnam up until the Soviet collapse in 1991. In the 1990s the communist party gave up centralised management of the economy but still has a monopoly of political power.

</doc>
<doc id="17764" url="http://en.wikipedia.org/wiki?curid=17764" title="Demographics of Latvia">
Demographics of Latvia

This article is about the demographic features of the population of the historical territory of Latvia, including population density, ethnic background, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Background.
Latvia was settled by the Baltic tribes some three millennia ago. The territories along the eastern Baltic first came under foreign domination at the beginning of the 13th century, with the formal establishment of Riga in 1201 under the German Teutonic Knights.
Latvia, in whole or in parts, remained under foreign rule for the next eight centuries, finding itself at the cross-roads of all the regional superpowers of their day, including Denmark (the Danes held on lands around the Gulf of Riga), Sweden, and Russia, with southern (Courland) Latvia being at one time a vassal to Poland-Lithuania as well as Latgale falling directly under Poland-Lithuania rule. Through all this time, Latvia remained largely under Baltic German hegemony, with Baltic Germans comprising the largest land-owners, a situation which did not change until Latvia's independence.
Historically, Latvia has had significant German, Russian, Jewish and Polish minorities. The majority (roughly two thirds) of Latvians, under Swedish influences, adopted Lutheranism, while the minority (the remaining third) of Latvians under Poland-Lithuania, Latgale in particular, retained their Catholicism. Aglona, in Latgale, has been the site of annual Catholic pilgrimage for centuries, even through to today.
Recently introduced immigration law in Latvia provides framework for immigration through investment in various financial areas or real estate. In 2012, solely 2,435 applications for residence permit by investment in real estate were received by Office of Citizenship and Migration Affairs. Main immigrant countries are Russia, Belarus, Ukraine and Lithuania (Lithuania is in the European Union, thus no investment is needed). Moreover Latvia receives residence permit applications from people of nationalities such as Afghans, Chinese, Libyans and people from various other distant countries.
Historical shifts.
Latvia's indigenous population has been ravaged numerous times throughout history. The earliest such event occurred during the conquest of Latvia by Peter the Great in the Great Northern War with Sweden.
In 1897, the first official census in this area indicated that Latvians formed 68.3% of the total population of 1.93 million; Russians accounted for 12%, Jews for 7.4%, Germans for 6.2%, and Poles for 3.4%. The remainder were Lithuanians, Estonians, Gypsies, and various other nationalities.
The demographics shifted greatly in the 20th century due to the world wars, the repatriation of the Baltic Germans, the Holocaust, and occupation by the Soviet Union. Today, only the Russian minority, which has tripled in numbers since 1935, remains important. The share of ethnic Latvians grew from 77% (1,467,035) in 1935 to 80% (1,508,800), after human loss in World War II and human deportation and other repressive measures, fell strongly to 52% (1,387,757) in 1989.
In 2005, there were even fewer Latvians than in 1989, though their share of the population was larger - 1,357,099 (58.8% of the inhabitants). People who arrived in Latvia during the Soviet era, and their descendants born before 21 August 1991, have to pass a naturalisation process to receive Latvian citizenship. Children born to residents after the restoration of independence in 1991 automatically receive citizenship. However, if both parents are non-citizens then the parents must take the extra step of choosing Latvian citizenship for their child—who is automatically entitled, but for whom citizenship is not automatic (neither granted nor imposed).
Over 130,000 persons have been naturalized as Latvian citizens since 1995, but 290,660 persons, as of March 2011, live in Latvia with non-citizen's passports. Large numbers of Russians, as well as some Ukrainians and Belarusians remained in Latvia after the fall of the Soviet Union.
According to the provisional results of the Population and Housing Census 2011, the total population of Latvia on March 1, 2011 was 2,067,887. Since the previous census in 2000 the country's population decreased by 309 thousand or 13%. The proportion of ethnic Latvians increased to 62.1% of the population. Livonians are the other indigenous ethnic group, with about 100 of them remaining. Latgalians are a distinctive subgroup of Latvians inhabiting or coming from Eastern Latvia.
According to rankings provided by the United States Census Bureau—International Data Base (IDB)—Country Rankings, Latvia is estimated to have a population of 1,544,000 in the year 2050.
Immigration.
Immigration in Latvia has traditionally been from neighboring countries such as Russia but now migrants also come from other areas such as Latin America and Africa. The Latvian government have sought to work with Russia to stem the problem.
 The Latvian government has been criticized for its treatment of illegals
For an immigrant not to become an illegal resident, a permit is required for a foreign national or a stateless person wishing to reside in the Republic of Latvia for more than 90 days within a 6-month period, thus if the person does not acquire himself a residence permit, he will be considered an illegal immigrant.
Population.
Age structure.
On 1 January 2011 the average age was 41.6 years—6 months more than the average age published earlier.
Vital statistics.
e=estimate, p=provisional
Ethnic groups.
Latvians have always been the largest ethnic group in Latvia during the past century, but minority peoples have always been numerous. Before WW II the proportion of non-Latvians was approximately 25%, the Russians being the largest minority (app. 10%), followed by Jews (approx. 5%), Germans and Poles (2–3%). After World War 2 only small numbers of Jews and Germans remained and following a massive immigration of Russians, Ukrainians and Belarusians, Latvians almost became a minority. In 1989, the proportion of Latvians had decreased to only 52% (75.5% in 1935). Despite the decreasing number of Latvians due to low fertility rates, the proportion of Latvians has considerably increased during the past two decades and reached 62.1% in 2011 (slightly higher than the 62.0% in 1959). This is due to large scale emigration of Russians, Ukrainians and Belarusians. The number of these peoples almost halved between 1989 and 2011.
Languages.
In the 2011 census, 1,164,894 persons in Latvia reported Latvian as their mother tongue; 698,757 respondents listed Russian as their mother tongue, representing 37.2% of the total population, whereas Latvian was recorded as the mother tongue for 62.1%. Latvian was spoken as a second language by 20.8% of the population, and 43.7% spoke Russian as a second language. In total, 71% of ethnic Latvians said they could speak Russian, and 52% of Russians could speak Latvian in census 2000.
Religion.
In a 2005 survey, 24.1% described themselves as Russian Orthodox, 20.7% Catholics, 20.0% Lutherans, 4.4% Old Believers, 10.6% non-denominational believers and 15.8% non-believers. Lutheranism was the majority religion before World War II, but has now fallen to third place behind Russian Orthodoxy and Catholicism.

</doc>
<doc id="17774" url="http://en.wikipedia.org/wiki?curid=17774" title="Demographics of Lebanon">
Demographics of Lebanon

This article is about the demographic features of the population of Lebanon, including population density, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
The cultural and linguistic heritage of the Lebanese people is a blend of both indigenous elements and the foreign cultures that have come to rule the land and its people over the course of thousands of years. In a 2013 interview, the lead investigator, Pierre Zalloua, pointed out that genetic variation preceded religious variation and divisions: "Lebanon already had well-differentiated communities with their own genetic peculiarities, but not significant differences, and religions came as layers of paint on top. There is no distinct pattern that shows that one community carries significantly more Phoenician than another".
About 99% of the population of Lebanon includes numerous Muslim sects and Christian denominations. Because the matter of religious balance is a sensitive political issue, a national census has not been conducted since 1932, before the founding of the modern Lebanese state. Consequently there is an absence of accurate data on the relative percentages of the population of the major religions and groups. There are from 8.6 to 14 million Lebanese and descendants of Lebanese worldwide and not in Lebanon; about 80% of them are Christians
Ethnic groups.
Lebanese.
Ethnic background is an important factor in Lebanon. The country encompasses a great mix of cultural, religious, and ethnic groups which have been building up for more than 6,000 years. The Arabs invaded and occupied Phoenicia in the 7th century AD from Arabia. The predominant cultural backgrounds and ancestry of the Lebanese vary from Aramaean (Ancient Syria) to Canaanite (Phoenician), and Greek (Byzantine). Lebanese are overall genetically similar to the people of Cyprus and Malta and to the other modern Levantine populations, such as the Syrians and Palestinians. The question of ethnic identity has come to revolve more around aspects of cultural self-identification more than descent. Religious affiliation has also become a substitute in some respects for ethnic affiliation. Generally it can be said that all religious sects and denominations comprise many different ethnic backgrounds, and that clear ethnic boundaries are difficult to define due to common conversions and inter-faith marriages. Moreover, in a 2013 interview, the lead investigator, Pierre Zalloua, pointed out that genetic variation preceded religious variation and divisions: "Lebanon already had well-differentiated communities with their own genetic peculiarities, but not significant differences, and religions came as layers of paint on top. There is no distinct pattern that shows that one community carries significantly more Phoenician than another".
Religious groups.
The Phoenicians are some of the oldest Christians in the world, preceded only by the Coptic Christians of Kush (ancient Ethiopia) and Kemet (ancient Egypt). The Maronite Christians, belong to the West Syriac Rite. Their Liturgical language is the Syriac-Aramaic language. The Melkite Greek Catholics and the Greek Orthodox, tend to focus more on the Greco-Hellenistic heritage of the region from the days of the Byzantine Empire, and the fact that Greek was maintained as a liturgical language until very recently. Some Lebanese even claim partial descent from Crusader knights who ruled Lebanon for a couple of centuries during the Middle Ages, also backed by recent genetic studies which confirmed this among Lebanese people, especially in the north of the country that was under the Crusader County of Tripoli. This identification with non-Arab civilizations also exists in other religious communities, albeit not to the same extent.
The sectarian system.
Lebanon's religious divisions are extremely complicated, and the country is made up by a multitude of religious groupings. The ecclesiastical and demographic patterns of the sects and denominations are complex. Divisions and rivalries between groups date back as far as 15 centuries, and still are a factor today. The pattern of settlement has changed little since the 7th century, but instances of civil strife and ethnic cleansing, most recently during the Lebanese Civil War, has brought some important changes to the religious map of the country. (See also History of Lebanon.)
Lebanon has by far the largest proportion of Christians of any Middle Eastern country, but both Christians and Muslims are sub-divided into many splinter sects and denominations. Population statistics are highly controversial. The various denominations and sects each have vested interests in inflating their own numbers. Sunnis, Shia, Maronites and Greek Orthodox (the four largest denominations) all often claim that their particular religious affiliation holds a majority in the country, adding up to over 150% of the total population, even before counting the other denominations. One of the rare things that most Lebanese religious leaders will agree on is to avoid a new general census, for fear that it could trigger a new round of denominational conflict. The last official census was performed in 1932.
Religion has traditionally been of overriding importance in defining the Lebanese population. Dividing state power between the religious denominations and sects, and granting religious authorities judicial power, dates back to Ottoman times (the millet system). The practice was reinforced during French mandate, when Christian groups were granted privileges. This system of government, while partly intended as a compromise between sectarian demands, has caused tensions that still dominate Lebanese politics to this day.
The Christian population majority is believed to have ended in the early 1960s, but government leaders would agree to no change in the political power balance. This led to Muslim demands of increased representation, and the constant sectarian tension slid into violent conflict in 1958 (prompting U.S. intervention) and again in the grueling Lebanese Civil War, in 1975–90.
The balance of power has been slightly adjusted in the 1943 National Pact, an informal agreement struck at independence, in which positions of power were divided according to the 1932 census. The Sunni elite was then accorded more power, but Maronites continued to dominate the system. The sectarian balance was again adjusted towards the Muslim side but simultaneously further reinforced and legitimized. Shia Muslims (by now the second largest sect) then gained additional representation in the state apparatus, and the obligatory Christian-Muslim representation in Parliament was downgraded from a 6:5 to a 1:1 ratio. Christians of various denominations were then generally thought to constitute about 40% of the population, although often Muslim leaders would cite lower numbers, and some Christians would claim that they still held a majority of the population.
18 recognized religious groups.
The present Lebanese Constitution officially acknowledges 18 religious groups (see below). These have the right to handle family law according to their own courts and traditions, and they are the basic players in Lebanon's complex sectarian politics.
Religious population statistics.
The 1932 census stated that Christians made up 54% of the population. Maronites, largest among the Christian denomination and then largely in control of the state apparatus, accounted for 29% of the total resident population. But since the 19th century, Muslim birth rates have been continually higher than Christian birth rates. Also, far larger numbers of Christians emigrated from Lebanon than Muslims.
A study conducted by Statistics Lebanon, a Beirut-based research firm, cited by the United States Department of State found that of Lebanon's population of approximately 4.3 million is estimated to be:
There are also very small numbers of other religions such as Judaism, Bahá'í Faith, Buddhism, Hindusim and Mormons.
The CIA World Factbook specifies that of those residing in Lebanon, 59.7% are Muslims (Sunni, Shia, Druze, Alawites, and Sufi) and 39% are Christians (mostly Maronites, Eastern Orthodox, Melkite Catholics, Protestant, Armenian Apostolic, Assyrian Church of the East, Syriac Orthodox, Chaldean Catholic, Syrian Catholic) and 1.3% "Other".
Muslims.
According to the CIA World Factbook, the Muslim population is estimated at around 59.5% within the Lebanese territory and of the 8.6–14 million Lebanese diaspora is believed by some to be about 20% of the total population. Sectarian Breakdown:
Christians.
It is estimated that the Christian population in Lebanon makes up about 40.5% within the Lebanese territory and of the 8.6–14 million Lebanese diaspora is believed by some to be about 80% of the total population. Sectarian Breakdown:
Other religions.
Other religions account for only an estimated 0.3% of the population mainly foreign temporary workers, according to the CIA Factbook. There remains a very small Jewish population, traditionally centered in Beirut. It has been larger: most Jews left the country after the Lebanese Civil War (1975–1990) as thousands of Lebanese did at that time.
Diaspora.
Apart from the four and a half million citizens of Lebanon proper, there is a sizeable Lebanese diaspora. There are more Lebanese people living outside of Lebanon (8.6-14 million), than within (4.3 million). The majority of the diaspora population consists of Lebanese Christians; however, there are some who are Muslim. They trace their origin to several waves of Christian emigration, starting with the exodus that followed the 1860 Lebanon conflict in Ottoman Syria.
Under the current Lebanese nationality law, diaspora Lebanese do not have an automatic right of return to Lebanon. Due to varying degrees of assimilation and high degree of interethnic marriages, most diaspora Lebanese have not passed on the Arabic language to their children, while still maintaining a Lebanese ethnic identity.
Many Lebanese families are economically and politically prominent in several Latin American countries (in 2007 Mexican Carlos Slim Helú, son of Lebanese immigrants, was determined to be the wealthiest man in the World by Fortune Magazine), and make up a substantial portion of the Lebanese American community in the United States. The largest Lebanese diaspora is located in Brazil, where about 6–7 million people have Lebanese descent (see Lebanese Brazilian). In Argentina, there is also a large Lebanese diaspora of approximately 1.5 million people having Lebanese descent. (see Lebanese Argentine). In Canada, there is also a large Lebanese diaspora of approximately 250,000-500,000 people having Lebanese descent. (see Lebanese Canadians).
The large size of Lebanon's diaspora may be partly explained by the historical and cultural tradition of seafaring and traveling, which stretches back to Lebanon's ancient Phoenician origins and its role as a "gateway" of relations between Europe and the Middle East. It has been commonplace for Lebanese citizens to emigrate in search of economic prosperity. Furthermore, on several occasions in the last two centuries the Lebanese population has endured periods of ethnic cleansing and displacement (for example, 1840–60 and 1975–90). These factors have contributed to the geographical mobility of the Lebanese people.
While under Syrian occupation, Beirut passed legislation which prevented second-generation Lebanese of the diaspora from automatically obtaining Lebanese citizenship. This has reinforced the émigré status of many diaspora Lebanese. There is currently a campaign by those Lebanese of the diaspora who already have Lebanese citizenship to attain the vote from abroad, which has been successfully passed in the Lebanese parliament and will be effective as of 2013 which is the next parliamentary elections. If suffrage was to be extended to these 1.2 million Lebanese émigré citizens, it would have a significant political effect, since as many as 80% of them are believed to be Christian.
Lebanese Civil War refugees and displaced persons.
With no official figures available, it is estimated that 600,000–900,000 persons fled the country during the Lebanese Civil War (1975–90). Although some have since returned, this permanently disturbed Lebanese population growth and greatly complicated demographic statistics.
Another result of the war was a large number of internally displaced persons. This especially affected the southern Shia community, as Israeli invasion of southern Lebanon in 1978, 1982 and 1996 prompted waves of mass emigration, in addition to the continual strain of occupation and fighting between Israel and Hezbollah (mainly 1982 to 2000).
Many Shias from Southern Lebanon resettled in the suburbs south of Beirut. After the war, the pace of Christian emigration accelerated, as many Christians felt discriminated against in a Lebanon under increasingly oppressive Syrian occupation.
According to a UNDP study, as much as 10% of the Lebanese had a disability in 1990. Other studies have pointed to the fact that this portion of society is highly marginalized due to the lack of educational and governmental support of their advancement.
Languages.
Commonly spoken languages in Lebanon include Lebanese Arabic, French and English. The minority languages mainly spoken between their respective populations are Armenian, Kurdish, Greek and many others.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Immigrants and ethnic groups.
There are substantial numbers of immigrants from other Arab countries (mainly Palestine, Syria, Iraq and Egypt) and non-Arab-speaking Muslim countries. Also, recent years have seen an influx of people from Ethiopia and South East Asian countries such as Indonesia, the Philippines, Malaysia, Sri Lanka, as well as smaller numbers of other immigrant minorities, Colombians and Brazilians (of Lebanese descent themselves). Most of these are employed as guest workers in the same fashion as Syrians and Palestinians, and entered the country to search for employment in the post-war reconstruction of Lebanon. Apart from the Palestinians, there are approximately 180,000 stateless persons in Lebanon.
Armenians, Jews and Iranians.
Lebanese Armenians, Jews and Iranians form more distinct ethnic minorities, all of them in possession of a separate languages (Armenian, Hebrew, Farsi) and a national home area (Armenia, Israel, Iran) outside of Lebanon. However, they total 5% of the population.
French and Italians.
During the French Mandate of Lebanon, there was a fairly large French minority and a tiny Italian minority. Most of the French and Italian settlers left after Lebanese independence in 1943 and only 22,000 French Lebanese and 4,300 Italian Lebanese continue to live in Lebanon. The most important legacy of the French Mandate is the frequent use and knowledge of the French language by most of the educated Lebanese people.
Palestinians.
Around 450,000 Palestinian refugees were registered in Lebanon with the UNRWA in 2014, who are refugees or descendants of refugees from the 1948 Arab–Israeli War. Some 53% live in 12 Palestine refugee camps, who "suffer from serious problems" such as poverty and overcrowding. Some of these may have emigrated during the civil war, but there are no reliable figures available. There are also a number of Palestinians who are not registered as UNRWA refugees, because they left earlier than 1948 or were not in need of material assistance. The exact number of Palestinians remain a subject of great dispute and the Lebanese government will not provide an estimate. A figure of 400,000 Palestinian refugees would mean that Palestinians constitute more than 10% of the resident population of Lebanon.
Palestinians living in Lebanon are considered foreigners and are under the same restrictions on employment applied to other foreigners. Prior to 2010, they were under even more restrictive employment rules which permitted, other than work for the U.N., only the most menial employment. They are not allowed to attend public schools, own property, or make an enforceable will. Palestinian refugees, who constitute nearly a tenth of the country's population, have long been denied basic rights in Lebanon. They are not allowed to attend public schools, own property or pass on inheritances, measures Lebanon says it has adopted to preserve their right to return to their property in what constitutes Israel now.
Their presence is controversial, and resisted by large segments of the Christian population, who argue that the primarily Sunni Muslim Palestinians dilute Christian numbers. Many Shia Muslims also look unfavorably upon the Palestinian presence since the refugee camps have tended to be concentrated in their home areas. The Lebanese Sunnis, however, would be happy to see these Palestinians given the Lebanese nationality, thus increasing the Lebanese Sunni population by well over 10% and tipping the fragile electoral balance much in favor of the Sunnis. Late prime minister Rafiq Hariri —himself a Sunni— had hinted on more than one occasion on the inevitability of granting these refugees Lebanese citizenship. Thus far the refugees are Lebanese citizenship as well as many rights enjoyed by the rest of the population, and are confined to severely overcrowded refugee camps, in which construction rights are severely constricted.
Palestinians may not work in a large number of professions, such as lawyers and doctors. However, after negotiations between Lebanese authorities and ministers from the Palestinian National Authority some professions for Palestinians were allowed (such as taxi driver and construction worker). The material situation of the Palestinian refugees in Lebanon is difficult, and they are believed to constitute the poorest community in Lebanon, as well as the poorest Palestinian community with the possible exception of Gaza Strip refugees. Their primary sources of income are UNRWA aid and menial labor sought in competition with Syrian guest workers.
The Palestinians are almost totally Sunni Muslim, though at some point Christians counted as high as 40% with Muslims at 60%. The numbers of Palestinian Christians has diminished in later years, as many have managed to leave Lebanon. During the Lebanese Civil War, Palestinian Christians sided with the rest of the Palestinian community, instead of allying with Lebanese Eastern Orthodox or other Christian communities.
60,000 Palestinians have received Lebanese citizenship, including most Christian Palestinians.
Syrians.
In 1976, the then Syrian president Hafez al-Assad sent troops into Lebanon to fight PLO forces on behalf of Christian militias. This led to escalated fighting until a cease-fire agreement later that year that allowed for the stationing of Syrian troops within Lebanon. The Syrian presence in Lebanon quickly changed sides; soon after they entered Lebanon they had flip-flopped and began to fight the Christian nationalists in Lebanon they allegedly entered the country to protect. The Kateab Party and the Lebanese Forces under Bachir Gemayel strongly resisted the Syrians in Lebanon. In 1989, 40,000 Syrian troops remained in central and eastern Lebanon under the supervision of the Syrian government. Although, the Taif Accord, established in the same year, called for the removal of Syrian troops and transfer of arms to the Lebanese army, the Syrian Army remained in Lebanon until the Lebanese Cedar Revolution in 2005 to end the Syrian occupation of Lebanon.
In 1994, the Lebanese government under the pressure of the Syrian government, gave Lebanese passport to almost 250,000 Syrian citizens.
There are nearly 1.1 million registered Syrian refugees in Lebanon.
Assyrians.
There are an estimated 40,000 to 80,000 Iraqi Assyrian refugees in Lebanon. The vast majority of them are undocumented, with a large number having been deported or put in prison. They belong to various denominations, including the Assyrian Church of the East, Chaldean Catholic Church, and Syriac Catholic Church.
Iraqis.
Due to the US-led invasion of Iraq, Lebanon has received a mass influx of Iraqi refugees numbering at around 100,000. The vast majority of them are undocumented, with a large number having been deported or put in prison.
Kurds.
There are an estimated 60,000 to 100,000 Kurdish refugees from Turkey and Syria within Lebanese teritory. Many of them are undocumented. As of 2012, around 40% of all Kurds in Lebanon do not have Lebanese citizenship.
Also, there are about 200,000 Mardalli in Lebanon, i.e. people originating from the Mardin province in Turkey, most of them live in Beirut. The Mardallis are often referred to as Kurds by the Lebanese people due to their close culture and similar vocabulary with the Kurdish peoples of Mardin. But the Mardallis are a mix population, as is the population in Mardin itself.

</doc>
<doc id="17826" url="http://en.wikipedia.org/wiki?curid=17826" title="Transport in Lithuania">
Transport in Lithuania

This article provides an overview of the transport infrastructure of Lithuania. For transport in the Soviet Union, see Transport in the Soviet Union.
Railways.
There is a total of 1,998 route km of railways, of which:
Lithuanian road system.
(2006):
<br>"total:"
21,328.09 km
<br>"paved:"
12,912.22 km
<br>"unpaved:"
8,415.87 km
A road system.
The A roads (Magistraliniai keliai) total 1748.84 km. 
Waterways.
The are 600 km that are perennially navigable.
Pipelines.
In 1992, there were 105 km of crude oil pipelines, and 760 km of natural gas pipelines.
Merchant marine.
The merchant marine consists of 47 ships of 1,000 GRT or over, together totaling 279,743 GRT/ tonnes deadweight (DWT).
Ships by type: 
Cargo 25, Combination bulk 8, Petroleum tanker 2, Railcar carrier 1, Refrigerated cargo 6, Roll on/roll off 2, Short-sea passenger 3.
"Note:" These totals include some foreign-owned ships registered here as a flag of convenience: Denmark 13 (2002 est.)
Airports.
In Lithuania, there are four international airports:

</doc>
<doc id="17838" url="http://en.wikipedia.org/wiki?curid=17838" title="Foreign relations of Luxembourg">
Foreign relations of Luxembourg

Luxembourg has long been a prominent supporter of European political and economic integration. In efforts foreshadowing European integration, Luxembourg and Belgium in 1921 formed the Belgium-Luxembourg Economic Union (BLEU) to create an inter-exchangeable currency and a common customs regime. Luxembourg is a member of the Benelux Economic Union and was one of the founding members of the United Nations and the European Economic Community (now the European Union). It also participates in the Schengen Group (named after the Luxembourg village where the agreements were signed), whose goal is the free movement of citizens among member states. At the same time, the majority of Luxembourgers have consistently believed that European unity makes sense only in the context of a dynamic transatlantic relationship, and thus have traditionally pursued a pro-NATO, pro-US foreign policy.
Luxembourg is the site of the European Court of Justice, the European Court of Auditors, the Statistical Office of the European Communities (Eurostat) and other vital EU organs. The Secretariat of the European Parliament is located in Luxembourg, but the Parliament usually meets in nearby Strasbourg.
Luxembourg maintains embassies in the following countries.
Austria, Belgium, China, the Czech Republic, Denmark, France, Germany, Greece, India, Italy, Japan, Malaysia, the Netherlands, Poland, Portugal, Russia, Spain, Switzerland, the United Kingdom and the United States.
Luxembourg intends to open embassies in the following countries.
Turkey and the United Arab Emirates
The following countries maintain embassies in Luxembourg.
Austria, Belgium, Canada, China, Croatia, the Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Japan, the Netherlands, Poland, Portugal, Romania, Russia, Spain, Sweden, Switzerland, the United Kingdom and the United States
Relations by country/organisation.
Australia.
Australia is represented in Luxembourg through its embassy in Brussels (Belgium). Luxembourg is represented in Australia through the embassy of the Netherlands in Canberra and through an honorary consulate in East Lindfield.
Estonia.
Luxembourg recognised Estonia on February 22, 1923 and re-recognised Estonia on August 27, 1991. Both countries re-established diplomatic relations on August 29, 1991. Estonia is represented in Luxembourg through its embassy in Brussels (Belgium) and an honorary consulate in Luxembourg. Luxembourg is represented in Estonia through its embassy in Prague (Czech Republic). 
In 1937, a prominent Estonian political leader Artur Sirk, while a fugitive in Luxembourg was found dead, having apparently committed suicide by jumping out a second-story window. Although the Luxembourg Gendarmerie report assumed suicide, because of inconsistencies in the report, the Estonian chargé d'affaires in Paris, Rudolph Mollerson was sent to investigate. Estonian historians including Pusta and Tomingas have argued that the death was an act of defenestration by agents of the first President of Estonia, Konstantin Päts.
As of December 31, 2007, foreign investments made in Estonia originating from Luxembourg totaled 225 million EUR accounting for 2% of the total volume of foreign direct investments. There are about 300 Estonians living in Luxembourg. An Estonian cultural association was founded in 1998. The Estonian President Arnold Rüütel's state visit to Luxembourg was in May 2003, prime minister Andrus Ansip's in 2006. The Luxembourg Prime Minister Jean-Claude Juncker visited Estonia in 1999 and 2007.
Israel.
In November 1947, Luxembourg voted in favor of the partition plan to create a Jewish state. Israel and Luxembourg established full diplomatic relations in 1949. Due to Luxembourg's small size, the Israeli embassy is located in Brussels and Luxembourg is represented politically by the Dutch embassy and economically by the Belgian embassy.
Kosovo.
Luxembourg recognized the independence of Kosovo on 21 February 2008. Luxembourg has promised to give Kosovo €30 million over the next five years. Luxembourg said that the money was mainly for focus on professional training for Pristina and Mitrovica North authorities.
Romania.
Both countries established diplomatic relations on December 10, 1910. Since August 19, 1991, Romania has an embassy in Luxembourg City. Luxembourg is represented in Romania through its embassy in Athens (Greece) and two honorary consulates (in Bucharest and Sibiu).
Russia.
Luxembourg has an embassy in Moscow and an honorary consulate in Saint Petersburg. Russia has an embassy in the city of Luxembourg. 
Both countries are full members of the Council of Europe, the Organization for Security and Co-operation in Europe, and the United Nations.
In the history of bilateral relations, the first Russian president to come on an official visit to Luxembourg was Vladimir Putin on 24 May 2007. As bilateral trade had more than tripled from USD 66.6 million in 2003 to USD 228.3 million in 2006, time had come to strengthen the ties between the two countries, energy and finance being the key areas of cooperation between Russia and Luxembourg.
Turkey.
Luxembourg is represented in Turkey through its embassy in Ankara, which was opened in 1987. 
There are 500 ethnic Turk's living in Luxembourg, 200 of whom have dual citizenship. The trade volume between the two States had reached $217 million in 2011. 
Luxembourg strongly supports Turkey's candidacy as a full European Union member.
United States.
The United States, fighting on the Allied side, contributed to Luxembourg's liberation in World War I and World War II. More than 5,000 American soldiers, including U.S. Army General George S. Patton, are buried at the Luxembourg American Cemetery and Memorial near the capital of Luxembourg City, and there are monuments in many towns to American liberators. The strong U.S.-Luxembourg relationship is expressed both bilaterally and through common membership in NATO, the Organization for Economic Co-operation and Development (OECD), and the Organization for Security and Co-operation in Europe (OSCE).
Vietnam.
Both countries signed diplomatic relations in 1973. Luxembourg's representation in Vietnam is through its embassy in Beijing, China. Vietnam is represented through its embassy in Brussels, Belgium.

</doc>
<doc id="17902" url="http://en.wikipedia.org/wiki?curid=17902" title="Leonhard Euler">
Leonhard Euler

Leonhard Euler ( ; ], ];
15 April 1707 – 18 September 1783) was a pioneering Swiss mathematician and physicist. He made important discoveries in fields as diverse as infinitesimal calculus and graph theory. He also introduced much of the modern mathematical terminology and notation, particularly for mathematical analysis, such as the notion of a mathematical function. He is also renowned for his work in mechanics, fluid dynamics, optics, astronomy, and music theory.
Euler is considered to be the pre-eminent mathematician of the 18th century and one of the greatest mathematicians to have ever lived. He is also one of the most prolific mathematicians; his collected works fill 60–80 quarto volumes. He spent most of his adult life in St. Petersburg, Russia, and in Berlin, Prussia. While he chose to be a mathematician over becoming a pastor like his father, he was a devout Christian throughout his life.
A statement attributed to Pierre-Simon Laplace expresses Euler's influence on mathematics: "Read Euler, read Euler, he is the master of us all."
Life.
Early years.
Euler was born on 15 April 1707, in Basel, Switzerland to Paul Euler, a pastor of the Reformed Church, and Marguerite Brucker, a pastor's daughter. He had two younger sisters named Anna Maria and Maria Magdalena. Soon after the birth of Leonhard, the Eulers moved from Basel to the town of Riehen, where Euler spent most of his childhood. Paul Euler was a friend of the Bernoulli family—Johann Bernoulli, who was then regarded as Europe's foremost mathematician, would eventually be the most important influence on young Leonhard. Euler's early formal education started in Basel, where he was sent to live with his maternal grandmother. At the age of 13 he enrolled at the University of Basel, and in 1723, received his Master of Philosophy with a dissertation that compared the philosophies of Descartes and Newton. At this time, he was receiving Saturday afternoon lessons from Johann Bernoulli, who quickly discovered his new pupil's incredible talent for mathematics. Euler was at this point studying theology, Greek, and Hebrew at his father's urging, in order to become a pastor, but Bernoulli convinced Paul Euler that Leonhard was destined to become a great mathematician. In 1726, Euler completed a dissertation on the propagation of sound with the title "De Sono". At that time, he was pursuing an (ultimately unsuccessful) attempt to obtain a position at the University of Basel. In 1727, he first entered the "Paris Academy Prize Problem" competition; the problem that year was to find the best way to place the masts on a ship. Pierre Bouguer, a man who became known as "the father of naval architecture" won, and Euler took second place. Euler later won this annual prize twelve times.
St. Petersburg.
Around this time Johann Bernoulli's two sons, Daniel and Nicolas, were working at the Imperial Russian Academy of Sciences in St Petersburg. On 10 July 1726, Nicolas died of appendicitis after spending a year in Russia, and when Daniel assumed his brother's position in the mathematics/physics division, he recommended that the post in physiology that he had vacated be filled by his friend Euler. In November 1726 Euler eagerly accepted the offer, but delayed making the trip to St Petersburg while he unsuccessfully applied for a physics professorship at the University of Basel.
Euler arrived in the Russian capital on 17 May 1727. He was promoted from his junior post in the medical department of the academy to a position in the mathematics department. He lodged with Daniel Bernoulli with whom he often worked in close collaboration. Euler mastered Russian and settled into life in St Petersburg. He also took on an additional job as a medic in the Russian Navy.
The Academy at St. Petersburg, established by Peter the Great, was intended to improve education in Russia and to close the scientific gap with Western Europe. As a result, it was made especially attractive to foreign scholars like Euler. The academy possessed ample financial resources and a comprehensive library drawn from the private libraries of Peter himself and of the nobility. Very few students were enrolled in the academy in order to lessen the faculty's teaching burden, and the academy emphasized research and offered to its faculty both the time and the freedom to pursue scientific questions.
The Academy's benefactress, Catherine I, who had continued the progressive policies of her late husband, died on the day of Euler's arrival. The Russian nobility then gained power upon the ascension of the twelve-year-old Peter II. The nobility were suspicious of the academy's foreign scientists, and thus cut funding and caused other difficulties for Euler and his colleagues.
Conditions improved slightly upon the death of Peter II, and Euler swiftly rose through the ranks in the academy and was made professor of physics in 1731. Two years later, Daniel Bernoulli, who was fed up with the censorship and hostility he faced at St. Petersburg, left for Basel. Euler succeeded him as the head of the mathematics department.
On 7 January 1734, he married Katharina Gsell (1707–1773), a daughter of Georg Gsell, a painter from the Academy Gymnasium. The young couple bought a house by the Neva River. Of their thirteen children, only five survived childhood.
Berlin.
Concerned about the continuing turmoil in Russia, Euler left St. Petersburg on 19 June 1741 to take up a post at the "Berlin Academy", which he had been offered by Frederick the Great of Prussia. He lived for twenty-five years in Berlin, where he wrote over 380 articles. In Berlin, he published the two works for which he would become most renowned: The "Introductio in analysin infinitorum", a text on functions published in 1748, and the "Institutiones calculi differentialis", published in 1755 on differential calculus. In 1755, he was elected a foreign member of the Royal Swedish Academy of Sciences.
In addition, Euler was asked to tutor Friederike Charlotte of Brandenburg-Schwedt, the Princess of Anhalt-Dessau and Frederick's niece. Euler wrote over 200 letters to her in the early 1760s, which were later compiled into a best-selling volume entitled "Letters of Euler on different Subjects in Natural Philosophy Addressed to a German Princess". This work contained Euler's exposition on various subjects pertaining to physics and mathematics, as well as offering valuable insights into Euler's personality and religious beliefs. This book became more widely read than any of his mathematical works, and was published across Europe and in the United States. The popularity of the 'Letters' testifies to Euler's ability to communicate scientific matters effectively to a lay audience, a rare ability for a dedicated research scientist.
Despite Euler's immense contribution to the Academy's prestige, he was eventually forced to leave Berlin. This was partly because of a conflict of personality with Frederick, who came to regard Euler as unsophisticated, especially in comparison to the circle of philosophers the German king brought to the Academy. Voltaire was among those in Frederick's employ, and the Frenchman enjoyed a prominent position within the king's social circle. Euler, a simple religious man and a hard worker, was very conventional in his beliefs and tastes. He was in many ways the antithesis of Voltaire. Euler had limited training in rhetoric, and tended to debate matters that he knew little about, making him a frequent target of Voltaire's wit. Frederick also expressed disappointment with Euler's practical engineering abilities:
I wanted to have a water jet in my garden: Euler calculated the force of the wheels necessary to raise the water to a reservoir, from where it should fall back through channels, finally spurting out in Sanssouci. My mill was carried out geometrically and could not raise a mouthful of water closer than fifty paces to the reservoir. Vanity of vanities! Vanity of geometry!
Eyesight deterioration.
Euler's eyesight worsened throughout his mathematical career. Three years after suffering a near-fatal fever in 1735, he became almost blind in his right eye, but Euler rather blamed the painstaking work on cartography he performed for the St. Petersburg Academy for his condition. Euler's vision in that eye worsened throughout his stay in Germany, to the extent that Frederick referred to him as "Cyclops". Euler later developed a cataract in his left eye, which was discovered in 1766. Just a few weeks after its discovery, he was rendered almost totally blind. However, his condition appeared to have little effect on his productivity, as he compensated for it with his mental calculation skills and exquisite memory. For example, Euler could repeat the Aeneid of Virgil from beginning to end without hesitation, and for every page in the edition he could indicate which line was the first and which the last. With the aid of his scribes, Euler's productivity on many areas of study actually increased. He produced on average, one mathematical paper every week in the year 1775.
Return to Russia.
The situation in Russia had improved greatly since the accession to the throne of Catherine the Great, and in 1766 Euler accepted an invitation to return to the St. Petersburg Academy and spent the rest of his life in Russia. However, his second stay in the country was marred by tragedy. A fire in St. Petersburg in 1771 cost him his home, and almost his life. In 1773, he lost his wife Katharina after 40 years of marriage. Three years after his wife's death, Euler married her half-sister, Salome Abigail Gsell (1723–1794). This marriage lasted until his death. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1782.
In St. Petersburg on 18 September 1783, after a lunch with his family, during a conversation with a fellow academician Anders Johan Lexell, about the newly discovered planet Uranus and its orbit, Euler suffered a brain hemorrhage and died a few hours later. A short obituary for the Russian Academy of Sciences was written by and a more detailed eulogy was written and delivered at a memorial meeting by Russian mathematician Nicolas Fuss, one of Euler's disciples. In the eulogy written for the French Academy by the French mathematician and philosopher Marquis de Condorcet, he commented,
"il cessa de calculer et de vivre"—... he ceased to calculate and to live.
He was buried next to Katharina at the Smolensk Lutheran Cemetery on Vasilievsky Island. In 1785, the Russian Academy of Sciences put a marble bust of Leonhard Euler on a pedestal next to the Director's seat and, in 1837, placed a headstone on Euler's grave. To commemorate the 250th anniversary of Euler's birth, the headstone was moved in 1956, together with his remains, to the 18th-century necropolis at the Alexander Nevsky Monastery.
Contributions to mathematics and physics.
Euler worked in almost all areas of mathematics, such as geometry, infinitesimal calculus, trigonometry, algebra, and number theory, as well as continuum physics, lunar theory and other areas of physics. He is a seminal figure in the history of mathematics; if printed, his works, many of which are of fundamental interest, would occupy between 60 and 80 quarto volumes. Euler's name is associated with a large number of topics.
Euler is the only mathematician to have "two" numbers named after him: the important Euler's Number in calculus, "e", approximately equal to 2.71828, and the Euler-Mascheroni Constant γ (gamma) sometimes referred to as just "Euler's constant", approximately equal to 0.57721. It is not known whether γ is rational or irrational.
Mathematical notation.
Euler introduced and popularized several notational conventions through his numerous and widely circulated textbooks. Most notably, he introduced the concept of a function and was the first to write "f"("x") to denote the function "f" applied to the argument "x". He also introduced the modern notation for the trigonometric functions, the letter "e" for the base of the natural logarithm (now also known as Euler's number), the Greek letter Σ for summations and the letter "i" to denote the imaginary unit. The use of the Greek letter "π" to denote the ratio of a circle's circumference to its diameter was also popularized by Euler, although it did not originate with him.
Analysis.
The development of infinitesimal calculus was at the forefront of 18th Century mathematical research, and the Bernoullis—family friends of Euler—were responsible for much of the early progress in the field. Thanks to their influence, studying calculus became the major focus of Euler's work. While some of Euler's proofs are not acceptable by modern standards of mathematical rigour (in particular his reliance on the principle of the generality of algebra), his ideas led to many great advances.
Euler is well known in analysis for his frequent use and development of power series, the expression of functions as sums of infinitely many terms, such as
Notably, Euler directly proved the power series expansions for "e" and the inverse tangent function. (Indirect proof via the inverse power series technique was given by Newton and Leibniz between 1670 and 1680.) His daring use of power series enabled him to solve the famous Basel problem in 1735 (he provided a more elaborate argument in 1741):
Euler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms. He also defined the exponential function for complex numbers, and discovered its relation to the trigonometric functions. For any real number φ (taken to be radians), Euler's formula states that the complex exponential function satisfies
A special case of the above formula is known as Euler's identity,
called "the most remarkable formula in mathematics" by Richard P. Feynman, for its single uses of the notions of addition, multiplication, exponentiation, and equality, and the single uses of the important constants 0, 1, "e", "i" and π. In 1988, readers of the "Mathematical Intelligencer" voted it "the Most Beautiful Mathematical Formula Ever". In total, Euler was responsible for three of the top five formulae in that poll.
De Moivre's formula is a direct consequence of Euler's formula.
In addition, Euler elaborated the theory of higher transcendental functions by introducing the gamma function and introduced a new method for solving quartic equations. He also found a way to calculate integrals with complex limits, foreshadowing the development of modern complex analysis. He also invented the calculus of variations including its best-known result, the Euler–Lagrange equation.
Euler also pioneered the use of analytic methods to solve number theory problems. In doing so, he united two disparate branches of mathematics and introduced a new field of study, analytic number theory. In breaking ground for this new field, Euler created the theory of hypergeometric series, q-series, hyperbolic trigonometric functions and the analytic theory of continued fractions. For example, he proved the infinitude of primes using the divergence of the harmonic series, and he used analytic methods to gain some understanding of the way prime numbers are distributed. Euler's work in this area led to the development of the prime number theorem.
Number theory.
Euler's interest in number theory can be traced to the influence of Christian Goldbach, his friend in the St. Petersburg Academy. A lot of Euler's early work on number theory was based on the works of Pierre de Fermat. Euler developed some of Fermat's ideas, and disproved some of his conjectures.
Euler linked the nature of prime distribution with ideas in analysis. He proved that the sum of the reciprocals of the primes diverges. In doing so, he discovered the connection between the Riemann zeta function and the prime numbers; this is known as the Euler product formula for the Riemann zeta function.
Euler proved Newton's identities, Fermat's little theorem, Fermat's theorem on sums of two squares, and he made distinct contributions to Lagrange's four-square theorem. He also invented the totient function φ("n"), the number of positive integers less than or equal to the integer "n" that are coprime to "n". Using properties of this function, he generalized Fermat's little theorem to what is now known as Euler's theorem. He contributed significantly to the theory of perfect numbers, which had fascinated mathematicians since Euclid. He proved that the relationship shown between perfect numbers and Mersenne primes earlier proved by Euclid was one-to-one, a result otherwise known as the Euclid–Euler theorem. Euler also conjectured the law of quadratic reciprocity. The concept is regarded as a fundamental theorem of number theory, and his ideas paved the way for the work of Carl Friedrich Gauss.
By 1772 Euler had proved that 231 − 1 = 2,147,483,647 is a Mersenne prime. It may have remained the largest known prime until 1867.
Graph theory.
In 1735, Euler presented a solution to the problem known as the Seven Bridges of Königsberg. The city of Königsberg, Prussia was set on the Pregel River, and included two large islands that were connected to each other and the mainland by seven bridges. The problem is to decide whether it is possible to follow a path that crosses each bridge exactly once and returns to the starting point. It is not possible: there is no Eulerian circuit. This solution is considered to be the first theorem of graph theory, specifically of planar graph theory.
Euler also discovered the formula V − E + F = 2 relating the number of vertices, edges and faces of a convex polyhedron, and hence of a planar graph. The constant in this formula is now known as the Euler characteristic for the graph (or other mathematical object), and is related to the genus of the object. The study and generalization of this formula, specifically by Cauchy and L'Huillier, is at the origin of topology.
Applied mathematics.
Some of Euler's greatest successes were in solving real-world problems analytically, and in describing numerous applications of the Bernoulli numbers, Fourier series, Venn diagrams, Euler numbers, the constants e and π, continued fractions and integrals. He integrated Leibniz's differential calculus with Newton's Method of Fluxions, and developed tools that made it easier to apply calculus to physical problems. He made great strides in improving the numerical approximation of integrals, inventing what are now known as the Euler approximations. The most notable of these approximations are Euler's method and the Euler–Maclaurin formula. He also facilitated the use of differential equations, in particular introducing the Euler–Mascheroni constant:
One of Euler's more unusual interests was the application of mathematical ideas in music. In 1739 he wrote the "Tentamen novae theoriae musicae," hoping to eventually incorporate musical theory as part of mathematics. This part of his work, however, did not receive wide attention and was once described as too mathematical for musicians and too musical for mathematicians.
Physics and astronomy.
Euler helped develop the Euler–Bernoulli beam equation, which became a cornerstone of engineering. Aside from successfully applying his analytic tools to problems in classical mechanics, Euler also applied these techniques to celestial problems. His work in astronomy was recognized by a number of Paris Academy Prizes over the course of his career. His accomplishments include determining with great accuracy the orbits of comets and other celestial bodies, understanding the nature of comets, and calculating the parallax of the sun. His calculations also contributed to the development of accurate longitude tables.
In addition, Euler made important contributions in optics. He disagreed with Newton's corpuscular theory of light in the "Opticks", which was then the prevailing theory. His 1740s papers on optics helped ensure that the wave theory of light proposed by Christiaan Huygens would become the dominant mode of thought, at least until the development of the quantum theory of light.
In 1757 he published an important set of equations for inviscid flow, that are now known as the Euler equations. In differential form, the equations are:
where
Euler is also well known in structural engineering for his formula giving the critical buckling load of an ideal strut, which depends only on its length and flexural stiffness:
where
Logic.
Euler is also credited with using closed curves to illustrate syllogistic reasoning (1768). These diagrams have become known as Euler diagrams.
An Euler diagram is a diagrammatic means of representing sets and their relationships. Euler diagrams consist of simple closed curves (usually circles) in the plane that depict sets. Each Euler curve divides the plane into two regions or "zones": the interior, which symbolically represents the elements of the set, and the exterior, which represents all elements that are not members of the set. The sizes or shapes of the curves are not important: the significance of the diagram is in how they overlap. The spatial relationships between the regions bounded by each curve (overlap, containment or neither) corresponds to set-theoretic relationships (intersection, subset and disjointness). Curves whose interior zones do not intersect represent disjoint sets. Two curves whose interior zones intersect represent sets that have common elements; the zone inside both curves represents the set of elements common to both sets (the intersection of the sets). A curve that is contained completely within the interior zone of another represents a subset of it. Euler diagrams were incorporated as part of instruction in set theory as part of the new math movement in the 1960s. Since then, they have also been adopted by other curriculum fields such as reading.
Personal philosophy and religious beliefs.
Euler and his friend Daniel Bernoulli were opponents of Leibniz's monadism and the philosophy of Christian Wolff. Euler insisted that knowledge is founded in part on the basis of precise quantitative laws, something that monadism and Wolffian science were unable to provide. Euler's religious leanings might also have had a bearing on his dislike of the doctrine; he went so far as to label Wolff's ideas as "heathen and atheistic".
Much of what is known of Euler's religious beliefs can be deduced from his "Letters to a German Princess" and an earlier work, "Rettung der Göttlichen Offenbahrung Gegen die Einwürfe der Freygeister" ("Defense of the Divine Revelation against the Objections of the Freethinkers"). These works show that Euler was a devout Christian who believed the Bible to be inspired; the "Rettung" was primarily an argument for the divine inspiration of scripture.
There is a famous legend inspired by Euler's arguments with secular philosophers over religion, which is set during Euler's second stint at the St. Petersburg academy. The French philosopher Denis Diderot was visiting Russia on Catherine the Great's invitation. However, the Empress was alarmed that the philosopher's arguments for atheism were influencing members of her court, and so Euler was asked to confront the Frenchman. Diderot was informed that a learned mathematician had produced a proof of the existence of God: he agreed to view the proof as it was presented in court. Euler appeared, advanced toward Diderot, and in a tone of perfect conviction announced this non-sequitur: "Sir, formula_19, hence God exists—reply!"
Diderot, to whom (says the story) all mathematics was gibberish, stood dumbstruck as peals of laughter erupted from the court. Embarrassed, he asked to leave Russia, a request that was graciously granted by the Empress. However amusing the anecdote may be, it is apocryphal, given that Diderot himself did research in mathematics.
The legend was apparently first told by
Dieudonné Thiébault with significant embellishment by Augustus De Morgan.
Commemorations.
Euler was featured on the sixth series of the Swiss 10-franc banknote and on numerous Swiss, German, and Russian postage stamps. The asteroid 2002 Euler was named in his honor. He is also commemorated by the Lutheran Church on their Calendar of Saints on 24 May—he was a devout Christian (and believer in biblical inerrancy) who wrote apologetics and argued forcefully against the prominent atheists of his time.
On 15 April 2013, Euler's 306th birthday was celebrated with a Google Doodle.
Selected bibliography.
Euler has an extensive bibliography. His best-known books include:
A definitive collection of Euler's works, entitled "Opera Omnia", has been published since 1911 by the Euler Commission of the Swiss Academy of Sciences. A complete chronological list of Euler's works is available at the following page: "" (PDF).
Further reading.
</dl>

</doc>
<doc id="17927" url="http://en.wikipedia.org/wiki?curid=17927" title="Logic programming">
Logic programming

Logic programming is a programming paradigm based on formal logic. A program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of "clauses":
and are read declaratively as logical implications:
H is called the "head" of the rule and B1, …, Bn is called the "body". Facts are rules that have no body, and are written in the simplified form:
In the simplest case in which H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be under the control of the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
Consider, for example, the following clause:
based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X that is fallible by finding an X that is human. Even facts have a procedural interpretation. For example, the clause:
can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by "assigning" socrates to X.
The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.
History.
The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.
Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in Artificial Intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.
Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.
Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
The Association for Logic Programming was founded to promote Logic Programming in 1986.
Prolog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of concurrent logic programming languages, constraint logic programming languages and datalog.
Concepts.
Logic and control.
Logic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan
where "Logic" represents a logic program and "Control" represents different theorem-proving strategies.
Problem solving.
In the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an "and". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an "or".
Any search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.
In the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.
Negation as failure.
For most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A "clause" in a normal logic program has the form:
and is read declaratively as a logical implication:
where H and all the Ai and Bi are atomic formulas. The negation in the negative literals not Bi is commonly referred to as "negation as failure", because in most implementations, a negative condition not Bi is shown to hold by showing that the positive condition Bi fails to hold. For example:
Given the goal of finding something that can fly:
there are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, The second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore X = mary is the only solution of the goal.
Micro-Planner had a construct, called "thnot", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator is normally built-in in modern Prolog's implementations. It is normally written as codice_1 or codice_2, where codice_3 is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as codice_4 fails when the variable codice_5 has been bound to the atom codice_6, but it succeeds in all other cases, including when codice_5 is unbound. This makes Prolog's reasoning non-monotonic: codice_8 always fails, while codice_9 can succeed, binding codice_5 to codice_6, depending on whether codice_5 was initially bound (note that standard Prolog executes goals in left-to-right order).
The logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say
as a definition of the predicate
where "iff" means "if and only if". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation by failure needs only the if-halves of the definitions without the axioms of equality.
For example the completion of the program above is:
The notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.
As an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in "extended logic programming", to formalise such phrases as "the contrary can not be shown", where "contrary" is classical negation and "can not be shown" is the epistemic interpretation of negation as failure.
Knowledge representation.
The fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.
Despite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it has been shown to correspond, with some further extensions, quite naturally to the semi-formal language of legislation. It is also a natural language for expressing common-sense laws of cause and effect, as in the situation calculus and event calculus.
Variants and extensions.
Prolog.
The programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.
It was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation
which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, …, Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.
Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the "de facto" standard and strongly influenced the definition of ISO standard Prolog.
Abductive logic programming.
Abductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be "open" or undefined. A clause in an abductive logic program has the form:
where H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:
where the Bi are arbitrary literals (defined or abducible, and atomic or negated). For example:
where the predicate normal is abducible.
Problem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something that can fly:
Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.
Metalogic programming.
Because mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called "vanilla" meta-interpreter:
 solve(true).
 solve((A,B)):- solve(A),solve(B).
 solve(A):- clause(A,B),solve(B).
where true represents an empty conjunction, and clause(A,B) means there is an object-level clause of the form A :- B.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic that is specified by means of inference rules.
Constraint logic programming.
Constraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:
where H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:
However, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:
Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
The solution is 2010 ≤ T, T ≤ 2012.
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.
Concurrent logic programming.
Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).
A concurrent logic program is a set of guarded Horn clauses of the form:
The conjunction G1, … , Gn is called the guard of the clause, and | is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:
However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, … , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceedes with the subgoals B1, …, Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of "don't care nondeterminism", rather than "don't know nondeterminism".
For example, the following concurrent logic program defines a predicate shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:
Here, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of | in the second and third clauses is the list constructor, whereas the second occurrence of | is the commitment operator.) The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:
The program will non-deterministically generate a single solution, for example Merge = [ace, queen, 1, king, 4, 2].
Arguably, concurrent logic programming is based on message passing and consequently is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that, concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced [Hewitt and Agha, 1988]. However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Consequently, the indeterminacy of computations implies that not all logical consequences of the program can be deduced.
Concurrent constraint logic programming.
Concurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to the use of only one.
Inductive logic programming.
Inductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.
Higher-order logic programming.
Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.
Linear logic programming.
Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli, ACL, and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.
Object-oriented logic programming.
F-logic extends logic programming with objects and the frame syntax. A number of systems are based on F-logic, including Flora-2, , and a highly scalable commercial system .
Transaction logic programming.
Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.

</doc>
<doc id="17931" url="http://en.wikipedia.org/wiki?curid=17931" title="Lola Graham">
Lola Graham

Lola Glenn Graham was born in Melbourne, Australia on 23 September 1918. She first came to public attention after winning a musical competition at age six by playing the piano. In 1930 she was employed by Melbourne radio station 3?? and continued to work in radio for most of her life. She also worked extensively in live musical theatre as a band member and accompanist. Her repertoire included working with such artists as Barry Humphries. She married Fred Menhennitt on 23 February 1957 and had two sons. She died on 2 January 1992 after a long battle with cancer.

</doc>
<doc id="17932" url="http://en.wikipedia.org/wiki?curid=17932" title="Liquid-crystal display">
Liquid-crystal display

A liquid-crystal display (LCD) is a flat panel display, electronic visual display, or video display that uses the light modulating properties of liquid crystals. Liquid crystals do not emit light directly.
LCDs are available to display arbitrary images (as in a general-purpose computer display) or fixed images which can be displayed or hidden, such as preset words, digits, and 7-segment displays as in a digital clock. They use the same basic technology, except that arbitrary images are made up of a large number of small pixels, while other displays have larger elements.
LCDs are used in a wide range of applications including computer monitors, televisions, instrument panels, aircraft cockpit displays, and signage. They are common in consumer devices such as DVD players, gaming devices, clocks, watches, calculators, and telephones, and have replaced cathode ray tube (CRT) displays in most applications.
They are available in a wider range of screen sizes than CRT and plasma displays, and since they do not use phosphors, they do not suffer image burn-in. LCDs are, however, susceptible to image persistence.
The LCD screen is more energy efficient and can be disposed of more safely than a CRT. Its low electrical power consumption enables it to be used in battery-powered electronic equipment. It is an electronically modulated optical device made up of any number of segments filled with liquid crystals and arrayed in front of a light source (backlight) or reflector to produce images in color or monochrome. Liquid crystals were first discovered in 1888.
By 2008, annual sales of televisions with LCD screens exceeded sales of CRT units worldwide, and the CRT became obsolete for most purposes.
Overview.
Each pixel of an LCD typically consists of a layer of molecules aligned between two transparent electrodes, and two polarizing filters (parallel and perpendicular), the axes of transmission of which are (in most of the cases) perpendicular to each other. Without the liquid crystal between the polarizing filters, light passing through the first filter would be blocked by the second (crossed) polarizer.
Before an electric field is applied, the orientation of the liquid-crystal molecules is determined by the alignment at the surfaces of electrodes. In a twisted nematic device (still the most common liquid-crystal device), the surface alignment directions at the two electrodes are perpendicular to each other, and so the molecules arrange themselves in a helical structure, or twist. This induces the rotation of the polarization of the incident light, and the device appears gray. If the applied voltage is large enough, the liquid crystal molecules in the center of the layer are almost completely untwisted and the polarization of the incident light is not rotated as it passes through the liquid crystal layer. This light will then be mainly polarized perpendicular to the second filter, and thus be blocked and the pixel will appear black. By controlling the voltage applied across the liquid crystal layer in each pixel, light can be allowed to pass through in varying amounts thus constituting different levels of gray.
The optical effect of a twisted nematic device in the voltage-on state is far less dependent on variations in the device thickness than that in the voltage-off state. Because of this, these devices are usually operated between crossed polarizers such that they appear bright with no voltage (the eye is much more sensitive to variations in the dark state than the bright state). These devices can also be operated between parallel polarizers, in which case the bright and dark states are reversed. The voltage-off dark state in this configuration appears blotchy, however, because of small variations of thickness across the device.
Both the liquid crystal material and the alignment layer material contain ionic compounds. If an electric field of one particular polarity is applied for a long period of time, this ionic material is attracted to the surfaces and degrades the device performance. This is avoided either by applying an alternating current or by reversing the polarity of the electric field as the device is addressed (the response of the liquid crystal layer is identical, regardless of the polarity of the applied field).
Displays for a small number of individual digits and/or fixed symbols (as in digital watches and pocket calculators) can be implemented with independent electrodes for each segment. In contrast full alphanumeric and/or variable graphics displays are usually implemented with pixels arranged as a matrix consisting of electrically connected rows on one side of the LC layer and columns on the other side, which makes it possible to address each pixel at the intersections. The general method of matrix addressing consists of sequentially addressing one side of the matrix, for example by selecting the rows one-by-one and applying the picture information on the other side at the columns row-by-row. "For details on the various matrix addressing schemes see" Passive-matrix and active-matrix addressed LCDs.
History.
The origins and the complex history of liquid-crystal displays from the perspective of an insider during the early days were described by Joseph A. Castellano in "Liquid Gold: The Story of Liquid Crystal Displays and the Creation of an Industry".
Another report on the origins and history of LCD from a different perspective until 1991 has been published by Hiroshi Kawamoto, available at the IEEE History Center.
A description of Swiss contributions to LCD developments, written by Peter J. Wild, can be looked up as "IEEE First-Hand History".
Illumination.
Since LCD panels produce no light of their own, they require external light to produce a visible image. In a "transmissive" type of LCD, this light is provided at the back of the glass "stack" and is called the backlight. While passive-matrix displays are usually not backlit (e.g. calculators, wristwatches), active-matrix displays almost always are.
The common implementations of LCD backlight technology are:
Today, most LCD screens are being designed with an LED backlight instead of the traditional CCFL backlight.
Connection to other circuits.
LCD panels typically use thinly-coated metallic conductive pathways on a glass substrate to form the cell circuitry to operate the panel. It is usually not possible to use soldering techniques to directly connect the panel to a separate copper-etched circuit board.
Instead, interfacing is accomplished using either adhesive plastic ribbon with conductive traces glued to the edges of the LCD panel, or with an elastomeric connector, which is a strip of rubber or silicone with alternating layers of conductive and insulating pathways, pressed between contact pads on the LCD and mating contact pads on a circuit board.
Passive and active-matrix.
Monochrome and later color passive-matrix LCDs were standard in most early laptops (although a few used plasma displays) and the original Nintendo Game Boy until the mid-1990s, when color active-matrix became standard on all laptops. The commercially unsuccessful Macintosh Portable (released in 1989) was one of the first to use an active-matrix display (though still monochrome).
Passive-matrix LCDs are still used today for applications less demanding than laptops and TVs. In particular, these are used on portable devices where less information content needs to be displayed, lowest power consumption (no backlight) and low cost are desired, and/or readability in direct sunlight is needed.
Displays having a passive-matrix structure are employing "super-twisted nematic" STN (invented by Brown Boveri Research Center, Baden, Switzerland, in 1983; scientific details were published) or double-layer STN (DSTN) technology (the latter of which addresses a color-shifting problem with the former), and color-STN (CSTN) in which color is added by using an internal filter.
STN LCDs have been optimized for passive-matrix addressing. They exhibit a sharper threshold of the contrast-vs-voltage characteristic than the original TN LCDs. This is important, because pixels are subjected to partial voltages even while not selected. Crosstalk between activated and non-activated pixels has to be handled properly by keeping the RMS voltage of non-activated pixels below the threshold voltage, while activated pixels are subjected to voltages above threshold. STN LCDs have to be continuously refreshed by alternating pulsed voltages of one polarity during one frame and pulses of opposite polarity during the next frame. Individual pixels are addressed by the corresponding row and column circuits. This type of display is called "passive-matrix addressed", because the pixel must retain its state between refreshes without the benefit of a steady electrical charge. As the number of pixels (and, correspondingly, columns and rows) increases, this type of display becomes less feasible. Slow response times and poor contrast are typical of passive-matrix addressed LCDs with too many pixels.
New "zero-power (bistable) LCDs" do not require continuous refreshing. Rewriting is only required for picture information changes. Potentially, passive-matrix addressing can be used with these new devices, if their write/erase characteristics are suitable.
High-resolution color displays, such as modern LCD computer monitors and televisions, use an active-matrix structure. A matrix of thin-film transistors (TFTs) is added to the electrodes in contact with the LC layer. Each pixel has its own dedicated transistor, allowing each column line to access one pixel. When a row line is selected, all of the column lines are connected to a row of pixels and voltages corresponding to the picture information are driven onto all of the column lines. The row line is then deactivated and the next row line is selected. All of the row lines are selected in sequence during a refresh operation. Active-matrix addressed displays look brighter and sharper than passive-matrix addressed displays of the same size, and generally have quicker response times, producing much better images.
Active-matrix technologies.
Twisted nematic (TN).
Twisted nematic displays contain liquid crystals that twist and untwist at varying degrees to allow light to pass through. When no voltage is applied to a TN liquid crystal cell, polarized light passes through the 90-degrees twisted LC layer. In proportion to the voltage applied, the liquid crystals untwist changing the polarization and blocking the light's path. By properly adjusting the level of the voltage almost any gray level or transmission can be achieved.
In-plane switching (IPS).
In-plane switching is an LCD technology that aligns the liquid crystals in a plane parallel to the glass substrates. In this method, the electrical field is applied through opposite electrodes on the same glass substrate, so that the liquid crystals can be reoriented (switched) essentially in the same plane, although fringe fields inhibit a homogeneous reorientation. This requires two transistors for each pixel instead of the single transistor needed for a standard thin-film transistor (TFT) display. Before LG Enhanced IPS was introduced in 2009, the additional transistors resulted in blocking more transmission area, thus requiring a brighter backlight and consuming more power, making this type of display less desirable for notebook computers. Currently Panasonic is using an enhanced version eIPS for their large size LCD-TV products as well as Hewlett-Packard in its WebOS based TouchPad tablet and their Chromebook 11.
IPS LCD vs AMOLED.
LG claimed the smartphone LG Optimus Black (IPS LCD (LCD NOVA)) has the brightness up to 700 nits, while the competitor has only IPS LCD with 518 nits and double an active-matrix OLED (AMOLED) display with 305 nits. LG also claimed the NOVA display to be 50 percent more efficient than regular LCDs and to consume only 50 percent of the power of AMOLED displays when producing white on screen. When it comes to contrast ratio, AMOLED display still performs best due to its underlying technology, where the black levels are displayed as pitch black and not as dark gray. On August 24, 2011, Nokia announced the Nokia 701 and also made the claim of the world's brightest display at 1000 nits. The screen also had Nokia's Clearblack layer, improving the contrast ratio and bringing it closer to that of the AMOLED screens.
Super In-plane switching (S-IPS).
Super-IPS was later introduced after in-plane switching with even better response times and color reproduction.
Advanced fringe field switching (AFFS).
Known as fringe field switching (FFS) until 2003,
advanced fringe field switching is similar to IPS or S-IPS offering superior performance and color gamut with high luminosity. AFFS was developed by Hydis Technologies Co., Ltd, Korea (formally Hyundai Electronics, LCD Task Force).
AFFS-applied notebook applications minimize color distortion while maintaining a wider viewing angle for a professional display. Color shift and deviation caused by light leakage is corrected by optimizing the white gamut which also enhances white/gray reproduction.
In 2004, Hydis Technologies Co., Ltd licensed AFFS to Japan's Hitachi Displays. Hitachi is using AFFS to manufacture high-end panels. In 2006, HYDIS licensed AFFS to Sanyo Epson Imaging Devices Corporation.
Shortly thereafter, Hydis introduced a high-transmittance evolution of the AFFS display, called HFFS (FFS+).
Hydis introduced AFFS+ with improved outdoor readability in 2007. AFFS panels are mostly utilized in the cockpits of latest commercial aircraft displays. But is no longer produced as of February 2015.
Vertical alignment (VA).
Vertical-alignment displays are a form of LCDs in which the liquid crystals naturally align vertically to the glass substrates. When no voltage is applied, the liquid crystals remain perpendicular to the substrate, creating a black display between crossed polarizers. When voltage is applied, the liquid crystals shift to a tilted position, allowing light to pass through and create a gray-scale display depending on the amount of tilt generated by the electric field. It has a deeper-black background, a higher contrast ratio, a wider viewing angle, and better image quality at extreme temperatures over traditional twisted-nematic displays.
Blue phase mode.
Blue phase mode LCDs have been shown as engineering samples early in 2008, but they are not in mass-production yet. The physics of blue phase mode LCDs suggest that very short switching times (~1 ms) can be achieved, so time sequential color control can possibly be realized and expensive color filters would be obsolete.
Quality control.
Some LCD panels have defective transistors, causing permanently lit or unlit pixels which are commonly referred to as stuck pixels or dead pixels respectively. Unlike integrated circuits (ICs), LCD panels with a few defective transistors are usually still usable. Manufacturers' policies for the acceptable number of defective pixels vary greatly. At one point, Samsung held a zero-tolerance policy for LCD monitors sold in Korea. As of 2005, though, Samsung adheres to the less restrictive ISO 13406-2 standard. Other companies have been known to tolerate as many as 11 dead pixels in their policies. Dead pixel policies are often hotly debated between manufacturers and customers. To regulate the acceptability of defects and to protect the end user, ISO released the ISO 13406-2 standard. However, not every LCD manufacturer conforms to the ISO standard and the ISO standard is quite often interpreted in different ways.
LCD panels are more likely to have defects than most ICs due to their larger size. For example, a 300 mm SVGA LCD has 8 defects and a 150 mm wafer has only 3 defects. However, 134 of the 137 dies on the wafer will be acceptable, whereas rejection of the whole LCD panel would be a 0% yield. In recent years, quality control has been improved. An SVGA LCD panel with 4 defective pixels is usually considered defective and customers can request an exchange for a new one. Some manufacturers, notably in South Korea where some of the largest LCD panel manufacturers, such as LG, are located, now have "zero defective pixel guarantee", which is an extra screening process which can then determine "A" and "B" grade panels. Many manufacturers would replace a product even with one defective pixel. Even where such guarantees do not exist, the location of defective pixels is important. A display with only a few defective pixels may be unacceptable if the defective pixels are near each other.
LCD panels also have defects known as "clouding" (or less commonly "mura"), which describes the uneven patches of changes in luminance. It is most visible in dark or black areas of displayed scenes.
Zero-power (bistable) displays.
The zenithal bistable device (ZBD), developed by QinetiQ (formerly DERA), can retain an image without power. The crystals may exist in one of two stable orientations ("Black" and "White") and power is only required to change the image. ZBD Displays is a spin-off company from QinetiQ who manufacture both grayscale and color ZBD devices.
Kent Displays has also developed a "no power" display that uses polymer stabilized cholesteric liquid crystal (ChLCD). In 2009 Kent demonstrated the use of a ChLCD to cover the entire surface of a mobile phone, allowing it to change colors, and keep that color even when power is cut off.
In 2004 researchers at the University of Oxford demonstrated two new types of zero-power bistable LCDs based on Zenithal bistable techniques.
Several bistable technologies, like the 360° BTN and the bistable cholesteric, depend mainly on the bulk properties of the liquid crystal (LC) and use standard strong anchoring, with alignment films and LC mixtures similar to the traditional monostable materials. Other bistable technologies, e.g. BiNem technology, are based mainly on the surface properties and need specific weak anchoring materials.
Specifications.
Important factors to consider when evaluating an LCD:
The spatial resolution of an LCD is expressed by the number of columns and rows of pixels (e.g., 1024×768). Each pixel is usually composed 3 sub-pixels, a red, a green, and a blue one. This had been one of the few features of LCD performance that was easily understood and not subject to interpretation. However, there are newer schemes that share sub-pixels among pixels and to add additional colors of sub-pixels. So going forward, spatial resolution may now be more subject to interpretation.
One external factor to consider in evaluating display resolution is the resolution of the viewer's eyes. Assuming 20/20 vision, the resolution of the eyes is about one minute of arc. In practical terms that means for an older standard definition TV set the ideal viewing distance was about 8 times the height (not diagonal) of the screen away. At that distance the individual rows of pixels merge into a solid. If the viewer were closer to the screen than that, they would be able to see the individual rows of pixels. When observed from farther away, the image of the rows of pixels still merge, but the total image becomes smaller as the distance increases. For an HDTV set with slightly more than twice the number of rows of pixels, the ideal viewing distance is about half what it is for a standard definition set. The higher the resolution, the closer the viewer can sit or the larger the set can usefully be sitting at the same distance as an older standard definition display.
For a computer monitor or some other LCD that is being viewed from a very close distance, resolution is often expressed in terms of dot pitch or pixels per inch. This is consistent with the printing industry (another form of a display). Magazines, and other premium printed media are often at 300 dots per inch. As with the distance discussion above, this provides a very solid looking and detailed image. LCDs, particularly on mobile devices, are frequently much less than this as the higher the dot pitch, the more optically inefficient the display and the more power it burns. Running the LCD is frequently half, or more, of the power consumed by a mobile device.
An additional consideration in spatial performance are viewing cone and aspect ratio. The Aspect ratio is the ratio of the width to the height (for example, 4:3, 5:4, 16:9 or 16:10). Older, standard definition TVs were 4:3. Newer High Definition televisions (HDTV) are 16:9, as are most new notebook computers. Movies are often filmed in much different (wider) aspect ratios, which is why there will frequently still be black bars at the top and bottom of an HDTV screen.
The Viewing Angle of an LCD may be important depending on its use or location. The viewing angle is usually measured as the angle where the contrast of the LCD falls below 10:1. At this point, the colors usually start to change and can even invert, red becoming green and so forth. Viewing angles for LCDs used to be very restrictive however, improved optical films have been developed that give almost 180 degree viewing angles from left to right. Top to bottom viewing angles may still be restrictive, by design, as looking at an LCD from an extreme up or down angle is not a common usage model and these photons are wasted. Manufacturers commonly focus the light in a left to right plane to obtain a brighter image here.
Refresh rate or the temporal resolution of an LCD is the number of times per second in which the display draws the data it is being given. Since activated LCD pixels do not flash on/off between frames, LCD monitors exhibit no refresh-induced flicker, no matter how low the refresh rate. High-end LCD televisions now feature up to 240 Hz refresh rate, which requires advanced digital processing to insert additional interpolated frames between the real images to smooth the image motion. However, such high refresh rates may not be actually supported by pixel response times and the result can be visual artifacts that distort the image in unpleasant ways.
Temporal performance can be further taxed if it is a 3D display. 3D displays work by showing a different series of images to each eye, alternating from eye to eye. Thus a 3D display must display twice as many images in the same period of time as a conventional display, and consequently the response time of the LCD is more important. 3D LCDs with marginal response times will exhibit image smearing.
These artifacts are most noticeable in a person's black and white vision (rod cells) than in color vision (cone cells). Thus they will be more likely to see flicker or any sort of temporal distortion in a display image by not looking directly at the display, because their eyes' rod cells are mostly grouped at the periphery of their vision.
When color depth is reported as color support, it is usually stated in terms of number of colors the LCD can show. The number of colors is the translation from the base 2-bit numbers into common base-10. For example, 8-bit color is 2 to the 8th power, which is 256 colors. 24-bit color is 2 to the 24th power, or 256 x 256 x 256, a total of 16,777,216 colors. The color resolution of the human eye depends on both the range of colors being sliced and the number of slices; but for most common displays the limit is about 28-bit color. LCD TVs commonly display more than that as the digital processing can introduce color distortions and the additional levels of color are needed to ensure true colors.
There are additional aspects to LCD color and color management, such as white point and gamma correction, which describe what color white is and how the other colors are displayed relative to white. LCD televisions also frequently have facial recognition software, which recognizes that an image on the screen is a face and both adjust the color and the focus differently from the rest of the image. These adjustments can have important effects on the consumer, but are not easily quantifiable; people like what they like and everyone does not like the same thing. There is no substitute for looking at the LCD one is going to buy before buying it. Portrait film, another form of display, has similar adjustments built into it. Many years ago, Kodak had to overcome initial rejection of its portrait film in Japan because of these adjustments. In the U.S., people generally prefer a more colorful facial image than in reality (higher color saturation). In Japan, consumers generally prefer a less saturated image. The film that Kodak initially sent to Japan was biased in the wrong direction for Japanese consumers. Television monitors have their built-in biases as well.
The first caveat is that contrast ratios are measured in a completely dark room. In actual use, the room is never completely dark, as one will always have the light from the LCD itself. Beyond that, there may be sunlight coming in through a window or other room lights that reflect off of the surface of the LCD and degrades the contrast. As a practical matter, the contrast of an LCD, or any display, is governed by the amount of surface reflections, not by the performance of the display.
The second caveat is that the human eye can only image a contrast ratio of a maximum of about 200:1. Black print on a white paper is about 15–20:1. That is why viewing angles are specified to the point where they fall below 10:1. A 10:1 image is not great, but is discernible.
Brightness is usually stated as the maximum output of the LCD. In the CRT era, Trinitron CRTs had a brightness advantage over the competition, so brightness was commonly discussed in TV advertising. With current LCD technology, brightness, though important, is usually similar from maker to maker and consequently is not discussed much, except for laptop LCDs and other displays that will be viewed in bright sunlight. In general, brighter is better, but there is always a trade-off between brightness and battery life in a mobile device.

</doc>
<doc id="17945" url="http://en.wikipedia.org/wiki?curid=17945" title="Lie group">
Lie group

In mathematics, a Lie group is a group that is also a differentiable manifold, with the property that the group operations are compatible with the smooth structure. Lie groups are named after Sophus Lie, who laid the foundations of the theory of continuous transformation groups. The term "groupes de Lie" first appeared in French in 1893 in the thesis of Lie’s student Arthur Tresse, page 3.
Lie groups represent the best-developed theory of continuous symmetry of mathematical objects and structures, which makes them indispensable tools for many parts of contemporary mathematics, as well as for modern theoretical physics. They provide a natural framework for analysing the continuous symmetries of differential equations (differential Galois theory), in much the same way as permutation groups are used in Galois theory for analysing the discrete symmetries of algebraic equations. An extension of Galois theory to the case of continuous symmetry groups was one of Lie's principal motivations.
Overview.
Lie groups are smooth differentiable manifolds and as such can be studied using differential calculus, in contrast with the case of more general topological groups. One of the key ideas in the theory of Lie groups is to replace the "global" object, the group, with its "local" or linearized version, which Lie himself called its "infinitesimal group" and which has since become known as its Lie algebra.
Lie groups play an enormous role in modern geometry, on several different levels. Felix Klein argued in his Erlangen program that one can consider various "geometries" by specifying an appropriate transformation group that leaves certain geometric properties invariant. Thus Euclidean geometry corresponds to the choice of the group E(3) of distance-preserving transformations of the Euclidean space R3, conformal geometry corresponds to enlarging the group to the conformal group, whereas in projective geometry one is interested in the properties invariant under the projective group. This idea later led to the notion of a G-structure, where "G" is a Lie group of "local" symmetries of a manifold. On a "global" level, whenever a Lie group acts on a geometric object, such as a Riemannian or a symplectic manifold, this action provides a measure of rigidity and yields a rich algebraic structure. The presence of continuous symmetries expressed via a Lie group action on a manifold places strong constraints on its geometry and facilitates analysis on the manifold. Linear actions of Lie groups are especially important, and are studied in representation theory.
In the 1940s–1950s, Ellis Kolchin, Armand Borel, and Claude Chevalley realised that many foundational results concerning Lie groups can be developed completely algebraically, giving rise to the theory of algebraic groups defined over an arbitrary field. This insight opened new possibilities in pure algebra, by providing a uniform construction for most finite simple groups, as well as in algebraic geometry. The theory of automorphic forms, an important branch of modern number theory, deals extensively with analogues of Lie groups over adele rings; p-adic Lie groups play an important role, via their connections with Galois representations in number theory.
Definitions and examples.
A real Lie group is a group that is also a finite-dimensional real smooth manifold, in which the group operations of multiplication and inversion are smooth maps. Smoothness of the group multiplication
means that μ is a smooth mapping of the product manifold "G"×"G" into "G". These two requirements can be combined to the single requirement that the mapping
be a smooth mapping of the product manifold into "G".
First examples.
All of the previous examples of Lie groups fall within the class of classical groups.
Related concepts.
A complex Lie group is defined in the same way using complex manifolds rather than real ones (example: SL(2, C)), and similarly, using an alternate metric completion of Q, one can define a "p"-adic Lie group over the "p"-adic numbers, a topological group in which each point has a "p"-adic neighborhood. Hilbert's fifth problem asked whether replacing differentiable manifolds with topological or analytic ones can yield new examples. The answer to this question turned out to be negative: in 1952, Gleason, Montgomery and Zippin showed that if "G" is a topological manifold with continuous group operations, then there exists exactly one analytic structure on "G" which turns it into a Lie group (see also Hilbert–Smith conjecture). If the underlying manifold is allowed to be infinite-dimensional (for example, a Hilbert manifold), then one arrives at the notion of an infinite-dimensional Lie group. It is possible to define analogues of many Lie groups over finite fields, and these give most of the examples of finite simple groups.
The language of category theory provides a concise definition for Lie groups: a Lie group is a group object in the category of smooth manifolds. This is important, because it allows generalization of the notion of a Lie group to Lie supergroups.
More examples of Lie groups.
Lie groups occur in abundance throughout mathematics and physics. Matrix groups or algebraic groups are (roughly) groups of matrices (for example, orthogonal and symplectic groups), and these give most of the more common examples of Lie groups.
Constructions.
There are several standard ways to form new Lie groups from old ones:
Related notions.
Some examples of groups that are "not" Lie groups (except in the trivial sense that any group can be viewed as a 0-dimensional Lie group, with the discrete topology), are:
Basic concepts.
The Lie algebra associated with a Lie group.
To every Lie group we can associate a Lie algebra whose underlying vector space is the tangent space of the Lie group at the identity element and which completely captures the local structure of the group. Informally we can think of elements of the Lie algebra as elements of the group that are "infinitesimally close" to the identity, and the Lie bracket is related to the commutator of two such infinitesimal elements. Before giving the abstract definition we give a few examples:
The concrete definition given above is easy to work with, but has some minor problems: to use it we first need to represent a Lie group as a group of matrices, but not all Lie groups can be represented in this way, and it is not obvious that the Lie algebra is independent of the representation we use. To get around these problems we give 
the general definition of the Lie algebra of a Lie group (in 4 steps):
This Lie algebra formula_7 is finite-dimensional and it has the same dimension as the manifold "G". The Lie algebra of "G" determines "G" up to "local isomorphism", where two Lie groups are called locally isomorphic if they look the same near the identity element.
Problems about Lie groups are often solved by first solving the corresponding problem for the Lie algebras, and the result for groups then usually follows easily. 
For example, simple Lie groups are usually classified by first classifying the corresponding Lie algebras.
We could also define a Lie algebra structure on "Te" using right invariant vector fields instead of left invariant vector fields. This leads to the same Lie algebra, because the inverse map on "G" can be used to identify left invariant vector fields with right invariant vector fields, and acts as −1 on the tangent space "Te".
The Lie algebra structure on "Te" can also be described as follows:
the commutator operation
on "G" × "G" sends ("e", "e") to "e", so its derivative yields a bilinear operation on "TeG". This bilinear operation is actually the zero map, but the second derivative, under the proper identification of tangent spaces, yields an operation that satisfies the axioms of a Lie bracket, and it is equal to twice the one defined through left-invariant vector fields.
Homomorphisms and isomorphisms.
If "G" and "H" are Lie groups, then a Lie group homomorphism "f" : "G" → "H" is a smooth group homomorphism. In the case of complex Lie groups, such a homomorphism is required to be a holomorphic map. However, these requirements are a bit stringent; over real or complex numbers, every continuous homomorphism between Lie groups turns out to be (real or complex) analytic.
The composition of two Lie homomorphisms is again a homomorphism, and the class of all Lie groups, together with these morphisms, forms a category. Moreover, every Lie group homomorphism induces a homomorphism between the corresponding Lie algebras. Let formula_9 be a Lie group homomorphism and let formula_10 be its derivative at the identity. If we identify the Lie algebras of "G" and "H" with their tangent spaces at the identity elements then formula_10 is a map between the corresponding Lie algebras:
One can show that formula_10 is actually a Lie algebra homomorphism (meaning that it is a linear map which preserves the Lie bracket). In the language of category theory, we then have a covariant functor from the category of Lie groups to the category of Lie algebras which sends a Lie group to its Lie algebra and a Lie group homomorphism to its derivative at the identity.
Two Lie groups are called "isomorphic" if there exists a bijective homomorphism between them whose inverse is also a Lie group homomorphism. Equivalently, it is a diffeomorphism which is also a group homomorphism.
Ado's theorem says every finite-dimensional Lie algebra is isomorphic to a matrix Lie algebra. For every finite-dimensional matrix Lie algebra, there is a linear group (matrix Lie group) with this algebra as its Lie algebra. So every abstract Lie algebra is the Lie algebra of some (linear) Lie group.
The "global structure" of a Lie group is not determined by its Lie algebra; for example, if "Z" is any discrete subgroup of the center of "G" then "G" and "G"/"Z" have the same Lie algebra (see the table of Lie groups for examples). 
A "connected" Lie group is simple, semisimple, solvable, nilpotent, or abelian if and only if its Lie algebra has the corresponding property.
If we require that the Lie group be simply connected, then the global structure is determined by its Lie algebra: for every finite-dimensional Lie algebra formula_7 over F there is a simply connected Lie group "G" with formula_7 as Lie algebra, unique up to isomorphism. Moreover every homomorphism between Lie algebras lifts to a unique homomorphism between the corresponding simply connected Lie groups.
The exponential map.
The exponential map from the Lie algebra M("n", R) of the general linear group GL("n", R) to GL("n", R) is defined by the usual power series:
for matrices "A". If "G" is any subgroup of GL("n", R), then the exponential map takes the Lie algebra of "G" into "G", so we have an exponential map for all matrix groups.
The definition above is easy to use, but it is not defined for Lie groups that are not matrix groups, and it is not clear that the exponential map of a Lie group does not depend on its representation as a matrix group. We can solve both problems using a more abstract definition of the exponential map that works for all Lie groups, as follows.
Every vector "v" in formula_7 determines a linear map from R to formula_7 taking 1 to "v", which can be thought of as a Lie algebra homomorphism. Because R is the Lie algebra of the simply connected Lie group R, this induces a Lie group homomorphism "c" : R → "G" so that
for all "s" and "t". The operation on the right hand side is the group multiplication in "G". The formal similarity of this formula with the one valid for the exponential function justifies the definition
This is called the exponential map, and it maps the Lie algebra formula_7 into the Lie group "G". It provides a diffeomorphism between a neighborhood of 0 in formula_7 and a neighborhood of "e" in "G". This exponential map is a generalization of the exponential function for real numbers (because R is the Lie algebra of the Lie group of positive real numbers with multiplication), for complex numbers (because C is the Lie algebra of the Lie group of non-zero complex numbers with multiplication) and for matrices (because M("n", R) with the regular commutator is the Lie algebra of the Lie group GL("n", R) of all invertible matrices).
Because the exponential map is surjective on some neighbourhood "N" of "e", it is common to call elements of the Lie algebra infinitesimal generators of the group "G". The subgroup of "G" generated by "N" is the identity component of "G".
The exponential map and the Lie algebra determine the "local group structure" of every connected Lie group, because of the Baker–Campbell–Hausdorff formula: there exists a neighborhood "U" of the zero element of formula_7, such that for "u", "v" in "U" we have
where the omitted terms are known and involve Lie brackets of four or more elements. In case "u" and "v" commute, this formula reduces to the familiar exponential law exp("u") exp("v") = exp("u" + "v").
The exponential map relates Lie group homomorphisms. That is, if formula_25 is a Lie group homomorphism and formula_26 the induced map on the corresponding Lie algebras, then for all formula_27 we have
In other words the following diagram commutes,
The exponential map from the Lie algebra to the Lie group is not always onto, even if the group is connected (though it does map onto the Lie group for connected groups that are either compact or nilpotent). For example, the exponential map of SL(2, R) is not surjective. Also, exponential map is not surjective nor injective for infinite-dimensional (see below) Lie groups modelled on "C"∞ Fréchet space, even from arbitrary small neighborhood of 0 to corresponding neighborhood of 1.
See also: derivative of the exponential map and normal coordinates.
Lie subgroup.
A Lie subgroup "H" of a Lie group "G" is a Lie group that is a subset of "G" and such that the inclusion map from "H" to "G" is an injective immersion and group homomorphism. According to Cartan's theorem, a closed subgroup of "G" admits a unique smooth structure which makes it an embedded Lie subgroup of "G"—i.e. a Lie subgroup such that the inclusion map is a smooth embedding.
Examples of non-closed subgroups are plentiful; for example take "G" to be a torus of dimension ≥ 2, and let "H" be a one-parameter subgroup of "irrational slope", i.e. one that winds around in "G". Then there is a Lie group homomorphism φ : R → "G" with "H" as its image. The closure of "H" will be a sub-torus in "G".
In terms of the exponential map of "G", in general, only some of the Lie subalgebras of the Lie algebra "g" of "G" correspond to closed Lie subgroups "H" of "G". There is no criterion solely based on the structure of "g" which determines which those are.
Early history.
According to the most authoritative source on the early history of Lie groups (Hawkins, p. 1), Sophus Lie himself considered the winter of 1873–1874 as the birth date of his theory of continuous groups. Hawkins, however, suggests that it was "Lie's prodigious research activity during the four-year period from the fall of 1869 to the fall of 1873" that led to the theory's creation ("ibid"). Some of Lie's early ideas were developed in close collaboration with Felix Klein. Lie met with Klein every day from October 1869 through 1872: in Berlin from the end of October 1869 to the end of February 1870, and in Paris, Göttingen and Erlangen in the subsequent two years ("ibid", p. 2). Lie stated that all of the principal results were obtained by 1884. But during the 1870s all his papers (except the very first note) were published in Norwegian journals, which impeded recognition of the work throughout the rest of Europe ("ibid", p. 76). In 1884 a young German mathematician, Friedrich Engel, came to work with Lie on a systematic treatise to expose his theory of continuous groups. From this effort resulted the three-volume "Theorie der Transformationsgruppen", published in 1888, 1890, and 1893.
Lie's ideas did not stand in isolation from the rest of mathematics. In fact, his interest in the geometry of differential equations was first motivated by the work of Carl Gustav Jacobi, on the theory of partial differential equations of first order and on the equations of classical mechanics. Much of Jacobi's work was published posthumously in the 1860s, generating enormous interest in France and Germany (Hawkins, p. 43). Lie's "idée fixe" was to develop a theory of symmetries of differential equations that would accomplish for them what Évariste Galois had done for algebraic equations: namely, to classify them in terms of group theory. Lie and other mathematicians showed that the most important equations for special functions and orthogonal polynomials tend to arise from group theoretical symmetries. In Lie's early work, the idea was to construct a theory of "continuous groups", to complement the theory of discrete groups that had developed in the theory of modular forms, in the hands of Felix Klein and Henri Poincaré. The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. There is a differential Galois theory, but it was developed by others, such as Picard and Vessiot, and it provides a theory of quadratures, the indefinite integrals required to express solutions.
Additional impetus to consider continuous groups came from ideas of Bernhard Riemann, on the foundations of geometry, and their further development in the hands of Klein. Thus three major themes in 19th century mathematics were combined by Lie in creating his new theory: the idea of symmetry, as exemplified by Galois through the algebraic notion of a group; geometric theory and the explicit solutions of differential equations of mechanics, worked out by Poisson and Jacobi; and the new understanding of geometry that emerged in the works of Plücker, Möbius, Grassmann and others, and culminated in Riemann's revolutionary vision of the subject.
Although today Sophus Lie is rightfully recognized as the creator of the theory of continuous groups, a major stride in the development of their structure theory, which was to have a profound influence on subsequent development of mathematics, was made by Wilhelm Killing, who in 1888 published the first paper in a series entitled "Die Zusammensetzung der stetigen endlichen Transformationsgruppen" ("The composition of continuous finite transformation groups") (Hawkins, p. 100). The work of Killing, later refined and generalized by Élie Cartan, led to classification of semisimple Lie algebras, Cartan's theory of symmetric spaces, and Hermann Weyl's description of representations of compact and semisimple Lie groups using highest weights.
In 1900 David Hilbert challenged Lie theorists with his Fifth Problem presented at the International Congress of Mathematicians in Paris.
Weyl brought the early period of the development of the theory of Lie groups to fruition, for not only did he classify irreducible representations of semisimple Lie groups and connect the theory of groups with quantum mechanics, but he also put Lie's theory itself on firmer footing by clearly enunciating the distinction between Lie's "infinitesimal groups" (i.e., Lie algebras) and the Lie groups proper, and began investigations of topology of Lie groups. The theory of Lie groups was systematically reworked in modern mathematical language in a monograph by Claude Chevalley.
The concept of a Lie group, and possibilities of classification.
Lie groups may be thought of as smoothly varying families of symmetries. Examples of symmetries include rotation about an axis. What must be understood is the nature of 'small' transformations, e.g., rotations through tiny angles, that link nearby transformations. The mathematical object capturing this structure is called a Lie algebra (Lie himself called them "infinitesimal groups"). It can be defined because Lie groups are manifolds, so have tangent spaces at each point.
The Lie algebra of any compact Lie group (very roughly: one for which the symmetries form a bounded set) can be decomposed as a direct sum of an abelian Lie algebra and some number of simple ones. The structure of an abelian Lie algebra is mathematically uninteresting (since the Lie bracket is identically zero); the interest is in the simple summands. Hence the question arises: what are the simple Lie algebras of compact groups? It turns out that they mostly fall into four infinite families, the "classical Lie algebras" A"n", B"n", C"n" and D"n", which have simple descriptions in terms of symmetries of Euclidean space. But there are also just five "exceptional Lie algebras" that do not fall into any of these families. E8 is the largest of these.
Lie groups are classified according to their algebraic properties (simple, semisimple, solvable, nilpotent, abelian), their connectedness (connected or simply connected) and their compactness.
The identity component of any Lie group is an open normal subgroup, and the quotient group is a discrete group. The universal cover of any connected Lie group is a simply connected Lie group, and conversely any connected Lie group is a quotient of a simply connected Lie group by a discrete normal subgroup of the center. Any Lie group "G" can be decomposed into discrete, simple, and abelian groups in a canonical way as follows. Write 
so that we have a sequence of normal subgroups
Then
This can be used to reduce some problems about Lie groups (such as finding their unitary representations) to the same problems for connected simple groups and nilpotent and solvable subgroups of smaller dimension.
Infinite-dimensional Lie groups.
Lie groups are often defined to be finite-dimensional, but there are many groups that resemble Lie groups, except for being infinite-dimensional. The simplest way to define infinite-dimensional Lie groups is to model them on Banach spaces, and in this case much of the basic theory is similar to that of finite-dimensional Lie groups. However this is inadequate for many applications, because many natural examples of infinite-dimensional Lie groups are not Banach manifolds. Instead one needs to define Lie groups modeled on more general locally convex topological vector spaces. In this case the relation between the Lie algebra and the Lie group becomes rather subtle, and several results about finite-dimensional Lie groups no longer hold.
Some of the examples that have been studied include:
The diffeomorphism group of spacetime sometimes appears in attempts to quantize gravity.

</doc>
<doc id="17951" url="http://en.wikipedia.org/wiki?curid=17951" title="Lake Superior">
Lake Superior

Lake Superior (French: "Lac Supérieur") is the largest of the Great Lakes of North America. The lake is shared by Canada's Ontario and the United States' Minnesota to the north and west, and Wisconsin and Michigan to the south. It is generally considered the largest freshwater lake in the world by surface area. It is the world's third-largest freshwater lake by volume and the largest by volume in North America.
Name.
The Ojibwe call the lake "gichi-gami" (pronounced as "gitchi-gami" and "kitchi-gami" in other dialects), meaning "be a great sea." Henry Wadsworth Longfellow wrote the name as "Gitche Gumee" in The Song of Hiawatha, as did Gordon Lightfoot in his song, "The Wreck of the Edmund Fitzgerald". According to other sources the actual Ojibwe name is "Ojibwe Gichigami" ("Ojibwe's Great Sea") or "Anishinaabe Gichigami" ("Anishinaabe's Great Sea"). The 1878 dictionary by Father Frederic Baraga, the first one written for the Ojibway language, gives the Ojibwe name as Otchipwe-kitchi-gami (reflecting "Ojibwe Gichigami"). The first French explorers approaching the great inland sea by way of the Ottawa River and Lake Huron during the 17th century referred to their discovery as "le lac supérieur". Properly translated, the expression means "Upper Lake," that is, the lake above Lake Huron. The lake was also called "Lac Tracy" (named for Alexandre de Prouville de Tracy) by 17th century Jesuit missionaries. The British, upon taking control of the region from the French in the 1760s following the French and Indian War, anglicized the lake's name to "Superior", "on account of its being superior in magnitude to any of the lakes on that vast continent."
Hydrography.
Lake Superior empties into Lake Huron via the St. Marys River and the Soo Locks. Lake Superior is the largest freshwater lake in the world in area (if Lakes Michigan and Huron are taken separately; "see Lake Michigan–Huron"), and the third largest in volume, behind Lake Baikal in Siberia and Lake Tanganyika in East Africa. The Caspian Sea, while larger than Lake Superior in both surface area and volume, is brackish; though presently isolated, prehistorically the Caspian has been repeatedly connected to and isolated from the Mediterranean via the Black Sea.
Lake Superior has a surface area of 31700 sqmi, which is approximately the size of South Carolina or Austria. It has a maximum length of 350 smi and maximum breadth of 160 smi. Its average depth is fathom ft m with a maximum depth of fathom ft m. Lake Superior contains 2,900 cubic miles (12,100 km³) of water. There is enough water in Lake Superior to cover the entire land mass of North and South America to a depth of 30 cm. The shoreline of the lake stretches 2726 mi (including islands).
American limnologist J. Val Klump was the first person to reach the lowest depth of Lake Superior on July 30, 1985, as part of a scientific expedition, which at 122 fathoms 1 foot (733 ft) below sea level is the lowest spot in the continental interior of the United States and the second-lowest spot in the interior of the North American continent after the deeper Great Slave Lake in Canada (1503 ft below sea level). (Though Crater Lake is the deepest lake in the United States and deeper than Lake Superior, Crater Lake's elevation is higher and consequently its deepest point is 4229 ft "above" sea level.)
While the temperature of the surface of Lake Superior varies seasonally, the temperature below 110 fathom is an almost constant 39 °F (4 °C). This variation in temperature makes the lake seasonally stratigraphic. Twice per year, however, the water column reaches a uniform temperature of 39 °F (4 °C) from top to bottom, and the lake waters thoroughly mix. This feature makes the lake dimictic. Because of its volume, Lake Superior has a retention time of 191 years.
Annual storms on Lake Superior regularly feature wave heights of over 20 ft. Waves well over 30 ft have been recorded.
Tributaries and outlet.
The lake is fed by over 200 rivers. The largest include the Nipigon River, the St. Louis River, the Pigeon River, the Pic River, the White River, the Michipicoten River, the Bois Brule River and the Kaministiquia River. Lake Superior drains into Lake Huron via the St. Marys River. There are rapids at the river's upper (Lake Superior) end where the river bed has a relatively steep gradient. The Soo Locks were built to enable ships to bypass the rapids and to overcome the 25 ft height difference between Lakes Superior and Huron.
Water levels.
The lake's average surface elevation is 600 ft above sea level. Until approximately 1887 the natural hydraulic conveyance through the St. Marys River rapids determined outflow from Lake Superior. By 1921 development in support of transportation and hydroelectric power resulted in gates, locks, power canals and other control structures completely spanning St. Marys rapids. The regulating structure is known as the Compensating Works and is operated according to a regulation plan known as Plan 1977-A. Water levels, including diversions of water from the Hudson Bay watershed, are regulated by the International Lake Superior Board of Control which was established in 1914 by the International Joint Commission.
Lake Superior's water level was at a new record low in September 2007, slightly less than the previous record low in 1926. However, the water levels returned within a few days.
Historic high water
The lake's water level fluctuates from month to month, with the highest lake levels in October and November. The normal high-water mark is 1.17 ft above datum (601.1 ft or 183.2 m). In the summer of 1985, Lake Superior reached its highest recorded level at 2.33 ft above datum. The winter of 1986 set new high-water records through the winter and spring months (January to June), ranging from 1.33 ft to 1.833 ft above Chart Datum.
Historic low water
The lake's lowest levels occur in March and April. The normal low-water mark is 0.33 ft below datum (601.1 ft or 183.2 m). In the winter of 1926 Lake Superior reached its lowest recorded level at 1.58 ft below datum. Additionally, the entire first half of the year (January to June) included record low months. The low water was a continuation of the dropping lake levels from the previous year, 1925, which set low-water records for October through December. During the nine-month period October 1925 to June 1926 water levels ranged from 1.58 ft to 0.33 ft below Chart Datum. In the summer of 2007 monthly historic lows were set; August at 0.66 ft, September at 0.58 ft.
Climate change.
According to a study by professors at the University of Minnesota Duluth, Lake Superior may have warmed faster than its surrounding area. Summer surface temperatures in the lake appeared to have increased by about 4.5 °F (2.5 °C) since 1979, compared with an approximately 2.7 °F (1.5 °C) increase in the surrounding average air temperature. The increase in the lake’s surface temperature may be related to the decreasing ice cover. Less winter ice cover allows more solar radiation to penetrate and warm the water. If trends continue, Lake Superior, which freezes over completely once every 20 years, could routinely be ice-free by 2040. This would be a significant departure from historical records as, according to Hubert Lamb, Samuel Champlain reported ice along the shores of Lake Superior in June 1608. Warmer temperatures could actually lead to more snow in the lake effect snow belts along the shores of the lake, especially in the Upper Peninsula of Michigan.
Geography.
The largest island in Lake Superior is Isle Royale in the state of Michigan. Isle Royale contains several lakes, some of which also contain islands. Other large famous islands include Madeline Island in the state of Wisconsin, Michipicoten Island in the province of Ontario, and Grand Island (the location of the Grand Island National Recreation Area) in the state of Michigan.
The larger cities on Lake Superior include: the twin ports of Duluth, Minnesota, and Superior, Wisconsin; Thunder Bay, Ontario; Marquette, Michigan; and the twin cities of Sault Ste. Marie, Michigan, and Sault Ste. Marie, Ontario. Duluth, at the western tip of Lake Superior, is the most inland point on the St. Lawrence Seaway and the most inland port in the world.
Among the scenic places on the lake are: the Apostle Islands National Lakeshore, Isle Royale National Park, Porcupine Mountains Wilderness State Park, Pukaskwa National Park, Lake Superior Provincial Park, Grand Island National Recreation Area, Sleeping Giant (Ontario) and Pictured Rocks National Lakeshore.
Great Lakes Circle Tour.
The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.
Climate.
Lake Superior's size reduces the severity of the seasons of its humid continental climate (more typically seen in locations like Nova Scotia). The water surface's slow reaction to temperature changes, seasonally ranging between 32 and 55 °F (0–13 °C) around 1970, helps to moderate surrounding air temperatures in the summer and winter, and creates lake effect snow in colder months. The hills and mountains that border the lake hold moisture and fog, particularly in the fall.
The lake's surface temperature has risen by 4.5 °F (2.5 °C) since 1979.
Geology.
The rocks of Lake Superior's northern shore date back to the early history of the earth. During the Precambrian (between 4.5 billion and 540 million years ago) magma forcing its way to the surface created the intrusive granites of the Canadian Shield. These ancient granites can be seen on the North Shore today. It was during the Penokean orogeny, part of the process that created the Great Lakes Tectonic Zone, that many valuable metals were deposited. The region surrounding the lake has proved to be rich in minerals. Copper, iron, silver, gold and nickel are or were the most frequently mined. Examples include the Hemlo gold mine near Marathon, copper at Point Mamainse, silver at Silver Islet and uranium at Theano Point.
The mountains steadily eroded, depositing layers of sediments that compacted and became limestone, dolostone, taconite and the shale at Kakabeka Falls.
The continent was later riven, creating one of the deepest rifts in the world. The lake lies in this long-extinct Mesoproterozoic rift valley, the Midcontinent Rift. Magma was injected between layers of sedimentary rock, forming diabase sills. This hard diabase protects the layers of sedimentary rock below, forming the flat-topped mesas in the Thunder Bay area.
Amethyst formed in some of the cavities created by the Midcontinent Rift and there are several amethyst mines in the Thunder Bay area.
Lava erupted from the rift and formed the black basalt rock of Michipicoten Island, Black Bay Peninsula, and St. Ignace Island.
During the Wisconsin glaciation 10,000 years ago, ice covered the region at a thickness of 1.25 mi. The land contours familiar today were carved by the advance and retreat of the ice sheet. The retreat left gravel, sand, clay and boulder deposits. Glacial meltwaters gathered in the Superior basin creating Lake Minong, a precursor to Lake Superior. Without the immense weight of the ice, the land rebounded, and a drainage outlet formed at Sault Ste. Marie, which would become known as St. Mary's River.
History.
The first people came to the Lake Superior region 10,000 years ago after the retreat of the glaciers in the last Ice Age. They are known as the Plano, and they used stone-tipped spears to hunt caribou on the northwestern side of Lake Minong.
The next documented people were known as the Shield Archaic (c. 5000–500 BC). Evidence of this culture can be found at the eastern and western ends of the Canadian shore. They used bows and arrows, dugout canoes, fished, hunted, mined copper for tools and weapons, and established trading networks. They are believed to be the direct ancestors of the Ojibwe and Cree.
The Laurel people (c. 500 BC to AD 500) developed seine net fishing, evidence being found at rivers around Superior such as the Pic and Michipicoten.
Another culture known as the Terminal Woodland Indians (c. AD 900–1650) has been found. They were Algonkian people who hunted, fished and gathered berries. They used snow shoes, birch bark canoes and conical or domed lodges. At the mouth of the Michipicoten River, nine layers of encampments have been discovered. Most of the Pukaskwa Pits were likely made during this time.
The Anishinaabe, which includes the Ojibwe or Chippewa, have inhabited the Lake Superior region for over five hundred years and were preceded by the Dakota, Fox, Menominee, Nipigon, Noquet and Gros Ventres. They called Lake Superior either "Ojibwe Gichigami" ("the Ojibwe's Great Sea") or "Anishnaabe Gichgamiing" ("the Anishinaabe's Great Sea"). After the arrival of Europeans, the Anishinaabe made themselves the middle-men between the French fur traders and other Native peoples. They soon became the dominant Indian nation in the region: they forced out the Sioux and Fox and won a victory against the Iroquois west of Sault Ste. Marie in 1662. By the mid-18th century, the Ojibwe occupied all of Lake Superior's shores.
In the 18th century, the fur trade in the region was booming, with the Hudson's Bay Company having a virtual monopoly. In 1783, however, the North West Company was formed to rival the Hudson's Bay Company. The North West Company built forts on Lake Superior at Grand Portage, Fort William, Nipigon, the Pic River, the Michipicoten River, and Sault Ste. Marie. But by 1821, with competition taking too great a toll on both, the companies merged under the Hudson's Bay Company name.
Many towns around the lake are either current or former mining areas, or engaged in processing or shipping. Today, tourism is another significant industry; the sparsely populated Lake Superior country, with its rugged shorelines and wilderness, attracts tourists and adventurers.
Shipping.
Lake Superior has been an important link in the Great Lakes Waterway, providing a route for the transportation of iron ore as well as grain and other mined and manufactured materials. Large cargo vessels called lake freighters, as well as smaller ocean-going freighters, transport these commodities across Lake Superior.
Because of ice, the Lake is closed to shipping from mid-January to late March. Exact dates for the shipping season vary each year, depending on weather conditions that form and break the ice.
Shipwrecks.
According to shipwreck historian Frederick Stonehouse, the southern shore of Lake Superior between Grand Marais, Michigan, and Whitefish Point is known as the "Graveyard of the Great Lakes" and more ships have been lost around the Whitefish Point area than any other part of Lake Superior. These shipwrecks are now protected by the Whitefish Point Underwater Preserve.
Storms that claimed multiple ships include the Mataafa Storm in 1905 and the Great Lakes Storm of 1913.
Wreckage of the "Cyprus" – a 420 ft ore carrier that sank on October 11, 1907, during a Lake Superior storm in 77 fathoms (460 ft) of water – was located in August 2007. All but Charles G. Pitz of the "Cyprus"’s 23 crew perished . The ore carrier sank in Lake Superior on her second voyage, while hauling iron ore from Superior, Wisconsin, to Buffalo, New York. Built in Lorain, Ohio, the "Cyprus" was launched August 17, 1907.
In 1918 the last warships to sink in the great lakes, French minesweepers "Inkerman" and "Cerisoles", vanished in a Lake Superior storm. With 78 crewmembers dead, their sinking marked the largest loss of life on Lake Superior to date.
The SS "Edmund Fitzgerald" was the last major shipwreck on Lake Superior, sinking 15 nmi from Whitefish Point in a storm on November 10, 1975. The wreck was immortalized Gordon Lightfoot in his ballad "The Wreck of the "Edmund Fitzgerald"". All 29 crew members perished when the ship sank, and no bodies were ever recovered. The "Fitzgerald" was swallowed up so intensely by Lake Superior that the 729 ft ship split in half; her two pieces are approximately 170 ft apart in a depth of 91 fathoms 4 feet (550 ft).
According to legend, "Lake Superior seldom gives up her dead". This is because of the unusually low temperature of the water, estimated at under 36 °F on average around 1970. Normally, bacteria feeding on a sunken decaying body will generate gas inside the body, causing it to float to the surface after a few days. However, Lake Superior's water is cold enough year-round to inhibit bacterial growth, and bodies tend to sink and never resurface. This is alluded to in Lightfoot's "The Wreck of the "Edmund Fitzgerald"" ballad. "Fitzgerald" adventurer Joe MacInnis reported that, in July 1994, explorer Frederick Shannon's Expedition 94 to the "Fitzgerald" discovered and filmed a man's body near the port side of her pilothouse, not far from the open door, "fully clothed, wearing an orange life jacket, and lying face down in the sediment."
Ecology.
Over 80 species of fish have been found in Lake Superior. Species native to the lake include: banded killifish, bloater, brook trout, burbot, cisco, lake sturgeon, lake trout, lake whitefish, longnose sucker, muskellunge, northern pike, pumpkinseed, rock bass, round whitefish, smallmouth bass, walleye, white sucker and yellow perch. In addition, many fish species have been either intentionally or accidentally introduced to Lake Superior: atlantic salmon, brown trout, carp, chinook salmon, coho salmon, freshwater drum, pink salmon, rainbow smelt, rainbow trout, round goby, ruffe, sea lamprey and white perch.
Lake Superior has fewer dissolved nutrients relative to its water volume compared to the other Great Lakes and so is less productive in terms of fish populations and is an oligotrophic lake. This is a result of the underdeveloped soils found in its relatively small watershed. However, nitrate concentrations in the lake have been continuously rising for more than a century. They are still much lower than levels considered dangerous to human health; but this steady, long-term rise is an unusual record of environmental nitrogen buildup. It may relate to anthropogenic alternations to the regional nitrogen cycle, but researchers are still unsure of the causes of this change to the lake's ecology.
As for other Great Lakes fish, populations have also been impacted by the accidental or intentional introduction of foreign species such as the sea lamprey and Eurasian ruffe. Accidental introductions have occurred in part by the removal of natural barriers to navigation between the Great Lakes. Overfishing has also been a factor in the decline of fish populations.

</doc>
<doc id="17974" url="http://en.wikipedia.org/wiki?curid=17974" title="Long gun">
Long gun

A long gun is a category of firearms and cannons with longer barrels than other classes. In small arms, a "long gun" is designed to be fired braced against the shoulder, in contrast to a handgun, while in artillery a "long gun" would be contrasted with a howitzer or carronade.
Small arms.
The actual length of the barrels of a long gun are subject various laws in many jurisdictions, for example by the National Firearms Act in the United States, which sets a minimum length of 16 inches (40 cm) for rifle barrels and 18 inches (45 cm) for shotgun barrels. Canada has a limit of 18.5 inches (47 cm) for either. In addition, Canada puts a minimum fireable length for long guns with detachable or folding stocks of 26 inches (66 cm). In the United States, the minimum length for long guns with detachable or folding stocks is 26 in with the stock in the extended position.
Examples of various classes of small arms generally considered long arms include, but are not limited to: rifles, shotguns, muskets, blunderbusses, carbines, wall guns, and musketoons.
Advantages and disadvantages of long guns.
Almost all long-arms have front grips (forearms) and shoulder stocks, which provides the user the ability to hold the firearm more steadily than a handgun. In addition, the long barrel of a long gun usually provides a longer sight plane for iron sights, providing the user with more precision when aiming. The presence of a stock makes the use of a telescopic sight or red dot sight more practical than with a hand gun.
The mass of a long gun is usually greater than that of a short gun, making the long gun more expensive to transport, and more difficult and tiring to carry. The increased moment of inertia makes the long gun slower and more difficult to traverse and elevate, and it is thus slower and more difficult to adjust the aim. However, this also results in greater stability in aiming. The greater amount of material in a long gun tends to make it more expensive to manufacture, other factors being equal. The greater size makes it more difficult to conceal, and more inconvenient to use in confined quarters, as well as requiring a larger storage space.
Shotguns are long guns that are designed to fire many small projectiles at once. This makes them very effective at close ranges, but with diminished usefulness at long range.
Naval long guns.
In historical navy usage, a long gun was the standard type of cannon mounted by a sailing vessel, called such to distinguish it from the much shorter carronades. In informal usage, the length was combined with the weight of shot, yielding terms like "long 9s", referring to full length cannons firing a 9-pound round shot.

</doc>
<doc id="17981" url="http://en.wikipedia.org/wiki?curid=17981" title="Law of definite proportions">
Law of definite proportions

In chemistry, the law of definite proportions, sometimes called Proust's Law or The Law of Definite Composition, states that a chemical compound always contains exactly the same proportion of elements by mass. An equivalent statement is the law of constant composition, which states that all samples of a given chemical compound have the same elemental composition by mass. For example, oxygen makes up about 8/9 of the mass of any sample of pure water, while hydrogen makes up the remaining 1/9 of the mass. Along with the law of multiple proportions, the law of definite proportions forms the basis of stoichiometry.
History.
This observation was first made by the French chemist Joseph Proust, based on several experiments conducted between 1798 and 1804. Based on such observations, Proust made statements like this one, in 1806:
I shall conclude by deducing from these experiments the principle I have established at the commencement of this memoir, viz. that iron like many other metals is subject to the law of nature which presides at every true combination, that is to say, that it unites with two constant proportions of oxygen. In this respect it does not differ from tin, mercury, and lead, and, in a word, almost every known combustible.
The law of definite proportions might seem obvious to the modern chemist, inherent in the very definition of a chemical compound. At the end of the 18th century, however, when the concept of a chemical compound had not yet been fully developed, the law was novel. In fact, when first proposed, it was a controversial statement and was opposed by other chemists, most notably Proust's fellow Frenchman Claude Louis Berthollet, who argued that the elements could combine in any proportion. The existence of this debate demonstrates that, at the time, the distinction between pure chemical compounds and mixtures had not yet been fully developed.
The law of definite proportions contributed to, and was placed on a firm theoretical basis by, the atomic theory that John Dalton promoted beginning in 1803, which explained matter as consisting of discrete atoms, that there was one type of atom for each element, and that the compounds were made of combinations of different types of atoms in fixed proportions.
A related early idea was Prout's hypothesis, formulated by English chemist William Prout, who proposed that the hydrogen atom was the fundamental atomic unit. From this hypothesis was derived the whole number rule, which was the rule of thumb that atomic masses were whole number multiples of the mass of hydrogen. This was later rejected in the 1820s and 30s following more refined measurements of atomic mass, notably by Jöns Jacob Berzelius, which revealed in particular that the atomic mass of chlorine was 35.45, which was incompatible with the hypothesis. Since the 1920s this discrepancy has been explained by the presence of isotopes; the atomic mass of any isotope is very close to satisfying the whole number rule, with the mass defect caused by differing binding energies being significantly smaller.
Non-stoichiometric compounds.
Although very useful in the foundation of modern chemistry, the law of definite proportions is not universally true. There exist non-stoichiometric compounds whose elemental composition can vary from sample to sample. An example is the iron oxide wüstite, which can contain between 0.83 and 0.95 iron atoms for every oxygen atom, and thus contain anywhere between 23% and 25% oxygen by mass. The ideal formula is FeO, but due to crystallographic vacancies it is reduced to about Fe0.95O. In general, Proust's measurements were not accurate enough to detect such variations.
In addition, the isotopic composition of an element can vary depending on its source, hence its contribution to the mass of even a pure stoichiometric compound may vary. This variation is used in geochemical dating since astronomical, atmospheric, oceanic, crustal and deep Earth processes may concentrate lighter or heavier isotopes preferentially. With the exception of hydrogen and its isotopes, the effect is usually small, but is measurable with modern instrumentation.
An additional note: many natural polymers vary in composition (for instance DNA, proteins, carbohydrates) even when "pure". Polymers are generally not considered "pure chemical compounds" except when their molecular weight is uniform (monodisperse) and their stoichiometry is constant. In this unusual case, they still may violate the law due to isotopic variations.

</doc>
<doc id="17988" url="http://en.wikipedia.org/wiki?curid=17988" title="Laurence Sterne">
Laurence Sterne

Laurence Sterne (24 November 1713 – 18 March 1768) was an Anglo-Irish novelist and an Anglican clergyman. He is best known for his novels "The Life and Opinions of Tristram Shandy, Gentleman" and "A Sentimental Journey Through France and Italy"; but he also published many sermons, wrote memoirs, and was involved in local politics. Sterne died in London after years of fighting consumption.
Early life and education.
Sterne was born in Clonmel, County Tipperary. His father, Roger Sterne, was an ensign in a British regiment recently returned from Dunkirk, which was disbanded on the day of Sterne's birth. Within six months the family had returned to Yorkshire, and in July 1715 they moved back to Ireland, having "decamped with Bag & Baggage for Dublin", in Sterne's words.
The first decade of Sterne's life was spent moving from place to place as his father was reassigned throughout Ireland. During this period Sterne never lived in one place for more than a year. In addition to Clonmel and Dublin, his family also lived in, among other places, Wicklow Town, Annamoe (County Wicklow), Drogheda (County Louth), Castlepollard (County Westmeath), and Carrickfergus (County Antrim). In 1724, his father took Sterne to Roger's wealthy brother, Richard, so that Sterne could attend Hipperholme Grammar School near Halifax; Sterne never saw his father again as Roger was ordered to Jamaica where he died of a fever in 1731. Sterne was admitted to a sizarship at Jesus College, Cambridge, in July 1733 at the age of 20. His great-grandfather Richard Sterne had been the Master of the college as well as the Archbishop of York. Sterne graduated with a degree of Bachelor of Arts in January 1737; and returned in the summer of 1740 to be awarded his Master of Arts degree.
Early career.
Sterne was ordained as a deacon in March 1737 and as a priest in August 1738. Shortly thereafter Sterne was awarded the vicarship living of Sutton-on-the-Forest in Yorkshire. Sterne married Elizabeth Lumley in 1741. Both were ill with consumption. In 1743, he was presented to the neighbouring living of Stillington by Rev. Richard Levett, Prebendary of Stillington, who was patron of the living. Subsequently Sterne did duty both there and at Sutton. He was also a prebendary of York Minster. Sterne's life at this time was closely tied with his uncle, Dr. Jaques Sterne, the Archdeacon of Cleveland and Precentor of York Minster. Sterne's uncle was an ardent Whig, and urged Sterne to begin a career of political journalism which resulted in some scandal for Sterne and, eventually, a terminal falling-out between the two men.
Jaques Sterne was a powerful clergyman but a mean-tempered man and a rabid politician. In 1741–42 Sterne wrote political articles supporting the administration of Sir Robert Walpole for a newspaper founded by his uncle but soon withdrew from politics in disgust. His uncle became his archenemy, thwarting his advancement whenever possible.
Sterne lived in Sutton for twenty years, during which time he kept up an intimacy which had begun at Cambridge with John Hall-Stevenson, a witty and accomplished "bon vivant", owner of Skelton Hall in the Cleveland district of Yorkshire.
Writing.
In 1759, to support his dean in a church squabble, Sterne wrote "A Political Romance" (later called "The History of a Good Warm Watch-Coat"), a Swiftian satire of dignitaries of the spiritual courts. At the demands of embarrassed churchmen, the book was burned. Thus, Sterne lost his chances for clerical advancement but discovered his real talents; until the completion of this first work, "he hardly knew that he could write at all, much less with humour so as to make his reader laugh".
Having discovered his talent, at the age of 46, he turned over his parishes to a curate, and dedicated himself to writing for the rest of his life. It was while living in the countryside, having failed in his attempts to supplement his income as a farmer and struggling with tuberculosis, that Sterne began work on his most famous novel, "The Life and Opinions of Tristram Shandy, Gentleman", the first volumes of which were published in 1759. Sterne was at work on his celebrated comic novel during the year that his mother died, his wife was seriously ill, and his daughter was also taken ill with a fever. He wrote as fast as he possibly could, composing the first 18 chapters between January and March 1759.
An initial, sharply satiric version was rejected by Robert Dodsley, the London printer, just when Sterne's personal life was upset. His mother and uncle both died. His wife had a nervous breakdown and threatened suicide. Sterne continued his comic novel, but every sentence, he said, was "written under the greatest heaviness of heart." In this mood, he softened the satire and recounted details of Tristram's opinions, eccentric family and ill-fated childhood with a sympathetic humour, sometimes hilarious, sometimes sweetly melancholic—a comedy skirting tragedy.
The publication of "Tristram Shandy" made Sterne famous in London and on the continent. He was delighted by the attention, and spent part of each year in London, being fêted as new volumes appeared. Indeed, Baron Fauconberg rewarded Sterne by appointing him as the perpetual curate of Coxwold, North Yorkshire.
Foreign travel.
Sterne continued to struggle with his illness, and departed England for France in 1762 in an effort to find a climate that would alleviate his suffering. Sterne was lucky to attach himself to a diplomatic party bound for Turin, as England and France were still adversaries in the Seven Years' War. Sterne was gratified by his reception in France where reports of the genius of "Tristram Shandy" had made him a celebrity. Aspects of this trip to France were incorporated into Sterne's second novel, "A Sentimental Journey Through France and Italy", which was published at the beginning of 1768. The novel was written during a period in which Sterne was increasingly ill and weak.
Death and "resurrection".
Less than a month after "Sentimental Journey" was published, early in 1768, Sterne's strength failed him, and he died in his lodgings at 41 Old Bond Street on 18 March, at the age of 54. He was buried in the churchyard of St George's, Hanover Square.
It was widely rumoured that Sterne's body was stolen shortly after it was interred and sold to anatomists at Cambridge University. Circumstantially, it was said that his body was recognised by Charles Collignon who knew him and discreetly reinterred back in St George's, in an unknown plot. A year later a group of Freemasons erected a memorial stone with a rhyming epitaph near to his original burial place. A second stone was erected in 1893, correcting some factual errors on the memorial stone. When the churchyard of St. George's was redeveloped in 1969, amongst 11,500 skulls disinterred, several were identified with drastic cuts from anatomising or a post-mortem examination. One was identified to be of a size that matched a bust of Sterne made by Nollekens.
The skull was held up to be his, albeit with "a certain area of doubt". Along with nearby skeletal bones, these remains were transferred to Coxwold churchyard in 1969 by the Laurence Sterne Trust.
The story of the reinterment of Sterne's skull in Coxwold is alluded to in Malcolm Bradbury's novel "To The Hermitage".
Works.
Sterne's early works were letters, he had two ordinary sermons published (in 1747 and 1750), and tried his hand at satire. He was involved in, and wrote about, local politics in 1742. His major publication prior to "Tristram Shandy" was the satire "A Political Romance" (1759), aimed at conflicts of interest within York Minster. A posthumously published piece on the art of preaching, "A Fragment in the Manner of Rabelais", appears to have been written in 1759. Sterne did not begin work on "Tristram Shandy" until he was 46 years old. Rabelais was by far Sterne's favourite author, and in his correspondence he made clear that he considered himself as Rabelais' successor in humour writing, distancing himself from Jonathan Swift:
I ... deny I have gone as far as Swift: he keeps a due distance from Rabelais; I keep a due distance from him.
Sterne is best known for his novel "The Life and Opinions of Tristram Shandy, Gentleman", for which he became famous not only in England, but throughout Europe. Translations of the work began to appear in all the major European languages almost upon its publication, and Sterne influenced European writers as diverse as Diderot and the German Romanticists. His work had also noticeable influence over Brazilian author Machado de Assis, who made exceptional (and outstandingly original) usage of the digressive technique in the masterful novel "The Posthumous Memoirs of Bras Cubas". Indeed, the novel, in which Sterne manipulates narrative time and voice, parodies accepted narrative form, and includes a healthy dose of "bawdy" humour, was largely dismissed in England as being too corrupt. Samuel Johnson's verdict in 1776 was that "Nothing odd will do long. "Tristram Shandy" did not last." This is strikingly different from the views of European critics of the day, who praised Sterne and "Tristram Shandy" as innovative and superior. Voltaire called it "clearly superior to Rabelais", and later Goethe praised Sterne as "the most beautiful spirit that ever lived." Both during his life and for a long time after, efforts were made by many to reclaim Sterne as an arch-sentimentalist; parts of "Tristram Shandy", such as the tale of Le Fever, were excerpted and published separately to wide acclaim from the moralists of the day. The success of the novel and its serialised nature also allowed many imitators to publish pamphlets concerning the Shandean characters and other Shandean-related material even while the novel was yet unfinished.
The novel itself is difficult to describe. The story starts with the narration, by Tristram, of his own conception. It proceeds by fits and starts, but mostly by what Sterne calls "progressive digressions" so that we do not reach Tristram's birth before the third volume. The novel is rich in characters and humour, and the influences of Rabelais and Cervantes are present throughout. The novel ends after 9 volumes, published over a decade, but without anything that might be considered a traditional conclusion. Sterne inserts sermons, essays and legal documents into the pages of his novel; and he explores the limits of typography and print design by including marbled pages and, most famously, an entirely black page within the narrative. Many of the innovations that Sterne introduced, adaptations in form that should be understood as an exploration of what constitutes the novel, were highly influential to Modernist writers like James Joyce and Virginia Woolf, and more contemporary writers such as Thomas Pynchon and David Foster Wallace. Italo Calvino referred to "Tristram Shandy" as the "undoubted progenitor of all avant-garde novels of our century." The Russian Formalist writer Viktor Shklovsky regarded "Tristram Shandy" as the archetypal, quintessential novel, of which all other novels are mere subsets: ""Tristram Shandy" is the most typical novel of world literature."
However, the leading critical opinions of "Tristram Shandy" tend to be markedly polarised in their evaluations of its significance. Since the 1950s, following the lead of D.W. Jefferson, there are those who argue that, whatever its legacy of influence may be, "Tristram Shandy" in its original context actually represents a resurgence of a much older, Renaissance tradition of "Learned Wit" – owing a debt to such influences as the Scriblerian approach.
"A Sentimental Journey Through France and Italy" is a less influential book, although it was better received by English critics of the day. The book has many stylistic parallels with "Tristram Shandy", and indeed, the narrator is one of the minor characters from the earlier novel. Although the story is more straightforward, "A Sentimental Journey" can be understood to be part of the same artistic project to which "Tristram Shandy" belongs.
Two volumes of Sterne's "Sermons" were published during his lifetime; more copies of his "Sermons" were sold in his lifetime than copies of "Tristram Shandy", and for a while he was better known in some circles as a preacher than as a novelist. The sermons, though, are conventional in both style and substance. Several volumes of letters were published after his death, as was "Journal to Eliza", a more sentimental than humorous love letter to a woman Sterne was courting during the final years of his life. Compared to many eighteenth-century authors, Sterne's body of work is quite small.
Abolitionists.
In 1766, at the height of the debate about slavery, the composer and former slave Ignatius Sancho wrote to Sterne encouraging him to use his pen to lobby for the abolition of the slave trade.
"That subject, handled in your striking manner, would ease the yoke (perhaps) of many—but if only one—Gracious God!—what a feast to a benevolent heart!"
In July 1766 Sancho's letter was received by Sterne shortly after he had just finished writing a conversation between his fictional characters Corporal Trim and his brother Tom in "Tristram Shandy" wherein Tom described the oppression of a black servant in a sausage shop in Lisbon which he had visited. Sterne's widely publicised response to Sancho's letter became an integral part of 18th century abolitionist literature:
There is a strange coincidence, Sancho, in the little events (as well as in the great ones) of this world: for I had been writing a tender tale of the sorrows of a friendless poor negro-girl, and my eyes had scarce done smarting with it, when your letter of recommendation in behalf of so many of her brethren and sisters, came to me—but why her brethren?—or your’s, Sancho! any more than mine? It is by the finest tints, and most insensible gradations, that nature descends from the fairest face about St. James’s, to the sootiest complexion in Africa: at which tint of these, is it, that the ties of blood are to cease? and how many shades must we descend lower still in the scale, ’ere mercy is to vanish with them?—but ’tis no uncommon thing, my good Sancho, for one half of the world to use the other half of it like brutes, & then endeavor to make ’em so.—Laurence Sterne, 27 July 1766
Bibliography.
His works, first collected in 1779, were edited, with newly discovered letters, by J. P. Browne (London, 1873). A less complete edition was edited by G. Saintsbury (London, 1894). The Florida Edition of Sterne's works is currently the leading scholarly edition – although the final volume (Sterne's letters) has yet to be published.

</doc>
<doc id="17999" url="http://en.wikipedia.org/wiki?curid=17999" title="Latin spelling and pronunciation">
Latin spelling and pronunciation

Latin spelling, or Latin orthography, is the spelling of Latin words written in the scripts of all historical phases of Latin, from Old Latin to the present. All scripts use the same alphabet, but conventional spellings may vary from phase to phase. The Roman alphabet, or Latin alphabet, was adapted from the Old Italic script to represent the phonemes of the Latin language. The Old Italic script had in turn been borrowed from the Greek alphabet, itself adapted from the Phoenician alphabet.
Latin pronunciation continually evolved over the centuries, making it difficult for speakers in one era to know how Latin was spoken in prior eras. A given phoneme may be represented by different letters in different periods. This article deals primarily with modern scholarship's best reconstruction of Classical Latin's phonemes (phonology) and the pronunciation and spelling used by educated people in the late Republic, and then touches upon later changes and other variants.
Letterforms.
The forms of the Latin alphabet used during the Classical period did not distinguish between upper case and lower case. Roman inscriptions typically use Roman square capitals, which resemble modern capitals, and handwritten text often uses old Roman cursive, which includes letterforms similar to modern lowercase.
This article uses small caps for Latin text, representing Roman square capitals, and long vowels are marked with acutes, representing apices. In the tables below, Latin letters and digraphs are paired with the phonemes they usually represent in the International Phonetic Alphabet.
Letters and phonemes.
In Latin spelling, individual letters mostly corresponded to individual phonemes, with three main exceptions:
In the tables below, Latin letters and digraphs are paired with the phonemes they usually represent in the International Phonetic Alphabet.
Consonants.
This is a table of the consonant sounds of Classical Latin. Sounds in parentheses are allophones, sounds with an asterisk exist mainly in loanwords, and sounds with a dagger (†) are phonemes in only some analyses.
Vowels.
Monophthongs.
Latin has ten native vowels, spelled "a", "e", "i", "o", "u". In Classical Latin, each vowel had short and long versions: /a ɛ ɪ ɔ ʊ/ and /aː eː iː oː uː/. The long versions of the close and mid vowels "e", "i", "o", "u" had a different vowel quality from the short versions, so that long /eː, oː/ were similar to short /ɪ, ʊ/. Some loanwords from Greek had the vowel "y", which was pronounced as /y yː/ by educated speakers, but approximated with the native vowels "u" and "i" by less educated speakers.
Long and short vowels.
Each vowel letter (with the possible exception of y) represents at least two phonemes. ⟨a⟩ can represent either short /a/ or long /aː/, ⟨e⟩ represents either /e/ or /eː/, etc.
Short mid vowels (/e o/) and close vowels (/i u/) were pronounced with a different quality than their long counterparts, being also more open: [ɛ], [ɔ], [ɪ] and [ʊ]. This opening made the short vowels ⟨i u⟩ [ɪ ʊ] similar in quality to long ⟨é ó⟩ [eː oː] respectively. ⟨i é⟩ and ⟨u ó⟩ were often written in place of each other in inscriptions:
Short /e/ most likely had a more open allophone before /r/ tending toward near-open [æ].
Short /e/ and /i/ were probably pronounced closer when they occurred before another vowel. mea was written as mia in inscriptions. Short /i/ before another vowel is often written with "i longa", as in dꟾes, indicating that its quality was similar to that of long /iː/, and is almost never confused with e in this position.
Adoption of Greek upsilon.
⟨y⟩ was used in Greek loanwords with upsilon ⟨Υ⟩. This letter represented the close front rounded vowel, both short and long: /y yː/. Latin did not have this sound as a distinctive phoneme, and speakers tended to pronounce such loanwords with /u uː/ in Old Latin, and /i iː/ in Classical and Late Latin, if they were unable to produce /y yː/.
Sonus medius.
An intermediate vowel sound (likely a close central vowel [ɨ] or possibly its rounded counterpart [ʉ]), called "sonus medius", can be reconstructed for the classical period. Such a vowel is found in docvmentvm, optimvs, lacrima (also spelled docimentvm, optvmvs, lacrvma) and other words. It developed out of a historical short /u/ which was later fronted due to vowel reduction. In the vicinity of labial consonants, this sound was not as fronted and may have retained some rounding.
Vowel nasalization.
Vowels followed by a nasal consonant were allophonically realised as long nasal vowels in some cases. This occurred in two environments:
These long nasal vowels had the same quality as ordinary long vowels. In Vulgar Latin, the vowels lost their nasalisation, and they merged with the long vowels (which were themselves shortened by that time). This is shown by many forms in the Romance languages, such as French "coûter" from Vulgar Latin cóstáre (originally con-stáre) and Italian "mese" from Vulgar Latin mése (Classical Latin mensem). On the other hand, the short vowel and /n/ was restored in French "enseigne" and "enfant" from insignia and infantem ("e" is the normal development of Latin short i), likely by analogy with other forms beginning in the prefix in-.
When a final -m occurred before a plosive or nasal in the next word, however, it was pronounced as a nasal at the place of articulation of the following consonant. For instance, tan dvrvm [tan ˈduː.rũː] was written for tam dv́rvm in inscriptions, and cvm nóbꟾs [kʊn ˈnoː.biːs] was a double entendre for cónv́biꟾs [koːˈnuː.bi.iːs].
Diphthongs.
⟨ae⟩, ⟨oe⟩, ⟨av⟩, ⟨ei⟩, ⟨ev⟩ represented diphthongs: ⟨ae⟩ represented /ae̯/, ⟨oe⟩ represented /oe̯/, ⟨av⟩ represented /au̯/, ⟨ei⟩ represented /ei̯/, and ⟨ev⟩ represented /eu̯/. ⟨vi⟩ sometimes represented the diphthong /ui̯/, as in cvi    and hvic.
In Old Latin, ⟨ae, oe⟩ were written as ⟨ai, oi⟩ and probably pronounced as [ai̯, oi̯], with a fully closed second element, similar to the final syllable in French   . In the late Old Latin period, the last element of the diphthongs was lowered to [e], so that the diphthongs were pronounced /ae̯/ and /oe̯/ in Classical Latin, similar to the diphthongs in English   and  . They were then monophthongized to /ɛː/ and /eː/, starting in rural areas at the end of the republican period. This process, however, does not seem to have been completed before the 3rd century AD in Vulgar Latin, and some scholars say that it may have been regular by the 5th century.
Vowel and consonant length.
Vowel and consonant length were more significant and more clearly defined in Latin than in modern English. Length is the duration of time that a particular sound is held before proceeding to the next sound in a word. Unfortunately, "vowel length" is a confusing term for English speakers, who in their language call "long vowels" what are in most cases diphthongs, rather than plain vowels. (This is a relic of the Great Vowel Shift, during which vowels that were once pronounced phonemically longer became these diphthongs.) In the modern spelling of Latin, especially in dictionaries and academic work, macrons are frequently used to mark long vowels: ⟨ā ē ī ō ū⟩, while the breve is sometimes used to indicate that a vowel is short: ⟨ă ĕ ĭ ŏ ŭ⟩.
Long consonants were usually indicated through doubling, but Latin orthography did not distinguish between the vocalic and consonantal uses of ⟨i⟩ and ⟨v⟩. Vowel length was indicated only intermittently in classical sources, through a variety of means. Later medieval and modern usage tended to omit vowel length altogether. A short-lived convention of spelling long vowels by doubling the vowel letter is associated with the poet Lucius Accius. Later spelling conventions marked long vowels with an apex (a diacritic similar to an acute accent), or in the case of long ⟨i⟩, by increasing the height of the letter (long i). Distinctions of vowel length became less important in later Latin, and have ceased to be phonemic in the modern Romance languages, where the previous long and short versions of the vowels have either been lost or replaced by other phonetic contrasts.
A minimal set showing both long and short vowels and long and short consonants is ánvs /ˈaː.nus/ "buttocks", annus /ˈannus/ "year", anvs /ˈa.nus/ "old woman"
Syllables and stress.
Old Latin stress.
In Old Latin, as in Proto-Italic, stress normally fell on the first syllable of a word. During this period, the word-initial stress triggered changes in the vowels of non-initial syllables, the effects of which are still visible in classical Latin. Compare for example:
In the earliest Latin writings, the original unreduced vowels are still visible. Study of this vowel reduction, as well as syncopation (dropping of short unaccented syllables) in Greek loan words, indicates that the stress remained word-initial until around the time of Plautus, the 3rd century BC. The placement of the stress then shifted to become the pattern found in classical Latin.
Classical Latin syllables and stress.
In Classical Latin, stress changed. It moved from the first syllable to one of the last three syllables, called the antepenult, the penult, and the ultima (short for "antepaenultima" "before almost last", "paenultima" "almost last", and "ultima syllaba" "last syllable"). Its position is determined by the syllable weight of the penult. If the penult is heavy, it is accented; if the penult is light and there are more than two syllables, the antepenult is accented. In a few words originally accented on the penult, accent is on the ultima because the two last syllables have been contracted, or the last syllable has been lost.
Syllable.
To determine stress, syllable weight of the penult must be determined. In order to determine syllable weight, words must be broken up into syllables. In the following examples, syllable structure is represented using these symbols: C (a consonant), K (a stop), R (a liquid), and V (a short vowel), VV (a long vowel or diphthong).
Nucleus.
Every short vowel, long vowel, or diphthong belongs to a single syllable. This vowel forms the syllable nucleus. Thus magistrárum has four syllables, one for every vowel (a i á u: V V VV V), aereus has three (ae e u: VV V V), tuó has two (u ó: V VV), and cui has one (ui: VV).
Onset and coda.
A consonant before a vowel, or a consonant cluster at the beginning of a word, is placed in the same syllable as the following vowel. This consonant or consonant cluster forms the syllable onset.
After this, if there is an additional consonant inside the word, it is placed at the end of the syllable. This consonant is the syllable coda. Thus if a consonant cluster of two consonants occurs between vowels, they are broken up between syllables: one goes with the syllable before, the other with the syllable after.
There are two exceptions. A consonant cluster of a stop ⟨p t c b d g⟩ followed by a liquid ⟨l r⟩ between vowels usually goes to the syllable after it, although it is also sometimes broken up like other consonant clusters.
Heavy and light syllables.
As shown in the examples above, Latin syllables have a variety of possible structures. Here are some of them. The first four examples are light syllables, and the last six are heavy. All syllables have at least one V (vowel). A syllable is heavy if it has another V or a VC after the first V. In the table below, the extra V or VC is bolded, indicating that it makes the syllable heavy.
Thus, a syllable is heavy if it ends in a long vowel or diphthong, a short vowel and a consonant, a long vowel and a consonant, or a diphthong and a consonant. Syllables ending in a diphthong and consonant are rare in Classical Latin.
The syllable onset has no relationship to syllable weight; both heavy and light syllables can have no onset or an onset of one, two, or three consonants.
A syllable that is heavy because it ends in a long vowel or diphthong is traditionally called syllaba nátv́rá longa "syllable long by nature", and a syllable that is heavy because it ends in a consonant is called positióne longa "long by position". These terms are translations of Greek συλλαβὴ μακρά φύσει and μακρὰ θέσει "syllable long by nature" and "position" (or "convention"). longa and μακρά are the same terms used for long vowels. This article uses the words "heavy" and "light" for syllables, and "long" and "short" for vowels, since the two are not the same.
Stress rule.
In a word of three or more syllables, the weight of the penult determines where the accent is placed. If the penult is light, accent is placed on the antepenult; if it is heavy, accent is placed on the penult. Below, stress is marked by placing the stress mark [ˈ] before the stressed syllable.
Iambic shortening.
Iambic shortening or brevis brevians is vowel shortening that occurs in words of the type "light–heavy", where the light syllable is stressed. By this sound change, words like egó, modó, bené, amá with long final vowel change to ego, modo, bene, ama with short final vowel.
Elision.
Where one word ended with a vowel (including a nasalized vowel, represented by a vowel plus ⟨m⟩) and the next word began with a vowel, the first vowel, at least in verse, was regularly elided; that is, it was omitted altogether, or possibly (in the case of /i/ and /u/) pronounced like the corresponding semivowel. When the second word was est or et, a different form of elision sometimes occurred (prodelision): the vowel of the preceding word was retained and the e was elided instead. Elision also occurred in Ancient Greek but in that language it is shown in writing by the vowel in question being replaced by an apostrophe, whereas in Latin elision is not indicated at all in the orthography, but can be deduced from the verse form. Only occasionally is it found in inscriptions, as in scriptust for scriptum est.
Latin spelling and pronunciation today.
Spelling.
Modern usage, even when printing classical Latin texts, varies in respect of "i" and "v". During the Renaissance the printing convention was to use "I" (upper case) and "i" (lower case) for both vocalic /i/ and consonantal /j/, to use "V" in the upper case and in the lower case to use "v" at the start of words and "u" subsequently within the word regardless of whether /u/ and /w/ was represented.
Many publishers (such as Oxford University Press) have adopted a purist convention of using "I" (upper case) and "i" (lower case) for both /i/ and /j/, and "V" (upper case) and "u" (lower case) for both /u/ and /w/.
An alternative approach, less common today, is to use "i" and "u" only for the vowels, and "j" and "v" for the approximants.
Most modern editions, however, adopt an intermediate position, distinguishing between "u" and "v" but not between "i" and "j". Usually the non-vocalic "v" after "q" or "g" is still printed as "u" rather than "v", probably because in this position it did not change from /w/ to /v/ in post-classical times.
Textbooks and dictionaries indicate the length of vowels by putting a macron or horizontal bar above the long vowel, but this is not generally done in regular texts. Occasionally, mainly in early printed texts up to the 18th century, one may see a circumflex used to indicate a long vowel where this makes a difference to the sense, for instance "Româ" /ˈroːmaː/ ('from Rome' ablative) compared to "Roma" /ˈroːma/ ('Rome' nominative). Sometimes, for instance in Roman Catholic service books, an acute accent over a vowel is used to indicate the stressed syllable. This would be redundant for one who knew the classical rules of accentuation, and also made the correct distinction between long and short vowels, but most Latin speakers since the 3rd century have not made any distinction between long and short vowels, while they have kept the accents in the same places, so the use of accent marks allows speakers to read aloud correctly even words that they have never heard spoken aloud.
Pronunciation.
Post-Medieval Latin.
Since around the beginning of the Renaissance period onwards, with the language being used as an international language among intellectuals, pronunciation of Latin in Europe came to be dominated by the phonology of local languages, resulting in a variety of different pronunciation systems.
Loan words and formal study.
When Latin words are used as loanwords in a modern language, there is ordinarily little or no attempt to pronounce them as the Romans did; in most cases, a pronunciation suiting the phonology of the receiving language is employed.
Latin words in common use in English are generally fully assimilated into the English sound system, with little to mark them as foreign, for example, "cranium", "saliva". Other words have a stronger Latin feel to them, usually because of spelling features such as the digraphs ⟨ae⟩ and ⟨oe⟩ (occasionally written as ligatures: ⟨æ⟩ and ⟨œ⟩, respectively), which both denote /iː/ in English. The digraph ⟨ae⟩ or ligature ⟨æ⟩ in some words tend to be given an /aɪ/ pronunciation, for example, "curriculum vitae".
However, using loan words in the context of the language borrowing them is a markedly different situation from the study of Latin itself. In this classroom setting, instructors and students attempt to recreate at least some sense of the original pronunciation. What is taught to native anglophones is suggested by the sounds of today's Romance languages, the direct descendants of Latin. Instructors who take this approach rationalize that Romance vowels probably come closer to the original pronunciation than those of any other modern language (see also the section below on "Derivative languages").
However, other languages—including Romance family members—all have their own interpretations of the Latin phonological system, applied both to loan words and formal study of Latin. But English, Romance, or other teachers do not always point out that the particular accent their students learn is not actually the way ancient Romans spoke.
Ecclesiastical pronunciation.
Because of the central position of Rome within the Catholic Church, an Italian pronunciation of Latin became commonly accepted, but studies by Frederick Brittain (published as "Latin in Church; the history of its pronunciation") show that this was not the case until the latter part of the 19th century. This pronunciation corresponds to that of the Latin-derived words in Italian. Before then, the pronunciation of Latin in church was the same as the pronunciation as Latin in other fields, and tended to reflect the sound values associated with the nationality of the speaker (Brittain, "Latin in Church; the history of its pronunciation").
The following are the main points that distinguish modern ecclesiastical pronunciation from Classical Latin pronunciation:
In his "Vox Latina: A guide to the Pronunciation of Classical Latin", William Sidney Allen remarked that this pronunciation, used by the Catholic Church in Rome and elsewhere, and whose adoption Pope Pius X recommended in a 1912 letter to the Archbishop of Bourges, "is probably less far removed from classical Latin than any other 'national' pronunciation"; but, as can be seen from the table above, there are, nevertheless, very significant differences. Pius X issued a Motu Proprio in 1903 stating that "[t]he language proper to the Roman Church is Latin". Although the document explicitly forbids "to sing anything whatever in the vernacular", it implies that the ecclesiastical pronunciation is the standard for all liturgical actions in the Church. The ecclesiastical pronunciation has since that time been the required pronunciation for any Catholic performing an action of the Church and is also the preferred pronunciation of Catholics whenever speaking Latin even if not as part of liturgy. The Pontifical Academy for Latin is a regulatory body in the Vatican that is charged with regulating Latin for use by Catholics similar to the way Académie française regulates the French language within the French state.
Outside of Austria and Germany it is the most widely used standard in choral singing which, with a few exceptions like Stravinsky's "Oedipus rex", is concerned with liturgical texts. Anglican choirs adopted it when classicists abandoned traditional English pronunciation after World War II. The rise of historically informed performance and the availability of guides such as Copeman's "Singing in Latin" has led to the recent revival of regional pronunciations.
Pronunciation shared by Vulgar Latin and Romance.
Because it gave rise to many modern languages, Latin did not strictly "die"; it merely evolved over the centuries in diverse ways. The local dialects of Vulgar Latin that emerged eventually became modern Italian, Spanish, French, Romanian, Portuguese, Catalan, Romansh, Dalmatian, Sardinian, and many others.
Key features of Vulgar Latin and Romance include:
Examples.
The following examples are both in verse, which demonstrates several features more clearly than prose.
From Classical Latin.
Virgil's "Aeneid", Book 1, verses 1–4. Quantitative metre. Translation: "I sing of arms and the man, who, driven by fate, came first from the borders of Troy to Italy and the Lavinian shores; he [was] much afflicted both on lands and on the deep by the power of the gods, because of fierce Juno's vindictive wrath."
Note the elisions in "mult(um)" and "ill(e)" in the third line. For a fuller discussion of the prosodic features of this passage, see Dactylic hexameter.
Some manuscripts have "Lavina" rather than "Lavinia" in the second line.
From Medieval Latin.
Beginning of Pange Lingua by St Thomas Aquinas (13th century). Rhymed accentual metre. Translation: "Extol, [my] tongue, the mystery of the glorious body and the precious blood, which the fruit of a noble womb, the king of nations, poured out as the price of the world."
1. Traditional orthography as in Roman Catholic service books (stressed syllable marked with an acute accent on words of three syllables or more).
2. "Italianate" ecclesiastical pronunciation
External links.
<br>

</doc>
<doc id="18120" url="http://en.wikipedia.org/wiki?curid=18120" title="Lysosome">
Lysosome

A lysosome (derived from the Greek words "lysis", meaning "to loosen", and "soma", "body") is a membrane-bound cell organelle found in most animal cells (they are absent in red blood cells). Structurally and chemically, they are spherical vesicles containing hydrolytic enzymes capable of breaking down virtually all kinds of biomolecules, including proteins, nucleic acids, carbohydrates, lipids, and cellular debris. They are known to contain more than 50 different enzymes, which are all optimally active at an acidic environment of about pH 4.5 (about the pH of black coffee). Thus lysosomes act as the waste disposal system of the cell by digesting unwanted materials in the cytoplasm, both from outside of the cell and obsolete components inside the cell. For this function they are popularly referred to as "suicide bags" or "suicide sacs" of the cell. Furthermore, lysosomes are responsible for cellular homeostasis for their involvements in secretion, plasma membrane repair, cell signalling and energy metabolism, which are related to health and diseases. Depending on their functional activity, their sizes can be very different—the biggest ones can be more than 10 times bigger than the smallest ones. They were discovered and named by Belgian biologist Christian de Duve, who eventually received the Nobel Prize in Physiology or Medicine in 1974.
Enzymes of the lysosomes are synthesised in the rough endoplasmic reticulum. The enzymes are released from Golgi apparatus in small vesicles which ultimately fuse with acidic vesicles called endosomes, thus becoming full lysosomes. In this process, the enzymes are specifically tagged with the molecule mannose 6-phosphate to differentiate them from other enzymes. Lysosomes are interlinked with three intracellular processes, namely phagocytosis, endocytosis and autophagy. Extracellular materials such as microorganisms taken up by phagocytosis, macromolecules by endocytosis, and unwanted cell organelles are fused with lysosomes in which they are broken down to their basic molecules. Thus lysosomes are the recycling units of a cell.
Synthesis of lysosomal enzymes is controlled by nuclear genes. Mutations in the genes for these enzymes are responsible for more than 30 different human genetic diseases, which are collectively known as lysosomal storage diseases. These diseases are due to deficiency in a single lysosomal enzyme, that prevents breakdown of target molecules; consequently the undegraded materials accumulate within the lysosomes and often giving rise to severe clinical symptoms. Further, such genetic defects are related to several neurodegenerative disorders, cancer, cardiovascular diseases, and ageing-related diseases.
Discovery.
Christian de Duve, then chairman of the Laboratory of Physiological Chemistry at the Catholic University of Louvain in Belgium, had been studying the mechanism of action of a pancreatic hormone insulin in liver cells. By 1949 he and his team had focused on the enzyme called glucose 6-phosphatase, which is the first crucial enzyme in sugar metabolism and the target of insulin. They already suspected that this enzyme played a key role in regulating blood sugar levels. However, even after a series of experiments, they failed to purify and isolate the enzyme from the cellular extracts. Therefore they tried a more arduous procedure of cell fractionation, by which cellular components are separated based on their sizes using centrifugation.
They succeeded in detecting the enzyme activity from the microsomal fraction. This was the crucial step in the serendipitous discovery of lysosomes. To estimate this enzyme activity, they used that of standardised enzyme acid phosphatase, and found that the activity was only 10% of the expected value. One day, the enzyme activity of purified cell fractions which had been refrigerated for five days was measured. Surprisingly, the enzyme activity was increased to normal of that of the fresh sample. The result was the same no matter how many times they repeated the estimation, and led to the conclusion that a membrane-like barrier limited the accessibility of the enzyme to its substrate, and that the enzymes were able to diffuse after a few days (and react with their substrate). They described this membrane-like barrier as a "saclike structure surrounded by a membrane and containing acid phosphatase."
It became clear that this enzyme from the cell fraction came from a membranous fractions, which were definitely cell organelles, and in 1955 De Duve named them "lysosomes" to reflect their digestive properties. The same year, Alex B. Novikoff from the University of Vermont visited de Duve´s laboratory, and successfully obtained the first electron micrographs of the new organelle. Using a staining method for acid phosphatase, de Duve and Novikoff confirmed the location of the hydrolytic enzymes of lysosomes using light and electron microscopic studies. de Duve won the Nobel Prize in Physiology or Medicine in 1974 for this discovery.
Function and structure.
Lysosomes are cellular organelles that contain acid hydrolase enzymes that break down waste materials and cellular debris. They can be described as the stomach of the cell. Lysosomes digest excess or worn-out organelles, food particles, and engulfed viruses or bacteria. The membrane around a lysosome allows the digestive enzymes to work at the pH they require. Lysosomes fuse with autophagic vacuoles (phagosomes) and dispense their enzymes into the autophagic vacuoles, digesting their contents. They are frequently nicknamed "suicide bags" or "suicide sacs" by cell biologists due to their autolysis.
The size of lysosomes varies from 0.1–1.2 μm. At pH 4.8, the interior of the lysosomes is acidic compared to the slightly basic cytosol (pH 7.2). The lysosome maintains this pH differential by pumping in protons (H+ ions) from the cytosol across the membrane via proton pumps and chloride ion channels. Vacuolar H+-ATPases are responsible for transport of protons, while the counter transport of chloride ions is performed by ClC-7 Cl−/H+ antiporter. In this way a steady acidic environment is maintained. The lysosomal membrane protects the cytosol, and therefore the rest of the cell, from the degradative enzymes within the lysosome. The cell is additionally protected from any lysosomal acid hydrolases that drain into the cytosol, as these enzymes are pH-sensitive and do not function well or at all in the alkaline environment of the cytosol. This ensures that cytosolic molecules and organelles are not destroyed in case there is leakage of the hydrolytic enzymes from the lysosome. 
Formation.
Many components of animal cells are recycled by transferring them inside or embedded in sections of membrane. For instance, in endocytosis (more specifically, macropinocytosis), a portion of the cell’s plasma membrane pinches off to form a vesicle that will eventually fuse with an organelle within the cell. Without active replenishment, the plasma membrane would continuously decrease in size. It is thought that lysosomes participate in this dynamic membrane exchange system and are formed by a gradual maturation process from endosomes.
The production of lysosomal proteins suggests one method of lysosome sustainment. Lysosomal protein genes are transcribed in the nucleus. mRNA transcripts exit the nucleus into the cytosol, where they are translated by ribosomes. The nascent peptide chains are translocated into the rough endoplasmic reticulum, where they are modified. Upon exiting the endoplasmic reticulum and entering the Golgi apparatus via vesicular transport, a specific lysosomal tag, mannose 6-phosphate, is added to the peptides. The presence of these tags allow for binding to mannose 6-phosphate receptors in the Golgi apparatus, a phenomenon that is crucial for proper packaging into vesicles destined for the lysosomal system.
Upon leaving the Golgi apparatus, the lysosomal enzyme-filled vesicle fuses with a late endosome, a relatively acidic organelle with an approximate pH of 5.5. This acidic environment causes dissociation of the lysosomal enzymes from the mannose 6-phosphate receptors. The enzymes are packed into vesicles for further transport to established lysosomes. The late endosome itself can eventually grow into a mature lysosome, as evidenced by the transport of endosomal membrane components from the lysosomes back to the endosomes.
Disease.
Lysosomes are responsible for a group of genetically inherited disorders called lysosomal storage diseases (LSD). They are a type of inborn errors of metabolism caused by malfunction of one of the enzymes. The rate of incidence is estimated to be 1 in 5,000 live births, and the true figure expected to be higher as many cases are likely to be undiagnosed or misdiagnosed. The primary cause is deficiency of an acidic hydrolase (a hydrolase which functions best in acidic environments). Other conditions are due to defects in lysosomal membrane proteins that fail to transport the enzyme, non-enzymatic soluble lysosomal proteins. The initial effect of such disorders is accumulation of specific macromolecules or monomeric compounds inside the endosomal–autophagic–lysosomal system. This results in abnormal signaling pathways, calcium homeostasis, lipid biosynthesis and degradation and intracellular trafficking, ultimately leading to pathogenetic disorders. The organs most affected are brain, viscera, bone and cartilage.
There is no direct medical treatment to cure LSDs. The most common LSD is Gaucher's disease, which is due to deficiency of the enzyme glucocerebrosidase. Consequently the enzyme substrate, the fatty acid glucosylceramide accumulates, particularly in white blood cells, which in turn affects spleen, liver, kidneys, lungs, brain and bone marrow. The disease is characterized by bruises, fatigue, anaemia, low blood platelets, osteoporosis, and enlargement of the liver and spleen.
Metachromatic leukodystrophy is another lysosomal storage disease that also affects sphingolipid metabolism.
Lysosomotropism.
Weak bases with lipophilic properties accumulate in acidic intracellular compartments like lysosomes. While the plasma and lysosomal membranes are permeable for neutral and uncharged species of weak bases, the charged protonated species of weak bases do not permeate biomembranes and accumulate within lysosomes. The concentration within lysosomes may reach levels 100 to 1000 fold higher than extracellular concentrations. This phenomenon is called "lysosomotropism" or "acid trapping". The amount of accumulation of lysosomotropic compounds may be estimated using a cell based mathematical model.
A significant part of the clinically approved drugs are lipophilic weak bases with lysosomotropic properties. This explains a number of pharmacological properties of these drugs, such as high tissue-to-blood concentration gradients or long tissue elimination half-lifes; these properties have been found for drugs such as haloperidol, levomepromazine, and amantadine. However, high tissue concentrations and long elimination half-lives are explained also by lipophilicity and absorption of drugs to fatty tissue structures. Important lysosomal enzymes, such as acid sphingomyelinase, may be inhibited by lysososomally accumulated drugs. Such compounds are termed FIASMAs (functional inhibitor of acid sphingomyelinase) and include for example fluoxetine, sertraline, or amitriptyline.
Controversy in botany.
By scientific convention, the term lysosome is applied to those vesicular organelles only in animals, and vacuoles to plants, fungi and algae. Discoveries in plant cells since the 1970s started to challenge this definition. Plant vacuoles are found to be much more diverse in structure and function than previously thought. Some vacuoles contain their own hydrolytic enzymes and perform the classic lysosomal activity, which is autophagy. These vacuoles are therefore seen as fulfilling the role of the animal lysosome. Based on de Duve's description that “only when considered as part of a system involved directly or indirectly in intracellular digestion does the term lysosome describe a physiological unit”, some botanists strongly argued that these vacuoles are lysosomes. However, this is not universally accepted as the vacuoles are strictly not similar to lysosomes, such as in their specific enzymes and lack of phagocytic functions. Vacuoles do not have catabolic activity and do not undergo exocytosis as lysosomes do.

</doc>
<doc id="18129" url="http://en.wikipedia.org/wiki?curid=18129" title="List of political parties named &quot;Labour Party&quot; or similar">
List of political parties named &quot;Labour Party&quot; or similar

The name Labour (or Labor) Party, or similar, is used by several political parties around the world, particularly in countries of the Commonwealth of Nations. They are most commonly, but not exclusively, social-democratic or democratic-socialist and traditionally allied to trade unions and the labour movement. Many labour parties are members of the Socialist International.

</doc>
<doc id="18173" url="http://en.wikipedia.org/wiki?curid=18173" title="List of libertarian political parties">
List of libertarian political parties

Many countries and subnational political entities have libertarian political parties. Although these parties may describe themselves as "libertarian," their ideologies differ considerably and not all of them support all elements of the libertarian agenda.

</doc>
<doc id="18209" url="http://en.wikipedia.org/wiki?curid=18209" title="Lossless compression">
Lossless compression

Lossless data compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy data compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).
Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).
Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.
Lossless compression techniques.
Most lossless compression programs do two things in sequence: the first step generates a "statistical model" for the input data, and the second step uses this model to map input data to bit sequences in such a way that "probable" (e.g. frequently encountered) data will produce shorter output than "improbable" data.
The primary encoding algorithms used to produce bit sequences are Huffman coding (also used by DEFLATE) and arithmetic coding. Arithmetic coding achieves compression rates close to the best possible for a particular statistical model, which is given by the information entropy, whereas Huffman compression is simpler and faster but produces poor results for models that deal with symbol probabilities close to 1.
There are two primary ways of constructing statistical models: in a "static" model, the data is analyzed and a model is constructed, then this model is stored with the compressed data. This approach is simple and modular, but has the disadvantage that the model itself can be expensive to store, and also that it forces using a single model for all data being compressed, and so performs poorly on files that contain heterogeneous data. "Adaptive" models dynamically update the model as the data is compressed. Both the encoder and decoder begin with a trivial model, yielding poor compression of initial data, but as they learn more about the data, performance improves. Most popular types of compression used in practice now use adaptive coders.
Lossless compression methods may be categorized according to the type of data they are designed to compress. While, in principle, any general-purpose lossless compression algorithm ("general-purpose" meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. Many of the lossless compression techniques used for text also work reasonably well for indexed images.
Text and image.
Statistical modeling algorithms for text (or text-like binary data such as executables) include:
Multimedia.
These techniques take advantage of the specific characteristics of images such as the common phenomenon of contiguous 2-D areas of similar tones.
Every pixel but the first is replaced by the difference to its left neighbor. This leads to small values having a much higher probability than large values.
This is often also applied to sound files, and can compress files that contain mostly low frequencies and low volumes.
For images, this step can be repeated by taking the difference to the top pixel, and then in videos, the difference to the pixel in the next frame can be taken.
A hierarchical version of this technique takes neighboring pairs of data points, stores their difference and sum, and on a higher level with lower resolution continues with the sums. This is called discrete wavelet transform. JPEG2000 additionally uses data points from other pairs and multiplication factors to mix them into the difference. These factors must be integers, so that the result is an integer under all circumstances. So the values are increased, increasing file size, but hopefully the distribution of values is more peaked. 
The adaptive encoding uses the probabilities from the previous sample in sound encoding, from the left and upper pixel in image encoding, and additionally from the previous frame in video encoding. In the wavelet transformation, the probabilities are also passed through the hierarchy.
Historical legal issues.
Many of these methods are implemented in open-source and proprietary tools, particularly LZW and its variants. Some algorithms are patented in the United States and other countries and their legal usage requires licensing by the patent holder. Because of patents on certain kinds of LZW compression, and in particular licensing practices by patent holder Unisys that many developers considered abusive, some open source proponents encouraged people to avoid using the Graphics Interchange Format (GIF) for compressing still image files in favor of Portable Network Graphics (PNG), which combines the LZ77-based deflate algorithm with a selection of domain-specific prediction filters. However, the patents on LZW expired on June 20, 2003.
Many of the lossless compression techniques used for text also work reasonably well for indexed images, but there are other techniques that do not work for typical text that are useful for some images (particularly simple bitmaps), and other techniques that take advantage of the specific characteristics of images (such as the common phenomenon of contiguous 2-D areas of similar tones, and the fact that color images usually have a preponderance of a limited range of colors out of those representable in the color space).
As mentioned previously, lossless sound compression is a somewhat specialized area. Lossless sound compression algorithms can take advantage of the repeating patterns shown by the wave-like nature of the data – essentially using autoregressive models to predict the "next" value and encoding the (hopefully small) difference between the expected value and the actual data. If the difference between the predicted and the actual data (called the "error") tends to be small, then certain difference values (like 0, +1, −1 etc. on sample values) become very frequent, which can be exploited by encoding them in few output bits.
It is sometimes beneficial to compress only the differences between two versions of a file (or, in video compression, of successive images within a sequence). This is called delta encoding (from the Greek letter Δ, which in mathematics, denotes a difference), but the term is typically only used if both versions are meaningful outside compression and decompression. For example, while the process of compressing the error in the above-mentioned lossless audio compression scheme could be described as delta encoding from the approximated sound wave to the original sound wave, the approximated version of the sound wave is not meaningful in any other context.
Lossless compression methods.
By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. For this reason, many different algorithms exist that are designed either with a specific type of input data in mind or with specific assumptions about what kinds of redundancy the uncompressed data are likely to contain.
Some of the most common lossless compression algorithms are listed below.
Video.
See this list of lossless video codecs.
Cryptography.
Cryptosystems often compress data (the "plaintext") "before" encryption for added security. When properly implemented, compression greatly increases the unicity distance by removing patterns that might facilitate cryptanalysis. However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. Thus, cryptosystems must utilize compression algorithms whose output does not contain these predictable patterns.
Genetics.
Genetics compression algorithms (not to be confused with genetic algorithms) are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and specific algorithms adapted to genetic data. In 2012, a team of scientists from Johns Hopkins University published the first genetic compression algorithm that does not rely on external genetic databases for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities.
Executables.
Self-extracting executables contain a compressed application and a decompressor. When executed, the decompressor transparently decompresses and runs the original application. This is especially often used in demo coding, where competitions are held for demos with strict size limits, as small as 1k.
This type of compression is not strictly limited to binary executables, but can also be applied to scripts, such as JavaScript.
Lossless compression benchmarks.
Lossless compression algorithms and their implementations are routinely tested in head-to-head benchmarks. There are a number of better-known compression benchmarks. Some benchmarks cover only the data compression ratio, so winners in these benchmarks may be unsuitable for everyday use due to the slow speed of the top performers. Another drawback of some benchmarks is that their data files are known, so some program writers may optimize their programs for best performance on a particular data set. The winners on these benchmarks often come from the class of context-mixing compression software.
The benchmarks listed in the 5th edition of the "Handbook of Data Compression" (Springer, 2009) are:
Matt Mahoney, in his February 2010 edition of the free booklet "Data Compression Explained", additionally lists the following:
 publishes a chart summary of the "frontier" in compression ratio and time.
Limitations.
Lossless data compression algorithms cannot guarantee compression for all input data sets. In other words, for any lossless data compression algorithm, there will be an input data set that does not get smaller when processed by the algorithm, and for any lossless data compression algorithm that makes at least one file smaller, there will be at least one file that it makes larger. This is easily proven with elementary mathematics using a counting argument, as follows:
Any lossless compression algorithm that makes some files shorter must necessarily make some files longer, but it is not necessary that those files become "very much" longer. Most practical compression algorithms provide an "escape" facility that can turn off the normal coding for files that would become longer by being encoded. In theory, only a single additional bit is required to tell the decoder that the normal coding has been turned off for the entire input; however, most encoding algorithms use at least one full byte (and typically more than one) for this purpose. For example, DEFLATE compressed files never need to grow by more than 5 bytes per 65,535 bytes of input.
In fact, if we consider files of length N, if all files were equally probable, then for any lossless compression that reduces the size of some file, the expected length of a compressed file (averaged over all possible files of length N) must necessarily be "greater" than N. So if we know nothing about the properties of the data we are compressing, we might as well not compress it at all. A lossless compression algorithm is useful only when we are more likely to compress certain types of files than others; then the algorithm could be designed to compress those types of data better.
Thus, the main lesson from the argument is not that one risks big losses, but merely that one cannot always win. To choose an algorithm always means implicitly to select a "subset" of all files that will become usefully shorter. This is the theoretical reason why we need to have different compression algorithms for different kinds of files: there cannot be any algorithm that is good for all kinds of data.
The "trick" that allows lossless compression algorithms, used on the type of data they were designed for, to consistently compress such files to a shorter form is that the files the algorithms are designed to act on all have some form of easily modeled redundancy that the algorithm is designed to remove, and thus belong to the subset of files that that algorithm can make shorter, whereas other files would not get compressed or even get bigger. Algorithms are generally quite specifically tuned to a particular type of file: for example, lossless audio compression programs do not work well on text files, and vice versa.
In particular, files of random data cannot be consistently compressed by any conceivable lossless data compression algorithm: indeed, this result is used to "define" the concept of randomness in algorithmic complexity theory.
It's provably impossible to create an algorithm that can losslessly compress any data. While there have been many claims through the years of companies achieving "perfect compression" where an arbitrary number "N" of random bits can always be compressed to "N" − 1 bits, these kinds of claims can be safely discarded without even looking at any further details regarding the purported compression scheme. Such an algorithm contradicts fundamental laws of mathematics because, if it existed, it could be applied repeatedly to losslessly reduce any file to length 0. Allegedly "perfect" compression algorithms are often derisively referred to as "magic" compression algorithms for this reason.
On the other hand, it has also been proven that there is no algorithm to determine whether a file is incompressible in the sense of Kolmogorov complexity. Hence it's possible that any particular file, even if it appears random, may be significantly compressed, even including the size of the decompressor. An example is the digits of the mathematical constant "pi", which appear random but can be generated by a very small program. However, even though it cannot be determined whether a particular file is incompressible, a simple theorem about incompressible strings shows that over 99% of files of any given length cannot be compressed by more than one byte (including the size of the decompressor).
Mathematical background.
Abstractly, a compression algorithm can be viewed as a function on sequences (normally of octets). Compression is successful if the resulting sequence is shorter than the original sequence (and the instructions for the decompression map). For a compression algorithm to be lossless, the compression map must form a bijection between "plain" and "compressed" bit sequences.
The pigeonhole principle prohibits a bijection between the collection of sequences of length "N" and any subset of the collection of sequences of length "N"−1. Therefore it is not possible to produce an algorithm that reduces the size of every possible input sequence.
Psychological background.
Most everyday files are relatively 'sparse' in an information entropy sense, and thus, most lossless algorithms a layperson is likely to apply on regular files compress them relatively well. This may, through misapplication of intuition, lead some individuals to conclude that a well-designed compression algorithm can compress "any" input, thus, constituting a "magic compression algorithm".
Points of application in real compression theory.
Real compression algorithm designers accept that streams of high information entropy cannot be compressed, and accordingly, include facilities for detecting and handling this condition. An obvious way of detection is applying a raw compression algorithm and testing if its output is smaller than its input. Sometimes, detection is made by heuristics; for example, a compression application may consider files whose names end in ".zip", ".arj" or ".lha" uncompressible without any more sophisticated detection. A common way of handling this situation is quoting input, or uncompressible parts of the input in the output, minimising the compression overhead. For example, the zip data format specifies the 'compression method' of 'Stored' for input files that have been copied into the archive verbatim.
The Million Random Number Challenge.
Mark Nelson, frustrated over many cranks trying to claim having invented a magic compression algorithm appearing in comp.compression, has constructed a 415,241 byte binary file of highly entropic content, and issued a public challenge of $100 to anyone to write a program that, together with its input, would be smaller than his provided binary data yet be able to reconstitute ("decompress") it without error.
The FAQ for the comp.compression newsgroup contains a challenge by Mike Goldman offering $5,000 for a program that can compress random data. Patrick Craig took up the challenge, but rather than compressing the data, he split it up into separate files all of which ended in the number "5", which was not stored as part of the file. Omitting this character allowed the resulting files (plus, in accordance with the rules, the size of the program that reassembled them) to be smaller than the original file. However, no actual compression took place, and the information stored in the names of the files was necessary to reassemble them in the correct order in the original file, and this information was not taken into account in the file size comparison. The files themselves are thus not sufficient to reconstitute the original file; the file names are also necessary. Patrick Craig agreed that no meaningful compression had taken place, but argued that the wording of the challenge did not actually require this. A full history of the event, including discussion on whether or not the challenge was technically met, is on Patrick Craig's web site.

</doc>
<doc id="18213" url="http://en.wikipedia.org/wiki?curid=18213" title="Los Angeles Dodgers">
Los Angeles Dodgers

The Los Angeles Dodgers are a professional baseball team located in Los Angeles, California. The Dodgers are members of the National League (NL) West division of Major League Baseball (MLB). Established in 1883, the team originated in Brooklyn, New York, where it was known by a number of nicknames before becoming the Brooklyn Dodgers definitively by 1932. The team moved to Los Angeles before the 1958 season. They played their first four seasons in Los Angeles at the Los Angeles Memorial Coliseum before moving to their current home of Dodger Stadium, the third-oldest ballpark in Major League Baseball (after Fenway Park and Wrigley Field).
The Dodgers have won six World Series titles and 21 National League pennants. Eight Cy Young Award winners have pitched for the Dodgers, winning a total of twelve Cy Young Awards (both MLB records). The team has also produced 12 Rookie of the Year Award winners, including four consecutive from 1979 to 1982 and five consecutive from 1992 to 1996, the longest consecutive streaks in Major League Baseball.
History.
In the 20th century, the team, then known as the Robins, won league pennants in 1916 and 1920, losing the World Series both times, first to Boston and then Cleveland. In 1941, as the Dodgers, they captured their third National League pennant, only to lose again to the New York Yankees. This marked the onset of the Dodgers–Yankees rivalry, as the Dodgers would face them in their next six World Series appearances. Led by Jackie Robinson, the first black Major League Baseball player of the modern era; and three-time National League Most Valuable Player Roy Campanella, also signed out of the Negro Leagues, the Dodgers captured their first World Series title in 1955 by defeating the Yankees for the first time, a story notably described in the 1972 book "The Boys of Summer".
Following the 1957 season the team left Brooklyn. In just their second season in Los Angeles, the Dodgers won their second World Series title, beating the Chicago White Sox in six games in 1959. Spearheaded by the dominant pitching style of Sandy Koufax and Don Drysdale, the Dodgers captured three pennants in the 1960s and won two more World Series titles, sweeping the Yankees in four games in 1963, and edging the Minnesota Twins in seven in 1965. The 1963 sweep was their second victory against the Yankees, and their first against them as a Los Angeles team. The Dodgers won four more pennants in 1966, 1974, 1977 and 1978, but lost in each World Series appearance. They went on to win the World Series again in 1981, thanks to pitching sensation Fernando Valenzuela. The early 1980s were affectionately dubbed "Fernandomania." In 1988, another pitching hero, Orel Hershiser, again led them to a World Series victory, aided by one of the most memorable home runs of all time, by their injured star outfielder Kirk Gibson coming off the bench to pinch hit with two outs in the bottom of the ninth inning of game 1, in his only appearance of the series.
The Dodgers share a fierce rivalry with the San Francisco Giants, the oldest rivalry in baseball, dating back to when the two franchises played in New York City. Both teams moved west for the 1958 season. The Brooklyn Dodgers and Los Angeles Dodgers have collectively appeared in the World Series 18 times, while the New York Giants and San Francisco Giants have collectively appeared 20 times and have been invited 21 times. The Giants have won two more World Series (8); the Dodgers have won 21 National League pennants, while the Giants hold the record with 23. Although the two franchises have enjoyed near equal success, the city rivalries are rather lopsided and in both cases, a team's championships have predated to the other's first one in that particular location. When the two teams were based in New York, the Giants won five World Series championships, and the Dodgers one. After the move to California, the Dodgers have won five in Los Angeles, the Giants have won three in San Francisco.
Team history.
Brooklyn Dodgers.
The Dodgers were originally founded in 1883 as the Brooklyn Atlantics, taking the name of a defunct team that had played in Brooklyn before them. The team joined the American Association in 1884 and won the AA championship in 1889 before joining the National League in 1890. They promptly won the NL Championship their first year in the League. The team was known alternatively as the Bridegrooms, Grooms, Superbas, Robins, and Trolley Dodgers before officially becoming the Dodgers in the 1930s.
In Brooklyn, the Dodgers won the NL pennant several times (1890, 1899, 1900, 1916, 1920, 1941, 1947, 1949, 1952, 1953, 1955, 1956) and the World Series in 1955. After moving to Los Angeles, the team won National League pennants in 1959, 1963, 1965, 1966, 1974, 1977, 1978, 1981, and 1988, with World Series championships in 1959, 1963, 1965, 1981, 1988. In all, the Dodgers have appeared in 18 World Series: 9 in Brooklyn and 9 in Los Angeles.
Jackie Robinson.
For most of the first half of the 20th century, no Major League Baseball team employed an African American player. Jackie Robinson became the first African American to play for a Major League Baseball team when he played his first major league game on April 15, 1947, as a member of the Brooklyn Dodgers. This was mainly due to general manager Branch Rickey's efforts. The deeply religious Rickey's motivation appears to have been primarily moral, although business considerations were also a factor. Rickey was a member of The Methodist Church, the antecedent denomination to The United Methodist Church of today, which was a strong advocate for social justice and active later in the Civil Rights movement.
This event was the harbinger of the integration of professional sports in the United States, the concomitant demise of the Negro Leagues, and is regarded as a key moment in the history of the American Civil Rights movement. Robinson was an exceptional player, a speedy runner who sparked the team with his intensity. He was the inaugural recipient of the Rookie of the Year award, which is now named the Jackie Robinson Award in his honor. The Dodgers' willingness to integrate, when most other teams refused to, was a key factor in their 1947–1956 success. They won six pennants in those 10 years with the help of Robinson, three-time MVP Roy Campanella, Cy Young Award winner Don Newcombe, Jim Gilliam and Joe Black. Robinson would eventually go on to become the first African-American elected to the Baseball Hall of Fame in 1962.
Move to Los Angeles.
Real estate businessman Walter O'Malley had acquired majority ownership of the Dodgers in 1950, when he bought the shares of his co-owners, Branch Rickey and the estate of James L. Smith. Before long he was working to buy new land in Brooklyn to build a more accessible and better arrayed ballpark than Ebbets Field. Beloved as it was, Ebbets Field was no longer well-served by its aging infrastructure and the Dodgers could no longer sell out the park even in the heat of a pennant race (despite largely dominating the league from 1946 to 1957).
O'Malley wanted to build a new, state of the art stadium in Brooklyn. But City Planner Robert Moses and other New York politicians refused to let him build the Brooklyn stadium he wanted. During the 1955 season he announced that the team would play seven regular season games and one exhibition game at Jersey City's Roosevelt Stadium in 1956. He expected that this move would put pressure on the city's politicians to build the Dodgers the park he wanted in Brooklyn. Yet Moses and the others considered this an empty threat, and did not believe O'Malley would go through with moving the team from New York City. That is when Los Angeles came into the picture.
After teams began to travel to and from games by air instead of train, it became possible to include locations in the far west. When Los Angeles officials attended the 1956 World Series looking to entice a team to move to the City of Angels, they were not even considering the Dodgers. Their original target had been the Washington Senators (who would in fact move to Bloomington, suburban Minneapolis, to become the Minnesota Twins in 1961). When O'Malley heard that LA was looking for a club, he sent word to the Los Angeles officials that he was interested in talking. LA offered him what New York would not: a chance to buy land suitable for building a ballpark, and own that ballpark, giving him complete control over all revenue streams. When the news came out, NY Mayor Robert F. Wagner, Jr. and Moses made a feeble effort to save the Dodgers, offering to build a ballpark on the World's Fair Grounds in Queens. Wagner was already on shaky ground, as the New York Giants were getting ready to move out of the crumbling Polo Grounds. However, O'Malley was interested in his park only under his conditions, and the plans for a new stadium in Brooklyn seemed like a pipe dream. Walter O'Malley was left with the difficult decision to move the Dodgers to California, convincing Giants owner Horace Stoneham to move to San Francisco instead of Minneapolis to keep the Giants-Dodgers rivalry alive on the West Coast. There was no turning back: the Dodgers were heading for Hollywood.
The Dodgers played their final game at Ebbets Field on September 24, 1957, which the Dodgers won 2–0 over the Pittsburgh Pirates.
Los Angeles Dodgers.
The Dodgers were the first Major League Baseball team to ever play in Los Angeles. On April 18, 1958, the Dodgers played their first LA game, defeating the former New York and now new San Francisco Giants, 6–5, before 78,672 fans at the Los Angeles Memorial Coliseum. Catcher Roy Campanella, left partially paralyzed in an off-season accident, was never able to play in Los Angeles.
Construction on Dodger Stadium was completed in time for Opening Day 1962. With its clean, simple lines and its picturesque setting amid hills and palm trees, the ballpark quickly became an icon of the Dodgers and their new California lifestyle. O'Malley was determined that there would not be a bad seat in the house, achieving this by cantilevered grandstands that have since been widely imitated. More importantly for the team, the stadium's spacious dimensions, along with other factors, gave defense an advantage over offense and the Dodgers moved to take advantage of this by assembling a team that would excel with its pitching.
Since moving to Los Angeles, the Dodgers have won nine more National League Championships and five World Series rings.
Other historical notes.
The team's nickname.
The Dodgers' official history tells us that, "[t]he term "Trolley Dodgers" was attached to the Brooklyn ballclub due to the complex maze of trolley cars that weaved its way through the borough of Brooklyn."
In 1892, the city of Brooklyn (Brooklyn was an independent city until annexed by New York City in 1898) began replacing its slow-moving, horse-drawn trolley lines with the faster, more powerful electric trolley lines. Within less than three years, by the end of 1895, electric trolley accidents in Brooklyn had resulted in more than 130 deaths and maimed well over 500 people. Brooklyn's high-profile, the significant number of widely-reported accidents, and a trolley strike in early 1895, combined to create a strong association in the public's mind between Brooklyn and trolley dodging.
Sportswriters started using the name "trolley dodgers" to refer to the Brooklyn team early in the 1895 season. The name was shortened to, on occasion, the "Brooklyn Dodgers" as early as 1898.
Sportswriters in the early 20th century began referring to the Dodgers as the "Bums", in reference to the team's fans and possibly because of the "street character" nature of Jack Dawkins, the "Artful Dodger" in Charles Dickens' "Oliver Twist".
Other team names used by the franchise were the Atlantics, Grays, Grooms, Bridegrooms, Superbas and Robins. All of these nicknames were used by fans and sportswriters to describe the team, but not in any official capacity. The team's legal name was the Brooklyn Base Ball Club. However, the Trolley Dodger nickname was used throughout this period, simultaneously with these other nicknames, by fans and sportswriters of the day. The team did not use the name in any formal sense until 1932, when the word "Dodgers" appeared on team jerseys. The "conclusive shift" came in 1933, when both home and road jerseys for the team bore the name "Dodgers".
Examples of how the many popularized names of the team were used are available from newspaper articles before 1932. A New York Times article describing a game in 1916 starts out: "Jimmy Callahan, pilot of the Pirates, did his best to wreck the hopes the Dodgers have of gaining the National League pennant", but then goes on to comment: "the only thing that saved the Superbas from being toppled from first place was that the Phillies lost one of the two games played". What is interesting about the use of these two nicknames is that most baseball statistics sites and baseball historians generally now refer to the pennant-winning 1916 Brooklyn team as the Robins. A 1918 New York Times article uses the nickname in its title: "Buccaneers Take Last From Robins", but the subtitle of the article reads: "Subdue The Superbas By 11 To 4, Making Series An Even Break".
Another example of the use of the many nicknames is found on the program issued at Ebbets Field for the 1920 World Series which identifies the matchup in the series as "Dodgers vs. Indians" despite the fact that the Robins nickname had been in consistent use for around six years. The "Robins" nickname was derived from the name of their Hall of Fame manager, Wilbert Robinson, who led the team from 1914 to 1931
Uniforms.
The Dodgers uniforms have remained relatively unchanged for over 70 years. The home jersey is white with "Dodgers" written in script across the chest in Dodger Blue. The road jersey is grey with "Los Angeles" written in script across the chest in Dodger Blue. The word "Dodgers" was first used on the front of the team's home jersey in 1933; the uniform was white with red pinstripes, and the stylized B on the left shoulder. The Dodgers also wore green outlined uniforms and green caps throughout the 1937 season but reverted to blue the following year.
The current design was adopted in 1939, and has remained the same ever since with only minor cosmetic changes. Since 1952, the home uniform has had a red uniform number under the "Dodgers" script. The road jerseys also have a red uniform number under the script. The most obvious change is the removal of "Brooklyn" from the road jerseys and the replacement of the stylized "B" with the interlocking "LA" on the caps in 1958. In 1970, the Dodgers removed the city name from the road jerseys and had "Dodgers" on both the home and away uniforms. The city script returned to the road jerseys in 1999, and the tradition-rich Dodgers flirted with an alternate uniform for the first time since 1944 (when all-blue satin uniforms were introduced). These 1999 alternate jerseys had a royal blue top with the "Dodgers" script in white across the chest, and the red number on the front. These were worn with white pants and a new cap with silver brim, top button and Dodger logo. These alternates proved unpopular and the team abandoned them after only one season. In 2014, the Dodgers introduced an alternate road jersey: a grey version of the home jersey (w/the Dodgers script).
Asian players.
The Dodgers have been groundbreaking in their signing of players from Asia; mainly, Japan, South Korea, and Taiwan. Former owner Peter O'Malley began reaching out in 1980 by starting clinics in China and South Korea, building baseball fields in two Chinese cities, and in 1998 becoming the first major league team to open an office in Asia. The Dodgers were the second team to start a Japanese player in recent history, pitcher Hideo Nomo, the first team to start a South Korean player, pitcher Chan Ho Park, and the first Taiwanese player, Chin-Feng Chen. In addition, they were the first team to send out three Asian pitchers, from different Asian countries, in one game: Park, Hong-Chih Kuo of Taiwan, and Takashi Saito of Japan. In the 2008 season, the Dodgers had the most Asian players on its roster of any major league team with five. They included Japanese pitchers Takashi Saito and Hiroki Kuroda; South Korean pitcher Chan Ho Park; and Taiwanese pitcher Hong-Chih Kuo and infielder Chin-Lung Hu. In 2005, the Dodgers' Hee Seop Choi became the first Asian player to compete in the Home Run Derby. For the 2013 season, the Dodgers signed starting pitcher Hyun-Jin Ryu with a six-year, $36,000,000 contract, after posting a bid of nearly $27,000,000 to acquire him from the KBO's Hanhwa Eagles.
Rivalries.
The Dodgers' rivalry with the San Francisco Giants dates back to the 19th century, when the two teams were based in New York; the rivalry with the New York Yankees took place when the Dodgers were based in New York, but was revived with their East Coast/West Coast World Series battles in 1963, 1977, 1978, and 1981. The Dodgers also had a heated rivalry with the Cincinnati Reds during the 1970s, 1980s and early 1990s. The rivalry with the Los Angeles Angels of Anaheim and the San Diego Padres dates back to the Angels' and Padres' respective inaugural seasons (Angels in 1961, Padres in 1969). Regional proximity is behind the rivalries with both the Angels and the Padres.
San Francisco Giants.
The Dodgers–Giants rivalry is one of the longest-standing rivalries in American baseball.
The feud between the Dodgers and the San Francisco Giants began in the late 19th century when both clubs were based in New York City, with the Dodgers playing in Brooklyn and the Giants playing at the Polo Grounds in Manhattan. After the 1957 season, Dodgers owner Walter O'Malley moved the team to Los Angeles for financial and other reasons. Along the way, he managed to convince Giants owner Horace Stoneham—who was considering moving his team to Minnesota—to preserve the rivalry by bringing his team to California as well. New York baseball fans were stunned and heartbroken by the move. Given that the cities of Los Angeles and San Francisco have been bitter rivals in economic, cultural, and political arenas for over a century and a half, the new venue in California became fertile ground for its transplantation.
Each team's ability to endure for over a century while moving across an entire continent, as well as the rivalry's leap from a cross-city to a cross-state engagement, have led to the rivalry being considered one of the greatest in sports history.
Unlike many other historic baseball match-ups in which one team remains dominant for most of their history, the Dodgers–Giants rivalry has exhibited a persistent balance in the respective successes of the two teams. While the Giants have more wins in franchise history, and lead all NL teams with 23 National League pennants, the Dodgers are second, having won 21; the Giants have won eight World Series titles, while the Dodgers have won six. The 2010 World Series was the Giants' first championship since moving to California, while the Dodgers' last title came in the 1988 World Series.
Los Angeles Angels of Anaheim.
This rivalry refers to a series of games played with the Los Angeles Angels of Anaheim. The series takes its name from the massive freeway system in the greater Los Angeles metropolitan area, the home of both teams; one could travel from one team's stadium to the other simply by traveling along Interstate 5. The term is akin to "Subway Series" which refers to meetings between New York City baseball teams. The term "Freeway Series" also inspired the official name of the regions' NHL rivalry: the "Freeway Face-Off".
San Diego Padres.
The rivalry between Dodgers and San Diego Padres is a divisional rivalry since both play in the NL West. Also, the two teams are both based in major cities of Southern California, and both separated by the Interstate 5. One of the most notable games between the two is the one on April 11, 2013 when they got into a bench-clearing brawl after Dodgers pitcher Zach Greinke hit Padres Carlos Quentin with a pitch. The rivalry has escalated slightly after the Dodgers traded fan favorite, Matt Kemp, to the Padres on December 11, 2014. The decision to trade Kemp was highly unfavorable with Dodger fans, as he was viewed by many as an unofficial captain for the team.
Historical rivalry.
New York Yankees.
The Dodgers–Yankees rivalry is one of the most well-known rivalries in Major League Baseball. The two teams have met eleven times in the World Series, more times than any other pair from the American and National Leagues. The initial significance was embodied in the two teams' proximity in New York City, when the Dodgers initially played in Brooklyn. After the Dodgers moved to Los Angeles in 1958, the rivalry retained its significance as the two teams represented the dominant cities on each coast of the United States, and since the 1980s, the two largest cities in the United States.
Although the rivalry's significance arose from the two teams' numerous World Series meetings, the Yankees and Dodgers have not met in the World Series since 1981. They would not play each other in a non-exhibition game until 2004, when they played a three-game interleague series. Their last meeting was in July 2013, when they split a two-game series in Los Angeles.
Fan support.
The Dodgers have a loyal fanbase, evidenced by the fact that the Dodgers were the first MLB team to attract more than 3 million fans in a season (in 1978), and accomplished that feat six more times before any other franchise did it once. The Dodgers drew at least 3 million fans for 15 consecutive seasons from 1996 to 2010, the longest such streak in all of MLB. On July 3, 2007, Dodgers management announced that total franchise attendance, dating back to 1901, had reached 175 million, a record for all professional sports. In 2007, the Dodgers set a franchise record for single-season attendance, attracting over 3.8 million fans. In 2009, the Dodgers led MLB in total attendance. The Dodger baseball cap is consistently in the top three in sales. During the 2011-2012 season, Frank McCourt, the owner of the Dodgers at that time, was going through a rough divorce with his wife over who should be the owner of the Dodger team. Instead, Frank McCourt paid $131 million to his wife as part of the divorce settlement. As a result, the team payroll was financially low for a big-budget team crippling the Dodgers in the free-agent market. Collectively, the team performance waned due to the distracting drama in the front office resulting in low attendance numbers.
Given the team's proximity to Hollywood, numerous celebrities can often be seen attending home games at Dodger Stadium. Celebrities such as Magic Johnson, Tiger Woods, Alyssa Milano, and Shia Labeouf are known to sit at field box seats behind home plate where they sign autographs for fellow Dodger fans. Actor Bryan Cranston is a lifelong Dodger fan.
The Dodgers set the world record for the largest attendance for a single baseball game during an exhibition game against the Boston Red Sox on March 28, 2008 at the Los Angeles Memorial Coliseum in honor of the Dodgers 50th anniversary, with 115,300 fans in attendance. All proceeds from the game benefited the official charity of the Dodgers, ThinkCure! which supports cancer research at Children's Hospital Los Angeles and City of Hope.
Radio and television.
Vin Scully has called Dodgers games since 1950. His longtime partners were Jerry Doggett (1956–1987) and Ross Porter (1977–2004). In 1976, he was selected by Dodgers fans as the Most Memorable Personality (on the field or off) in the team's history. He is also a recipient of the Baseball Hall of Fame's Ford C. Frick Award for broadcasters (inducted in 1982). Unlike the modern style in which multiple sportscasters have an on-air conversation (usually with one functioning as play-by-play announcer and the other[s] as color commentator), Scully, Doggett and Porter generally called games solo, trading with each other inning-by-inning. In the 1980s and 1990s, Scully would call the entire radio broadcast except for the third and seventh inning, allowing the other Dodger commentators to broadcast an inning.
When Doggett retired after the 1987 season, he was replaced by Hall-of-Fame Dodgers pitcher Don Drysdale, who previously broadcast games for the California Angels and Chicago White Sox. Drysdale died in his hotel room following a heart attack before a game in Montreal in 1993. This was a difficult broadcast for Scully and Porter who could not mention it on-air until Drysdale's family had been notified and the official announcement made. He was replaced by former Dodgers outfielder Rick Monday. Porter's tenure ended after the 2004 season, after which the format of play-by-play announcers and color commentators was installed, led by Monday and newcomer Charley Steiner. Scully, however, continues to announce solo.
Scully calls roughly 100 games per season (all home games and road games in California and Arizona) for both flagship radio station KLAC and on television for SportsNet LA. Scully is simulcast for the first three innings of each of his appearances, then announces only for the TV audience. If Scully is calling the game, Steiner takes over play-by-play on radio beginning with the fourth inning, with Monday as color commentator. If Scully is not calling the game, Steiner and Orel Hershiser call the entire game on television while Monday and Kevin Kennedy do the same on radio. In the event the Dodgers are in post-season play, Scully calls the first three and last three innings of the radio broadcast alone and Steiner and Monday handle the middle innings.
The Dodgers also broadcast on radio in Spanish, and the play-by-play is voiced by another Frick Award winner, Jaime Jarrín, who has been with the Dodgers since 1959. The color analyst for some games is former Dodger pitcher Fernando Valenzuela, for whom Jarrin once translated post-game interviews. The Spanish-language radio flagship station is KTNQ.
Achievements.
Ford C. Frick Award recipients.
Names in bold received the award based primarily on their work as Dodgers broadcasters.
 Played as Dodgers
Retired numbers.
Koufax, Campanella, and Robinson were the first Dodgers to have their numbers retired, in a ceremony at Dodger Stadium on June 4, 1972. This was the year in which Koufax was inducted into the Baseball Hall of Fame; Robinson and Campanella were already Hall-of-Famers.
Alston's number was retired in the year following his retirement as the Dodgers manager, six years before he was inducted into the Hall of Fame.
Gilliam died suddenly in 1978 after a 28-year career with the Dodgers organization. The Dodgers retired his number two days after his death, prior to Game 1 of the 1978 World Series. He is the only non-Hall-of-Famer to have his number retired by the Dodgers.
Beginning in 1980, the Dodgers have retired the numbers of longtime Dodgers (Snider, Reese, Drysdale, Lasorda, and Sutton) during the seasons in which each was inducted into the Hall of Fame.
In 1997, 50 years after he broke the color barrier and 25 years after the Dodgers retired his number, Robinson's No.42 was retired throughout Major League Baseball. Robinson is the only major league baseball player to have this honor bestowed upon him. Starting in the 2007 season, Jackie Robinson Day (April 15, commemorating Opening Day of Robinson's rookie season of 1947) has featured many or all players and coaches wearing the number 42 as a tribute to Robinson.
The Dodgers have not issued No.34 since the departure of Fernando Valenzuela in 1991, although it has not been officially retired.
Personnel.
Managers.
Since 1884, the Dodgers have used a total of 30 Managers, the most current being Don Mattingly, who was appointed at the conclusion of the 2010 season as the successor to Joe Torre.
The managers of the Los Angeles Dodgers (1958–present) are as follows:
Public address announcers.
From the Dodgers' move to Los Angeles from Brooklyn in 1958, the Dodgers employed a handful of well-known public address announcers; the most famous of which was John Ramsey, who served as the PA voice of the Dodgers from 1958 until his retirement in 1982; as well as announcing at other venerable Los Angeles venues, including the Los Angeles Memorial Coliseum and Sports Arena, and the Forum. Ramsey died in 1990.
From 1958 to 1982, Doug Moore, a local businessman; Philip Petty, an Orange County Superior Court Judge; and Dennis Packer; served as back-up voices for John Ramsey for the Dodgers, California Angels, Los Angeles Chargers, USC football and Los Angeles Rams. Packer was Ramsey's primary backup for the Los Angeles Lakers and Los Angeles Kings until Ramsey's retirement from the Forum in 1978. Thereafter, Packer became the public address announcer for the Lakers, Kings, indoor soccer and indoor tennis events at the Forum.
Nick Nickson, a radio broadcaster for the Los Angeles Kings, replaced John Ramsey as the Dodger Stadium public address announcer in 1983 and served in that capacity through the 1989 season to work with the Kings full-time.
Dennis Packer and Pete Arbogast were emulators of John Ramsey, using the same stentorian style of announcing Ramsey was famous for. Packer and Arbogast shared the stadium announcing chores for the 1994 FIFA World Cup matches at the Rose Bowl. Arbogast won the Dodgers job on the day that Ramsey died on January 25, 1990, by doing a verbatim imitation of Ramsey's opening and closing remarks that were standard at each game. He left following the 1993 season to concentrate with his duties as the radio voice of USC sports. Arbogast's replacement was Mike Carlucci, who remained as the Dodgers' PA voice until 2001.
Through 2014, the Dodgers public address announcer was Eric Smith, who also announces for the Los Angeles Clippers and USC Trojans.
On April 3, 2015 the Dodgers announced that former radio broadcaster Todd Leitz would become their new public address announcer. Leitz was an anchor and news reporter in Los Angeles at KNX 1070 AM for 10 years, and a news reporter at KABC 790 for two years.
Other.
Vin Scully is permanently honored in the Hall's "Scribes & Mikemen" exhibit as a result of winning the Ford C. Frick Award in 1982. As with all Frick Award recipients, he is not officially considered an inducted member of the Hall of Fame.
Sue Falsone, served as the first female physical therapist in Major League baseball, and from 2012 to 2013, was the first female head athletic trainer.
External links.
class="navbox collapsible autocollapse" style="width:100;"
!colspan="3" style="background-color:#DCDCDC;"

</doc>
<doc id="18295" url="http://en.wikipedia.org/wiki?curid=18295" title="Legacy system">
Legacy system

In computing a legacy system is an old method, technology, computer system, or application program, "of, relating to, or being a previous or outdated computer system." Often a pejorative term, referencing a system as "legacy" often implies that the system is out of date or in need of replacement.
Overview.
The first use of the term legacy to describe computer systems probably occurred in the 1970s. By the 1980s it was commonly used to refer to existing computer systems to distinguish them from the design and implementation of new systems. Legacy was often heard during a conversion process, for example, when moving data from the legacy system to a new database.
While this term may indicate that some engineers may feel that a system is out of date, a legacy system may continue to be used for a variety of reasons. It may simply be that the system still provides for the users' needs. In addition, the decision to keep an old system may be influenced by economic reasons such as return on investment challenges or vendor lock-in, the inherent challenges of change management, or a variety of other reasons other than functionality. Backward compatibility (such as the ability of newer systems to handle legacy file formats and character encodings) is a goal that software developers often include in their work.
Even if it is no longer used, a legacy system may continue to impact the organization due to its historical role. Historic data may not have been converted into the new system format and may exist within the new system with the use of a customized schema crosswalk, or may exist only in a data warehouse. In either case, the effect on business intelligence and operational reporting can be significant. A legacy system may include procedures or terminology which are no longer relevant in the current context, and may hinder or confuse understanding of the methods or technologies used.
Organizations can have compelling reasons for keeping a legacy system, such as:
Problems posed by legacy computing.
Legacy systems are considered to be potentially problematic by some software engineers for several reasons (for example, see Bisbal et al., 1999). 
Improvements on legacy software systems.
Where it is impossible to replace legacy systems through the practice of application retirement, it is still possible to enhance (or "re-face") them. Most development often goes into adding new interfaces to a legacy system. The most prominent technique is to provide a Web-based interface to a terminal-based mainframe application. This may reduce staff productivity due to slower response times and slower mouse-based operator actions, yet it is often seen as an "upgrade", because the interface style is familiar to unskilled users and is easy for them to use. John McCormick discusses such strategies that involve middleware.
Printing improvements are problematic because legacy software systems often add no formatting instructions, or they use protocols that are not usable in modern PC/Windows printers. A print server can be used to intercept the data and translate it to a more modern code. Rich Text Format (RTF) or PostScript documents may be created in the legacy application and then interpreted at a PC before being printed.
Biometric security measures are difficult to implement on legacy systems. A workable solution is to use a telnet or http proxy server to sit between users and the mainframe to implement secure access to the legacy application.
The change being undertaken in some organizations is to switch to Automated Business Process (ABP) software which generates complete systems. These systems can then interface to the organizations' legacy systems and use them as data repositories. This approach can provide a number of significant benefits: the users are insulated from the inefficiencies of their legacy systems, and the changes can be incorporated quickly and easily in the ABP software .
Model-driven reverse and forward engineering approaches can be also used for the improvement of legacy software. Model-driven tools and methodologies can support the migration of legacy software to Cloud computing environments and allow for its modernization, in the notion of Software as a service, exploiting the advanced business and technical characteristics of clouds.
NASA example.
Andreas Hein, from the University of Stuttgart, researched the use of legacy systems in space exploration. According to Hein, legacy systems are attractive for reuse if an organization has the capabilities for verification, validation, testing, and operational history. These capabilities must be integrated into various software life cycle phases such as development, implementation, usage, or maintenance. For software systems, the capability to use and maintain the system are crucial. Otherwise the system will become less and less understandable and maintainable.
According to Hein, verification, validation, testing, and operational history increases the confidence in a system's reliability and quality. However, accumulating this history is often expensive. NASA's now retired Space Shuttle program used a large amount of 1970s-era technology. Replacement was cost-prohibitive because of the expensive requirement for flight certification. The original hardware completed the expensive integration and certification requirement for flight, but any new equipment would have had to go through that entire process again. This long and detailed process required extensive tests of the new components in their new configurations before a single unit could be used in the Space Shuttle program. Thus any new system that started the certification process becomes a "de facto" legacy system by the time it is approved for flight.
Additionally, the entire Space Shuttle system, including ground and launch vehicle assets, was designed to work together as a closed system. Since the specifications did not change, all of the certified systems and components performed well in the roles for which they were designed. Even before the Shuttle was scheduled to be retired in 2010, NASA found it advantageous to keep using many pieces of 1970s technology rather than to upgrade those systems and recertify the new components.
Additional uses of the term "Legacy" in computing.
The term "legacy support" is often used in conjunction with legacy systems. The term may refer to a feature of modern software. For example, Operating systems with "legacy support" can detect and use older hardware. The term may also be used to refer to a business function; e.g. A software or hardware vendor that is supporting, or providing software maintenance, for older products.
A "legacy" product may be a product that is no longer sold, has lost substantial market share, or is a version of a product that is not current. A legacy product may have some advantage over a modern product making it appealing for customers to keep it around. A product is only truly "obsolete" if it has an advantage to nobody – if no person making a rational decision would choose to acquire it new.
The term "legacy mode" often refers specifically to backward compatibility. A software product that is capable of performing as though it were a previous version of itself, is said to be "running in legacy mode." This kind of feature is common in operating systems and internet browsers, where many applications depend on these underlying components.
The computer mainframe era saw many applications running in legacy mode. In the modern business computing environment, n-tier, or 3-tier architectures are more difficult to place into legacy mode as they include many components making up a single system.
Virtualization technology is a recent innovation allowing legacy systems to continue to operate on modern hardware by running older operating systems and browsers on a software system that emulates legacy hardware.
Brownfield architecture.
The field of Information Technology has borrowed the term "brownfield" from the building industry, where undeveloped land (and especially unpolluted land) is described as "greenfield" and previously developed land – which is often polluted and abandoned – is described as "brownfield".
Alternative view.
There is an alternate point of view — growing since the "Dot Com" bubble burst in 1999 — that legacy systems are simply computer systems that are both installed and working. In other words, the term is not pejorative, but the opposite. Bjarne Stroustrup, creator of the C++ language, addressed this issue succinctly:
"Legacy code" often differs from its suggested alternative by actually working and scaling.—
IT analysts estimate that the cost of replacing business logic is about five times that of reuse, and that is not counting the risks involved in wholesale replacement. Ideally, businesses would never have to rewrite most core business logic; debits must equal credits — they always have, and they always will. New software may increase the risk of system failures and security breaches.
The IT industry is responding to these concerns. "Legacy modernization" and "legacy transformation" refer to the act of reusing and refactoring existing core business logic by providing new user interfaces (typically Web interfaces), sometimes through the use of techniques such as screen scraping and service-enabled access (e.g. through web services). These techniques allow organizations to understand their existing code assets (using discovery tools), provide new user and application interfaces to existing code, improve workflow, contain costs, minimize risk, and enjoy classic qualities of service (near 100% uptime, security, scalability, etc.).
The re-examination of attitudes toward legacy systems is also inviting more reflection on what makes legacy systems as durable as they are. Technologists are relearning that sound architecture, practiced up front, helps businesses avoid costly and risky rewrites in the first place. The most common legacy systems tend to be those which embraced well-known IT architectural principles, with careful planning and strict methodology during implementation. Poorly designed systems often don't last, both because they wear out and because their reliability or usability are low enough that no one is inclined to make an effort to extend their term of service when replacement is an option. Thus, many organizations are rediscovering the value of both their legacy systems themselves and those systems' philosophical underpinnings.
Further reading.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="18309" url="http://en.wikipedia.org/wiki?curid=18309" title="Lucifer">
Lucifer

Lucifer ( ) is the King James Version rendering of the Hebrew word הֵילֵל in Isaiah . This word, transliterated "hêlêl" or "heylel", occurs only once in the Hebrew Bible and according to the KJV-influenced Strong's Concordance means "shining one, morning star". The word "Lucifer" is taken from the Latin Vulgate, which translates הֵילֵל as "lucifer", meaning "the morning star, the planet Venus", or, as an adjective, "light-bringing". The Septuagint renders הֵילֵל in Greek as ἑωσφόρος ("heōsphoros"), a name, literally "bringer of dawn", for the morning star.
Later Christian tradition came to use the Latin word for "morning star", "lucifer", as a proper name ("Lucifer") for the Devil; as he was before his fall. As a result, "'Lucifer' has become a by-word for Satan/the Devil in the Church and in popular literature", as in Dante Alighieri's "Inferno" and John Milton's "Paradise Lost". However, the Latin word never came to be used almost exclusively, as in English, in this way, and was applied to others also, including Christ. The image of a morning star fallen from the sky is generally believed among scholars to have a parallel in Canaanite mythology.
However, according to both Christian and Jewish exegesis, in the Book of Isaiah, chapter 14, the King of Babylon, Nebuchadnezzar II, conqueror of Jerusalem, is condemned in a prophetic vision by the prophet Isaiah and is called the "Morning Ha" (planet Venus). In this chapter the Hebrew text says הֵילֵל בֶּן-שָׁחַר ("Helel ben Shaḥar", "shining one, son of the morning"). "Helel ben Shaḥar" may refer to the Morning Star, but the text in Isaiah 14 gives no indication that Helel was a star or planet.
Etymology, Lucifer or morning star.
Translation of הֵילֵל as "Lucifer", as in the King James Version, has been abandoned in modern English translations of Isaiah 14:12. Present-day translations have "morning star" (New International Version, New Century Version, New American Standard Bible, Good News Translation, Holman Christian Standard Bible, Contemporary English Version, Common English Bible, Complete Jewish Bible), "daystar" (New Jerusalem Bible, English Standard Version, The Message, "Day Star" New Revised Standard Version), "shining one" (New Life Version, New World Translation, JPS Tanakh) or "shining star" (New Living Translation).
The term appears in the context of an oracle against a dead king of Babylon, who is addressed as הילל בן שחר ("hêlêl ben šāḥar"), rendered by the King James Version as "O Lucifer, son of the morning!" and by others as "morning star, son of the dawn".
In a modern translation from the original Hebrew, the passage in which the phrase "Lucifer" or "morning star" occurs begins with the statement: "On the day the Lord gives you relief from your suffering and turmoil and from the harsh labour forced on you, you will take up this taunt against the king of Babylon: How the oppressor has come to an end! How his fury has ended!" After describing the death of the king, the taunt continues:
J. Carl Laney has pointed out that in the final verses here quoted, the king of Babylon is described not as a god or an angel but as a man.
For the unnamed "king of Babylon" a wide range of identifications have been proposed. They include a Babylonian ruler of the prophet Isaiah's own time the later Nebuchadnezzar II, under whom the Babylonian captivity of the Jews began, or Nabonidus, and the Assyrian kings Tiglath-Pileser, Sargon II and Sennacherib. Herbert Wolf held that the "king of Babylon" was not a specific ruler but a generic representation of the whole line of rulers.
Isaiah 14:12.
Mythology behind Isaiah 14:12.
In ancient Canaanite mythology, the morning star is pictured as a god, Attar, who attempted to occupy the throne of Ba'al and, finding he was unable to do so, descended and ruled the underworld. The original myth may have been about a lesser god Helel trying to dethrone the Canaanite high god El who lived on a mountain to the north. Hermann Gunkel's reconstruction of the myth told of a mighty warrior called Hêlal, whose ambition it was to ascend higher than all the other stellar divinities, but who had to descend to the depths; it thus portrayed as a battle the process by which the bright morning star fails to reach the highest point in the sky before being faded out by the rising sun.
Similarities have been noted with the East Semitic story of Ishtar's or Inanna's descent into the underworld, Ishtar and Inanna being associated with the planet Venus. A connection has been seen also with the Babylonian myth of Etana. The "Jewish Encyclopedia" comments:
The Greek myth of Phaethon, whose name, like that of הֵילֵל, means "Shining One", has also been seen as similar.
The Eerdmans Commentary on the Bible points out that no evidence has been found of any Canaanite myth of a god being thrown from heaven, as in Isaiah 14:12. It concludes that the closest parallels with Isaiah's description of the king of Babylon as a fallen morning star cast down from heaven are to be found not in any lost Canaanite and other myths but in traditional ideas of the Jewish people themselves, echoed in the Biblical account of the fall of Adam and Eve, cast out of God's presence for wishing to be as God, and the picture in of the "gods" and "sons of the Most High" destined to die and fall. This Jewish tradition has echoes also in Jewish pseudepigrapha such as 2 Enoch and the "Life of Adam and Eve".
Latin word "lucifer".
As an adjective, the Latin word "lucifer" meant "light-bringing" and was applied to the moon. As a noun, it meant "morning star", or, in Roman mythology, its divine personification as "the fabled son of Aurora and Cephalus, and father of Ceyx", or (in poetry) "day".
The second of the meanings attached to the word when used as a noun corresponds to the image in Greek mythology of "Eos", the goddess of dawn, giving birth to the morning star Phosphorus.
 is not the only place where the Vulgate uses the word "lucifer". It uses the same word four more times, in contexts where it clearly has no reference to a fallen angel: (meaning "morning star"), ("the light of the morning"), ("the signs of the zodiac") and ("the dawn"). To speak of the morning star, "lucifer" is not the only expression that the Vulgate uses: three times it uses "stella matutina": (referring to the actual morning star), and (of uncertain reference) and (referring to Jesus).
Indications that in Christian tradition the Latin word "Lucifer", unlike the English word, did not necessarily call a fallen angel to mind exist also outside the text of the Vulgate. Two bishops bore that name: Saint Lucifer of Cagliari, and Lucifer of Siena.
In Latin, the word is applied to John the Baptist and is used as a title of Christ himself in several early Christian hymns. The morning hymn "Lucis largitor splendide" of Hilary contains the line: "Tu verus mundi lucifer" (you are the true light bringer of the world). Some interpreted the mention of the morning star ("lucifer") in Ambrose's hymn "Aeterne rerum conditor" as referring allegorically to Christ and the mention of the cock, the herald of the day ("praeco") in the same hymn as referring to John the Baptist. Likewise, in the medieval hymn "Christe qui lux es et dies", some manuscripts have the line "Lucifer lucem proferens".
The Latin word "lucifer" is also used of Christ in the Easter Proclamation prayer to God regarding the paschal candle: "Flammas eius lucifer matutinus inveniat: ille, inquam, lucifer, qui nescit occasum. Christus Filius tuus, qui, regressus ab inferis, humano generi serenus illuxit, et vivit et regnat in saecula saeculorum" (May this flame be found still burning by the Morning Star: the one Morning Star who never sets, Christ your Son, who, coming back from death's domain, has shed his peaceful light on humanity, and lives and reigns for ever and ever). In the works of Latin grammarians, Lucifer, like Daniel, was discussed as an example of a personal name.
Literal meaning.
The Hebrew words הֵילֵל בֶּן-שָׁחַר ("Helel ben Shaḥar", "day-star, son of the morning") in Isaiah 14:12 are part of a prophetic vision against an oppressive king of Babylon. Jewish exegesis of Isaiah 14:12–15 took a humanistic approach by identifying the king of Babylon as Nebuchadnezzar II. Verse 20 says that this king of Babylon will not be "joined with them [all the kings of the nations] in burial, because thou hast destroyed thy land, thou hast slain thy people; the seed of evil-doers shall not be named for ever", but rather be cast out of the grave, while "All the kings of the nations, all of them, sleep in glory, every one in his own house".
Intertestamental Period.
In the Second temple period literature the main possible reference is found in 2 Enoch, also known as Slavonic Enoch:
2 Enoch 29:3 Here Satanail was hurled from the height together with his angels
However the editor of the standard modern edition (Charlesworth "OTP" Vol.1) pipelines the verse as a probable later Christian interpolation on the grounds that "Christian explanations of the origin of evil linked Lk 10:18 with Isa 14 and eventually Gen.3 so vs 4 could be a Christian interpolation... Jewish theology concentrated on Gen 6., and this is prominent in the Enoch cycle as in other apocalypses." Further the name used in 2 Enoch, Satanail, is not directly related to the Isaiah 14 text and the surrounding imagery of fire and stones suggests Ezekiel 28.
Other instances of "Lucifer" in the Old Testament Pseudepigrapha are related simply to the "star" Venus, in the Sibylline Oracles battle of the constellations (line 517) "Lucifer fought mounted on the back of Leo", or the entirely rewritten Christian version of the Greek Apocalypse of Ezra 4:32 which has a reference to Lucifer as Antichrist.
An association of Isaiah 14:12-18 with a personification of the evil, called the Devil developed outside of mainstream (rabbinic) Judaism in Pseudepigrapha and Christian writings. Old Testament Pseudepigrapha are works produced after the closing of the Hebrew Bible canon, they flourished toward the end of the Second Temple period under Roman occupation, particularly with the "apocalypses". Old Testament Pseudepigrapha are not accepted as part of Jewish tradition, but are in custodianship of the church. This period before the closing of the Christian canon is also called the Intertestamental Period when the deuterocanonical books were written.
Especially Isaiah 14:12, became a dominant conception of a fallen angel motif in . Rabbis, in Medieval Judaism, made every attempt to protect the Jewish community from their currency, strictly rejecting these Enochic phantasms. Rabbinical Judaism rejected any belief in rebel or fallen angels. In the 11th century, the "Pirqe de-Rabbi Eliezer", an aggadic-midrashic work on the Torah containing exegesis and retellings of biblical stories, illustrates the origin of the "fallen angel myth" by giving two accounts, one relates to the angel in the garden in Eden, who seduces Eve, and the other relates to the angels, the "benei elohim", who cohabit with the daughters of man (Genesis 6:1-4).
Allegorical interpretation in Christianity.
Apart from the literal meaning of Isaiah 14:12, which applies to a king of Babylon, Christian writers applied the words allegorically to Satan. Sigve K Tonstad argues that in the New Testament itself the War in Heaven theme of , in which the dragon "who is called the devil and Satan … was thrown down to the earth", derives from the passage in Isaiah 14. Origen (184/185 – 253/254) interpreted such Old Testament passages as being about manifestations of the Devil; but of course, writing in Greek, not Latin, he did not identify the Devil with the name "Lucifer". Tertullian (c. 160 – c. 225), who wrote in Latin, also understood ("I will ascend above the tops of the clouds; I will make myself like the Most High") as spoken by the Devil, but "Lucifer" is not among the numerous names and phrases he used to describe the Devil. Even at the time of the Latin writer Augustine of Hippo (354 – 430), "Lucifer" had not yet become a common name for the Devil.
Some time later, the metaphor of the morning star that Isaiah 14:12 applied to a king of Babylon gave rise to the general use of the Latin word for "morning star", capitalized, as the original name of the Devil before his fall from grace, linking Isaiah 14:12 with ("I saw Satan fall like lightning from heaven") and interpreting the passage in Isaiah as an allegory of Satan's fall from heaven.
However, the understanding of the morning star in Isaiah 14:12 as a metaphor referring to a king of Babylon continued also to exist among Christians. Theodoret of Cyrus (c. 393 – c. 457) wrote that Isaiah calls the king "morning star", not as being the star, but as having had the illusion of being it. The same understanding is shown in Christian translations of the passage, which in English generally use "morning star" rather than treating the word as a proper name, "Lucifer". So too in other languages, such as French, German, Portuguese, and Spanish. Even the Vulgate text in Latin is printed with lower-case "lucifer" (morning star), not upper-case "Lucifer" (proper name).
Calvin said: "The exposition of this passage, which some have given, as if it referred to Satan, has arisen from ignorance: for the context plainly shows these statements must be understood in reference to the king of the Babylonians." Luther also considered it a gross error to refer this verse to the devil.
Christians who identify Lucifer with Satan or the Devil.
Adherents of the King James Only movement and others who hold that Isaiah 14:12 does indeed refer to the devil have decried the modern translations.
Treating "Lucifer" as a name for the devil or Satan, they may use that name when speaking of such accounts of the devil or Satan as the following:
- Satan inciting David to number Israel (1 Chronicles )
- Job tested by Satan (Book of Job)
- Satan ready to accuse the high priest Joshua (Zechariah )
- Sin brought into the world through the devil's envy (Wisdom )
- "The prince of the power of the air, the spirit that is now at work in the sons of disobedience" (Ephesians )
- "The god of this world" (2 Corinthians ).
- The devil disputing with Michael about the body of Moses (Jude )
- The dragon of the Book of Revelation "who is called the devil and Satan" ()
They may also use the name Lucifer when speaking of Satan's motive for rebelling and of the nature of his sin, which Origen, Chrysostom, Jerome, Ambrose, and Augustine attributed to the devil's pride, and Irenaeus, Tertullian, Justin Martyr, Cyprian, and again Augustine attributed to the devil's envy of humanity created in the image of God. Jealousy of humans, created in the divine image and given authority over the world is the motive that a modern writer, who denies that there is any such person as Lucifer, says that Tertullian attributed to the Devil, and, while he cited Tertullian and Augustine as giving envy as the motive for the fall, an 18th-century French Capuchin preacher himself described the Rebel Angel as jealous of Adam's exaltation, which he saw as a diminution of his own status.
Islam.
In Islam the Devil is known as Iblīs (Arabic: إبليس‎, plural: ابالسة "abālisah") or Shayṭān (Arabic: شيطان‎, plural: شياطين "shayāṭīn"). He has no name corresponding in meaning to that of the Latin word "lucifer" to associate him with the Morning Star, but the accounts of him resemble the fallen-angel accounts in Enochic and Christian literature. Iblis is banished from heaven for refusing to prostrate himself before Adam. Thus, he sins "after" the creation of man. He asks God for a respite until the Last Day rather than being consigned to the Fire of Hell immediately. God grants this request, and Iblis then swears revenge by tempting human beings and turning them away from God. God tells him that any humans who follow him will join him in the Fire of Hell at Judgement, but that Iblis will have no power over all mankind except who wants to follow Iblis. This story is cited multiple times in the Qur'an for different reasons.
Islamic literature presents Iblis as God worshipping and very pious until he refused to prostrate to Adam due to his jealousy and pride. Iblis was a type of supernatural being known as the Jinn, who were made out of smokeless fire and created before humankind. 
Occultism.
Luciferianism is a belief system that venerates the essential characteristics that are affixed to Lucifer. The tradition, influenced by Gnosticism, usually reveres Lucifer not as the Devil, but as a liberator or guiding spirit or even the true god as opposed to Jehovah.
In Anton LaVey's "The Satanic Bible", Lucifer is one of the four crown princes of hell, particularly that of the East, the 'lord of the air', and is called the bringer of light, the morning star, intellectualism, and enlightenment.
Author Michael W. Ford has written on Lucifer as a "mask" of the adversary, a motivator and illuminating force of the mind and subconscious.
Taxil's hoax.
Léo Taxil (1854–1907) claimed that Freemasonry is associated with worshipping Lucifer. In what is known as the Taxil hoax, he alleged that leading Freemason Albert Pike had addressed "The 23 Supreme Confederated Councils of the world" (an invention of Taxil), instructing them that Lucifer was God, and was in opposition to the evil god Adonai. Supporters of Freemasonry contend that, when Albert Pike and other Masonic scholars spoke about the "Luciferian path," or the "energies of Lucifer," they were referring to the Morning Star, the light bearer, the search for light; the very antithesis of dark, satanic evil. Taxil promoted a book by Diana Vaughan (actually written by himself, as he later confessed publicly) that purported to reveal a highly secret ruling body called the Palladium, which controlled the organization and had a satanic agenda. As described by "Freemasonry Disclosed" in 1897:
With frightening cynicism, the miserable person we shall not name here [Taxil] declared before an assembly especially convened for him that for twelve years he had prepared and carried out to the end the most sacrilegious of hoaxes. We have always been careful to publish special articles concerning Palladism and Diana Vaughan. We are now giving in this issue a complete list of these articles, which can now be considered as not having existed.
Taxil's work and Pike's address continue to be quoted by anti-masonic groups.
In "Devil-Worship in France", Arthur Edward Waite compared Taxil's work to what today we would call a tabloid story, replete with logical and factual inconsistencies.

</doc>
<doc id="18352" url="http://en.wikipedia.org/wiki?curid=18352" title="AvtoVAZ">
AvtoVAZ

AvtoVAZ (Russian: АвтоВАЗ) is the Russian automobile manufacturer formerly known as VAZ: Volzhsky Avtomobilny Zavod (ВАЗ, Во́лжский автомоби́льный заво́д), but better known to the world under the trade name Lada. The company was established in the late 1960s in collaboration with Fiat. The current company name is "AvtoVAZ", which stands for "Avtomobili Volzhskogo Avtomobilnogo Zavoda" ("Cars of Volga Automobile Plant"). AvtoVAZ is the largest company in the Eastern European and Russian automotive industry.
It produces nearly one million cars a year, including the Kalina family (hatchback, wagon and crossover), Lada Granta family (sedan and liftback), Lada Priora family (sedan, hatchback, wagon and coupe), Lada 4x4 (former Lada "Niva") and Lada Largus (Renault-Nissan platform). It also produces the vehicles of Renault-Nissan alliance brands: Renault Logan 2, Datsun on-Do (the Nissan sub-brand) and Nissan Almera New. However, the original Fiat 124-based VAZ-2101, and its derivatives, remain the models most associated with its Lada brand.
The VAZ factory is one of the biggest in the world, with over 90 mi of production lines, and is unique in that most of the components for the cars are made in-house.
The original Lada was intended as a "people's car" for consumers of the Eastern Bloc - lacking in most luxuries expected in Western-made cars of its era. Ladas were sold as a budget 'no-frills' vehicle in several Western nations during the 1970s and 1980s, including Canada, the United Kingdom, France, Belgium, Luxembourg and the Netherlands, though trade sanctions banned their export to the United States. Sales to Italy were forbidden by the agreement between the Soviet government and Fiat, to protect Fiat from cheap imports in its home market.
Origins.
With the aim of producing more cars, a brand-new integrated plant was decided upon in 1966, with Viktor Polyakov (later minister of "Minavtoprom") as director, and Vladimir Solovyev was chief designer. It was set up as a collaboration between Italy and the Soviet Union and built on the banks of the Volga River in 1966. A new town, Tolyatti, named after the Italian Communist Party leader Palmiro Togliatti, was built around the factory. The Lada was envisaged as a "people's car" like the Citroën 2CV or the VW Type 1. Production was intended to be 220,000 cars a year, beginning in 1971; car production actually began before the plant was finished in 1970. The VAZ trademark, at first, was a grey Volga boat on a red pentagonal background, with "Togliatti" superposed in Cyrillic (Тольятти); the first badges, manufactured in Turin, mistakenly had the Cyrillic "Я" rendered "R", instead (Тольтти), making them collector's items.
The lightweight Italian Fiat 124 was adapted in order to survive treacherous Russian driving conditions. Among many changes, aluminium brake drums were added to the rear, and the original Fiat engine was dropped in favour of a newer design also purchased from Fiat. This new engine had a modern overhead camshaft design, but was never used in Fiat cars. The suspension was raised to clear rough Russian roads and the bodyshell was made from thicker, heavier steel. The first Lada models were equipped with a starting handle in case the battery went flat in Siberian conditions, though this was later dropped. Another feature specifically intended to help out in cold conditions was a manual auxiliary fuel pump. About 22,000 VAZ-2101s were built in 1970, with capacity at the end of 1973 reaching 660,000 a year; 21 December, the one millionth 2101 was built. A third production line was added in October 1974, boosting output to 2,230 cars a day. The same year, total VAZ production reached 1.5 million.
Exports to the West began in 1974; under the original agreement with Fiat, the car could not be sold in competition with the 124 until its replacement (the Fiat 131 Mirafiori) had been released and all Fiat production of the 124 had ceased.
Engines fitted to the original Ladas start with the 1.2 L carubretted in the original and go up to the 1.7 L export model set up with a General Motors single point fuel injection system. Diesel engines were later fitted for the domestic market only. The drivetrain is a simple rear-wheel drive setup with a live rear axle. The engine is an inline four with two valves per cylinder and a single overhead camshaft.
The Fiat-based Ladas feature various headlight, trim and body styles. The original, Fiat style models included VAZ-2101 sedan and VAZ-2102 station wagon. 1972 saw the introduction of a deluxe version of the sedan, VAZ-2103, which was based on Fiat 124 Special 1968 and featured a new 1.5 L engine and twin headlights. In 1974, the original VAZ-2101 was updated with new engines and interiors; VAZ-2102 underwent the same improvements in 1976. The body style with two round headlights was manufactured until 1988; all others remain in production in slightly updated form.
The VAZ-2106 introduced in December 1975 was an updated version of VAZ-2103, really which was based on 1972 Fiat 124 Special T, featuring different interiors and new 1.6 L engine. The 2106 is the oldest and the most popular rear-wheel drive AvtoVAZ model; its production ended in 2001 from Tolyatti, but continued at Izhavto (Izhevsk), ending there in December 2005.
In 1974, VAZ was given permission to begin producing Wankel engines under licence from NSU. Work began in 1976, with a single-rotor Lada appearing in 1978; the first 250 of these went on sale in summer 1980.
The VAZ-2105, still based on the 2101 but updated to 1980s styling, was introduced in 1980 and was marketed outside the Soviet Union under the Riva or Laika names, depending on country. Square headlights and new body panels distinguish this style from the old models. A deluxe version, VAZ-2107, was out in 1982; it featured a better engine, refined interiors and a Mercedes-like radiator grille. In 1984, the VAZ-2104 station wagon completed the line-up. In 2002 station wagon 2104 production was transferred to IzhAvto. Production of the 2105 was completed on 30 December 2010, and production of deluxe sedan 2107 was transferred to IzhAvto on March 2011.
In the domestic market, these "classic" models were called "Zhiguli" (Жигули). The Lada name was used for exports only, but a large share of Ladas was reexported from Eastern Bloc countries, so the brand was well known in the domestic market as well.
AvtoVAZ designers proved they had some original ideas when the VAZ-2121 Niva was introduced in 1978. This highly popular car was made with off-road use in mind, featuring a gearbox with a four-wheel-drive selector lever as well as a low- and high- range selector lever. It has an original body style and the most powerful 1.7 L engine in the VAZ range. The Niva has also been available with 1.9 L Peugeot sourced diesel engine. The Niva is still in production.
Based on the success of the Niva, the design department prepared a new family of front-wheel drive models by 1984, which was of a completely domestic design. Production started with the VAZ-21083 "Sputnik" three-door hatchback; the series was later renamed Samara. The Samara engine was mostly designed and produced in-house, had a new single overhead camshaft (SOHC) design and was driven by a more modern rubber belt. The combustion chambers were developed in collaboration with Porsche. The line-up features a completely new body and interiors, front MacPherson strut independent suspension and rear torsion bar, rack and pinion steering, and an updated five-speed gearbox. The five-door VAZ-21093 hatchback followed in 1987, and the four-door 1.5 L sedan, VAZ-21099, was introduced in 1990. The same year, the front sides and radiator grille were restyled on the whole Samara range.
A white 2108 would be VAZ's nine millionth Lada built, on 24 May 1985, with the ten millionth, on 9 October 1986, also a 2108. The twelve millionth, a right-hand drive 2109, was produced 6 July 1989.
The 2108-2109 models were in production until 2001, when they were restyled with new side panels, interiors and 1.5 L fuel injection engines (though fuel injection was available as early as 1995). The Lada 2109 hatchback was rebadged as Lada 2114, and Lada 21099 sedan was rebadged as the Lada 2115. The 2104-21099 model range was transferred to IzhMash and ZAZ and is still being manufactured. In 2004 VAZ also introduced Lada 2113, a restyled version of Lada 2108, but this car has never used much popularity, as the Lada 2108 was only popular for a short time.
The VAZ-1111 Oka micro-car, which resembles the Fiat Panda (though relation to it), was introduced in 1988, and in 1991 the production was transferred to the KamAZ and SeAZ factories.
The VAZ-2120 Nadezhda minivan is based on the original Niva and has been in low-volume production since 1998. A five-door version of the Niva, the VAZ-2131, has been in production since 1995.
The break-up of the USSR delayed the production of new 110-series by a couple of years. The VAZ-2110 sedan was introduced in 1996, the 2111 station wagon followed in 1998 and the 2112 hatchback completed the range in 2001. These models are basically based on Samara mechanicals, with a new body and fuel injected engines as standard features, though carbureted versions have also been available up until 2001. The 110-series remains in production and has been continually updated over the years. For example, engines used to be 1.5 L units with either 8 or 16 valves, but these have now been upgraded to 1.6 L units that meet stricter emissions rules.
VAZ in 2008 was the largest automotive plant in Europe, able to build 750,000 cars a year. The plant covers 600 ha, with three assembly lines each 1,700 m long; at peak production, it employed 180,000. And, unlike most Western factories, it is vertically integrated, producing almost every component in the plant itself.
In 2013, AVTOVAZ sold 481 thousand cars LADA, sales declined by 19% compared to the year 2012.
Revenues declined by 13 billion roubles up to 177 billion roubles. The loss amounted to 7.9 billion rubles. The deterioration of the results of financial and economic activity of the AVTOVAZ group mainly connected with reduction of sales at the Russian car market.
In the first half of 2014, the proceeds from sales of AVTOVAZ amounted to 91.1 billion rubles, net loss - 2,75 billion. For the six months sales of the company amounted to 220 251 LADA, which is 5% less than in the first half of last year The main factors that influenced these results, the steel market collapse and the fall in the ruble.
Market share.
Tightening emissions and safety legislation meant that AvtoVAZ withdrew from most Western markets by the late 1997; often, there were also problems with spare parts. Since then AvtoVAZ has been concentrated on home market. Today Lada has 15,7% of the Russian market, in the segment up to 600 thousand rubles, they occupy 37%. In the USA they were never sold due to the cold war, but they were available in Canada (where the Niva was quite popular). The rise in popularity of Far Eastern imports from newly established manufacturers such as Daewoo, Proton, Kia and Hyundai contributed to Lada's demise in the West. These Korean and Malaysian-manufactured vehicles offered modern, Japanese developed technology and standard equipment such as automatic transmissions which Lada could not compete with, and by the turn of the millennium, had completely taken over the market niche that Lada had survived in for over 20 years.
Though the original Lada, and as of the early part of the new millennium, the Samara, have now been withdrawn, the Lada 110 and the Niva are still sold in certain Western European markets, as are the more modern models (Lada Kalina, Lada Granta, Lada 4x4). The Lada is widely available in many Central and South American countries as well as in Africa, the Middle East and in all of the former Soviet Union and Communist Bloc nations.
Recent developments.
As AvtoVAZ was allowed to sell cars to private dealers in the late 1980s, Boris Berezovsky arranged to resell the cars to the public through his "LogoVAZ" dealerships. In 1993 he started a campaign to collect funds for the "people's automobile" and created the "AVVA" venture, which stands for "All-Russian Automobile Alliance"; the AvtoVAZ held a major share in the venture. The plans were to build a completely new plant for production of the VAZ-1116 supermini. However, the financial crisis of 1998 put these plans to an end. The development concepts of 1116 instead became the foundation of the Lada Kalina range.
GM-AvtoVAZ, a joint-venture with General Motors, adopted an updated version of the Niva, VAZ-2123, that was considered for production since the 1990s. Named Chevrolet Niva, it is being built on the venture's plant since 2001 and is exported to Europe and Latin America. In 2004, the Chevrolet Viva, a four-door version of the Opel Astra G, was introduced.
VAZ has also tried to get into the sportier markets: several Ladas were factory-tuned and given a Momo steering wheel. A convertible was also produced. In 2003, VAZ presented the concept car Lada Revolution, an open single seater sports car powered by a 1.6 L engine producing 215 hp. There are other experimental cars, such as the VAZ-210834 Tarzan SUV concept, VAZ-1922 monster truck and VAZ-2359 pick-up, all based on Niva. The VAZ-211223 110-series coupe, with the sister models 111 and 112 have been developed with a modern and luxurious look and feel, have been mass-produced, and are popular in Russia today.
Some models (mostly the police version) have a Wankel engine (like the Mazda RX-7), though development (and production) of this engine stopped in 2004. The main causes are special requirements for service and repair (mostly available only in Moscow & Togliatti) and very high fuel & lubricating oil consumption.
2005 saw the introduction of the new Kalina supermini lineup to the market. AvtoVAZ has built a new modern plant for this model and is hoping to sell some 200,000 cars annually. Test production of the Lada 1118 sedan started in November 2004 and full-scale assembly was launched in May 2005. The Lada 1119 hatchback and Lada 1117 station wagon with updated DOHC 1.6L engines followed in 2006.
The restyled 110-series model, Lada 2170 Priora, is produced since March 2007.
"Project C", which has come to be known as the Lada 2116 or Lada Silhouette, is a family car jointly developed with input from both Porsche and Renault, is intended to finally replace the Classic models. Spy shots of the car appeared in 2007, suggesting a 2008 launch. AvtoVAZ began to move production of the Classic models (which were still selling strongly in Russia) out of Togliatti at the end of 2010 - fuelling further speculation that this was to free up production capacity for the 2116. In the end, the 2116 never reached production.
AvtoVAZ was considering the local production of Ecotec Family 1 (FAM-1) engines using the equipment transferred from Szentgotthard, Hungary plant. A transmissions plant was to be bought from Daewoo Moto India, a former Daewoo Motors subsidiary that was not sold to GM. The engines and transmissions were to be used in both GM-AvtoVAZ and Lada cars. As of Summer 2005, these plans were cancelled and VAZ is seeking another way to acquire some modern powerplant technology.
After some shakeups in the management caused by a recent acquisition from Rosoboronexport, AvtoVAZ is currently in talks with Renault to negotiate a CKD assembly of the Renault Logan. They have also contracted Magna International to design a new car platform and equip a new plant for its production.
AvtoVAZ suffered considerably in the 2008-2009 world economic crisis. In October 2008, the company was reported to possess over 100,000 unsold units, and desperately needed money to repay short-term debts. On March 31, the value of AvtoVAZ shares jumped by almost 30%, due to Prime Minister Vladimir Putin's proclaimed determination to support the auto giant. Putin visited Togliatti, expressed his approval of the management for not initiating massive layoffs, and promised more than $1 billion in loans, cash, and guarantees. In May, 2009, Putin bought an AvtoVAZ Niva SUV to show his support for the hard-pressed domestic producer.
On March 10, 2010 the Board of Directors of "AvtoVAZ" approved a business plan for the period until 2020, by which expected to increase vehicle production to 1.2 million units per year by the end of the 2010s, as well as investments up to 3 billion euros.
On 3 May 2012, the Renault-Nissan alliance has signed letter of intent to raise its stake in Avtovaz to a majority by taking a majority share of 67.13% of a joint venture with the Russian state-controlled company, Russian Technologies, to own 74.5% of Avtovaz. This would raise the share of the Renault-Nissan Alliance in AvtoVAZ to 51.01%. Renault and Nissan will invest $750,000,000 in the joint venture.
In 2012, it was announced that Avtovaz and Sollers plan to jointly produce vehicles in Kazakhstan. The plant, which will be open in 2016, will be built in Ust-Kamenogorsk, in the eastern part of the country, and will produce around 120,000 cars a year.
In 2013, Bo Andersson joined JSC AvtoVAZ as the President.
Models.
Each model has an internal index that reflects the level of modifications, based on the engine and other options installed. For example, the VAZ-21103 variant has the 1.5 L 16V engine, while the VAZ-21104 uses the latest 1.6 L 16V fuel injection engine. Since 2001, trim levels are also indicated by including a number after the main index: '-00' means base trim level, '-01' means standard trim and '-02' designates deluxe version; for example, VAZ-21121-02 means Lada 112 hatchback with an 1.6L SOHC engine and deluxe trim.
The car's name is formed from 'VAZ-"index" "model name". The "classic" Fiat 124-derived models were known on the domestic market as Zhiguli (Жигули) until the late-1990s, when the name was dropped; thus, the 2104-2107 range, as well as 110-series, actually lack a model name. The restyled Sputnik range was renamed Samara, but the Niva and the Oka retained their names. By the 2000s (decade), the "VAZ" designation was dropped from market names in favour of "Lada" and simplified export naming conventions were adopted, so VAZ-2104 effectively became Lada 2104, VAZ-2110 became Lada 110, VAZ-2114 became Lada Samara hatchback or Lada 114 and so on, though model indices continue to be used in both technical and marketing materials.
The model names varied from market to market and as such should not be used except to indicate a certain export market. Instead, it is advisable to refer solely to the model number as these are the same for all markets.
Oka.
The Oka is a Russian city car designed by AvtoVAZ and sometimes branded as a Lada. This model was built in Russia by SeverstalAvto and SeAZ (the Serpuhov Car Factory), as well as in Azerbaijan by the Gyandzha Auto Plant. Series production of the OKA was stopped in Russia in 2008 when SeAZ released the last batch of OKA's with Chinese EURO-2 engines.
Chevrolet Niva.
The Chevrolet Niva is a GM modification produced at GM-AvtoVAZ, a joint venture between AvtoVAZ and General Motors, at its factory in Tolyatti from 2002. Although the body and the interiors are new, it is still based on the original VAZ 2121 engine, transmission and most mechanicals of the Lada Niva.
Nissan Almera.
In December 2012, the second generation of the Nissan Bluebird Sylphy began full-scale manufacturing at the AvtoVAZ plant as the new Nissan Almera.
It received its world premiere at the 2012 Moscow International Motor Show on 29 August 2012, and uses the same design as the Bluebird Sylphy, but a redesigned dashboard interior, adapted from the first generation Dacia Logan. It has a 1.6-litre petrol engine (75 kW), with a five-speed manual or a four-speed automatic transmission.
Models gallery.
Older Fiat-based models.
All are based on the Fiat 124

</doc>
<doc id="18374" url="http://en.wikipedia.org/wiki?curid=18374" title="Lake Eyre">
Lake Eyre

Lake Eyre ( ), officially known as Kati Thanda–Lake Eyre, is the lowest natural point in Australia, at approximately 15 m below sea level (AHD), and, on the rare occasions that it fills, the largest lake in Australia and 18th largest in the world. The shallow lake is the depocentre of the vast Lake Eyre basin and is found in South Australia, some 700 km north of Adelaide. The lake was named in honour of Edward John Eyre, who was the first European to see it, in 1840. The lake's official name was changed in December 2012 to combine the name "Lake Eyre" with the indigenous name, Kati Thanda. Native title over the lake and surrounding region is held by the Arabana people.
Geography.
Kati Thanda–Lake Eyre is located in the deserts of central Australia, in northern South Australia. The Lake Eyre Basin is a large endorheic system surrounding the lakebed, the lowest part of which is filled with the characteristic salt pan caused by the seasonal expansion and subsequent evaporation of the trapped waters. Even in the dry season there is usually some water remaining in Kati Thanda–Lake Eyre, normally collecting in over 200 smaller sub-lakes within its margins. The lake was formed by aeolian processes after tectonic upwarping occurred to the south subsequent to the end of the Pleistocene epoch.
During the rainy season the rivers from the north-east part of the Lake Eyre Basin (in outback (south-west and central) Queensland) flow towards the lake through the Channel Country. The amount of water from the monsoon determines whether water will reach the lake and if it does, how deep the lake will get. The average rainfall in the area of the lake is 100 to 150 mm per year.
The −15 m altitude usually attributed to Kati Thanda–Lake Eyre refers to the deepest parts of the lake floor, in Belt Bay and the Madigan Gulf. The shoreline lies at −9 m. The lake is the area of maximum deposition of sediment in the Lake Eyre Basin.
Lake Eyre is divided into two sections which are joined by the Goyder Channel. These are known as Lake Eyre North, which is 144 km in length and 65 km wide and Lake Eyre South, which measures 65 by 24 km. The salt crusts are thickest (up to 50 cm) in the southern Belt Bay, Jackboot Bay and Madigan Gulf sub-basins of Lake Eyre North.
Since 1883 proposals have been forwarded to flood Lake Eyre with seawater brought to the basin via canal or pipeline. The purpose was, in part, to increase evaporation and thereby increase rainfall in the region downwind of an enlarged Lake Eyre. Due to the basin's low elevation below sea level and the region's high annual evaporation rate (between 2,500 and 3,500mm) such schemes have generally been considered impractical as it is likely that accumulation of salt deposits would rapidly block the engineered channel.
Floods.
Typically a 1.5 m flood occurs every three years, a 4 m flood every decade, and a fill or near fill a few times a century. The water in the lake soon evaporates with a minor or medium flood drying by the end of the following summer. Most of the water entering the lakes arrives via Warburton River.
In strong La Niña years the lake can fill. Since 1885 this has occurred in 1886–1887, 1889–1890, 1916–1917, 1950, 1955, 1974–1977, and 1999-2001, with the highest flood of 6 m in 1974. Local rain can also fill Kati Thanda–Lake Eyre to 3–4 m (10–13 ft) as occurred in 1984 and 1989. Torrential rain in January 2007 took about six weeks to reach the lake but put only a small amount of water into it.
When recently flooded the lake is almost fresh and native freshwater fish, including bony bream ("Nematolosa erebi"), the Lake Eyre Basin sub-species of golden perch ("Macquaria ambigua") and various small hardyhead species ("Craterocephalus" spp.) can survive in it. The salinity increases as the 450 mm salt crust dissolves over a period of six months resulting in a massive fish kill. When over 4 m deep the lake is no more salty than the sea, but salinity increases as the water evaporates, with saturation occurring at about a 500 mm depth. The Lake takes on a pink hue when saturated due to the presence of beta-carotene pigment caused by the algae "Dunaliella salina".
2009.
The 2009 Lake Eyre flood peaked at 1.5 m deep in late May which is a quarter of its maximum recorded depth of 6 m. 9 km3 of water crossed the Queensland–South Australian border with most of it coming from massive floods in the Georgina River. However, owing to the very low rainfall in the lower reaches of these rivers (contrasting with heavy rainfall in the upper catchments) the greater proportion soaked into the desert or evaporated en route to the lake leaving less than 4 km3 (1 cu mi) in the lake which covered an area of 800 km2 or 12% of the lake. As the flood did not start filling the lake's deepest point (Belt Bay) until late March little bird life appeared preferring instead to nest in the upper reaches of the Lake Eyre Basin, north of Birdsville, where large lakes appeared in January as a result of monsoonal rain.
2010.
The high rainfall in summer sent flood water into the Diamantina, Georgina and Cooper Creek catchments of the Lake Eyre basin, with the Cooper Creek reaching the lake for the first time since 1990. The higher rainfall has prompted many different birds to migrate back to the area for breeding.
2011.
Heavy local rain in early March in the Stuart Creek and Warriner catchments filled Lake Eyre South, with Lake Eyre North about 75 per cent covered with water firstly from the Neales and Macumba and later from the Warburton River.
Yacht club.
The Lake Eyre Yacht Club is a dedicated group of sailors who sail on the lake's floods, including recent trips in 1997, 2000, 2001, 2004, 2007 and 2009. A number of 6 m trailer sailers sailed on Kati Thanda–Lake Eyre in 1975, 1976, and 1984 when the flood depth reached 3–6 m (10–20 ft). In July 2010 The Yacht Club held its first regatta since 1976 and its first on Lake Killamperpunna, a freshwater lake on Cooper Creek. The Cooper had reached Kati Thanda–Lake Eyre for the first time since 1990.
When the lake is full, a notable phenomenon is that around midday the surface can often become very flat. The surface then reflects the sky in a way that leaves both the horizon and water surface virtually impossible to see. The commodore of the Lake Eyre Yacht Club has stated that sailing during this time has the appearance of sailing in the sky.
Land speed record attempts.
Kati Thanda–Lake Eyre has been a site for various land speed record attempts on its salt flats, especially those by Donald Campbell with the Bluebird-Proteus CN7.
Wildlife.
Phytoplankton in the lake includes "Nodularia spumigena" and a number of species of "Dunaliella".
Birds.
Pelicans are drawn to a filled lake from as far afield as Papua New Guinea. During the 1989—90 flood it was estimated that 200,000 pelicans, 80% of Australia's total population, came to feed at Lake Eyre.
Protected area status.
Statutory.
The extent of the lake is covered by two protected areas declared by the Government of South Australia - the Kati Thanda-Lake Eyre National Park and the Elliot Price Conservation Park.
Non-statutory.
Wetlands.
Lake Eyre is on the list of wetlands of national importance known as A Directory of Important Wetlands in Australia.
Important bird area.
Lake Eyre has been identified by BirdLife International as an Important Bird Area (IBA) known as the Lake Eyre Important Bird Area because, when flooded, it supports major breeding events of the Banded stilt and Australian pelican, as well as over 1% of the world populations of Red-necked avocets, Sharp-tailed sandpipers, Red-necked stints, Silver gulls and Caspian terns.

</doc>
<doc id="18388" url="http://en.wikipedia.org/wiki?curid=18388" title="Laocoön">
Laocoön

Laocoön (; Ancient Greek: Λαοκόων, ]), the son of Acoetes, is a figure in Greek and Roman mythology and the Epic Cycle. He was a Trojan priest who was attacked, with his two sons, by giant serpents sent by the gods. Though not mentioned by Homer, the story of Laocoön had been the subject of a tragedy, now lost, by Sophocles and was mentioned by other Greek writers, though the events around the attack by the serpents vary considerably. The most famous account of these is now in Virgil's "Aeneid" where Laocoön was a priest of Poseidon (or Neptune for the Romans), who was killed with both his sons after attempting to expose the ruse of the Trojan Horse by striking it with a spear.
Virgil gives Laocoön the famous line "Equō nē crēdite, Teucrī / Quidquid id est, timeō Danaōs et dōna ferentēs", or "Do not trust the Horse, Trojans / Whatever it is, I fear the Greeks even bearing gifts." This line is the source of the saying: "Beware of Greeks bearing gifts."
In Sophocles, however, he was a priest of Apollo, who should have been celibate but had married. The serpents killed only the two sons, leaving Laocoön himself alive to suffer. In other versions he was killed for having committed an impiety by making love with his wife in the presence of a cult image in a sanctuary, or simply making a sacrifice in the temple with his wife present. In this second group of versions, the snakes were sent by Poseidon and in the first by Poseidon and Athena, or Apollo, and the deaths were interpreted by the Trojans as proof that the horse was a sacred object. The two versions have rather different morals: Laocoön was either punished for doing wrong, or for being right.
Death.
The most detailed description of Laocoön's grisly fate was provided by Quintus Smyrnaeus in "Posthomerica", a later, literary version of events following the "Iliad". According to Quintus, Laocoön begged the Trojans to set fire to the horse to ensure it was not a trick. Athena, angry with him and the Trojans, shook the ground around Laocoön's feet and painfully blinded him. The Trojans, watching this unfold, assumed Laocoön was punished for the Trojans' mutilating and doubting Sinon, the undercover Greek soldier sent to convince the Trojans to let him and the horse inside their city walls. Thus, the Trojans wheeled the great wooden Horse in. Laocoön did not give up trying to convince the Trojans to burn the horse, and Athena makes him pay even further. She sends two giant sea serpents to strangle and kill him and his two sons. In another version of the story, it was said that Poseidon sent the sea serpents to strangle and kill Laocoön and his two sons.
According to Apollodorus, it was Apollo who sent the two sea serpents. Laocoön had insulted Apollo by sleeping with his wife in front of the "divine image".
Virgil used the story in the "Aeneid." According to Virgil, Laocoön advised the Trojans to not receive the horse from the Greeks. They disregarded Laocoön's advice and were taken in by the deceitful testimony of Sinon. The enraged Laocoön threw his spear at the Horse in response. Minerva then sent sea-serpents to strangle Laocoön and his two sons, Antiphantes and Thymbraeus, for his actions. "Laocoön, ostensibly sacrificing a bull to Neptune on behalf of the city (lines 201ff.), becomes himself the tragic victim, as the simile (lines 223–24) makes clear. In some sense, his death must be symbolic of the city as a whole," S. V. Tracy notes. According to the Hellenistic poet Euphorion of Chalcis, Laocoön is in fact punished for procreating upon holy ground sacred to Poseidon; only unlucky timing caused the Trojans to misinterpret his death as punishment for striking the Horse, which they bring into the city with disastrous consequences. The episode furnished the subject of Sophocles' lost tragedy, "Laocoön".
In "Aeneid", Virgil describes the circumstances of Laocoön's death:
The death of Laocoön was famously depicted in a much-admired marble "Laocoön and his Sons", attributed by Pliny the Elder to the Rhodian sculptors Agesander, Athenodoros, and Polydorus, which stands in the Vatican Museums, Rome. Copies have been executed by various artists, notably Baccio Bandinelli. These show the complete sculpture (with conjectural reconstructions of the missing pieces) and can be seen in Rhodes, at the Palace of the Grand Master of the Knights of Rhodes, Rome, the Uffizi Gallery in Florence and in front of the Archaeological Museum, Odessa, Ukraine, amongst others.
The marble Laocoön provided the central image for Lessing's "Laocoön", 1766, an aesthetic polemic directed against Winckelmann and the comte de Caylus. Daniel Albright reengages the role of the figure of Laocoön in aesthetic thought in his book "Untwisting the Serpent: Modernism in Literature, Music, and Other Arts". [cite El Greco painting]
In addition to other literary references, John Barth employs a bust of Laocoön in his novella, "The End of the Road". The R.E.M. song "Laughing" references Laocoön, rendering him female ("Laocoön and her two sons"). The marble's pose is parodied in the comic book "Asterix and the Laurel Wreath". American author Joyce Carol Oates also references Laocoön in her 1989 novel "American Appetites". In Stave V of "A Christmas Carol", by Charles Dickens (1843), Scrooge awakes on Christmas morning, "making a perfect Laocoon of himself with his stockings". Barbara Tuchman's "The March of Folly" begins with an extensive analysis of the Laocoön story. The American feminist poet and author Marge Piercy includes a poem titled, "Laocoön is the name of the figure", in her collection "Stone, Paper, Knife" (1983), relating love lost and beginning.
In Hector Berlioz opera "Les Troyens", the death of Laocoon is a pivotal moment of the first act after Aeneas entrance, sung by eight singers and a double choir ("ottetto et double chœur"). It begins with the verse "Châtiment effroyable" ("frightful punishment").
References.
Classical sources.
Compiled by Tracy, , which also mentions a fragmentary line possibly by Nicander.

</doc>
<doc id="18408" url="http://en.wikipedia.org/wiki?curid=18408" title="LAME">
LAME

LAME is a free software codec used to encode/compress audio into the lossy MP3 file format.
History.
The name LAME is a recursive acronym for "LAME Ain't an MP3 Encoder". Around mid-1998, Mike Cheng created LAME 1.0 as a set of modifications against the "8Hz-MP3" encoder source code. After some quality concerns raised by others, he decided to start again from scratch based on the "dist10" MPEG reference software sources. His goal was only to speed up the dist10 sources, and leave its quality untouched. That branch (a patch against the reference sources) became Lame 2.0. The project quickly became a team project. Mike Cheng eventually left leadership and started working on tooLAME (an MP2 encoder).
Mark Taylor then started pursuing increased quality in addition to better speed, and released version 3.0 featuring gpsycho, a new psychoacoustic model he developed.
A few key improvements, in chronological order:
Patents and legal issues.
Like all MP3 encoders, LAME implements some technology covered by patents owned by the Fraunhofer Society and other entities. The developers of LAME do not themselves license the technology described by these patents. Distributing compiled binaries of LAME, its libraries, or programs that derive from LAME in countries that recognize those patents may be patent infringing.
The LAME developers state that, since their code is only released in source code form, it should only be considered as an educational description of an MP3 encoder, and thus does not infringe any patent by itself when released as source code only. At the same time, they advise users to obtain a patent license for any relevant technologies that LAME may implement before including a compiled version of the encoder in a product. Some software is released using this strategy: companies use the LAME library, but obtain patent licenses.
In November 2005, there were reports that the Extended Copy Protection rootkit included on some Sony Compact Discs included portions of the LAME library without complying with the terms of the LGPL.

</doc>
<doc id="18422" url="http://en.wikipedia.org/wiki?curid=18422" title="Linear algebra">
Linear algebra

Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.
The set of points with coordinates that satisfy a linear equation form a hyperplane in an "n"-dimensional space. The conditions under which a set of "n" hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors.
Linear algebra is central to both pure and applied mathematics. For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations. Functional analysis studies the infinite-dimensional version of the theory of vector spaces. Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations. 
Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, and the social sciences (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.
History.
The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations. Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.
The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his “Theory of Extension” which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".
In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra". The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra first took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.
The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.
Educational history.
Linear algebra first appeared in graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s. Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s. In France during the 1960s, educators attempted to teach linear algebra through affine dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum. In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.
Scope of study.
Vector spaces.
The main structures of linear algebra are vector spaces. A vector space over a field "F" is a set "V" together with two binary operations. Elements of "V" are called "vectors" and elements of "F" are called "scalars". The first operation, "vector addition", takes any two vectors "v" and "w" and outputs a third vector "v" + "w". The second operation, "scalar multiplication", takes any scalar "a" and any vector "v" and outputs a new vector "av". The operations of addition and multiplication in a vector space must satisfy the following axioms. In the list below, let "u", "v" and "w" be arbitrary vectors in "V", and "a" and "b" scalars in "F".
The first four axioms are those of "V" being an abelian group under vector addition. Vector spaces may be diverse in nature, for example, containing functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.
Linear transformations.
Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces "V" and "W" over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map
that is compatible with addition and scalar multiplication:
for any vectors "u","v" ∈ "V" and a scalar "a" ∈ F.
Additionally for any vectors "u", "v" ∈ "V" and scalars "a", "b" ∈ F:
When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.
Linear transformations have geometric significance. For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.
Subspaces, span, and basis.
Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors "v"1, "v"2, …, "vk":
where "a"1, "a"2, …, "a""k" are scalars. The set of all linear combinations of vectors "v"1, "v"2, …, "vk" is called their span, which forms a subspace.
A linear combination of any system of vectors with all zero coefficients is the zero vector of "V". If this is the only way to express the zero vector as a linear combination of "v"1, "v"2, …, "vk" then these vectors are linearly independent. Given a set of vectors that span a space, if any vector "w" is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove "w" from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space "V", which we call a basis of "V". Any set of vectors that spans "V" contains a basis, and any linearly independent set of vectors in "V" can be extended to a basis. It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers considered as a vector space over the rationals, but no explicit basis has been constructed.
Any two bases of a vector space "V" have the same cardinality, which is called the dimension of "V". The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of "V" has finite number of elements, "V" is called a finite-dimensional vector space. If "V" is finite-dimensional and "U" is a subspace of "V", then dim "U" ≤ dim "V". If "U"1 and "U"2 are subspaces of "V", then
One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.
Matrix theory.
A particular basis {"v"1, "v"2, …, "vn"} of "V" allows one to construct a coordinate system in "V": the vector with coordinates ("a"1, "a"2, …, "an") is the linear combination
The condition that "v"1, "v"2, …, "vn" span "V" guarantees that each vector "v" can be assigned coordinates, whereas the linear independence of "v"1, "v"2, …, "vn" assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to "v"). In this way, once a basis of a vector space "V" over F has been chosen, "V" may be identified with the coordinate "n"-space F"n". Under this identification, addition and scalar multiplication of vectors in "V" correspond to addition and scalar multiplication of their coordinate vectors in F"n". Furthermore, if "V" and "W" are an "n"-dimensional and "m"-dimensional vector space over F, and a basis of "V" and a basis of "W" have been fixed, then any linear transformation "T": "V" → "W" may be encoded by an "m" × "n" matrix "A" with entries in the field F, called the matrix of "T" with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.
There is an important distinction between the coordinate "n"-space R"n" and a general finite-dimensional vector space "V". While R"n" has a standard basis {"e"1, "e"2, …, "en"}, a vector space "V" typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of "V").
One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.
Eigenvalues and eigenvectors.
In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation "T" is to find "characteristic lines" that are invariant sets under "T". If "v" is a non-zero vector such that "Tv" is a scalar multiple of "v", then the line through 0 and "v" is an invariant set under "T" and "v" is called a characteristic vector or eigenvector. The scalar λ such that "Tv" = λ"v" is called a characteristic value or eigenvalue of "T".
To find an eigenvector or an eigenvalue, we note that
where I is the identity matrix. For there to be nontrivial solutions to that equation, det("T" − λ I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation "T" taking a vector space "V" into itself we can find a basis for "V" consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if "v"1, "v"2, …, "vn" are linearly independent eigenvectors of a mapping of "n"-dimensional spaces "T" with (not necessarily distinct) eigenvalues λ1, λ2, …, λ"n", and if "v" = "a"1"v"1 + ... + "an vn", then,
Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).
Inner-product spaces.
Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an "inner product" is a map
that satisfies the following three axioms for all vectors "u", "v", "w" in "V" and all scalars "a" in "F":
Note that in R, it is symmetric.
We can define the length of a vector "v" in "V" by 
and we can prove the Cauchy–Schwarz inequality:
In particular, the quantity
and so we can call this quantity the cosine of the angle between the two vectors.
Two vectors are orthogonal if formula_17. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if "v" = "a"1 "v"1 + ... + "an vn", then formula_18.
The inner product facilitates the construction of many useful concepts. For instance, given a transform "T", we can define its Hermitian conjugate "T*" as the linear transform satisfying
If "T" satisfies "TT*" = "T*T", we call "T" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span "V".
Applications.
Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.
Solution of linear systems.
Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:
The Gaussian-elimination algorithm is as follows: eliminate "x" from all equations below "L"1, and then eliminate "y" from all equations below "L"2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.
In the example, "x" is eliminated from "L"2 by adding (3/2)"L"1 to "L"2. "x" is then eliminated from "L"3 by adding "L"1 to "L"3. Formally:
The result is:
Now "y" is eliminated from "L"3 by adding −4"L"2 to "L"3:
The result is:
This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.
The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that
Then, "z" can be substituted into "L"2, which can then be solved to obtain
Next, "z" and "y" can be substituted into "L"1, which can be solved to obtain
The system is solved.
We can, in general, write any system of linear equations as a matrix equation:
The solution of this system is characterized as follows: first, we find a particular solution "x"0 of this equation using Gaussian elimination. Then, we compute the solutions of "Ax" = 0; that is, we find the null space "N" of "A". The solution set of this equation is given by formula_30. If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since "N" is trivial if and only if det "A" ≠ 0, the equation has a unique solution if and only if det "A" ≠ 0.
Least-squares best fit line.
The least squares method is used to determine the best fit line for a set of data. This line will minimize the sum of the squares of the residuals.
Fourier series expansion.
Fourier series are a representation of a function "f": [−π, π] → R as a trigonometric series:
This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.
The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product
The functions "gn"("x") = sin("nx") for "n" > 0 and "hn"("x") = cos("nx") for "n" ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient "ak", we take the inner product with "hk":
and by orthonormality, formula_34; that is, 
Quantum mechanics.
Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions φ: R3 → C such that formula_36 is finite), and it evolves according to the Schrödinger equation. Energy is represented as the operator formula_37, where "V" is the potential energy. "H" is also known as the Hamiltonian operator. The eigenvalues of "H" represents the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of "H". The component of "H" in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
Geometric introduction.
Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two dimensional plane "E". When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.
Point coordinates in the plane "E" are ordered pairs of real numbers, ("x","y"), and a line is defined as the set of points ("x","y") that satisfy the linear equation
where "a", "b" and "c" are not all zero.
Then,
or
where x = ("x", "y", 1) is the 3 × 1 set of homogeneous coordinates associated with the point ("x", "y").
Homogeneous coordinates identify the plane "E" with the "z" = 1 plane in three dimensional space. The x−y coordinates in "E" are obtained from homogeneous coordinates y = ("y"1, "y"2, "y"3) by dividing by the third component (if it is nonzero) to obtain y = ("y"1/"y"3, "y"2/"y"3, 1).
The linear equation, λ, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point "αx1 + "βx2 is also on the line, for any real "α" and "β".
Now consider the equations of the two lines "λ"1 and "λ"2,
which forms a system of linear equations. The intersection of these two lines is defined by x = ("x", "y", 1) that satisfy the matrix equation,
or using homogeneous coordinates,
The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates,
the solutions are multiples of the following solution:
if the rows of B are linearly independent (i.e., "λ"1 and "λ"2 represent distinct lines).
Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns. Notice that this yields a point in the "z" = 1 plane only when the 2 × 2 submatrix associated with "x"3 has a non-zero determinant.
It is interesting to consider the case of three lines, λ1, λ2 and λ3, which yield the matrix equation,
which in homogeneous form yields,
Clearly, this equation has the solution x = (0,0,0), which is not a point on the "z" = 1 plane "E". For a solution to exist in the plane "E", the coefficient matrix "C" must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.
Introduction to linear transformations.
Another way to approach linear algebra is to consider linear functions on the two dimensional real plane "E"=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in "E" and consider the linear function λ: "E"→R, given by
or
This transformation has the important property that if Ay=d, then
This shows that the sum of vectors in "E" map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation. For this case, where the image space is a real number the map is called a linear functional.
Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on "E", so that x=xi+yj. It is now possible to see that 
Thus, the columns of the matrix A are the image of the basis vectors of "E" in R.
This is true for any pair of vectors used to define coordinates in "E". Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in "E". This means a vector x has coordinates (α,β), such that x=αv+βw. Then, we have the linear functional
where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as
Coordinates relative to a basis.
This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in "E". Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is two find the real numbers α, β, so that x=αv+βw, that is
To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis v, w, which are given by,
The functionals σ and τ compute the components of x along the basis vectors v and w, respectively, that is,
which can be written in matrix form as
These coordinate functionals have the properties,
These equations can be assembled into the single matrix equation,
Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.
Inverse image.
The set of points in the plane "E" that map to the same image in R under the linear functional λ define a line in "E". This line is the image of the inverse map, λ−1: R→"E". This inverse image is the set of the points x=(x, y) that solve the equation,
Notice that a linear functional operates on known values for x=(x, y) to compute a value "c" in R, while the inverse image seeks the values for x=(x, y) that yield a specific value "c".
In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation
Solve for y and obtain the inverse image as the set of points,
For convenience the free parameter x has been relabeled t.
The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,
Notice that if h is a solution to this homogeneous equation, then "t" h is also a solution.
The set of points of a linear functional that map to zero define the "kernel" of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.
Generalizations and related topics.
Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.
In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space "V"∗ consisting of linear maps "f": "V" → "F" where "F" is the field of scalars. Multilinear maps "T": "Vn" → "F" can be described via tensor products of elements of "V"∗.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector product "V" × "V" → "V", the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L"p" spaces.
Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.
Algebraic geometry considers the solutions of systems of polynomial equations.
There are several related topics in the field of Computer Programming that utilizes much of the techniques and theorems Linear Algebra encompasses and refers to.

</doc>
<doc id="18473" url="http://en.wikipedia.org/wiki?curid=18473" title="Luca Pacioli">
Luca Pacioli

Fra Luca Bartolomeo de Pacioli (sometimes "Paccioli" or "Paciolo"; 1447–1517) was an Italian mathematician, Franciscan friar, collaborator with Leonardo da Vinci, and the seminal contributor to the field now known as accounting. He is referred to as "The Father of Accounting and Bookkeeping" and he was the first person to publish a work on double-entry system of book-keeping. He was also called Luca di Borgo after his birthplace, Borgo Sansepolcro, Tuscany. 
Life.
Luca Pacioli was born between 1446 and 1448 in Sansepolcro (Tuscany) where he received an abbaco education. This was education in the vernacular (i.e. the local tongue) rather than Latin and focused on the knowledge required of merchants. He moved to Venice around 1464, where he continued his own education while working as a tutor to the three sons of a merchant. It was during this period that he wrote his first book, a treatise on arithmetic for the boys he was tutoring. Between 1472 and 1475, he became a Franciscan friar.
In 1475, he started teaching in Perugia, first as a private teacher, from 1477 holding the first chair in mathematics. He wrote a comprehensive textbook in the vernacular for his students. He continued to work as a private tutor of mathematics and was, in fact, instructed to stop teaching at this level in Sansepolcro in 1491. In 1494, his first book to be printed, "Summa de arithmetica, geometria. Proportioni et proportionalita", was published in Venice. In 1497, he accepted an invitation from Duke Ludovico Sforza to work in Milan. There he met, collaborated with, lived with, and taught mathematics to Leonardo da Vinci. In 1499, Pacioli and Leonardo were forced to flee Milan when Louis XII of France seized the city and drove out their patron. Their paths appear to have finally separated around 1506. Pacioli died at about the age of 70 in 1517, most likely in Sansepolcro where it is thought that he had spent much of his final years.
Mathematics.
Pacioli published several works on mathematics, including:
Translation of Piero della Francesca's work.
The majority of the second volume of "Summa de arithmetica, geometria. Proportioni et proportionalita" was a slightly rewritten version of one of Piero della Francesca's works. The third volume of Pacioli's "De divina proportione" was an Italian translation of Piero della Francesca's Latin writings "On [the] Five Regular Solids". In neither case, did Pacioli include an attribution to Piero. He was severely criticized for this and accused of plagiarism by sixteenth-century art historian and biographer Giorgio Vasari. R. Emmett Taylor (1889–1956) said that Pacioli may have had nothing to do with the translated volume "De divina proportione", and that it may just have been appended to his work. However, no such defence can be presented concerning the inclusion of Piero della Francesca's material in Pacioli's Suma.
Chess.
Pacioli also wrote an unpublished treatise on chess, "De ludo scacchorum" ("On the Game of Chess"). Long thought to have been lost, a surviving manuscript was rediscovered in 2006, in the 22,000-volume library of Count Guglielmo Coronini. A facsimile edition of the book was published in Pacioli's home town of Sansepolcro in 2008. Based on Leonardo da Vinci's long association with the author and his having illustrated "De divina proportione", some scholars speculate that Leonardo either drew the chess problems that appear in the manuscript or at least designed the chess pieces used in the problems.

</doc>
<doc id="18491" url="http://en.wikipedia.org/wiki?curid=18491" title="Lead and follow">
Lead and follow

In some types of partner dance, lead and follow are designations for the two dancers comprising a dance couple. In the case of mixed-sex couples, the male is traditionally the lead and the female is the follow. The lead is responsible for guiding the couple and initiating transitions to different dance steps and, in improvised dances, for choosing the dance steps to perform. The lead conveys his choices and direction to the follow through subtle physical and visual signals, thereby allowing the couple to be smoothly coordinated. These choices are communicated by partner connection.
Theory.
The amount of direction given by the lead depends on several factors, including dance style, social context of the dance, and experience and personalities of the dancers. Some partner dances (e.g., Lindy Hop) employ an open position that encourages improvisation by the follow. Others, such as Argentine Tango, involve a close embrace (or closed position) that requires the follow to strictly conform to the lead's direction.
Social partner dance principles.
Partner dancing requires awareness and clear communication; this is essential both for safety and for the overall success of the dance. If following in the dance, it helps to maintain a centered readiness to the leader. This helps the follower be ready for cues both visually and physically: a lifted hand likely means an upcoming turn, for example, or a hand brought in front of the abdomen might signal a send-away. The leader of the dance will best support the follower by giving clear direction. 
For the Leader and Follower to interact with each other, communication needs to occur between the dance couple. Because it's not practical to discuss moves, physical contact is the most effective means. More advanced dancers will take many cues from each other through this connection, with the Follower using it to communicate feedback to the Leader just as the Leader uses it to suggest moves to their partner. The most accomplished dancers use connection as a line of communication which allows the leader to incorporate the follower's ideas, abilities, and creative suggestions into their own styling and selection of moves.
In many partner dances, the lead's steps differ from the follower's. In face-to-face positions, the follower generally "mirrors" the lead's footwork. For example, if the lead begins on their left foot, the follow will begin on their right foot. In choreographed pieces and other situations where the follow is in a tandem position or shadow position, the lead and follow will use the same footwork. Usually both partners move together as a unit, but in some dances the partners move in opposite directions - together and apart again.
In partner dancing, dancers seek to work together to create synchronised or complementary movements. The lead is largely responsible for "initiating" movement, whereas the follow's role is to "maintain" this movement (though they may choose not to). Many dancers describe this process as involving the initiation of momentum or 'energy' (by the lead) and then the subsequent maintenance, exaggeration, decreasing or dissolving of this momentum by both partners.
This momentum or energy may be manifested as movement (in its most obvious form), or in a range of more complex interactions between partners:
It is also helpful for dancers to regard their partners in terms of their "points of balance" to help the Lead initiate movements for their Follow partners. These points of balance include the front-facing side of the shoulders, the front facing side of the hips, and the Follow's center (the abdomen). If the Lead wants his or her partner to turn, he would take the Follower's hand from the hip area to the shoulder area while indicating the direction in which the turn should occur. If the Lead wants to bring the Follow close, the Lead is to apply tension and draw the hand in and down toward his or her own hip; to send the Follow away, the Lead would guide the hand toward the Follow's center and add compression, signaling the move away. 
Weight transfer.
For partner dancers, using weight transfers is a way for a Lead to communicate a 'lead' for a dance step to a Follow.
In another example, for a Lead to have their Follow walk forwards while connected, the Lead begins by taking his or her center back, indicating a backward walking move. As the partners arms/points of contact move away from each other, they develop tension, which the follow may either break by dropping their arms or breaking the hold, or 'follow' by moving.
A more experienced Lead may realize (if only on an unconscious level) that the most effective execution of even this "simple" step is achieved by preparing for movement before the step begins.
The Lead-Follow connection facilitates this. The principles of Leading and Following are explored to their most extreme limits in contact improvisation of modern dance, though they are as ancient a process as a parent carrying a child.
Advanced swing dancers do this to enhance their dance connection and to add more fun into the dance. Another way of "breaking the routine" of the dance is syncopation (the second meaning, making more steps than required by the standard description of the dance pattern). Syncopations are easier for the lead to cope with, since the lead does not have to change the intended dance figure, although experienced dancers try and match the fancy footwork of the partner, at least in rhythm. So, in a sense, syncopation may be perceived as mild hijacking. This is not as difficult as it might seem, since good dancers match their footwork to musical accents.
Closed Position: The basic dance position has the leader and follower facing one another, with the leader's right hand on the follower's back and the left hand holding the follower's right hand. The leader's left hand may be raised just below shoulder level to create the frame, or loose by the leader's side, depending on the style of dance. The right foot should point between the partner's feet.
Open Position: An open position will have the leader and follower facing one another, holding with one or both hands. Partners have slightly more distance between them than in closed position.
Apart Position: No hands held or contact made.
Promenade Position: A closed position that forms a V-shape. The leader will turn approximately 1/8 of a turn to the left and the follower 1/8 of a turn to the right, with heads following in respective directions. These degrees of turn vary, depending on the style of dance. Partners are walking with toes pointed in the same direction.
Counter-Promenade Position: Similar V-position as above, with partners walking in same direction yet with toes facing opposite directions (towards one another).
Outside Partner Position: A closed position where both of the leader's feet step on the outside track of the follower's feet; right leg will be against right leg.
Alignment: Indicate the directions the feet face in relationship to the room. Utilizes the Line Of Dance, which creates orientation positions in the room where the dance is taking place.
Turn: Direction of the turn indicates which shoulder will move backward as one turns. In a Left turn, the left shoulder will move backward. In the Right turn, the right shoulder will move backward.
Sway: Sway indicates body movement in the opposite direction of the moving foot. Sway Left stretches away from the moving right foot, while Sway Right stretches away from the moving left foot. Straight indicates no sway, holding an upright position.
Counter-Sway: Body moves in the same direction of the moving foot.
Rise and Fall: Refers to the body movement ascending and descending to create dynamic levels in the dance. 
Contra Body Movement (CBM): Describes the body movement when the opposite shoulder moves in the same direction as the opposite or moving leg. 
Contra Body Movement Position (CBMP): Describes the foot position when the moving foot is in the same line of position as the opposite foot (as in being on a tightrope). 
Fall Away: Both dancers walk backwards.
Travel.
Obstruction avoidance.
A general rule is that both lead and follow watch each other's back in a dance hall situation. Collision avoidance is one of the cases when the follow is required to "backlead" or at least to communicate about the danger to the lead. In travelling dances, such as Waltz, common Follow signals of danger are an unusual resistance to the Lead, or a slight tap by the shoulder. In open-position dances, such as Swing or Latin dances, maintaining eye contact with the partner is an important safety communication link.
Recovery from miscommunication.
Sometimes a miscommunication is possible between the Lead and Follow. A general rule here is do not wrestle and never stop dancing. Techniques of the recovery of connection and synchronization vary from dance to dance, but there are some common tricks.
History of gender roles.
Traditionally, the male dance partner is the Lead and the female dance partner is the Follow, though this is not always the case. Many social dance forms have a long history of same-sex and role-crossing partnerships, and there have been some changes to the strict gendering of partner dances in some competition or performance contexts. An example is a "Jack and Jack" dance contest.
Another form of partner dance where there is a Lead and Follow is Ballet. Roger Copeland(1993) describes how in ballet the male (Lead) is in control manipulating the movement through his Follow, the ballerina. The Lead is consistently in control which leaves the Follow in a submissive position(p.140).
Lead.
Methods to lead.
Body lead vs. arm lead.
A "body lead" occurs where the leader initiates a lead by moving their body, which moves their arm(s), and thus transmits a lead to the follow. 'Body lead' means much the same as 'weight transfer'.
An "arm lead" occurs where the leader moves their arm(s) without moving their body, or moves their body in a different direction to their arm.
While an 'arm lead' without the transfer of weight (or movement of the body) on the part of the leader is often a marker of an inexperienced or poorly taught dancer, the process of leading and following, particularly at an advanced level, often involves the contra- and contrasting uses of weight transfers and 'arm moves'. As an example, a leader may lead a follow back onto their right foot through the leader's own weight transfer forwards onto their left foot; yet at the same time turn the follower's torso to the left from above the hips.
Techniques of leading.
The Lead has to communicate the direction of the movement to the Follow. Traditionally, the Lead's right hand on the follow's back, near the lowest part of the shoulder-blade. This is the strongest part of the back and the lead can easily pull the Follow's body inwards. To enable the Lead to communicate a step forward (backward for the Follow) the Follow has to constantly put a little weight against the Lead's right hand. When the Lead goes forward, the Follow will naturally go backwards.
An important leading mechanism is the Lead's left hand, which usually holds the Follow's right hand. At no point should it be necessary for any partner to firmly grab the other's hand. It is sufficient to press the hand or even only finger tips slightly against each other, the Follows hand following the Leads hand.
Another important leading mechanism is hip contact. Though not possible in traditional Latin dances like Rumba, Cha-cha, Tango Argentino because of partner separation, hip contact is a harmonious and sensual way of communicating movement to the partner, used primarily in Standard or Ballroom Dances (English / slow Waltz, European Tango, Quickstep etc.) and Caribbean dances.
Follow.
Backleading.
'Backleading' is when a Follow is executing steps without waiting for, or contrary to, or interfering with the Lead's lead. Both are considered bad dancing habits because it makes the Follow difficult to lead and dance with.
Backleading can be a teaching tool that is often used intentionally by an instructor when dancing with a student lead, in order to help them learn the desired technique.
Backleading sounds similar to "hijacking", and indeed it is often used in place of "hijacking". However the two terms have significant differences, stemming from intentions. The first, superficial, difference: hijacking is usually an occasional "outburst" of the Follow, which otherwise diligently follows the lead, while a "backlead" may do this almost on every other step. The second, a more significant one: hijacking is an actual Lead, i.e., a hijacker does their stuff and watches for the 'Lead' to 'Follow' (reversed roles!), while backleading is taking care only about one's own dancing.
Hijacking.
Sometimes the follow steals the lead and they reverse roles for some time. This is called hijacking (also known as lead stealing). Hijacking requires experience and good connection, since without proper timing it may look like sloppy dancing. A signal for hijacking is typically an unusually changed (mostly, increased) stress in the connection from the follow's side. "Unusually" means more than typically required for the execution of the current step (by these partners). For a follow to hijack, they must be sure that the lead will understand or at least guess the follow's intentions.

</doc>
<doc id="18492" url="http://en.wikipedia.org/wiki?curid=18492" title="Lexeme">
Lexeme

A lexeme (  ) is a unit of lexical meaning that exists regardless of the number of inflectional endings it may have or the number of words it may contain. It is a basic unit of meaning, and the headwords of a dictionary are all lexemes. Put more technically, a lexeme is an abstract unit of morphological analysis in linguistics, that roughly corresponds to a set of forms taken by a single word. For example, in the English language, "run", "runs", "ran" and "running" are forms of the same lexeme, conventionally written as RUN. A related concept is the lemma (or citation form), which is a particular form of a lexeme that is chosen by convention to represent a canonical form of a lexeme. Lemmas are used in dictionaries as the headwords, and other forms of a lexeme are often listed later in the entry if they are not common conjugations of that word.
A lexeme belongs to a particular syntactic category, has a certain meaning (semantic value), and in inflecting languages, has a corresponding inflectional paradigm; that is, a lexeme in many languages will have many different forms. For example, the lexeme RUN has a present third person singular form "runs", a present non-third-person singular form "run" (which also functions as the past participle and non-finite form), a past form "ran", and a present participle "running". (It does not include "runner, runners, runnable," etc.) The use of the forms of a lexeme is governed by rules of grammar; in the case of English verbs such as RUN, these include subject-verb agreement and compound tense rules, which determine which form of a verb can be used in a given sentence.
A lexicon consists of lexemes.
In many formal theories of language, lexemes have subcategorization frames to account for the number and types of complements. They occur within sentences and other syntactic structures.
The notion of a lexeme is very central to morphology, and thus, many other notions can be defined in terms of it. For example, the difference between inflection and derivation can be stated in terms of lexemes:
Decomposition.
The lexemes of a language are often composed of smaller units with individual meaning called morphemes, according to root morpheme + derivational morphemes + desinence (not necessarily in this order), where:
The compound root morpheme + derivational morphemes is often called the stem. The decomposition stem + desinence can then be used to study inflection.

</doc>
<doc id="18495" url="http://en.wikipedia.org/wiki?curid=18495" title="Lightworks">
Lightworks

Lightworks is a professional non-linear editing system (NLE) for editing and mastering digital video in various formats, including 2K and 4K resolutions, and television in PAL, NTSC, and high-definition formats. Lightworks was an early developer of computer-based non-linear editing systems, and has been in development since 1989.
The program is currently available for Windows, Linux and Mac OS. The development of an open source version, as well as ports to Linux and Mac OS X were announced in May 2010. No source code has yet been released, but the project roadmap has indicated that source code release will happen eventually. 
Overview.
Using a control interface similar to the industry standard Steenbeck flatbed controller, Lightworks is a non-linear editing system. When introduced in the early 1990s, it provided a number of unique features, such as "sync slip", synchronized varispeed playback with audio scrubbing, synchronized multi-channel playback, and an object-oriented user interface with a dedicated hardware console. Some of these features are still absent in competing systems.
Editors using Lightworks have produced numerous internationally renowned and Oscar and Emmy Award-winning feature films and television programs, including "The King's Speech", Martin Scorsese's "Hugo" and "The Departed", "", "Pulp Fiction", "Braveheart" and "Batman".
History.
OLE Limited was founded in 1989 by Paul Bamborough, Nick Pollock and Neil Harris. In 1994 it was sold to Tektronix, who were not successful at developing the company's products. In 1999 it was sold on to the newly formed Lightworks Inc., then owned by Fairlight Japan, and then purchased by Gee Broadcast in May 2004.
Gee Broadcast ownership, 2004-2009.
Under Gee Broadcast ownership, new product releases resumed with the release of the Lightworks "Touch" range, and the "Alacrity" and "Softworks" ranges for SD & HD editing. Softworks offered the Lightworks User Interface and toolset in a software only package for laptops or office workstations. Softworks and Alacrity supported mixed formats and resolutions in real time and project output in different resolutions without re-rendering. Alacrity supported dual outputs while the same facility was available for Softworks users as an option.
EditShare ownership, 2009-present.
In August 2009 the UK and US based company "EditShare" acquired Gee Broadcast and the Lightworks editing platform from, along with their video server system "GeeVS".
At the annual convention of the National Association of Broadcasters, NAB Show, on 11 April 2010, EditShare announced that they plan to transform Lightworks into Lightworks Open Source. It was presented at IBC in Amsterdam September 2010.
On November 9, 2010, EditShare announced that Lightworks would be downloadable on November 29 of the same year, at first exclusively for the users who had registered during the initial announcement, but subsequently publishing the software as "public beta".
EditShare planned the release of the open source version in Q4 of 2011, after they finished code review. They plan to make money from proprietary plugins offered in their associated online shop, including plugins needed to access professional video formats. Shortly before the scheduled release date of 29. November 2011, EditShare announced that an open source release of the software would be temporarily delayed, but did not announce a new release date. The announcement noted that they were not yet satisfied with the stability of the new version.
Windows version released at NAB 2012.
After an 18-month beta program, EditShare released Lightworks 11, for Windows only, on the 28th of May, 2012. The non beta release of Lightworks includes a host of new features for editors, and runs on wide range of PC hardware. The software was re-designed and re-written for portability (versions for GNU/Linux and Mac OS X have been promised) and now supports many more codecs including AVCHD, H.264, AVC-Intra, DNxHD, ProRes, Red R3D, DPX, XDCAM HD 50, XDCAM EX, DVD, Blu-ray, and 4K, but only for the paid Pro version. The free version supports DV, MPEG, Uncompressed and other codecs for both import and export.
Windows version 11.1 released 29 May 2013.
On 29 May 2013, v11.1 stable release was made available for download. A major development in the Pro version is much improved performance of the H.264/AVC codec in MP4 and MOV containers. This makes it possible to edit this format natively, even with less powerful CPUs. This should interest HDSLR and GoPro camera users. Native editing of H.264 MTS files has been possible since version 11.0.3.
This version of Lightworks has also replaced HASP with the new EditShare Licensing System (ELS), which eliminates some installation problems. Lightworks Free users can now download the 64 bit version, which was previously limited to Pro users. The Free version now also comes with a 30 day Pro Trial period.
Linux version announced at IBC 2012.
EditShare demonstrated the Linux version at the NAB in Las Vegas in April 2012, and posted a video of it running on Ubuntu on their YouTube channel. At IBC in Amsterdam in September, an updated Linux demo was presented, and EditShare announced that the initial Linux alpha version would become available on October 30. Lightworks 11 alpha for Linux was released on the 30 April 2012, but only to a limited audience. The Linux version of Lightworks was made available as a Public Beta on 30 April 2013.
Lightworks 12 beta released for Windows, Linux and Mac.
On August 8, 2014, the first beta of Lightworks version 12 working on Windows, Linux and Mac was released.

</doc>
<doc id="18521" url="http://en.wikipedia.org/wiki?curid=18521" title="Latina (disambiguation)">
Latina (disambiguation)

Latina is the feminine form of the term Latino.
Latina may also refer to:

</doc>
<doc id="18524" url="http://en.wikipedia.org/wiki?curid=18524" title="Latin America">
Latin America

Latin America is a region of the Americas, that comprises countries or provinces where Romance languages are spoken; primarily Spanish and Portuguese, but also French. Latin America has an area of approximately 19,197,000 km2 (7,412,000 sq mi), almost 13% of the earth's land surface area.
As of 2013, its population was estimated at more than 604 million and in 2014, Latin America has a combined nominal GDP of 5,573,397 million USD and a GDP PPP of 7,531,585 million USD. The term "Latin America" was first used by two men in the same year: a Colombian poet, José María Torres Caicedo, and a Chilean socialist, Francisco Bilbao, both in 1856.
Etymology and definitions.
The idea that a part of the Americas has a linguistic affinity with the Romance cultures as a whole can be traced back to the 1830s, in the writing of the French Saint-Simonian Michel Chevalier, who postulated that this part of the Americas was inhabited by people of a "Latin race", and that it could, therefore, ally itself with "Latin Europe" in a struggle with "Teutonic Europe", "Anglo-Saxon America" and "Slavic Europe". The idea was later taken up by Latin American intellectuals and political leaders of the mid- and late-nineteenth century, who no longer looked to Spain or Portugal as cultural models, but rather to France. The term was first used in Paris in an 1856 conference by the Chilean politician Francisco Bilbao and the same year by the Colombian writer José María Torres Caicedo in his poem "Two Americas". The term Latin America was supported by the French Empire of Napoleon III during the French invasion of Mexico as a way to include France among countries with influence in the Americas and to exclude Anglophone countries and played a role in his campaign to imply cultural kinship of the region with France, transform France into a cultural and political leader of the area, and install Maximilian of Habsburg as emperor of the Second Mexican Empire. This term was also used in 1861 by French scholars in "La revue des races Latines," a magazine dedicated to the Pan-Latinism movement.
In contemporary usage:
The distinction between "Latin America" and "Anglo-America" is a convention based on the predominant languages in the Americas by which Romance-language and English-speaking cultures are distinguished. Neither area is culturally or linguistically homogeneous; in substantial portions of Latin America (e.g., highland Peru, Bolivia, Guatemala, and Paraguay), Native American cultures and, to a lesser extent, Amerindian languages, are predominant, and in other areas, the influence of African cultures is strong (e.g., the Caribbean basin – including parts of Colombia and Venezuela).
Subdivisions.
Latin America can be subdivided into several subregions based on geography, politics, demographics and culture. If defined as all of the Americas south of the United States, the basic geographical subregions are North America, Central America, the Caribbean and South America; the latter contains further politico-geographical subdivisions such as the Southern Cone, the Guianas and the Andean states. It may be subdivided on linguistic grounds into Hispanic America, Portuguese America and French America.
History.
Pre-Columbian history.
The earliest known settlement was identified at Monte Verde, near Puerto Montt in Southern Chile. Its occupation dates to some 14,000 years ago and there is some disputed evidence of even earlier occupation. Over the course of millennia, people spread to all parts of the continents. By the first millennium AD/CE, South America's vast rainforests, mountains, plains and coasts were the home of tens of millions of people. The earliest settlements in the Americas are of the Las Vegas Culture from about 8000 BC and 4600 BC, a sedentary group from the coast of Ecuador, the forefathers of the more known Valdivia culture, of the same era. Some groups formed more permanent settlements such as the Chibchas (or "Muiscas" or "Muyscas") and the Tairona groups. These groups are in the circum Caribbean region. The Chibchas of Colombia, the Quechuas and Aymaras of Bolivia and Perú were the three indigenous groups that settled most permanently.
The region was home to many indigenous peoples and advanced civilizations, including the Aztecs, Toltecs, Caribs, Tupi, Maya, and Inca. The golden age of the Maya began about 250, with the last two great civilizations, the Aztecs and Incas, emerging into prominence later on in the early fourteenth century and mid-fifteenth centuries, respectively. The Aztec empire was ultimately the most powerful civilization known throughout the Americas, until its downfall in part by the Spanish invasion.
European colonization.
With the arrival of the Europeans following Christopher Columbus' voyages, the indigenous elites, such as the Incas and Aztecs, lost power to the heavy European invasion. Hernándo Cortés seized the Aztec elite's power with the help of local groups who had favored the Aztec elite, and Francisco Pizarro eliminated the Incan rule in Western South America. The European powers of Spain and Portugal colonized the region, which along with the rest of the uncolonized world, was divided into areas of Spanish and Portuguese control by the line of demarcation in 1494, which gave Spain all areas to the west, and Portugal all areas to the east (the Portuguese lands in South America subsequently becoming Brazil. By the end of the sixteenth century Spain and Portugal had been joined by others, including France, in occupying large areas of North, Central and South America, ultimately extending from Alaska to the southern tips of the Patagonia. European culture, customs and government were introduced, with the Roman Catholic Church becoming the major economic and political power to overrule the traditional ways of the region, eventually becoming the only official religion of the Americas during this period.
Epidemics of diseases brought by the Europeans, such as smallpox and measles, wiped out a large portion of the indigenous population. Historians cannot determine the number of natives who died due to European diseases, but some put the figures as high as 85% and as low as 25%. Due to the lack of written records, specific numbers are hard to verify. Many of the survivors were forced to work in European plantations and mines. Intermixing between the indigenous peoples and the European colonists was very common, and, by the end of the colonial period, people of mixed ancestry (mestizos) formed majorities in several colonies.
Independence (1804–1825).
In 1804, Haiti became the first Latin American nation to gain independence, following a violent slave revolt led by Toussaint L'ouverture on the French colony of Saint-Domingue. The victors abolished slavery. Haitian independence inspired independence movements in Spanish America.
By the end of the eighteenth century, Spanish and Portuguese power waned on the global scene as other European powers took their place, notably Britain and France. Resentment grew among the majority of the population in Latin America over the restrictions imposed by the Spanish government, as well as the dominance of native Spaniards (Iberian-born "Peninsulares") in the major social and political institutions. Napoleon's invasion of Spain in 1808 marked a turning point, compelling Criollo elites to form juntas that advocated independence. Also, the newly independent Haiti, the second oldest nation in the New World after the United States, further fueled the independence movement by inspiring the leaders of the movement, such as Miguel Hidalgo y Costilla of Mexico, Simón Bolívar of Venezuela and José de San Martín of Argentina, and by providing them with considerable munitions and troops.
Fighting soon broke out between juntas and the Spanish colonial authorities, with initial victories for the advocates of independence. Eventually these early movements were crushed by the royalist troops by 1810, including those of Miguel Hidalgo y Costilla in Mexico in the year 1810. Later on Francisco de Miranda in Venezuela by 1812. Under the leadership of a new generation of leaders, such as Simón Bolívar "The Liberator", José de San Martín of Argentina, and other "Libertadores" in South America, the independence movement regained strength, and by 1825, all Spanish America, except for Puerto Rico and Cuba, had gained independence from Spain. Brazil achieved independence with a constitutional monarchy established in 1822. In the same year in Mexico, a military officer, Agustín de Iturbide, led a coalition of conservatives and liberals who created a constitutional monarchy, with Iturbide as emperor. This First Mexican Empire was short-lived, and was followed by the creation of a republic in 1823.
Consolidation and liberal–conservative conflicts (1818–1990).
Conservative and liberal conflicts in Latin America in the 19th Century.
After the independence of many Latin American countries, there was conflict between the people and the government, much of which can be reduced to the contrasting ideologies between liberalism and conservatism. Conservatism was the dominant system of government prior to the revolutions and it was founded on having social classes, including governing by kings. Liberalists wanted to see a change in the ruling systems, and to move away from monarchs and social classes in order to promote equality.
When liberal Guadalupe Victoria became the first president of Mexico in 1824, conservatists relied on their belief that the state had been better off before the new government came into power, so, by comparison, the old government was better in the eyes of the Conservatives. Following this sentiment, the conservatives pushed to take control of the government, and they succeeded. General Santa Anna was elected president in 1833. The following decade, the Mexican–American War (1846–48) caused Mexico to lose a significant amount of territory to the United States. This loss led to a rebellion by the enraged liberal forces against the conservative government.
In 1837, conservative Rafael Carrera conquered Guatemala and separated from the Central American Union. The instability that followed the disintegration of the union led to the independence of the other Central American countries.
In Brazil, rural aristocrats were in conflict with the urban conservatives. Portuguese control over Brazilian ports continued after Brazil's independence. Following the conservative idea that the old government was better, urbanites tended to support conservatism because more opportunities were available to them as a result of the Portuguese presence.
Simón Bolívar became president of Gran Colombia in 1819 after the region gained independence from Spain. He led a military-controlled state. Citizens did not like the government's position under Bolívar: The people in the military were unhappy with their roles, and the civilians were of the opinion that the military had too much power. After the dissolution of Gran Colombia, New Grenada continued to have conflicts between conservatives and liberals. These conflicts were each concentrated in particular regions, with conservatives particularly in the southern mountains and the Valley of Cauca. In the mid-1840s some leaders in Caracas organized a liberal opposition. Antonio Leocadio Guzman was an active participant and journalist in this movement and gained much popularity among the people of Caracas.
In Argentina, the conflict manifested itself as a conflict between the centralists and federalists, which are equivalent to conservatives and liberals, respectively. Uruguay gained its independence in a war with Brazil, after which a central government was established in Argentina. After the first president of the centralized government in Argentina resigned, the civil war between the centralists and federalists continued. When the provinces became the Argentinian Federation with no head of state, Juan Manuel de Rosas was given the powers of debt payment and international relations. He refused to enact a national constitution which resulted in greater conflict and more civil war. The differences in Uruguay were manifested as blancos and colorados, where the conservatives were represented by the blancos and the colorados represented the business interest in Montevideo. The conflicts between these two groups resulted in a civil war which is known as the Guerra Grande.
British influence in Latin America during 19th century.
Losing the North American colonies at the end of the 18th century left Great Britain in need of new markets to supply resources in the early 19th century. In order to solve this problem, Great Britain turned to the Spanish colonies in South America for resources and markets. In 1806 a small British force surprise attacked the capitol of the viceroyalty in  Río de la Plata.  As a result the local garrison protecting the capitol was destroyed in an attempt to defend against the British conquest. The British were able to capture numerous amounts of precious metals, before a French naval force intervened on behalf of the Spanish King and took down the invading force. However, this caused much turmoil in the area as militia took control of the area from the viceroy. The next year the British attacked once again with a much larger force attempting to reach and conquer Montevideo. They failed to reach Montevideo but succeeded in establishing an alliance with the locals. As a result the British were able to take control of the Indian markets.
This newly gained British dominance hindered the development of Latin American industries and strengthened the dependence on the world trade  network. Britain now replaced Spain as the region's largest trading  partner.  Great Britain invested significant capital in Latin America in order to develop the area as a market for processed goods. From the early 1820s to 1850, the post-independence economies of Latin American countries were lagging and stagnant. Eventually, enhanced trade among Britain and Latin America led to state development such as infrastructure improvements. These improvements included roads and railroads which grew the trades between countries and outside nations such as Great Britain. By 1870 exports dramatically increased attracting capital from abroad (including Europe and USA.)
French involvement in Latin America during the 19th century.
Between 1821 and 1910, Mexico battled through various civil wars between the established Conservative government and the Liberal reformists ("Mexico Timeline- Page 2)". On May 8, 1827 Baron Damas, the French Minister of Foreign Affairs, and Sebastián Camacho, a Mexican diplomat, signed an agreement called "The Declarations" which contained provisions regarding commerce and navigation between France and Mexico. At this time the French government did not recognise Mexico as an independent entity. It was not until 1861 that the liberalist rebels, led by Benito Juárez, took control of Mexico City, consolidating liberal rule. However, the constant state of warfare left Mexico with a tremendous amount of debt owed to Spain, England, and France, all of whom funded the Mexican war effort (Neeno). As newly appointed president, Benito Juárez suspended payment of debts for next two years, to focus on a rebuilding and stabilization initiative in Mexico under the new government. On December 8, 1861, Spain, England and France landed in Veracruz in order to seize unpaid debts from Mexico. However Napoleon III, with intentions of establishing a French client state to further push his economic interests, pressured the other two powers to withdraw in 1862 (Greenspan; "French Intervention in Mexico…"). France under Napoleon III remained and established Maximilian of Habsburg, Archduke of Austria, as Emperor of Mexico. The march by the French to Mexico City enticed heavy resistance by the Mexican government, it resulted in open war-fare. The Battle of Puebla in 1862 in particular presented an important turning point in which Ignacio Zaragoza led the Mexican army to victory as they pushed back the French offensive ("Timeline of the Mexican Revolution"). The victory came to symbolize Mexico's power and national resolve against foreign occupancy and as a result delayed France's later attack on Mexico City for an entire year (Cinco de Mayo (Mexican History)). With heavy resistance by Mexican rebels and the fear of United States intervention against France, forced Napoleon III to withdraw from Mexico, leaving Maximilian to surrender, where he would be later executed by Mexican troops under the rule of Porfirio Díaz.
Napoleon III's desire to expand France's economic empire influenced the decision to seize territorial domain over the Central American region. The port city of Veracruz, Mexico and France's desire to construct a new canal were of particular interest. Bridging both New World and East Asian trade routes to the Atlantic were key to Napoleon III's economic goals to the mining of precious rocks and the expansion of France's textile industry. Napoleon's fear of the United States' economic influence over the Pacific trade region, and in turn all New World economic activity, pushed France to intervene in Mexico under the pretense of collecting on Mexico's debt. Eventually France began plans to build the Panama Canal in 1881 until 1904 when the United States took over and proceeded with its construction and implementation ("Read Our Story").
United States involvement in Latin America during the 19th Century.
Monroe Doctrine.
The Monroe Doctrine was included in President James Monroe's 1823 annual message to Congress. The doctrine warns European nations that the United States will no longer tolerate the colonization of Latin American countries. It was originally drafted to meet the present major concerns, but eventually became the precept of U.S. foreign policy in the Western Hemisphere. The doctrine was put into effect in 1865 when the U.S. government supported Mexican president, Benito Juárez, diplomatically and militarily. Some Latin American countries viewed the U.S. interventions, allowed by the Monroe Doctrine when the U.S. deems necessary, with suspicion.
Another important aspect of United States involvement in Latin America is the case of the filibuster William Walker. In 1855, he traveled to Nicaragua hoping to overthrow the government and take the land for the United States. With only the aid of 56 followers, he was able to take over the city of Granada, declaring himself commander of the army and installing Patricio Rivas as a puppet president. However, Rivas's presidency ended when he fled Nicaragua; Walker rigged the following election to ensure that he became the next president. His presidency did not last long, however, as he was met with much opposition from political groups in Nicaragua and neighbouring countries. On May 1, 1857, Walker was forced by a coalition of Central American armies to surrender himself to a United States Navy officer who repatriated him and his followers. When Walker subsequently returned to Central America in 1860, he was apprehended by the Honduran authorities and executed.
Mexican–American War (1846–48).
The Mexican–American War, another instance of U.S involvement in Latin America, was a war between the United States and Mexico that started in April 1846 and lasted until February 1848. The main cause of the war was the United States' annexation of Texas in 1845 and a dispute afterwards about whether the border between Mexico and the United States ended where Mexico claimed, at the Nueces River, or ended where the United States claimed, at the Rio Grande. Peace was negotiated between the United States and Mexico with the Treaty of Guadalupe-Hidalgo, which stated that Mexico was to cede land which would later become part of California and New Mexico as well as give up all claims to Texas, for which the United States would pay $15,000,000. However, tensions between the two countries were still high and over the next six years things only got worse with raids along the border and attacks by Native Americans against Mexican citizens. To defuse the situation, the United States agreed to purchase 29,670 squares miles of land from Mexico for $10,000,000 so a southern railroad could be built to connect the Pacific and Atlantic coasts. This would become known as the Gadsden Purchase. A critical component of U.S. intervention in Latin American affairs took form in the Spanish–American War, which drastically affected the futures of Cuba and Puerto Rico in the Americas, as well as Guam and the Philippines, by dismantling some of the last remaining Spanish colonial possessions throughout the world.
World War I and the Zimmermann Telegram.
The Zimmermann Telegram was a 1917 diplomatic proposal from the German Empire for Mexico to join an alliance with Germany in the event of the United States entering World War I against Germany. The proposal was intercepted and decoded by British intelligence. Revelation of the contents outraged American public opinion. President Woodrow Wilson moved to arm American merchant ships to defend themselves against German submarines, which had started to attack them. The news helped generate support for the United States declaration of war on Germany in April of that year.
The message came as a coded telegram dispatched by the Foreign Secretary of the German Empire, Arthur Zimmermann, on January 16, 1917. The message was sent to the German ambassador of Mexico, Heinrich von Eckardt. Zimmermann sent the telegram in anticipation of the by Germany on 1 February, an act which Germany presumed would lead to war. The telegram instructed Ambassador Eckardt that if the U.S. appeared certain to enter the war, he was to approach the Mexican Government with a proposal for military alliance, with funding from Germany. As part of the alliance, Germany would assist Mexico to reconquer Texas and the Southwest. Eckardt was instructed to urge Mexico to help broker an alliance between Germany and Japan. Mexico, in the middle of the Mexican Revolution, far weaker militarily, economically and politically than the U.S., ignored the proposal; after the U.S. entered the war, it officially rejected it.
Brazil's participation in World War II.
After World War I, in which Brazil was an ally of the United States, Great Britain, and France, the country realized it needed a more capable army but didn't have the technology to create it. In 1919, the French Military Mission was established by the French Commission in Brazil. Their main goal was to contain the inner rebellions in Brazil. They tried to assist the army by bringing them up to the European military standard but constant civil missions did not prepare them for World War II.
Brazil President, Getúlio Vargas, wanted to industrialize Brazil, allowing it to be more competitive with other countries. He reached out to Germany, Italy, France, and the United States to act as trade allies. Many Italian and German people immigrated to Brazil many years before World War II began thus creating a Nazi influence. The immigrants held high positions in government and the armed forces. It was recently found that 9,000 war criminals escaped to South America, including Croats, Ukrainians, Russians and other western Europeans who aided the Nazi war machine. Most, perhaps as many as 5,000, went to Argentina; between 1,500 and 2,000 are thought to have made it to Brazil; around 500 to 1,000 to Chile; and the rest to Paraguay and Uruguay. It was not a secret that Vargas had an admiration for Hitler's Nazi Germany and its Führer. He even let German Luftwaffe build secret air forces around Brazil, but he knew that he could never favor the Nazis because of their racism towards the large black population in Brazil. This alliance with Germany became Brazil's second best trade alliance behind the United States.
Brazil continued to try to remain neutral to the United States and Germany because it was trying to make sure it could continue to be a place of interest for both opposing countries. Brazil attended continental meetings in Buenos Aires, Argentina (1936); Lima, Peru (1938); and Havana, Cuba (1940) that obligated them to agree to defend any part of the Americas if they were to be attacked. Eventually Brazil decided to stop trading with Germany once Germany started attacking offshore trading ships resulting in Germany declaring a blockade against the Americas in the Atlantic Ocean. Furthermore, Germany also ensured that they would be attacking the Americas soon.
Once the German submarines attacked unarmed Brazilian trading ships, President Vargas met with United States President Franklin D. Roosevelt to discuss how they could retaliate. On January 22, 1942, Brazil officially ended all relations with Germany, Japan, and Italy, becoming a part of the Allies.
The Brazilian Expeditionary Force was sent to Naples, Italy to fight for democracy. Brazil was the only Latin American country to send troops to Europe. Initially, Brazil wanted to only provide resources and shelter for the war to have a chance of gaining a high postwar status but ended up sending 25,000 men to fight.
After World War II, the United States and Latin America continued to have a close relationship. For example, USAID created family planning programs in Latin America combining the NGOs already in place, providing the women in largely Catholic areas access to contraception.
Involvement in World War II.
There was Nazi influence in certain parts of the region, but Jewish migration from Europe during the war continued. Only a small number of people recognized or knew about the Holocaust. Furthermore, numerous military bases were built during the war by the United States, but some also by the Germans. Even now, unexploded bombs from the second world war that need to be made safe still remain.
Cold War (1946–90).
Economy.
The Great Depression caused Latin America to grow at a slow rate, separating it from leading industrial democracies. The two world wars and U.S. Depression also made Latin American countries favor internal economic development, leading Latin America to adopt the policy of import substitution industrialization. Countries also renewed emphasis on exports. Brazil began selling automobiles to other countries, and some Latin American countries set up plants to assemble imported parts, letting other countries take advantage of Latin America's low labor costs. Colombia began to export flowers, emeralds and coffee grains and gold, becoming the world's second leading flower exporter.
Economic integration was called for, to attain economies that could compete with the economies of the United States or Europe. Starting in the 1960s with the Latin American Free Trade Association and Central American Common Market, Latin American countries worked toward economic integration.
Reforms.
Large countries like Argentina called for reforms to lessen the disparity of wealth between the rich and the poor, which has been a long problem in Latin America that stunted economic growth.
Advances in public health caused an explosion of population growth, making it difficult to provide social services. Education expanded, and social security systems introduced, but benefits usually went to the middle class, not the poor. As a result, disparity of wealth increased. Increasing inflation and other factors caused countries to be unwilling to fund social development programs to help the poor.
Bureaucratic authoritarianism.
Bureaucratic authoritarianism was practiced in Brazil after 1964, in Argentina, and in Chile under Augusto Pinochet, in a response to harsh economic conditions. It rested on the conviction that no democracy could take the harsh measures to curb inflation, reassure investors, and quicken economic growth quickly and effectively. Though inflation fell sharply, industrial production dropped with the decline of official protection.
US relations.
After World War II and the beginning of a Cold War between the United States and the Soviet Union, US diplomats became interested in Asia, Africa, and Latin America, and frequently waged proxy wars against the Soviet Union in these countries. The US sought to stop the spread of communism. Latin American countries generally sided with the US in the Cold War period, even though they complained of being neglected since the US's concern with communism were focused in Europe and Asia, not Latin America. Some Latin American governments also complained of the US support in the overthrow of some nationalist governments, and intervention through the CIA. In 1947, the US Congress passed the National Security Act, which created the National Security Council in response to the United States's growing obsession with anti-communism.
In 1954, when Jacobo Arbenz of Guatemala accepted the support of communists and attacked holdings of the United Fruit Company, the US decided to assist Guatemalan counter-revolutionaries in overthrowing Arbenz. These interventionist tactics featured use of the CIA rather than the military, which was used in Latin America for the majority of the Cold War in events including the overthrow of Salvador Allende. Latin America was more concerned with issues of economic development, while the United States focused on fighting communism, even though the presence of communism was small in Latin America.
Cuban Revolution.
By 1959, Cuba was afflicted with a corrupt dictatorship under Batista, and Fidel Castro ousted Batista that year and set up the first communist state in the hemisphere. The United States imposed a trade embargo on Cuba, and combined with Castro's expropriation of private enterprises, this was detrimental to the Cuban economy.
Around Latin America, rural guerrilla conflict and urban terrorism increased, inspired by the Cuban example. The United States put down these rebellions by supporting Latin American countries in their counter guerrilla operations through the Alliance for Progress launched by President John F. Kennedy. This thrust appeared to be successful. A Marxist, Salvador Allende, became president of Chile in 1970, but was overthrown three years later in a military coup backed by the United States. Despite civil war, high crime and political instability, most Latin American countries eventually adopted democracies besides Cuba.
Bay of Pigs Invasion.
Encouraged by the success of Guatemala in the 1954 Guatemalan coup d'état, in 1960, the U.S. decided to support an attack on Cuba by anti-Castro rebels. The Bay of Pigs invasion was an unsuccessful invasion of Cuba in 1961, financed by the U.S. through the CIA, to overthrow Fidel Castro. The incident proved to be very embarrassing for the new Kennedy administration.
Alliance for Progress.
President John F. Kennedy initiated the Alliance for Progress in 1961, to establish economic cooperation between the U.S. and Latin America. The Alliance would provide $20 billion for reform in Latin America, and counterinsurgency measures. Instead, the reform failed because of the simplistic theory that guided it and the lack of experienced American experts who could understand Latin American customs.
Nicaraguan Revolution.
Following the American occupation of Nicaragua in 1912, as part of the Banana Wars, the Somoza family political dynasty came to power, and would rule Nicaragua until their ouster in 1979 during the Nicaraguan Revolution. The era of Somoza family rule was characterized by strong U.S. support for the government and its military[15] as well as a heavy reliance on U.S. based multi-national corporations. The Nicaraguan Revolution (Spanish: Revolución Nicaragüense or Revolución Popular Sandinista) encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–79, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990 and the Contra War which was waged between the FSLN and the Contras from 1981–1990.
The Revolution marked a significant period in Nicaraguan history and revealed the country as one of the major proxy war battlegrounds of the Cold War with the events in the country rising to international attention.
Although the initial overthrow of the Somoza regime in 1978–79 was a bloody affair, the Contra War of the 1980s took the lives of tens of thousands of Nicaraguans and was the subject of fierce international debate. During the 1980s both the FSLN (a Leftist collection of political parties) and the Contras (a rightist collection of counter-revolutionary groups) received large amounts of aid from the Cold War super-powers (respectively, the Soviet Union and the United States).
Washington Consensus.
The set of specific economic policy prescriptions that were considered the "standard" reform package were promoted for crisis-wracked developing countries by Washington, D.C.-based institutions such as the International Monetary Fund (IMF), World Bank, and the US Treasury Department during the 1980s and 1990s.
In recent years, several Latin American countries led by socialist or other left wing governments – including Argentina and Venezuela – have campaigned for (and to some degree adopted) policies contrary to the Washington Consensus set of policies. (Other Latin countries with governments of the left, including Brazil, Chile and Peru, have in practice adopted the bulk of the policies.) Also critical of the policies as actually promoted by the International Monetary Fund have been some US economists, such as Joseph Stiglitz and Dani Rodrik, who have challenged what are sometimes described as the "fundamentalist" policies of the International Monetary Fund and the US Treasury for what Stiglitz calls a "one size fits all" treatment of individual economies.
The term has become associated with neoliberal policies in general and drawn into the broader debate over the expanding role of the free market, constraints upon the state, and US influence on other countries' national sovereignty.
This politico-economical initiative was institutionalized in North America by the 1994 NAFTA, and elsewhere in the Americas through a series of like agreements. The comprehensive Free Trade Area of the Americas project, however, was rejected by most South American countries at the 2005 4th Summit of the Americas.
Turn to the left.
Due to the São Paulo Forum, in most countries, since the 2000s left-wing political parties have risen to power. The presidencies of Hugo Chávez in Venezuela, Ricardo Lagos and Michelle Bachelet in Chile, Lula da Silva and Dilma Rousseff in Brazil, Néstor Kirchner and his wife Cristina Fernández in Argentina, Tabaré Vázquez and José Mujica in Uruguay, Evo Morales in Bolivia, Daniel Ortega in Nicaragua, Rafael Correa in Ecuador, Fernando Lugo in Paraguay, Manuel Zelaya in Honduras (removed from power by a coup d'état), Mauricio Funes and Salvador Sánchez Cerén in El Salvador are all part of this wave of left-wing politicians who often declare themselves socialists, Latin Americanists, or anti-imperialists (often implying opposition to US policies towards the region). A development of this has been the creation of the eight-member ALBA alliance, or "The Bolivarian Alliance for the Peoples of Our America" (Spanish: "Alianza Bolivariana para los Pueblos de Nuestra América") by some of the countries already mentioned. By June 2014, Honduras (Juan Orlando Hernández), Guatemala (Otto Pérez Molina), Colombia (Juan Manuel Santos) and Panama (Ricardo Martinelli) had right-wing governments.
The return of social movements.
In 1982, Mexico announced that it could not meet its foreign debt payment obligations, inaugurating a debt crisis that would "discredit" Latin American economies throughout the decade. This debt crisis would lead to neoliberal reforms that would instigate many social movements in the region. A "reversal of development" reigned over Latin America, seen through negative economic growth, declines in industrial production, and thus, falling living standards for the middle and lower classes. Governments made financial security their primary policy goal over social security, enacting new neoliberal economic policies that implemented privatization of previously national industries and informalization of labor. In an effort to bring more investors to these industries, these governments also embraced globalization through more open interactions with the international economy. Significantly, as democracy spread across much of Latin America, the realm of government more inclusive (a trend that proved conductive to social movements), the economic ventures remained exclusive to a few elite groups within society. Neoliberal restructuring consistently redistributed income upward while denying political responsibility to provide social welfare rights, and though development projects took place throughout the region, both inequality and poverty increased. Feeling excluded from these new projects, the lower classes took ownership of their own democracy through a revitalization of social movements in Latin America.
Both urban and rural populations had serious grievances as a result of the above economic and global trends and have voiced them in mass demonstrations. Some of the largest and most violent of these have been protests against cuts in urban services, such as the Caracazo in Venezuela and the Argentinazo in Argentina.
Rural movements have made diverse demands related to unequal land distribution, displacement at the hands of development projects and dams, environmental and indigenous concerns, neoliberal agricultural restructuring, and insufficient means of livelihood. These movements have benefited considerably from transnational support from conservationists and INGOs. The Movement of Rural Landless Workers (MST), is perhaps the largest contemporary Latin American social movement. As indigenous populations are primarily rural, indigenous movements account for a large portion of rural social movements, including the Zapatista rebellion in Mexico, the Confederation of Indigenous Nationalities of Ecuador (CONAIE), indigenous organizations in the Amazon region of Ecuador and Bolivia, pan-Mayan communities in Guatemala, and mobilization by the indigenous groups of Yanomami peoples in the Amazon, Kuna peoples in Panama, and Altiplano Aymara and Quechua peoples in Bolivia. Other significant types of social movements include labor struggles and strikes, such as recovered factories in Argentina, as well as gender-based movements such as the Mothers of the Plaza de Mayo in Argentina and protests against maquila production, which is largely a women's issue because of how it draws on women for cheap labor.
Commodity boom and increasing relations with China.
The 2000s commodities boom caused positive effects for many Latin American economies. Another trend is the rapidly increasing importance of the relations with China.
Demographics.
Largest cities.
The following is a list of the ten largest metropolitan areas in Latin America.
Ethnic groups.
The inhabitants of Latin America are of a variety of ancestries, ethnic groups, and races, making the region one of the most diverse in the world. The specific composition varies from country to country: many have a predominance of European-Amerindian or more commonly referred to as Mestizo or Castizo depending on the admixture, population; in others, Amerindians are a majority; some are dominated by inhabitants of European ancestry; and some countries' populations are primarily Mulatto, in Latin America the only Black majority nation is Haiti. Asian and Afro-Amerindian (historically sometimes called Zambo) minorities are also identified regularly. People with European ancestry are the largest single group, and along with people of part-European ancestry, they combine to make up approximately 80% of the population, or even more.
Language.
Spanish and Portuguese are the predominant languages of Latin America. Spanish is spoken as first language by about 60% of the population, Portuguese is spoken by about 34% of the population and about 6% of the population speak other languages such as Quechua, Mayan languages, Guaraní, Aymara, Nahuatl, English, French, Dutch and Italian. Portuguese is spoken only in Brazil (Brazilian Portuguese), the biggest and most populous country in the region. Spanish is the official language of most of the rest of the countries on the Latin American mainland (Spanish language in the Americas), as well as in Cuba, Puerto Rico (where it is co-official with English), and the Dominican Republic. French is spoken in Haiti and in the French overseas departments of Guadeloupe, Martinique and Guiana, and the French overseas collectivity of Saint Pierre and Miquelon; it is also spoken by some Panamanians of Afro-Antillean descent. Dutch is the official language in Suriname, Aruba, and the Netherlands Antilles. (As Dutch is a Germanic language, these territories are not necessarily considered part of Latin America.)
Native American languages are widely spoken in Peru, Guatemala, Bolivia, Paraguay and Mexico, and to a lesser degree, in Panama, Ecuador, Brazil, Colombia, Venezuela, Argentina, and Chile amongst other countries. In Latin American countries not named above, the population of speakers of indigenous languages tend to be very small or even non-existent (e.g. Uruguay). Mexico is possibly the only country that contains a wider variety of indigenous languages than any Latin American country, but the most spoken language is Nahuatl.
In Peru, Quechua is an official language, alongside Spanish and any other indigenous language in the areas where they predominate. In Ecuador, while holding no official status, the closely related Quichua is a recognized language of the indigenous people under the country's constitution; however, it is only spoken by a few groups in the country's highlands. In Bolivia, Aymara, Quechua and Guaraní hold official status alongside Spanish. Guaraní, along with Spanish, is an official language of Paraguay, and is spoken by a majority of the population (who are, for the most part, bilingual), and it is co-official with Spanish in the Argentine province of Corrientes. In Nicaragua, Spanish is the official language, but on the country's Caribbean coast English and indigenous languages such as Miskito, Sumo, and Rama also hold official status. Colombia recognizes all indigenous languages spoken within its territory as official, though fewer than 1% of its population are native speakers of these languages. Nahuatl is one of the 62 native languages spoken by indigenous people in Mexico, which are officially recognized by the government as "national languages" along with Spanish.
Other European languages spoken in Latin America include: English, by some groups in Puerto Rico, as well as in nearby countries that may or may not be considered Latin American, like Belize and Guyana; German, in southern Brazil, southern Chile portions of Argentina, Venezuela and Paraguay; Italian, in Brazil, Argentina, Venezuela, and Uruguay; Ukrainian and Polish in southern Brazil, and Welsh, in southern Argentina.
Yidish and Hebrew are possible to be heard around Buenos Aires and São Paulo especially. Non-European or Asian languages include Japanese in Brazil and Peru, Korean in Brazil, Arabic in Argentina, Brazil, Colombia and Venezuela and Chinese throughout South America.
In several nations, especially in the Caribbean region, creole languages are spoken. The most widely spoken creole language in Latin America and the Caribbean is Haitian Creole, the predominant language of Haiti; it is derived primarily from French and certain West African tongues with Amerindian, English, Portuguese and Spanish influences as well. Creole languages of mainland Latin America, similarly, are derived from European languages and various African tongues.
The Garifuna language is spoken along the Caribbean coast in Honduras, Guatemala, Nicaragua and Belize mostly by the Garifuna people a mixed race Zambo people who were the result of mixing between Indigenous Caribbeans and escaped Black slaves. Primarily an Arawakan language, it has influences from Caribbean and European languages.
Religion.
The vast majority of Latin Americans are Christians, mostly Roman Catholics belonging to the Latin Rite. About 70% of the Latin American population consider themselves Catholic.
Migration.
Due to economic, social and security developments that are affecting the region in recent decades, the focus is now the change from net immigration to net emigration. About 10 million Mexicans live in the United States. 28.3 million Americans listed their ancestry as Mexican as of 2006. According to the 2005 Colombian census or DANE, about 3,331,107 Colombians currently live abroad. The number of Brazilians living overseas is estimated at about 2 million people. An estimated 1.5 to two million Salvadorans reside in the United States. At least 1.5 million Ecuadorians have gone abroad, mainly to the United States and Spain. Approximately 1.5 million Dominicans live abroad, mostly in the United States. More than 1.3 million Cubans live abroad, most of them in the United States. It is estimated that over 800,000 Chileans live abroad, mainly in Argentina, the United States, Canada, Australia and Sweden. An estimated 700,000 Bolivians were living in Argentina as of 2006 and another 33,000 in the United States. Central Americans living abroad in 2005 were 3,314,300, of which 1,128,701 were Salvadorans, 685,713 were Guatemalans, 683,520 were Nicaraguans, 414,955 were Hondurans, 215,240 were Panamanians, 127,061 were Costa Ricans and 59,110 were Belizeans.
For the period 2000–2005, Chile, Costa Rica, Panama, and Venezuela were the only countries with global positive migration rates, in terms of their yearly averages.
Education.
Despite significant progress, education access and school completion remains unequal in Latin America. The region has made great progress in educational coverage; almost all children attend primary school and access to secondary education has increased considerably. Quality issues such as poor teaching methods, lack of appropriate equipment and overcrowding exist throughout the region. These issues lead to adolescents dropping out of the educational system early. Most educational systems in the region have implemented various types of administrative and institutional reforms that have enabled reach for places and communities that had no access to education services in the early 1990s. Compared to prior generations, Latin American youth have seen an increase in their levels of education. On average, they have completed two years schooling more than their parents.
However, there are still 23 million children in the region between the ages of 4 and 17 outside of the formal education system. Estimates indicate that 30% of preschool age children (ages 4–5) do not attend school, and for the most vulnerable populations, the poor and rural, this calculation exceeds 40 percent. Among primary school age children (ages 6 to 12), coverage is almost universal; however there is still a need to incorporate 5 million children in the primary education system. These children live mostly in remote areas, are indigenous or Afro-descendants and live in extreme poverty.
Among people between the ages of 13 and 17 years, only 80% are full-time students in the education system; among them only 66% advance to secondary school. These percentages are lower among vulnerable population groups: only 75% of the poorest youth between the ages of 13 and 17 years attend school. Tertiary education has the lowest coverage, with only 70% of people between the ages of 18 and 25 years outside of the education system. Currently, more than half of low income children or living in rural areas fail to complete nine years of education.
Crime and violence.
Latin America and the Caribbean have been cited by numerous sources to be the most dangerous regions in the world. Studies have shown that Latin America contains the majority of the world's most dangerous cities. Many analysts attribute the reason to why the region has such an alarming crime rate and criminal culture is largely due to social and income inequality within the region, they say that growing social inequality is fueling crime in the region. Many agree that the prison crisis will not be resolved until the gap between the rich and the poor is addressed.
Crime and violence prevention and public security are now important issues for governments and citizens in Latin America and the Caribbean region. Homicide rates in Latin America are the highest in the world. From the early 1980s through the mid-1990s, homicide rates increased by 50 percent. The major victims of such homicides are young men, 69 percent of whom are between the ages of 15 and 19 years old. Countries with the highest homicide rate per year per 100,000 inhabitants were: Honduras 91.6, El Salvador 69.2, Venezuela 45.1, Belize 41.4, Guatemala 38.5, Puerto Rico 26.2, Dominican Republic 25, Mexico 23.7, and Ecuador 18.2. Compared to the world average homicide rating of 6.9, the country with the highest rating in Latin America is more than 13x's the world average. The top 10 highest homicide rates per 100,000 inhabitants ever recorded since 1995 were entirely made up of countries from Latin America and they were El Salvador, Honduras, and Colombia with El Salvador scoring the highest homicide rate ever recorded at 139.1 back in 1995. In Colombia alone, one person was murdered every 10 minutes in 2005. Amnesty International has even named Latin America as the most dangerous region in the world for journalists to work. Crime-related violence in Latin America represents the most threat to public health, striking more victims than HIV/AIDS or other infectious diseases.
Countries with low crime rates in Latin America are Argentina, Chile, Cuba, Costa Rica, Nicaragua and Uruguay.
Economy.
Size.
According to Goldman Sachs' BRIC review of emerging economies, by 2050 the largest economies in the world will be as follows: China, United States, India, Brazil, and Mexico.
Standard of living.
The following table lists all the countries in Latin America indicating a valuation of the country's Human Development Index, GDP at purchasing power parity per capita, measurement of inequality through the Gini index, measurement of poverty through the Human Poverty Index, measurement of extreme poverty based on people living under 1.25 dollars a day, life expectancy, murder rates and a measurement of safety through the Global Peace Index. Green cells indicate the best performance in each category while red indicates the lowest.
Inequality.
Poverty continues to be one of the region's main challenges; according to the ECLAC, Latin America is the most unequal region in the world. Inequality is undermining the region's economic potential and the well-being of its population, since it increases poverty and reduces the impact of economic development on poverty reduction. Children in Latin America are often forced to seek work on the streets when their families can no longer afford to support them, leading to a substantial population of street children in Latin America. According to some estimates, there are 40 million street children in Latin America.
Inequality in Latin America has deep historical roots in the Latin European racially based Casta system instituted in Latin America in colonial times that have been difficult to eradicate since the differences between initial endowments and opportunities among social groups have constrained the poorest's social mobility, thus making poverty to be transmitted from generation to generation, becoming a vicious cycle. High inequality is rooted in the deepest exclusionary institutions of the Casta system that have been perpetuated ever since colonial times and that have survived different political and economic regimes. Inequality has been reproduced and transmitted through generations because Latin American political systems allow a differentiated access on the influence that social groups have in the decision making process, and it responds in different ways to the least favored groups that have less political representation and capacity of pressure. Recent economic liberalisation also plays a role as not everyone is equally capable of taking advantage of its benefits. Differences in opportunities and endowments tend to be based on race, ethnicity, rurality and gender. Because inequality in gender and location are near universal, race and ethnicity play a larger, more integral role in the unequal discriminatory practices in Latin America. These differences have a strong impact on the distribution of income, capital and political standing.
In 2008, According to UNICEF, Latin America and the Caribbean region had the highest combined income inequality in the world with a measured net Gini coefficient of 48.3, an unweighted average which is considerably higher than the world's Gini coefficient average of 39.7. Gini is the statistical measurement used to measure income distribution across entire nations and their populations and their income inequality. The other regional averages were: sub-Saharan Africa (44.2), Asia (40.4), Middle East and North Africa (39.2), Eastern Europe and Central Asia (35.4), and high-income nations (30.9).
According to a study by the World Bank,the richest decile of the population of Latin America earn 48% of the total income, while the poorest 10% of the population earn only 1.6% of the income. In contrast, in developed countries, the top decile receives 29% of the total income, while the bottom decile earns 2.5%. The countries with the highest inequality in the region (as measured with the Gini index in the UN Development Report) in 2007 were Haiti (59.5), Colombia (58.5), Bolivia (58.2), Honduras (55.3), Brazil (55.0), and Panama (54.9), while the countries with the lowest inequality in the region were Venezuela (43.4), Uruguay (46.4) and Costa Rica (47.2).
According to the World Bank, the poorest countries in the region were (as of 2008): Haiti, Nicaragua, Bolivia and Honduras. Undernourishment affects to 47% of Haitians, 27% of Nicaraguans, 23% of Bolivians and 22% of Hondurans.
Many countries in Latin America have responded to high levels of poverty by implementing new, or altering old, social assistance programs such as conditional cash transfers. These include Mexico's Progresa Oportunidades, Brazil's Bolsa Escola and Bolsa Familia, Panama's Red de Oportunidades and Chile's Chile Solidario. In general, these programs provide money to poor families under the condition that those transfers are used as an investment on their children's human capital, such as regular school attendance and basic preventive health care. The purpose of these programs is to address the inter-generational transmission of poverty and to foster social inclusion by explicitly targeting the poor, focusing on children, delivering transfers to women, and changing social accountability relationships between beneficiaries, service providers and governments. These programs have helped to increase school enrollment and attendance and they also have shown improvements in children's health conditions. Most of these transfer schemes are now benefiting around 110 million people in the region and are considered relatively cheap, costing around 0.5% of their GDP. In some countries e.g. in Peru decentralisation is hoped to help addressing social justice and poverty better. NGOs which addressed those problems on the local level before could help with that.
Trade blocs.
The major trade blocs (or agreements) in the region are the Pacific Alliance and the Union of South American Nations, composed of the integrated Mercosur and Andean Community of Nations (CAN). Minor blocs or trade agreements are the G3 Free Trade Agreement, the Dominican Republic – Central America Free Trade Agreement (DR-CAFTA) and the Caribbean Community (CARICOM). However, major reconfigurations are taking place along opposing approaches to integration and trade; Venezuela has officially withdrawn from both the CAN and G3 and it has been formally admitted into the Mercosur (pending ratification from the Paraguayan legislature). The president-elect of Ecuador has manifested his intentions of following the same path. This bloc nominally opposes any Free Trade Agreement (FTA) with the United States, although Uruguay has manifested its intention otherwise. Chile has already signed an FTA with Canada, and along with Peru, Colombia and Mexico are the only four Latin American nations that have an FTA with the United States, the latter being a member of the North American Free Trade Agreement (NAFTA).
Tourism.
Income from tourism is key to the economy of several Latin American countries. Thanks to its proximity to the USA, Mexico receives the largest number of international tourists, with 22.3 million visitors in 2010, followed by Argentina, with 5.2 million in 2010; Brazil, with 5.1 million; Puerto Rico, with 3.6 million, Chile with 2.7 million, Colombia with 2.385 million; Dominican Republic, with 4.1 million and Panama with 2.06 million. Places such as Cancún, Galápagos Islands, Machu Picchu, Chichen Itza, Cartagena de Indias, Cabo San Lucas, Acapulco, Rio de Janeiro, Salvador, Margarita Island, São Paulo, Salar de Uyuni, Punta del Este, Santo Domingo, Labadee, San Juan, La Habana, Panama City, Iguazu Falls, Puerto Vallarta, Poás Volcano National Park, Punta Cana, Viña del Mar, Mexico City, Quito, Bogotá, Santa Marta, San Andrés, Buenos Aires, Lima, Maceió, Florianópolis, Cuzco, Ponce and Patagonia are popular among international visitors in the region.
Culture.
Latin American culture is a mixture of many cultural expressions worldwide. It is the product of many diverse influences:
Art.
Beyond the rich tradition of indigenous art, the development of Latin American visual art owed much to the influence of Spanish, Portuguese and French Baroque painting, which in turn often followed the trends of the Italian Masters. In general, this artistic Eurocentrism began to fade in the early twentieth century, as Latin-Americans began to acknowledge the uniqueness of their condition and started to follow their own path.
From the early twentieth century, the art of Latin America was greatly inspired by the Constructivist Movement. The Movement quickly spread from Russia to Europe and then into Latin America. Joaquín Torres García and Manuel Rendón have been credited with bringing the Constructivist Movement into Latin America from Europe.
An important artistic movement generated in Latin America is "muralism" represented by Diego Rivera, David Alfaro Siqueiros, José Clemente Orozco and Rufino Tamayo in Mexico, Santiago Martinez Delgado and Pedro Nel Gómez in Colombia and Antonio Berni in Argentina. Some of the most impressive "Muralista" works can be found in Mexico, Colombia, New York City, San Francisco, Los Angeles and Philadelphia.
Painter Frida Kahlo, one of the most famous Mexican artists, painted about her own life and the Mexican culture in a style combining Realism, Symbolism and Surrealism. Kahlo's work commands the highest selling price of all Latin American paintings.
Colombian sculptor and painter Fernando Botero is also widely known by his works which, on first examination, are noted for their exaggerated proportions and the corpulence of the human and animal figures.
Film.
Latin American film is both rich and diverse. Historically, the main centers of production have been Mexico, Argentina, Brazil, and Cuba.
Latin American film flourished after sound was introduced in cinema, which added a linguistic barrier to the export of Hollywood film south of the border. The 1950s and 1960s saw a movement towards Third Cinema, led by the Argentine filmmakers Fernando Solanas and Octavio Getino. More recently, a new style of directing and stories filmed has been tagged as "New Latin American Cinema".
Mexican cinema started out in the silent era from 1896 to 1929 and flourished in the Golden Era of the 1940s. It boasted a huge industry comparable to Hollywood at the time with stars such as María Félix, Dolores del Río, and Pedro Infante. In the 1970s, Mexico was the location for many cult horror and action movies. More recently, films such as "Amores Perros" (2000) and "Y tu mamá también" (2001) enjoyed box office and critical acclaim and propelled Alfonso Cuarón and Alejandro González Iñarritu to the front rank of Hollywood directors. Alejandro González Iñárritu directed in (2010) Biutiful and Brirdman (2014), Alfonso Cuarón directed Harry Potter and the Prisoner of Azkaban in (2004) and "Gravity" (2013). Guillermo del Toro close friend and also a front rank Hollywood director in Hollywood and Spain, directed Pan's Labyrinth (2006) and produce El Orfanato (2007). Carlos Carrera (The Crime of Father Amaro), and screenwriter Guillermo Arriaga are also some of the most known present-day Mexican film makers. "Rudo y Cursi" released in December (2008) in Mexico directed by Carlos Cuarón.
Argentine cinema has also been prominenent since the first half of the 20th century and today averages over 60 full-length titles yearly. The industry suffered during the 1976–1983 military dictatorship; but re-emerged to produce the Academy Award winner "The Official Story" in 1985. A wave of imported U.S. films again damaged the industry in the early 1990s, though it soon recovered, thriving even during the Argentine economic crisis around 2001. Many Argentine movies produced during recent years have been internationally acclaimed, including "Nueve reinas" (2000), "Son of the bride" (2001), "El abrazo partido" (2004), "El otro" (2007), the 2010 Foreign Language Academy Award winner "El secreto de sus ojos" and Wild Tales (2014).
In Brazil, the "Cinema Novo" movement created a particular way of making movies with critical and intellectual screenplays, a clearer photography related to the light of the outdoors in a tropical landscape, and a political message. The modern Brazilian film industry has become more profitable inside the country, and some of its productions have received prizes and recognition in Europe and the United States, with movies such as "Central do Brasil" (1999), "Cidade de Deus" (2002) and "Tropa de Elite" (2007).
Puerto Rican cinema has produced some notable films, such as Una Aventura Llamada Menudo, Los Diaz de Doris and Casi Casi. An influx of Hollywood films affected the local film industry in Puerto Rico during the 1980s and 1990s, but several Puerto Rican films have been produced since and it has been recovering.
Cuban cinema has enjoyed much official support since the Cuban revolution and important film-makers include Tomás Gutiérrez Alea.
It is also worth noting that many Latin Americans have achieved significant success within Hollywood, for instance Carmen Miranda (Portuguese-Brazilian), Salma Hayek (Mexican), and Benicio del Toro (Puerto Rican), while Mexican Americans such as Robert Rodriguez have also made their mark in film production.
Literature.
Pre-Columbian cultures were primarily oral, though the Aztecs and Mayans, for instance, produced elaborate codices. Oral accounts of mythological and religious beliefs were also sometimes recorded after the arrival of European colonizers, as was the case with the Popol Vuh. Moreover, a tradition of oral narrative survives to this day, for instance among the Quechua-speaking population of Peru and the Quiché (K'iche') of Guatemala.
From the very moment of Europe's discovery of the continents, early explorers and conquistadores produced written accounts and crónicas of their experience – such as Columbus's letters or Bernal Díaz del Castillo's description of the conquest of Mexico. During the colonial period, written culture was often in the hands of the church, within which context Sor Juana Inés de la Cruz wrote memorable poetry and philosophical essays. Towards the end of the 18th Century and the beginning of the 19th, a distinctive criollo literary tradition emerged, including the first novels such as Lizardi's "El Periquillo Sarniento" (1816).
The 19th century was a period of "foundational fictions" (in critic Doris Sommer's words), novels in the Romantic or Naturalist traditions that attempted to establish a sense of national identity, and which often focussed on the indigenous question or the dichotomy of "civilization or barbarism" (for which see, say, Domingo Sarmiento's "Facundo" (1845), Juan León Mera's "Cumandá" (1879), or Euclides da Cunha's "Os Sertões" (1902)). The 19th century also witnessed the realist work of Machado de Assis, who made use of surreal devices of metaphor and playful narrative construction, much admired by critic Harold Bloom.
At the turn of the 20th century, "modernismo" emerged, a poetic movement whose founding text was Nicaraguan poet Rubén Darío's "Azul" (1888). This was the first Latin American literary movement to influence literary culture outside of the region, and was also the first truly Latin American literature, in that national differences were no longer so much at issue. José Martí, for instance, though a Cuban patriot, also lived in Mexico and the United States and wrote for journals in Argentina and elsewhere.
However, what really put Latin American literature on the global map was no doubt the literary boom of the 1960s and 1970s, distinguished by daring and experimental novels (such as Julio Cortázar's "Rayuela" (1963)) that were frequently published in Spain and quickly translated into English. The Boom's defining novel was Gabriel García Márquez's "Cien años de soledad" (1967), which led to the association of Latin American literature with magic realism, though other important writers of the period such as the Peruvian Mario Vargas Llosa and Carlos Fuentes do not fit so easily within this framework. Arguably, the Boom's culmination was Augusto Roa Bastos's monumental "Yo, el supremo" (1974). In the wake of the Boom, influential precursors such as Juan Rulfo, Alejo Carpentier, and above all Jorge Luis Borges were also rediscovered.
Contemporary literature in the region is vibrant and varied, ranging from the best-selling Paulo Coelho and Isabel Allende to the more avant-garde and critically acclaimed work of writers such as Diamela Eltit, Giannina Braschi, Ricardo Piglia, or Roberto Bolaño. There has also been considerable attention paid to the genre of testimonio, texts produced in collaboration with subaltern subjects such as Rigoberta Menchú. Finally, a new breed of chroniclers is represented by the more journalistic Carlos Monsiváis and Pedro Lemebel.
The region boasts six Nobel Prize winners: in addition to the two Chilean poets Gabriela Mistral (1945) and Pablo Neruda (1971), there is also the Colombian writer Gabriel García Márquez (1982), the Guatemalan novelist Miguel Ángel Asturias (1967), the Mexican poet and essayist Octavio Paz (1990), and the Peruvian novelist Mario Vargas Llosa (2010).
Music and dance.
Latin America has produced many successful worldwide artists in terms of recorded global music sales. Among the most successful have been Gloria Estefan (Cuba), Mercedes Sosa (Argentina), Roberto Carlos (Brazil), Carlos Santana (Mexico) of whom have sold over 90 million records, Luis Miguel (Mexico), Shakira (Colombia) and Vicente Fernández (Mexico) with over 50 million records sold worldwide.
Other notable successful mainstream acts through the years, include Soda Stereo, Celia Cruz, Thalía, Ricky Martin, Marc Anthony, Selena, Lynda Thomas and Menudo.
Caribbean Hispanic music, such as merengue, bachata, salsa, and more recently reggaeton, from such countries as the Dominican Republic, Puerto Rico, Trinidad and Tobago, Cuba, and Panama has been strongly influenced by African rhythms and melodies. Haiti's compas is a genre of music that draws influence and is thus similar to its Caribbean Hispanic counterparts, with an element of jazz and modern sound as well.
Another well-known Latin American musical genre includes the Argentine and Uruguayan tango (with Carlos Gardel as the greatest exponent), as well as the distinct nuevo tango, a fusion of tango, acoustic and electronic music popularized by bandoneón virtuoso Ástor Piazzolla. Samba, North American jazz, European classical music and choro combined to form "bossa nova" in Brazil, popularized by guitarrist João Gilberto and pianist Antonio Carlos Jobim.
Other influential Latin American sounds include the Antillean soca and calypso, the Honduras (Garifuna) punta, the Colombian cumbia and vallenato, the Chilean cueca, the Ecuadorian boleros, and rockoleras, the Mexican ranchera and the mariachi which is the epitome of Mexican soul, the Nicaraguan palo de Mayo, the Peruvian marinera and tondero, the Uruguayan candombe, the French Antillean zouk (derived from Haitian compas) and the various styles of music from pre-Columbian traditions that are widespread in the Andean region.
The classical composer Heitor Villa-Lobos (1887–1959) worked on the recording of native musical traditions within his homeland of Brazil. The traditions of his homeland heavily influenced his classical works. Also notable is the recent work of the Cuban Leo Brouwer and guitar work of the Venezuelan Antonio Lauro and the Paraguayan Agustín Barrios. Latin America has also produced world-class classical performers such as the Chilean pianist Claudio Arrau, Brazilian pianist Nelson Freire and the Argentine pianist and conductor Daniel Barenboim.
Arguably, the main contribution to music entered through folklore, where the true soul of the Latin American and Caribbean countries is expressed. Musicians such as Yma Súmac, Chabuca Granda, Atahualpa Yupanqui, Violeta Parra, Víctor Jara, Jorge Cafrune, Facundo Cabral, Mercedes Sosa, Jorge Negrete, Luiz Gonzaga, Caetano Veloso, Susana Baca, Chavela Vargas, Simon Diaz, Julio Jaramillo, Toto la Momposina as well as musical ensembles such as Inti Illimani and Los Kjarkas are magnificent examples of the heights that this soul can reach.
Latin pop, including many forms of rock, is popular in Latin America today (see Spanish language rock and roll).
More recently, Reggaeton, which blends Jamaican reggae and dancehall with Latin America genres such as bomba and plena, as well as that of hip hop, is becoming more popular, in spite of the controversy surrounding its lyrics, dance steps (Perreo) and music videos. It has become very popular among populations with a "migrant culture" influence – both Latino populations in the United States, such as southern Florida and New York City, and parts of Latin America where migration to the United States is common, such as Puerto Rico, Trinidad and Tobago, Dominican Republic, Colombia, Ecuador, El Salvador, and Mexico.

</doc>
<doc id="18603" url="http://en.wikipedia.org/wiki?curid=18603" title="Lydia Kavina">
Lydia Kavina

Lydia Yevgenyevna Kavina (Лидия Евгеньевна Кавина; born 8 September 1967) is a Russian theremin player, and is currently the leading performing musician on the instrument.
The granddaughter of Léon Theremin's first cousin, a Soviet anthropologist and primatologist , Kavina was born in Moscow and began studying the instrument under the direction of Léon Theremin when she was nine years old. Five years later, she gave her first theremin concert, which marked the beginning of a musical career that has so far led to more than 1000 theatre, radio, and television performances around the world.
Kavina has appeared as a solo performer at such prestigious venues as the "Bolshoi Zal" (Great Hall) of the Moscow Conservatory, Moscow International Art Centre with National Philarmonic of Russia under Vladimir Spivakov and Bellevue Palace in Berlin, the residence of the German President. She has also performed at leading festivals, including Caramoor with the Orchestra St. Luke's, New York's Lincoln Center Festival, Holland Music Festival, Martinu Festival, Electronic Music Festival in Burge and Moscow “Avantgarde”.
Kavina performs most of the classical theremin repertoire, including popular works for theremin by Bohuslav Martinů, Joseph Schillinger, and "Spellbound" by Miklos Rozsa, as well as "Equatorial" by Edgard Varèse and the lesser known "Testament" by Nicolas Obouchov.
In addition to giving concerts, Kavina is a composer of music for theremin and teaches the instrument in Western Europe, Russia and the United States. Together with the London Philharmonic Orchestra, she played theremin for Howard Shore's soundtrack of the Oscar-winning film "Ed Wood", as well as for "eXistenZ" (also by Shore) and "The Machinist". Additionally, Kavina has recorded several compact discs and is the subject of an instructional video from the theremin manufacturer Moog Music. She was also featured in stage productions such as "Alice" and "The Black Rider" (both conceived and directed by Robert Wilson, with music by Tom Waits) at the Thalia Theater in Hamburg, and in collaboration with the Russian experimental surf band Messer Chups.
Lydia Kavina is an active promoter of new experimental music for the theremin. In collaboration with Barbara Buchholz and Kamerensemble Neue Musik Berlin, Kavina performed a number of concerts of contemporary works for theremin in Germany in 2005–2007 as part of the "Touch! Don't Touch! – Music for Theremin" project.
The most notable project of her recent career is her theremin solo in "The Little Mermaid", a ballet by Lera Auerbach, choreographed by John Neumeier in Copenhagen New Opera Haus and Hamburg State Opera (2007).
Kavina has completed a number of her own compositions for theremin including a Concerto for Theremin and Symphony Orchestra, first performed by the Boston Modern Orchestra under the direction of Gil Rose.
Kavina holds a degree in composition from The Moscow Conservatory, where she also completed a postgraduate assistantship program.
CDs.
Collaborations

</doc>
<doc id="18606" url="http://en.wikipedia.org/wiki?curid=18606" title="Liberation Tigers of Tamil Eelam">
Liberation Tigers of Tamil Eelam

The Liberation Tigers of Tamil Eelam (Tamil: தமிழீழ விடுதலைப் புலிகள் "Tamiḻīḻa viṭutalaip pulikaḷ", Sinhalese: දෙමළ ඊළාම් විමුක්ති කොටි "Dhemala īlām vimukthi koti", commonly known as the LTTE or the Tamil Tigers) is a now-defunct militant organisation that was based in northern Sri Lanka. Founded in May 1976 by Velupillai Prabhakaran, it waged a secessionist nationalist insurgency to create an monoethnic, independent state in the north and east of Sri Lanka for Tamil people. This campaign led to the Sri Lankan Civil War, which ran from 1983 until 2009, when the LTTE was decisively defeated by the Sri Lankan Military under the leadership of President Mahinda Rajapaksa.
At the height of its power, the LTTE possessed a well-developed militia and carried out many high-profile attacks, including the assassinations of several high-ranking Sri Lankan and Indian politicians. The LTTE was the only militant group to assassinate two world leaders: former Indian Prime Minister Rajiv Gandhi in 1991 and Sri Lankan President Ranasinghe Premadasa in 1993. The LTTE invented suicide belts and pioneered the use of women in suicide attacks. It was the first militant group to acquire air power and used light aircraft in some of its attacks. It is currently proscribed as a terrorist organisation by 32 countries, including United States, European Union and India, but has support amongst some Tamils in Tamil Nadu in India. Velupillai Prabhakaran headed the organisation from its inception until his death in 2009.
Historical inter ethnic imbalances between majority Sinhalese and minority Tamil populations are alleged to have created the background for the origin of the LTTE. Post independent Sri Lankan governments attempted to rectify the disproportionate favouring and empowerment of Tamil minority by the colonial rulers, which led to exclusivist ethnic policies including the ″Sinhala only act″ and gave rise to separatist ideologies among many Tamil leaders. By the 1970s, initial non violent political struggle for an independent mono ethnic Tamil state was leveraged as justification for a violent secessionist insurgency led by the LTTE. Over the course of the conflict, the Tamil Tigers frequently exchanged control of territory in north-east Sri Lanka with the Sri Lankan military, with the two sides engaging in fierce military confrontations. It was involved in four unsuccessful rounds of peace talks with the Sri Lankan government over the course of the conflict. At its peak in 2000, the LTTE was in control of 76% of the landmass in the Northern and Eastern provinces of Sri Lanka.
At the start of the final round of peace talks in 2002, the Tamil Tigers, with control of 15,000 km2 area, ran a virtual mini-state. After the breakdown of the peace process in 2006, the Sri Lankan military launched a major offensive against the Tigers, defeating the LTTE militarily and bringing the entire country under its control. Victory over the Tigers was declared by Sri Lankan President Mahinda Rajapaksa on 16 May 2009, and the LTTE admitted defeat on 17 May 2009. Prabhakaran was killed by government forces on 19 May 2009 and Selvarasa Pathmanathan succeeded Prabhakaran as leader of the LTTE, who was later arrested in Malaysia and handed over to the Sri Lankan government in August 2009.
History.
Background.
In the early 1970s, United Front government of Sirimavo Bandaranaike introduced the policy of standardisation to rectify the low numbers of Sinhalese being accepted into university in Sri Lanka. A student named Satiyaseelan formed "Tamil Manavar Peravai" (Tamil Students League) to counter this biased move. This group comprised Tamil youth who advocated the rights of students to have fair enrolment. Inspired by the failed 1971 insurrection of Janatha Vimukthi Peramuna, it was the first Tamil insurgent group of its kind. It consisted of around 40 Tamil youth, including Ponnuthurai Sivakumaran (later, the leader of the Sivakumaran group), K. Pathmanaba (one of the founder members of EROS) and Velupillai Prabhakaran, an 18-year-old youth from single caste-oriented Valvettithurai (VVT). In 1972, Prabhakaran teamed up with Chetti Thanabalasingam, Jaffna to form the Tamil New Tigers (TNT), with Thanabalasingham as its leader. After he was killed, Prabhakaran took over. At the same time, Nadarajah Thangathurai and Selvarajah Yogachandran (better known by his "nom de guerre" Kuttimani) were also involved in discussions about an insurgency. They would later (in 1979) create a separate organisation named Tamil Eelam Liberation Organization (TELO) to campaign for the establishment of an independent Tamil Eelam. These groups, along with another prominent figure of the armed struggle, Ponnuthurai Sivakumaran, were involved in several hit-and-run operations against pro-government Tamil politicians, Sri Lanka Police and civil administration during the early 1970s. These attacks included throwing bombs at the residence and the car of SLFP Jaffna Mayor, Alfred Duraiyappah, placing a bomb at a carnival held in the stadium of Jaffna city (now "Duraiyappah stadium") and Neervely bank robbery. The Tamil conference incident in 1974 also sparked the anger of these militant groups. Both Sivakumaran and Prabhakaran attempted to assassinate Duraiyappah in revenge for the incident. Sivakumaran committed suicide on 5 June 1974, to evade capture by Police. But on 27 July 1975, Prabhakaran was able to assassinate Duraiyappah, who was branded as a "traitor" by TULF and the insurgents alike. Prabhakaran himself shot and killed the Mayor when he was visiting the Krishnan temple at Ponnalai.
Founding and rise to power.
The LTTE was founded on 5 May 1976 as the successor to the Tamil New Tigers. Uma Maheswaran became its leader, and Prabhakaran, its military commander. A 5-member committee was also appointed. Prabhakaran sought to "refashion the old TNT/new LTTE into an elite, ruthlessly efficient, and highly professional fighting force", notes the terrorism expert Rohan Gunaratna. Prabhakaran kept the numbers of the group small, and maintained a high standard of training. The LTTE carried out low-key attacks against various government targets, including policemen and local politicians. The ideology of the Tamil Tigers emerged from Marxist–Leninist though its leadership is considered atheist.
TULF support.
Tamil United Liberation Front leader Appapillai Amirthalingam, who was in 1977 elected as the Opposition leader of Sri Lanka Parliament clandestinely supported the LTTE. Amirthalingam believed that if he could exercise control over the Tamil insurgent groups, it would enhance his political position and pressurise government to agree to his demand, which was to grant political autonomy to Tamils. Thus, he even provided letters of reference to the LTTE and to other Tamil insurgent groups to raise funds. Both Uma Maheswaran (a former surveyor) and Urmila Kandiah, first female member of the LTTE were prominent members of the TULF youth wing. Maheswaran was the secretary of TULF Tamil Youth Forum, Colombo branch. Amirthalingam introduced Prabhakaran to N. S. Krishnan, who later became the first international representative of LTTE. It was Krishnan, who introduced Prabhakaran to Anton Balasingham, who later became the chief political strategist and chief negotiator of LTTE, which split for the first time in 1979. Uma Maheswaran was found out having a love affair with Urmila Kandiah. It was against the code of conduct of LTTE. Prabhakaran ordered him to leave the organisation. Uma Maheswaran left LTTE and formed People's Liberation Organisation of Tamil Eelam (PLOTE) in 1980.
Meanwhile in 1980, J. R. Jayawardene government agreed to devolve power by the means of District Development Councils upon the request of TULF. But by this time, LTTE and other insurgent groups were not prepared to accept any solution less than a separate state. They had no faith in any sort of political solution. Thus the TULF and other Tamil political parties were steadily marginalised and insurgent groups emerged as the major force in the north. During this period of time several other insurgent groups came into the arena, such as EROS (1975), TELO (1979), PLOTE (1980), EPRLF (1980) and TELA (1982). LTTE ordered civilians to boycott the local government elections of 1983 in which even TULF contested. Voter turnout became as low as 10%. Thereafter, Tamil political parties had very little room to represent Tamil people as insurgent groups took over their position.
Thirunelveli attack, 1983.
The LTTE carried out its first major attack on 23 July 1983, when they ambushed Sri Lanka Army patrol Four Four Bravo at Thirunelveli, Jaffna. Thirteen Sri Lankan servicemen were killed in the attack, leading to the Black July.
Some consider Black July to be a planned rampage against the Tamil community of Sri Lanka, in which the JVP movement and sections of government were implicated.
Many outraged Tamil youths joining Tamil militant groups to fight the Sri Lankan government, in what is considered a major catalyst to the insurgency in Sri Lanka.
Indian support.
In reaction to various geo-political ("see Indian intervention in the Sri Lankan Civil War") and economic factors, from August 1983 to May 1987, India, through its intelligence agency Research and Analysis Wing (RAW), provided arms, training and monetary support to 6 Sri Lankan Tamil insurgent groups including LTTE. During that period, 32 camps were set up all over India to train these 495 LTTE insurgents, including 90 women who were trained in 10 batches. First batch of Tigers were trained in Establishment 22 based in Chakrata, Uttarakhand. The second batch, including LTTE intelligence chief Pottu Amman, trained in Himachal Pradesh. Prabakaran himself visited the first and the second batch of Tamil Tigers to see them training. Eight other batches of LTTE were trained in Tamil Nadu. Ironically, Thenmozhi Rajaratnam "alias" Dhanu, who carried out the assassination of Rajiv Gandhi and Sivarasan—the key conspirator were among the militants trained by RAW, in Nainital, India.
In April 1984, the LTTE formally joined a common militant front, the Eelam National Liberation Front (ENLF), a union between LTTE, the Tamil Eelam Liberation Organization (TELO), the Eelam Revolutionary Organisation of Students (EROS), the People's Liberation Organisation of Tamil Eelam (PLOTE) and the Eelam People's Revolutionary Liberation Front (EPRLF).
Clashes with other insurgent groups.
TELO usually held the Indian view of problems and pushed for India's view during peace talks with Sri Lanka and other groups. LTTE denounced the TELO view and claimed that India was only acting on its own interest. As a result, the LTTE broke from the ENLF in 1986. Soon fighting broke out between the TELO and the LTTE and clashes occurred over the next few months. As a result almost the entire TELO leadership and at least 400 TELO militants were killed by the LTTE. The LTTE attacked training camps of the EPRLF a few months later, forcing it to withdraw entirely from the Jaffna peninsula. The LTTE then demanded that all remaining Tamil insurgents join the LTTE. Notices were issued to that effect in Jaffna and in Madras, where the Tamil groups were headquartered. With the major groups including the TELO and EPRLF eliminated, the remaining twenty or so Tamil insurgent groups were then absorbed into the LTTE, making Jaffna an LTTE-dominated city.
LTTE's practice such as wearing a cyanide vial for consumption if captured appealed to the Tamil people as dedication and sacrifice.
Another practice that increased support by Tamil people was LTTE's members taking an oath of loyalty which stated LTTE's goal of establishing a state for the Sri Lankan Tamils. In 1987 LTTE established the Black Tigers, a unit responsible for conducting suicide attacks against political, economic, and military targets,<ref name='bbc-11/26/02'></ref> and launched its first suicide attack against a Sri Lankan Army camp, killing 40 soldiers. LTTE members are prohibited from smoking cigarettes and consuming alcohol in any form. LTTE members must avoid their family members and avoid communication with them. Initially LTTE members were prohibited from having love affairs or sexual relationships as it could deter their prime motive. But this policy had changed since Prabhakaran married Mathivathani Erambu in October 1984.
IPKF period.
In July 1987, faced with growing anger among its own Tamils and a flood of refugees, India intervened directly in the conflict for the first time by initially airdropping food parcels into Jaffna. After negotiations, India and Sri Lanka entered into the Indo-Sri Lanka Accord. Though the conflict was between the Tamil and Sinhalese people, India and Sri Lanka signed the peace accord instead of India influencing both parties to sign a peace accord among themselves. The peace accord assigned a certain degree of regional autonomy in the Tamil areas, with Eelam People's Revolutionary Liberation Front (EPRLF) controlling the regional council and called for the Tamil militant groups to lay down their arms. India was to send a peacekeeping force, named the Indian Peace Keeping Force (IPKF), part of the Indian Army, to Sri Lanka to enforce the disarmament and to watch over the regional council.
War against IPKF.
Although the Tamil militant organisations did not have a role in the Indo-Lanka agreement, most groups, including EPRLF, TELO, EROS, and PLOTE, accepted it. But LTTE rejected the accord because they opposed EPRLF's Varadaraja Perumal as the chief ministerial candidate for the merged North Eastern Province. The LTTE named three alternate candidates for the position, which India rejected. The LTTE subsequently refused to hand over their weapons to the IPKF. After three months of tensions, LTTE declared war on IPKF on 7 October 1987.
Thus LTTE found itself engaged in military conflict with the Indian Army, and launched its first attack on an Indian army rations truck on 8 October, killing five Indian para-commandos who were on board by strapping burning tires around their necks. The government of India decided that the IPKF should disarm the LTTE by force. The Indian Army launched number of assaults on the LTTE, including a month-long campaign dubbed "Operation Pawan" to win control of the Jaffna Peninsula. The ruthlessness of this campaign, and the Indian army's subsequent anti-LTTE operations, made it extremely unpopular among many Tamils in Sri Lanka.
Premadasa government support.
The Indian intervention was also unpopular among the Sinhalese majority. Prime Minister Ranasinghe Premadasa during his presidential election campaign in 1988, pledged to withdraw IPKF as soon as he is elected president. After being elected, in April 1989, he started negotiations with LTTE. President Premadasa ordered Sri Lanka Army to clandestine handed over arms consignments to the LTTE to fight the IPKF and its proxy, the Tamil National Army (TNA). These consignments include RPG guns, mortars, self-loading rifles, T81 automatic rifles, T56 automatic rifles, pistols, hand grenades, ammunition, and communications sets. Moreover, millions of dollars was also passed on to the LTTE.
After IPKF.
IPKF became bogged down in the fighting with the Tamil Tigers for nearly three years, experiencing heavy losses. The last members of the IPKF, which was estimated to have had a strength of well over 100,000 at its peak, left the country in March 1990 upon the request of President Premadasa. A shaky peace initially held between the government and the LTTE, and peace talks progressed towards providing devolution for Tamils in the north and east of the country. A ceasefire held between LTTE and the government from June 1989 to June 1990, but broke down as LTTE massacred 600 police officers in the Eastern Province.
Fighting continued throughout the 1990s, and was marked by two key assassinations carried out by the LTTE: those of former Indian Prime Minister Rajiv Gandhi in 1991, and Sri Lankan President Ranasinghe Premadasa in 1993, using suicide bombers on both occasions. The fighting briefly halted in 1994 following the election of Chandrika Kumaratunga as President of Sri Lanka and the onset of peace talks, but fighting resumed after LTTE sank two Sri Lanka Navy boats in April 1995. In a series of military operations that followed, the Sri Lanka Army recaptured the Jaffna Peninsula. Further offensives followed over the next three years, and the military captured vast areas in the north of the country from the LTTE, including areas in the Vanni region, the town of Kilinochchi, and many smaller towns. From 1998 onward, the LTTE regained control of these areas, which culminated in the capture in April 2000 of the strategically important Elephant Pass base complex, located at the entrance of the Jaffna Peninsula, after prolonged fighting against the Sri Lanka Army.
Mahattaya, a one-time deputy leader of LTTE, was accused of treason by the LTTE and killed in 1994. He is said to have collaborated with the Indian Research and Analysis Wing to remove Prabhakaran from the LTTE leadership.
2002 ceasefire.
In 2002, the LTTE dropped its demand for a separate state. Instead, it demanded a form of regional autonomy. Following the landslide election defeat of Kumaratunga and the coming to power of Ranil Wickramasinghe in December 2001, the LTTE declared a unilateral ceasefire. The Sri Lankan Government agreed to the ceasefire, and in March 2002 the Ceasefire Agreement (CFA) was signed. As part of the agreement, Norway and other Nordic countries agreed to jointly monitor the ceasefire through the Sri Lanka Monitoring Mission.
Six rounds of peace talks between the Government of Sri Lanka and LTTE were held, but they were temporarily suspended after the LTTE pulled out of the talks in 2003 claiming "certain critical issues relating to the ongoing peace process". In 2003 the LTTE proposed an Interim Self Governing Authority (ISGA). This move was welcomed by the international community but rejected by the Sri Lankan President. The LTTE boycotted the presidential election in December 2005. While LTTE claimed that the people under its control were free to vote, it is alleged that they used threats to prevent the population from voting. The United States condemned this act.
The new government of Sri Lanka came into power in 2006 and demanded to abrogate the ceasefire agreement, stating that the only possible solution to the ethnic conflict was a military solution, and that the only way to achieve this was by eliminating the Liberation Tigers of Tamil. Further peace talks were scheduled in Oslo, Norway, on 8 and 9 June 2006, but cancelled when the LTTE refused to meet directly with the government delegation, stating its fighters were not being allowed safe passage to travel to the talks. Norwegian mediator Erik Solheim told journalists that the LTTE should take direct responsibility for the collapse of the talks. Rifts grew between the government and LTTE, and resulted in a number of ceasefire agreement violations by both sides during 2006. Suicide attacks, military skirmishes, and air raids took place during the latter part of 2006. Between February 2002 to May 2007, the Sri Lanka Monitoring Mission documented 3,830 ceasefire violations by the LTTE, with respect to 351 by the security forces. Military confrontation continued into 2007 and 2008. On January 2008 the government officially pulled out of the Cease Fire Agreement.
Dissension.
In the biggest show of dissent from within the organisation, a senior LTTE commander named Colonel Karuna ("nom de guerre" of Vinayagamoorthi Muralitharan) broke away from the LTTE in March 2004 and formed the TamilEela Makkal Viduthalai Pulikal (later Tamil Makkal Viduthalai Pulikal), amid allegations that the northern commanders were overlooking the needs of the eastern Tamils. The LTTE leadership accused him of mishandling funds and questioned him about his recent personal behaviour. He tried to take control of the eastern province from the LTTE, which caused clashes between the LTTE and TMVP. The LTTE has suggested that TMVP was backed by the government, and the Nordic SLMM monitors corroborated this.
Military defeat.
Mahinda Rajapaksa was elected as the president of Sri Lanka in 2005. After a brief period of negotiations, LTTE pulled out of peace talks indefinitely. Sporadic violence had continued and on 25 April 2006, LTTE tried to assassinate Sri Lankan Army Commander Lieutenant General Sarath Fonseka. Following the attack, the European Union decided to proscribe the LTTE as a terrorist organisation. A new crisis leading to the first large-scale fighting since signing of the ceasefire occurred when the LTTE closed the sluice gates of the Mavil Oya (Mavil Aru) reservoir on 21 July 2006, and cut the water supply to 15,000 villages in government controlled areas. This dispute was developed into a full-scale war by August 2006.
Defeat in the East.
Eelam War IV had commenced in the East. Mavil Aru fell into the hands of Sri Lanka Army by 15 August 2006. Systematically, Sampoor, Vakarai, Kanjikudichchi Aru and Batticaloa also fell into the hands of military. Finally the military captured Thoppigala, the Tiger stronghold in Eastern Province on 11 July 2007. Even IPKF had failed to capture it from LTTE during its offensive in 1988.
Defeat in the North.
Sporadic fighting in the North had been going on for months, but the intensity of the clashes increased after September 2007. Gradually, the defence lines of the LTTE began to fall. The advancing military confined the LTTE into rapidly diminishing areas in the North. Prabhakaran was seriously injured during air strikes carried out by the Sri Lanka Air Force on a bunker complex in Jayanthinagar on 26 November 2007.<ref name='mod-vp-12/19/07'></ref> Earlier, on 2 November 2007, S. P. Thamilselvan, who was the head of the rebels' political wing, was killed during another government air raid. On 2 January 2008, the Sri Lankan government officially abandoned the ceasefire agreement. By 2 August 2008, LTTE lost the entire Mannar District following the fall of Vellankulam town. Troops captured Pooneryn and Mankulam during the final months of 2008.
On 2 January 2009, the President of Sri Lanka, Mahinda Rajapaksa, announced that the Sri Lankan troops had captured Kilinochchi, the city which the LTTE had used for over a decade as its de facto administrative capital. On the same day, President Rajapaksa called upon LTTE to lay down arms and surrender. It was stated that the loss of Kilinochchi had caused substantial damage to the LTTE's public image, and that the LTTE was likely to collapse under military pressure on multiple fronts. As of 8 January 2009, the LTTE abandoned its positions on the Jaffna peninsula to make a last stand in the jungles of Mullaitivu, their last main base.<ref name='mod-bbc-01/08/09'></ref> The entire Jaffna Peninsula was captured by the Sri Lankan Army by 14 January. On 25 January 2009, SLA troops "completely captured" Mullaitivu town, the last major LTTE stronghold.<ref name='mod-bbc-01/25/09'></ref>
The Sri Lankan government accused the LTTE of causing a human disaster by trapping civilians in the shrinking area under their control. With the LTTE on the brink of defeat, the fate of their leader Velupillai Prabhakaran remained uncertain. On 12 May 2009, BBC reported that the LTTE was now clinging to 840 acre of land near the town of Mullaitivu, which is roughly the same area as New York City's Central Park. UN secretary General Ban Ki Moon appealed to the LTTE that children should not be held hostage, recruited as child soldiers, or put in harm's way. Claude Heller of United Nations Security Council said, "we demand that the LTTE immediately lay down arms, renounce terrorism, allow a UN-assisted evacuation of the remaining civilians in the conflict area, and join the political process". The council president, speaking on behalf of the 15 members, said they "strongly condemned the LTTE, a terrorist organisation, for the use of civilians as human shields and for not allowing them to leave the area". On 13 May 2009, the UN security council condemned the LTTE, denounced its use of civilians as human shields, and urged them to acknowledge the legitimate right of the government of Sri Lanka to combat terrorism by laying down their arms and allowing the tens of thousands of civilians to leave the conflict zone. On 14 May 2009, The United Nations acting representative for Sri Lanka Amin Awad said that 6,000 civilians had fled or were trying to flee, but that LTTE was firing on them to prevent them from escaping.
President Mahinda Rajapaksa declared military victory over the Tamil Tigers on 16 May 2009, after 26 years of conflict. The rebels offered to lay down their weapons in return for a guarantee of safety. Sri Lanka's disaster relief and human-rights minister Mahinda Samarasinghe stated, "the military phase is over. The LTTE has been militarily defeated. Now the biggest hostage rescue operation in the world has come to a conclusion, The figure I have here is since 20 April 179,000 hostages have been rescued". On 17 May 2009, LTTE's head of the Department of International Relations, Selvarasa Pathmanathan conceded defeat, saying in an email statement, "this battle has reached its bitter end". Several LTTE fighters committed suicide when they became surrounded.
Aftermath.
With the end of the hostilities, 11,664 LTTE members, including 595 child soldiers surrendered to the Sri Lankan military. Approximately 150 hardcore LTTE cadres and 1,000 mid-level cadres escaped to India. Government took action to rehabilitate the surrendered cadres under a National Action Plan for the Re-integration of Ex-combatants while allegations of torture, rape, and murder have been reported by international human rights bodies. They were divided into three categores; hardcore, non-combatants, and those who were forcefully recruited (including child soldiers). Twenty-four rehabilitation centres were set up in Jaffna, Batticaloa, and Vavuniya. Among the apprehended cadres, there had been about 700 hardcore members. Some of these cadres were integrated into State Intelligence Services to tackle the internal and external networks of LTTE. By August 2011, government had released more than 8,000 cadres, and 2,879 remained.
Continued operations.
After the demise of LTTE leader Prabhakaran and the entire top brass of the organisation, Selvarasa Pathmanathan "alias" KP was its sole first generation leader left alive. KP assumed duty as the new leader of LTTE on 21 July 2009. A statement was issued, allegedly from the Executive Committee of the LTTE, stating that Pathmanathan had been appointed leader of the LTTE. But 15 days after the announcement, on 5 August 2009, a Sri Lankan military intelligence unit, with the collaboration of local authorities, captured Pathmanathan in the Tune Hotel, Downtown Kuala Lumpur, Malaysia. Sri Lanka Ministry of Defense alleges that Perinpanayagam Sivaparan "alias" Nediyavan of the Tamil Eelam People's Alliance (TEPA) in Norway, Suren Surendiran of British Tamils Forum (BTF), Father S. J. Emmanuel of Global Tamil Forum (GTF), Visvanathan Rudrakumaran of Transnational Government of Tamil Eelam (TGTE) and Sekarapillai Vinayagamoorthy "alias" Kathirgamathamby Arivazhagan "alias" Vinayagam, a former senior intelligence leader are trying to revive the organisation among the Tamil diaspora. Subsequently in May 2011, Nediyavan, who advocates an armed struggle against Sri Lankan state, was arrested and released on bail in Norway, pending further investigation.
Divisions.
Two major divisions of the LTTE were, the military wing and the political wing.
The military wing consisted of at least 11 separate divisions including the conventional fighting forces, Charles Anthony Brigade and Jeyanthan Brigade; the dreaded suicide wing called the Black Tigers; naval wing Sea Tigers, air-wing Air Tigers, LTTE leader Prabhakaran's personal security divisions, Imran Pandian regiment and Ratha regiment; auxiliary military units such as Kittu artillery brigade, Kutti Sri mortar brigade, Ponnamman mining unit and hit-and-run squads like Pistol gang. Charles Anthony brigade was the first conventional fighting formation created by LTTE. Sea Tiger division was founded in 1984, under the leadership of Thillaiyampalam Sivanesan "alias" Soosai. LTTE acquired its first light aircraft in the late 1990s. Vaithilingam Sornalingam "alias" Shankar was instrumental in creating the Air Tigers. It carried out 9 air attacks since 2007, including one last suicide air raid targeting Sri Lanka Air Force headquarters, Colombo in February 2009. LTTE is the only terrorist-proscribed organisation to acquire aircraft. LTTE intelligence wing consisted of Tiger Organisation Security Intelligence Service "aka" TOSIS, run by Pottu Amman, and a separate military intelligence division.
Politically the LTTE was never serious about a political solution, it operated a systematic and powerful political wing, which functioned like a separate state in the LTTE controlled area. In 1989, it established a political party named People's Front of Liberation Tigers, under Gopalaswamy Mahendraraja "alias" Mahattaya. It was abandoned soon after. Later, S. P. Thamilselvan was appointed the head of the political wing. He was a member of the LTTE delegation for Norwegian brokered peace talks too. After the death of Thamilselvan in November 2007, Balasingham Nadesan was appointed as its leader. Major sections within the political wing include International peace secretariat, led by Pulidevan, LTTE Police, LTTE court, Bank of Tamil Eelam, Sports division and "Voice of Tigers" Radio broadcasting station of LTTE.
LTTE is also known for the use of female cadres for military engagements. Its women's' wing consisted of Malathi and Sothiya Brigades.
The LTTE also controlled a powerful international wing called the "KP branch", controlled by Selvarasa Pathmanathan, "Castro branch", controlled by Veerakathy Manivannam "alias" Castro, and "Aiyannah group" led by Ponniah Anandaraja alias Aiyannah.
Global network.
LTTE had developed a massive international network since the days of N. S. Krishnan, who served as its first international representative. In the late 1970s, TULF parliamentarian and opposition leader A. Amirthalingam provided letters of reference for fundraising, and V. N. Navaratnam, who was an executive committee member of the Inter-Parliamentary Union (IPU), introduced many influential and wealthy Tamils living overseas to Tamil insurgent leaders. Navaratnam also introduced LTTE members to the members of Polisario Front, a national liberation movement in Morocco, at a meeting held in Oslo, Norway. In 1978, during the world tour of Amirthalingam (with London-based Eelam activist S. K. Vaikundavasan), he formed the World Tamil Coordinating Committee (WTCC), which later turned out to be an LTTE front organisation. The global contacts of LTTE grew steadily since then. At the height of its power, LTTE had 42 offices all over the world. The global network of LTTE engages in propaganda, fundraising, arms procurement, and shipping.
There were three types of organisations to do propaganda and fund raising—i.e., Front, Cover, and Sympathetic. Prior to the ethnic riots of 1983, attempts to raise funds for a sustaining military campaign were not realised. It was the mass exodus of Tamil civilians to India and western countries following Black July ethnic riots, which made it possible. As the armed conflict evolved and voluntary donations dwindled, LTTE used force and threats to collect money. LTTE was worth US$200–300 million at its peak. The group's global network owned numerous business ventures in various countries. These include investment in real estate, shipping, grocery stores, gold and jewellery stores, gas stations, restaurants, production of films, mass media organisations (TV, radio, print), industries, etc. It was also in control of numerous charitable organisations including Tamils Rehabilitation Organisation, which was banned and funds were frozen by United States Treasury in 2007 for covertly financing terrorism.
Arms Procurement and shipping activities of LTTE were largely clandestine. Prior to 1983, it procured weapons mainly from Afghanistan via the Indo-Pakistani border. Explosives were purchased from commercial markets in India. From 1983 to 1987, LTTE acquired substantial amount of weapons from RAW and from Lebanon, Cyprus, Singapore, and Malaysia-based arms dealers. LTTE received its first consignment of arms from Singapore in 1984 on board the MV "Cholan", the first ship owned by the organisation. Funds were received and cargo was cleared at Chennai Port with the assistance of M. G. Ramachandran, the Chief Minister of Tamil Nadu. In November 1994, the LTTE was able to purchase 60 tonnes of explosives (50 tonnes of TNT and 10 tonnes of RDX) from Rubezone Chemical plant in Ukraine, providing a forged Bangladeshi Ministry of Defense end-user certificate. Payments for the explosives were made from a Citibank account in Singapore held by Selvarasa Pathmanathan. Consignment was transported on board MV Sewne. Same explosives were used for the Central Bank bombing in 1996. Myanmar, Thailand, Malaysia, Cambodia and Indonesia remained the most trusted outposts of LTTE, after India alienated it after the Rajiv Gandhi assassination.
Since late 1997, North Korea became the principal country to provide arms, ammunition, and explosives to the LTTE. The deal with North Korean government was carried out by Ponniah Anandaraja "alias" Aiyannah, a member of World Tamil Coordinating Committee of the United States and later, the accountant of LTTE. He worked at the North Korean embassy in Bangkok since late 1997. LTTE had nearly 20 second-hand ships, which were purchased in Japan, and registered in Panama and other Latin American countries. Most of the times, these ships transported general cargo, including paddy, sugar, timber, glass, and fertiliser. But when an arms deal was finalised, they travelled to North Korea, load the cargo and brought it to the equator. Then the ships are based there. Then on board merchant tankers, weapons were transferred to the sea of Alampil, just outside the territorial waters in Sri Lanka's Exclusive Economic Zone. After that, small teams of Sea Tigers brought the cargo ashore. Sri Lanka Navy, during 2005–08 destroyed at least 11 of these cargo ships belonged to LTTE in the international waters.
LTTE's last shipment of weapons came in March 2009, towards the end of the war. Merchant vessel "Princess Iswari" went from Indonesia to North Korea under captain Kamalraj Kandasamy "alias" Vinod, loaded the weapons and it came back to international waters beyond Sri Lanka. But due to the heavy naval blockades set up by Sri Lankan Navy, it could not deliver the arms consignment. Thus it dumped the weapons in the sea. The same ship, after changing its name to MV Ocean Lady, appeared in Vancouver with 76 migrants, in October 2009. In December 2009, Sri Lanka Navy apprehended a merchant vessel belonged to LTTE, "Princess Chrisanta" in Indonesia and brought it back to Sri Lanka.
The United States Senate Committee on Foreign Relations (USSFRC) and Ethiopian based Jimma Times claimed that the Eritrean government had provided direct military assistance, including light aircraft to LTTE, during the 2002–03 period when the LTTE was having negotiations with Sri Lankan government via the Norwegian mediators. It was also alleged that Erik Solheim, the chief Norwegian facilitator, helped LTTE to establish this relationship. None of these claims have since been verified. These allegations and a suspicion from within the Sri Lankan armed forces, that LTTE had considerable connections and assets in Eritrea and that its leader Prabhakaran may try to flee to Eritrea in the final stages of war, prompted the Sri Lankan government to establish diplomatic relations with Eritrea in 2009. None of the allegations nor suspicions have since materialised into verifiable fact.
Proscription as a terrorist group.
32 countries have listed the LTTE as a terrorist organisation. As of January 2009, these include:
The first country to ban the LTTE was its former ally, India. The Indian change of policy came gradually, starting with the IPKF-LTTE conflict, and culminating with the assassination of Rajiv Gandhi. India opposes the new state Tamil Eelam that LTTE wants to establish, saying that it would lead to Tamil Nadu's separation from India though the leaders of Tamil Nadu are opposing it. Sri Lanka itself lifted the ban on the LTTE before signing the ceasefire agreement in 2002. This was a prerequisite set by the LTTE for the signing of the agreement. Indian Government extended the ban on LTTE considering their strong anti-India posture and threat to the security to Indian nationals.
The European Union banned LTTE as a terrorist organisation on 17 May 2006. In a statement, the European Parliament said that the LTTE did not represent all the Tamils and called on it to "allow for political pluralism and alternate democratic voices in the northern and eastern parts of Sri Lanka".
In October 2014, the European Court of Justice annulled the anti-terrorism sanctions and several other restrictions placed on the LTTE in 2006. The court noted that the basis of proscribing the LTTE had been based on "imputations derived from the press and the Internet" rather than on direct investigation of the group's actions, as required by law. Later, in March 2015, EU reimposed the sanctions and restrictions.
Assassinations.
The LTTE has been condemned by various groups for assassinating political and military opponents. The victims include Tamil moderates who coordinated with Sri Lanka Government, Tamil paramilitary groups assisting Sri Lankan Army. The assassination of the Sri Lankan president Ranasinghe Premadasa is attributed to LTTE. The seventh Prime Minister of the Republic of India, Rajiv Gandhi, was assassinated by an LTTE suicide bomber Thenmozhi Rajaratnam on 21 May 1991. On 24 October 1994, LTTE detonated a bomb during a political rally in Thotalanga-Grandpass, which in turn wiped out most of the prominent politicians of the United National Party, including presidential candidate Gamini Dissanayake MP, Cabinet ministers Weerasinghe Mallimarachchi and G. M. Premachandra, Ossie Abeygunasekara MP and Gamini Wijesekara MP.
LTTE sympathizers justify some of the assassinations by arguing that the people attacked were combatants or persons closely associated with Sri Lankan military intelligence. Specifically in relation to the TELO, the LTTE has said that it had to perform preemptive self-defence because the TELO was in effect functioning as a proxy for India.
Suicide bombings.
One of the main divisions of LTTE included the Black Tigers, an elite fighting wing of the movement, whose mission included carrying out suicide attacks against enemy targets. From ancient times, the Tamil civilization saw War as an honorable sacrifice and fallen heroes and kings were worshiped in the form of a Hero stone. Heroic martyrdom was glorified in ancient Tamil literature. The Tamil kings and warriors followed an honour code similar to that of Japanese Samurais and committed suicide to save the honor. The Black Tigers wing of the LTTE is said to reflect some of these elements of Tamil martial traditions including the practice of the worship of fallen heroes (Maaveerar Naal) and martial martyrdom. All soldiers of LTTE carried a suicide pill around their necks to escape captivity and torture by enemy forces.
According to the International Institute for Strategic Studies, LTTE was the first insurgent organisation to use concealed Explosive belts and vests. The specialised unit that carried out suicide attacks was named the Black Tigers. According to the information published by the LTTE, the Black Tigers carried out 378 suicide attacks between 5 July 1987, and 20 November 2008. Out of the deceased, 274 were male and 104 were female.
Many of these attacks have involved military objectives in the north and east of the country, although civilians have been targeted on numerous occasions, including during a high-profile attack on Colombo's International Airport in 2001 that caused damage to several commercial airliners and military jets, killing 16 people. The LTTE was responsible for a 1998 attack on the Buddhist shrine and UNESCO world heritage site Sri Dalada Maligawa in Kandy that killed eight worshipers. The attack was symbolic in that the shrine, which houses a sacred tooth of the Buddha, is the holiest Buddhist shrine in Sri Lanka. Other Buddhist shrines have been attacked, notably the Sambuddhaloka Temple in Colombo that killed nine worshipers.
Black Tiger wing had carried out attacks on various high-profile leaders both inside and outside Sri Lanka. It had successfully targeted three world leaders, the only insurgent group to do so. That includes assassination of Rajiv Gandhi, the former Prime Minister of India on 21 May 1991, assassination of Ranasinghe Premadasa, the President of Sri Lanka on 1 May 1993, and failed assassination attempt of Chandrika Kumaratunga, the Sri Lankan President on 18 December 1999, which resulted in the loss of her right eye.
The slain Black Tiger cadres were highly glorified and their families were given the "Maha Viru family" status. Those cadres were given a chance to have his/her last supper with the LTTE leader Prabhakaran, which was a rare honour one would get in the LTTE controlled area. This, in turn motivated LTTE cadres to join the Black Tiger wing.
On 28 November 2007, an LTTE suicide bomber named Sujatha Vagawanam detonated a bomb hidden inside her brassiere in an attempt to kill Sri Lankan minister Douglas Devananda. This was recorded in the security cameras inside Devananda's office. It is one of the few unsuspected detonations of an explosive by a suicide bomber recorded by a camera. ()
Human rights violations.
The United States Department of State states that its reason for banning LTTE as a proscribed terrorist group is based on allegations that LTTE does not respect human rights and that it does not adhere to the standards of conduct expected of a resistance movement or what might be called "freedom fighters". The FBI has described the LTTE as "amongst the most dangerous and deadly extremist outfits in the world". Other countries have also proscribed LTTE under the same rationale. Numerous countries and international organisations have accused the LTTE of attacking civilians and recruiting children.
Attacks on civilians.
The LTTE has launched attacks on civilian targets several times. Notable attacks include the Aranthalawa Massacre, Anuradhapura massacre, Kattankudy mosque massacre, the Kebithigollewa massacre, and the Dehiwala train bombing. Civilians have also been killed in attacks on economic targets, such as the Central Bank bombing.
Child soldiers.
The LTTE has been accused of recruiting and using child soldiers to fight against Sri Lankan government forces. The LTTE was accused of having up to 5,794 child soldiers in its ranks since 2001. Amid international pressure, the LTTE announced in July 2003 that it would stop conscripting child soldiers, but both UNICEF and Human Rights Watch have accused it of reneging on its promises, and of conscripting Tamil children orphaned by the tsunami. On 18 June 2007, the LTTE released 135 children under 18 years of age. UNICEF, along with the United States, states that there has been a significant drop in LTTE recruitment of children, but claimed in 2007 that 506 child recruits remain under the LTTE. A report released by the LTTE's Child Protection Authority (CPA) in 2008 stated that less than 40 soldiers under age 18 remained in its forces. In 2009 a Special Representative of the Secretary-General of the United Nations said the Tamil Tigers "continue to recruit children to fight on the frontlines", and "use force to keep many civilians, including children, in harms way".
The LTTE argues that instances of child recruitment occurred mostly in the east, under the purview of former LTTE regional commander Colonel Karuna. After leaving the LTTE and forming the TMVP, it is alleged that Karuna continued to forcibly kidnap and induct child soldiers.
Ethnic cleansing.
The LTTE is responsible for forcibly removing, or ethnic cleansing, of Sinhalese and Muslim inhabitants from areas under its control, and using violence against those who refuse to leave. The eviction of Muslim residents happened in the north in 1990, and the east in 1992. The main reason behind the expulsion of Muslims was the fact that local Muslim community did not support the Tamil Eelam struggle of LTTE.
Muslims in the North of Sri Lanka contributed to the Tamil movement on several occasions. Muslim ironmongers in Mannar fashioned weapons for the LTTE. In its 1976 Vaddukoddai Resolution, LTTE condemned the Sri Lankan government for "unleashing successive bouts of communal violence on both the Tamils and Muslims". But later, LTTE undertook its anti-Muslim campaigns as it began to view Muslims as outsiders, rather than a part of the Tamil nation. Local Tamil leaders were disturbed by the LTTE's call for the eviction of Muslims in 1970. In 2005, the International Federation of Tamils claimed that the Sri Lankan military purposefully stoked tensions between Tamils and Muslims, in an attempt to undermine Tamil security. As Tamils turned to the LTTE for support, the Muslims were left with the Sri Lankan state as their sole defender, and so in the eyes of the LTTE, the Muslims had legitimised the role of the state, and were thus viewed as Sri Lankans.
Beginning in 1985, the LTTE forcibly occupied 35000 acre of Muslim-owned farmland in the north of Sri Lanka, before systematically evicting the Muslims from areas under LTTE control. Although anti-Muslim pogroms had occurred in the north and east of Sri Lanka since 1985, the LTTE embarked on a campaign to expel Muslims from the north in 1989. The first eviction notice was sent to the Muslims of Chavakacheri on 15 October 1989, after the LTTE entered the local mosque and threatened Muslims a few weeks earlier. Afterward, the houses of evicted Muslims were ransacked and looted. On 28 October 1989, the Muslims of Mannar were ordered to leave, by the LTTE. Before leaving, they had to seek permission and clearance at the LTTE office. LTTE was to decide their exit route."
The deadline was extended by four days after pleas from local Tamil Catholics, who were left to look after many Muslims' property in anticipation of looting by the Sri Lankan army. The Catholics themselves were later robbed by the LTTE of both their own, and the Muslims’ property. On the 28th, while Muslims were preparing to leave, the LTTE barred Hindus from entering Muslim villages and dealing with them. The areas were reopened on 3 November, after Muslims had been packed onto the boats of Muslim fishermen and sent southwards along the coast. After a lull in ethnic cleansing, on 3 August 1990, the LTTE sealed off a Shiite mosque in Kattankady, the Meera Jumma and Husseinia, and opened fire through the mosque's windows, leaving 147 Muslim worshipers dead, out of 300 gathered for Friday prayers. Fifteen days later, LTTE gunmen shot dead between 122 and 173 Muslim civilians in the town of Eravur.
Ethnic cleansing culminated on 30 October 1990, when the LTTE forcibly expelled the entire Muslim population of Jaffna. LTTE commanders from the east announced at 7:30 a.m. that all Muslims in Jaffna were to report to Osmania stadium, where they were to be addressed by two LTTE leaders, Karikalana and Anjaneyar. After listening to the leaders denigrate Muslims for allegedly attacking Tamils in the east, the leaders explained to the community that they had two hours to evacuate the city. The community was released from the stadium at 10 a.m., and by noon, and were only allowed to carry 500 rupees, while the rest of their possessions were seized by the LTTE after they were forced to report to LTTE checkpoints upon exiting Jaffna. In total, over 14,400 Muslim families, roughly 72,000 people, were forcibly evicted from LTTE-controlled areas of the Northern Province. This includes 38,000 people from Mannar, 20,000 from Jaffna and Kilinochchi, 9,000 from Vavuniya and 5,000 from Mullaitivu.
In 1992 the LTTE embarked on a campaign to create a contiguous Tamil homeland that stretched from the North of Sri Lanka and downwards along the Eastern Coast. A large Muslim population inhabited a narrow strip of land between the two entities, and so a pattern of ethnic cleansing emerged in Eastern Sri Lanka. "The LTTE unleashed violence against the Muslims of Alinchipothanai and killed 69 Muslim villagers. This led to a retaliatory violence against the Tamils in Muthugala, where 49 Tamils were killed allegedly by the Muslim Home guards". Later in the year, the LTTE attacked four Muslim villages (Palliyagodalla, Akbarpuram, Ahmedpuram, and Pangurana) and killed 187 Muslims. The "Australian Muslim Times" commented on 30 October 1992: "The massacres, eviction and the atrocities by the Tamil Tigers are carried out to derive the Muslim Community from their traditional land in the Eastern province as they have done it in the northern province and then set up a separate state only for Tamils".
In 2002 LTTE leader Vellupillai Prabhakaran formally apologised for the expulsion of Muslims from the north and asked the Muslims to return. Some families returned and reopened the Osmania College and two mosques in 2003. Since the apology, TamilNet, which is widely seen as an LTTE mouthpiece, has featured numerous stories of Muslim civilians coming under attack from Sinhalese forces. During the summer of 1990, the LTTE killed over 370 Muslims in the north and east of Sri Lanka in 11 mass killings. The LTTE is accused of organising massacres of Sinhala villagers who settled in the northeast under the dry lands policy. Expulsion of civilians did not confine to Muslim community. Sri Lanka's 1981 population census recorded 19,334 Sinhala civilians in Jaffna District. But with the end of the war in 2009, hardly any Sinhala civilian remained in their places of origin in Jaffna.
Execution of prisoners of war.
LTTE had executed prisoners of war on a number of occasions, in spite of the declaration in 1988, that it would abide by the Geneva Conventions. One such incident was the mass murder of unarmed 600 Sri Lankan Police officers in 1990, in Eastern Province, after they surrendered to the LTTE upon the request of President Ranasinghe Premadasa. Police officers were promised safe conduct and subsequent release; they were instead taken to the jungle, blindfolded, and had their hands tied behind their backs, before being made to lie down on the ground to be subsequently shot. In 1993, LTTE killed 200 Sri Lanka Army soldiers, captured in the naval base at Pooneryn, during the Battle of Pooneryn.
War crimes.
There are allegations that war crimes were committed by the Sri Lankan military and the rebel Liberation Tigers of Tamil Eelam during the Sri Lankan Civil War, particularly during the final months of the conflict in 2009. The alleged war crimes include attacks on civilians and civilian buildings by both sides; executions of combatants and prisoners by both sides; enforced disappearances by the Sri Lankan military and paramilitary groups backed by them; acute shortages of food, medicine, and clean water for civilians trapped in the war zone; and child recruitment by the Tamil Tigers.
A panel of experts appointed by UN Secretary-General (UNSG) Ban Ki-moon to advise him on the issue of accountability with regard to any alleged violations of international human rights and humanitarian law during the final stages of the civil war found "credible allegations" which, if proven, indicated that war crimes and crimes against humanity were committed by the Sri Lankan military and the Tamil Tigers. The panel has called on the UNSG to conduct an independent international inquiry into the alleged violations of international law.
Other criminal activities.
One factor that has greatly benefited the LTTE has been its sophisticated international support network. While some of the funding obtained by the LTTE is from legitimate fundraising, a significant portion is obtained through criminal activities, extortion among Tamil diaspora, involving sea piracy, human trafficking, drug trafficking and gunrunning.<ref name='nydn-tgb-10/16/07'></ref>
Sea piracy.
The LTTE has been accused of hijacking several vessels and ships in waters outside Sri Lanka, including "Ocean Trader" (in October 1994), "Irish Mona" (in August 1995), "Princess Wave" (in August 1996), "Athena" (in May 1997), "Misen" (in July 1997), "Morong Bong" (in July 1997), MV "Cordiality" (in September 1997), "Princess Kash" (in August 1998), "Newko" (in July 1999), "Uhana" (in June 2000), Fuyuan Ya 225 (Chinese trawler, in March 2003), "MV Farah III" (in December 2006) and "City of Liverpool" (in January 2007). The MV "Sik Yang", a 2,818-ton Malaysian-flag cargo ship which sailed from Tuticorin, India on 25 May 1999, went missing in waters near Sri Lanka. The fate of the ship's crew of 15 is unknown. It was suspected that the vessel was hijacked by the LTTE to be used as a phantom vessel. Later, in 1999 it was confirmed that the vessel had been hijacked by the LTTE.
Likewise, the crew of a Jordanian ship, "MV Farah III", that ran aground near LTTE-controlled territory off the island's coast, accused the Tamil Tigers of risking their lives and forcing them to abandon the vessel which was carrying 14,000 tonnes of Indian rice.
Arms smuggling.
The LTTE members operated a cargo company called "Otharad Cargo" in the United Arab Emirates. There are reports that the LTTE met Taliban members and discussed the "Sharjah network", which existed in the Sharjah emirate of the United Arab Emirates. The Sharjah network was used by Victor Bout, an arms-smuggling Russian intelligence agent, to provide Taleban with weapons deliveries and other flights between Sharjah and Kandahar. Otharad Cargo reportedly received several consignments of military hardware from the Sharjah network.
The Mackenzie Institute claimed that LTTE's secretive international operations of the smuggling of weapons, explosives, and "dual use" technologies which is attributed to the "KP Branch", headed by Selvarasa Pathmanathan prior to 2002. It also claims that the most expertly executed operation of the KP Branch was the theft of 32,400 rounds of 81 mm mortar ammunition purchased from Tanzania destined for the Sri Lanka Army. Being aware of the purchase of 35,000 mortar bombs, the LTTE made a bid to the manufacturer through a numbered company and arranged a vessel of their own to pick up the load. Once the bombs were loaded into the ship, the LTTE changed the name and registration of their ship. The vessel was taken to Tiger-held territory in Sri Lanka's north instead of transporting it to its intended destination. In 2002, Prabhakaran appointed Castro as the international chief of LTTE. He overtook the responsibilities of arms smuggling and related activities from Pathmanathan.
People smuggling.
Most of the smuggling of Tamil people to western countries was carried out by LTTE. It had largely benefited from this. The prices charged by LTTE to go to countries such as Canada was extremely higher than the normal cost to travel. In addition, money had to be paid to obtain "exit visas" to leave LTTE controlled areas. After the war, LTTE's main business has been people smuggling. A cost of LKR 4 million per immigrant was "enforced" by LTTE operatives. LTTE's people smuggling ships included "MV Ocean Lady", which appeared in October 2009 off Canada's British Columbia coast with 76 Tamil asylum seekers; "MV Sun Sea", arrived in August 2010 off British Columbia, with 492 asylum seekers and "MV Alicia", carrying 80 illegal immigrants, but was intercepted by Indonesian authorities in July 2011, allegedly heading towards Canada or New Zealand.
Extortion.
LTTE had coerced Sri Lankan Tamil diaspora and Tamil civilians in Sri Lanka to give it money, by threatening the safety of their relatives or property in areas under its control.
Money laundering.
In 2008 - 2009, a report on ″Money laundering and the financing of terrorism″ to the European Union Committee stated a case study related to the LTTE which evidenced the implantation of this terrorist group in number of EU member states. In January 2011, Swiss authorities arrested several LTTE members on money laundering. They were all released later.
Passport forgery.
In the early 1990s, Canadian authorities uncovered a passport forgery scheme run by Canadian Tamils with links to the LTTE, including one of its founding members. In December 2010, Spanish and Thai police uncovered another passport forgery scheme attributed to LTTE.
Drug trafficking.
A number of intelligence agencies have accused that LTTE is involved in drug trafficking. In 2010, citing Royal Canadian Mounted Police sources, "Jane's Intelligence Review" said the LTTE controls a portion of the one billion dollar drug market in the Canadian city of Montreal. It also states, narcotics smuggling using its merchant ships, is one of the main ways of earning money out of its $300 million annual income. U.S. Department of Justice states that LTTE has historically served as the drug couriers moving narcotics into Europe. Indian authorities accused LTTE operatives used to bring narcotics to Mumbai from Mandsaur District of Madhya Pradesh, Rajasthan and Punjab border. Then the drugs were transported to coastal towns in Tamil Nadu such as Tuticorin, Rameswaram, Ramanathapuram, Nagapattinam and Kochi,in Kerala State.
Credit card fraud.
LTTE was also involved in credit card fraud, in the United Kingdom. In 2010, STF arrested the mastermind behind this fraud, Neshanadan Muruganandan "alias" Anandan. LTTE had cloned credit cards using PIN and card numbers obtained from unsuspecting card holders in the United Kingdom, and funds were transferred out of their accounts later. In 2007, Norwegian authorities sentenced six LTTE members for skimming more than 5.3 million Norwegian kroner in a similar credit card scam.
Internet terrorism and cyber crimes.
The LTTE is accused to be the first conventional terrorist group to use internet terrorism and they are also accused to have pioneered online fund raising through solicitation and various cyber crimes including identity theft and credit card fraud. The Pro Tamil movement maintained a prominent web presence for propaganda campaigns and fund raising to sustain LTTE's violent campaign and to make the international public opinion regarding the war in Sri Lanka a very Tamil version. This extensive web base also provided the LTTE a platform for various cyber crimes.
In 1997, the attack on Sri Lankan government and consulate network was the first recorded incident on internet terrorism by a conventional terrorist group. A Tamil tiger wing called ″Internet Black Tigers″ were involved in this attack and they were also responsible for repeated attacks on official sites of numerous other governments.
Further reading.
</dl>

</doc>
<doc id="18673" url="http://en.wikipedia.org/wiki?curid=18673" title="Long-Term Capital Management">
Long-Term Capital Management

Long-Term Capital Management L.P. (LTCM) was a hedge fund management firm based in Greenwich, Connecticut that used absolute-return trading strategies combined with high financial leverage. The firm's master hedge fund, Long-Term Capital Portfolio L.P., collapsed in the late 1990s, leading to an agreement on September 23, 1998 among 16 financial institutions — which included Bankers Trust, Barclays, Bear Stearns, Chase Manhattan Bank, Credit Agricole, Credit Suisse First Boston, Deutsche Bank, Goldman Sachs, JP Morgan, Lehman Brothers, Merrill Lynch, Morgan Stanley, Paribas, Salomon Smith Barney, Societe Generale, and UBS — for a $3.6 billion recapitalization (bailout) under the supervision of the Federal Reserve.
LTCM was founded in 1994 by John W. Meriwether, the former vice-chairman and head of bond trading at Salomon Brothers. Members of LTCM's board of directors included Myron S. Scholes and Robert C. Merton, who shared the 1997 Nobel Memorial Prize in Economic Sciences for a "new method to determine the value of derivatives". Initially successful with annualized return of over 21% (after fees) in its first year, 43% in the second year and 41% in the third year, in 1998 it lost $4.6 billion in less than four months following the 1997 Asian financial crisis and 1998 Russian financial crisis requiring financial intervention by the Federal Reserve, with the fund liquidating and dissolving in early 2000.
Founding.
John W. Meriwether headed Salomon Brothers' bond trading desk until he resigned in 1991 amid a trading scandal.
In 1993 he created Long-Term Capital as a hedge fund and recruited several Salomon bond traders—Larry Hilibrand and Victor Haghani in particular would wield substantial clout—and two future winners of the Nobel Memorial Prize, Myron S. Scholes and Robert C. Merton. Other principals included Eric Rosenfeld, Greg Hawkins, William Krasker, Dick Leahy, James McEntee, Robert Shustak, and David W. Mullins Jr.
The company consisted of Long-Term Capital Management (LTCM), a company incorporated in Delaware but based in Greenwich, Connecticut. LTCM managed trades in Long-Term Capital Portfolio LP, a partnership registered in the Cayman Islands. The fund's operation was designed to have extremely low overhead; trades were conducted through a partnership with Bear Stearns and client relations were handled by Merrill Lynch.
Meriwether chose to start a hedge fund to avoid the financial regulation imposed on more traditional investment vehicles, such as mutual funds, as established by the Investment Company Act of 1940—funds which accepted stakes from 100 or fewer individuals with more than $1 million in net worth each were exempt from most of the regulations that bound other investment companies. In late 1993, Meriwether approached several "high-net-worth individuals" in an effort to secure start-up capital for Long-Term Capital Management. With the help of Merrill Lynch, LTCM secured hundreds of millions of dollars from business owners, celebrities and even private university endowments. The bulk of the money, however, came from companies and individuals connected to the financial industry. By 24 February 1994, the day LTCM began trading, the company had amassed just over $1.01 billion in capital.
Trading strategies.
The company used complex mathematical models to take advantage of fixed income arbitrage deals (termed "convergence trades") usually with U.S., Japanese, and European government bonds. Government bonds are a "fixed-term debt obligation," meaning that they will pay a fixed amount at a specified time in the future. Differences in the bonds' present value are minimal, so according to economic theory any difference in price will be eliminated by arbitrage. Unlike differences in share prices of two companies, which could reflect differing underlying fundamentals, price differences between a 30-year treasury bond and a 29-and-three-quarter-year-old treasury bond should be minimal—both will see a fixed payment roughly 30 years in the future. However, small discrepancies arose between the two bonds because of a difference in liquidity. By a series of financial transactions, essentially amounting to buying the cheaper 'off-the-run' bond (the 29-and-three-quarter-year-old bond) and shorting the more expensive, but more liquid, 'on-the-run' bond (the 30-year bond just issued by the Treasury), it would be possible to make a profit as the difference in the value of the bonds narrowed when a new bond was issued.
LTCM also attempted creating a splinter fund in 1996 called LTCM-X that would invest in even higher risk trades and focus on Latin American markets. LTCM turned to UBS bank to invest in and write the warrant for this new spin-off company.
As LTCM's capital base grew, they felt pressed to invest that capital and had run out of good bond-arbitrage bets. This led LTCM to undertake more aggressive trading strategies. Although these trading strategies were market neutral, i.e. they were not dependent on overall interest rates or stock prices going up (or down), they were not convergence trades as such. By 1998, LTCM had extremely large positions in areas such as merger arbitrage (betting whether mergers would be completed or not) and S&P 500 options (net short long-term S&P volatility). LTCM had become a major supplier of S&P 500 vega, which had been in demand by companies seeking to essentially insure equities against future declines.
Because these differences in value were minute—especially for the convergence trades—the fund needed to take highly-leveraged positions to make a significant profit. At the beginning of 1998, the firm had equity of $4.72 billion and had borrowed over $124.5 billion with assets of around $129 billion, for a debt-to-equity ratio of over 25 to 1. It had off-balance sheet derivative positions with a notional value of approximately $1.25 trillion, most of which were in interest rate derivatives such as interest rate swaps. The fund also invested in other derivatives such as equity options.
Downturn.
Although much success within the financial markets arises from immediate-short term turbulence and the ability of fund managers to identify informational asymmetries, factors giving rise to the downfall of the fund were established before the 1997 East Asian financial crisis. In May and June 1998 returns from the fund were -6.42% and -10.14% respectively, reducing LTCM's capital by $461 million. This was further aggravated by the exit of Salomon Brothers from the arbitrage business in July 1998. Such losses were accentuated through the 1998 Russian financial crisis in August and September 1998, when the Russian government defaulted on their bonds. Panicked investors sold Japanese and European bonds to buy U.S. treasury bonds. The profits that were supposed to occur as the value of these bonds converged became huge losses as the value of the bonds diverged. By the end of August, the fund had lost $1.85 billion in capital.
As a result of these losses, LTCM had to liquidate a number of its positions at a highly unfavorable moment and suffer further losses. A good illustration of the consequences of these forced liquidations is given by Lowenstein (2000). He reports that LTCM established an arbitrage position in the dual-listed company (or "DLC") Royal Dutch Shell in the summer of 1997, when Royal Dutch traded at an 8%-10% premium relative to Shell. In total $2.3 billion was invested, half of which was "long" in Shell and the other half was "short" in Royal Dutch.
LTCM was essentially betting that the share prices of Royal Dutch and Shell would converge. This might have happened in the long run, but due to its losses on other positions, LTCM had to unwind its position in Royal Dutch Shell. Lowenstein reports that the premium of Royal Dutch had increased to about 22%, which implies that LTCM incurred a large loss on this arbitrage strategy. LTCM lost $286 million in equity pairs trading and more than half of this loss is accounted for by the Royal Dutch Shell trade.
The company, which was providing annual returns of almost 40% up to this point, experienced a flight-to-liquidity. In the first three weeks of September, LTCM's equity tumbled from $2.3 billion at the start of the month to just $400 million by September 25. With liabilities still over $100 billion, this translated to an effective leverage ratio of more than 250-to-1.
1998 bailout.
Long-Term Capital Management did business with nearly everyone important on Wall Street. Indeed, much of LTCM's capital was composed of funds from the same financial professionals with whom it traded. As LTCM teetered, Wall Street feared that Long-Term's failure could cause a chain reaction in numerous markets, causing catastrophic losses throughout the financial system. After LTCM failed to raise more money on its own, it became clear it was running out of options. On September 23, 1998, Goldman Sachs, AIG, and Berkshire Hathaway offered then to buy out the fund's partners for $250 million, to inject $3.75 billion and to operate LTCM within Goldman's own trading division. The offer was stunningly low to LTCM's partners because at the start of the year their firm had been worth $4.7 billion. Warren Buffett gave Meriwether less than one hour to accept the deal; the time lapsed before a deal could be worked out.
Seeing no options left, the Federal Reserve Bank of New York organized a bailout of $3.625 billion by the major creditors to avoid a wider collapse in the financial markets. The principal negotiator for LTCM was general counsel James G. Rickards. The contributions from the various institutions were as follows:
In return, the participating banks got a 90% share in the fund and a promise that a supervisory board would be established. LTCM's partners received a 10% stake, still worth about $400 million, but this money was completely consumed by their debts. The partners once had $1.9 billion of their own money invested in LTCM, all of which was wiped out.
The fear was that there would be a chain reaction as the company liquidated its securities to cover its debt, leading to a drop in prices, which would force other companies to liquidate their own debt creating a vicious cycle.
The total losses were found to be $4.6 billion. The losses in the major investment categories were (ordered by magnitude):
Long-Term Capital was audited by Price Waterhouse LLP. After the bailout by the other investors, the panic abated, and the positions formerly held by LTCM were eventually liquidated at a small profit to the rescuers.
Some industry officials said that Federal Reserve Bank of New York involvement in the rescue, however benign, would encourage large financial institutions to assume more risk, in the belief that the Federal Reserve would intervene on their behalf in the event of trouble. Federal Reserve Bank of New York actions raised concerns among some market observers that it could create moral hazard.
LTCM's strategies were compared (a contrast with the market efficiency aphorism that there are no $100 bills lying on the street, as someone else has already picked them up) to "picking up nickels in front of a bulldozer" — a likely small gain balanced against a small chance of a large loss, like the payouts from selling an out-of-the-money naked call option.
Aftermath.
After the bailout, Long-Term Capital Management continued operations. In the year following the bailout, it earned 10%. By early 2000, the fund had been liquidated, and the consortium of banks that financed the bailout had been paid back; but the collapse was devastating for many involved. Mullins, once considered a possible successor to Alan Greenspan, saw his future with the Reserve dashed. The theories of Merton and Scholes took a public beating. In its annual reports, Merrill Lynch observed that mathematical risk models "may provide a greater sense of security than warranted; therefore, reliance on these models should be limited."
After helping unwind LTCM, Meriwether launched JWM Partners. Haghani, Hilibrand, Leahy, and Rosenfeld signed up as principals of the new firm. By December 1999, they had raised $250 million for a fund that would continue many of LTCM's strategies—this time, using less leverage. With the Credit Crisis, JWM Partners LLC was hit with 44% loss from September 2007 to February 2009 in its Relative Value Opportunity II fund. As such, JWM Hedge Fund was shut down in July 2009.
In 1998, the chairman of Union Bank of Switzerland resigned as a result of a $780 million loss due to the collapse of Long-Term Capital Management.

</doc>
<doc id="18706" url="http://en.wikipedia.org/wiki?curid=18706" title="Lee Van Cleef">
Lee Van Cleef

Clarence Leroy "Lee" Van Cleef, Jr. (January 9, 1925 – December 16, 1989), was an American film actor who appeared mostly in Westerns and action pictures. His sharp features and piercing eyes led to his being cast as a villain in scores of films, such as "Kansas City Confidential", "High Noon", "The Man Who Shot Liberty Valance", and "The Good, the Bad and the Ugly". 
Youth.
Van Cleef was born on January 9, 1925 in Somerville, New Jersey, the son of Marion Levinia (née Van Fleet) and Clarence LeRoy Van Cleef. At the age of 17, he obtained his high school diploma early in his senior year in order to enlist in the United States Navy in September, 1942. After basic training and further training at the Naval Fleet Sound School, he was assigned to a submarine chaser and then to a minesweeper, USS "Incredible", on which he worked as sonarman.
The ship initially patrolled the Caribbean, then moved to the Mediterranean, participating in the landings in southern France. In January 1945 "Incredible" moved to the Black Sea, and performed sweeping duties out of the Soviet Navy base at Sevastopol, Crimea. Afterwards the ship performed air-sea rescue patrols in the Black Sea, before returning to Palermo, Sicily. By the time of his discharge in March 1946, he had achieved the rank of Sonarman First Class (SO1), and earned his Mine Sweeper Patch. He had been awarded the Bronze Star and the Good Conduct Medal. By virtue of his deployments Van Cleef also qualified for the European-African-Middle Eastern Campaign Medal, Asiatic-Pacific Campaign Medal and American Campaign Medal and the World War II Victory Medal.
Career.
After leaving the Navy, Van Cleef read for a part in "Our Town" at the Little Theater Group in Clinton, New Jersey.. He got the part. From there, he continued to meet with the group and audition for parts. The next biggest part was that of the boxer, Joe Pendleton, in the play "Heaven Can Wait". During this time he was observed by visiting talent scouts who were impressed by Van Cleef's stage presence and delivery. One of these scouts later took him to New York City talent agent Maynard Morris of the MCA agency who then sent him to the Alvin Theater for an audition. The play was "Mister Roberts".
During a performance of "Mister Roberts" in Los Angeles, he was noticed by film director Stanley Kramer who offered Van Cleef a role in his upcoming film "High Noon". Kramer originally wanted Van Cleef for the role of the deputy Harvey Pell, but as he wanted Van Cleef to have his "distinctive nose" fixed, Van Cleef declined the role in favor of the part of the silent gunslinger Jack Colby. He was then cast mostly in villainous roles, due to his sharp cheeks and chin, piercing eyes and hawk-like nose, from the part of Tony Romano in "Kansas City Confidential" (1952), and culminating 14 years later in Sergio Leone's "The Good, the Bad and the Ugly" (1966). 
Aside from westerns and the science fiction films, three of his early major roles were in noir films, "Kansas City Confidential" (1952), "Vice Squad" (1953) and "The Big Combo" (1955). Van Cleef appeared six times between 1951 and 1955 on the children's syndicated western series, "The Adventures of Kit Carson", starring Bill Williams. He was cast three times, including the role of Rocky Hatch in the episode "Greed Rides the Range" (1952), of another syndicated western series, "The Range Rider". In 1952, he was cast in the episode "Formula for Fear" of the western aviation series "Sky King". He appeared in episode 82 of the TV series "The Lone Ranger" in 1952. In 1954, Van Cleef appeared as Jesse James in the syndicated series, "Stories of the Century".
In 1955, he was cast twice on another syndicated western series, "Annie Oakley". That same year, he guest-starred on the CBS western series, "Brave Eagle". In 1955, he played one of the two villains in an episode of "The Adventures of Champion" the Wonder Horse. In 1958, he was cast as Ed Murdock, a rodeo performer trying to reclaim the title in the event as Madison Square Garden in New York City, on "Richard Diamond, Private Detective".
Van Cleef played different minor characters on four episodes of ABC's "The Rifleman", with Chuck Connors, between 1959 and 1962, and twice on ABC's "Tombstone Territory". In 1958, he was cast as Deputy Sid Carver in the episode "The Great Stagecoach Robbery" of another syndicated western series, "Frontier Doctor", starring Rex Allen. Van Cleef appeared in 1959 as Luke Clagg in the episode "Strange Request" of the NBC western series "Riverboat".
Van Cleef played a sentry on an episode of the ABC sitcom "The Real McCoys", with Walter Brennan. Van Cleef was cast with Pippa Scott and again with Chuck Connors in the 1960 episode "Trial by Fear" of the CBS anthology series "The DuPont Show with June Allyson". A young Van Cleef also made an appearance as Frank Diamond in "The Untouchables", in an episode entitled "The Unhired Assassin." He also appeared in an episode of the ABC/Warner Brothers western series "The Alaskans".
Van Cleef guest-starred on the CBS western series "Have Gun - Will Travel", on the ABC/WB series "Colt .45", on the NBC western series "Cimarron City" and "Laramie", and on Rod Cameron's syndicated crime dramas "City Detective" and "State Trooper". He guest-starred in an episode of John Bromfield's syndicated crime drama, "Sheriff of Cochise". Van Cleef starred as minor villains and henchmen in various westerns, including "The Tin Star" and "Gunfight at the O.K. Corral".
In 1958, a severe car crash nearly cost Van Cleef his life and career. A resulting knee injury made his physicians think that he would never ride a horse again. This injury plagued Van Cleef for the rest of his life and caused him great pain. His recovery was long and arduous and halted his acting for a time. He then began a business in interior decoration with second wife Joan, as well as pursuing his talent for painting, primarily of sea and landscapes.
In 1960, he appeared as a villainous swindler in the "Bonanza" episode, "The Bloodline" (December 31, 1960) and also made an appearance on "Gunsmoke".In 1961 he played a role on episode 7 ("The Grave") of the 3rd season of "The Twilight zone". He played a villainous henchman in the 1962 John Ford movie "The Man Who Shot Liberty Valance". . In 1963, he appeared on "Perry Mason" (episode: "The Case of the Golden Oranges"). That same year he appeared in "The Day of the Misfits" on "The Travels of Jaimie McPheeters". In 1965, his career revived when Sergio Leone cast Van Cleef, whose career was still in the doldrums, as one of the two protagonists, alongside Clint Eastwood, in "For a Few Dollars More".
Leone then chose Van Cleef to appear again with Eastwood, this time as the primary villain Angel Eyes in the now seminal western "The Good, the Bad and the Ugly" (1966). With his roles in Leone's films, Van Cleef became a major star of Spaghetti Westerns, playing central, and often surprisingly heroic, roles in films such as "Death Rides a Horse", "Day of Anger", "The Big Gundown", "The Sabata Trilogy", and "The Grand Duel". Van Cleef also had a supporting role in John Carpenter's cult film "Escape from New York". In 1984, Van Cleef was cast as a ninja master in the NBC adventure series "The Master", but it was canceled after thirteen episodes. All in all, he is credited with 90 movie roles and 109 other television appearances over a 38-year span.
Personal life.
Van Cleef was married three times. He and his first wife, Patsy Ruth, were married from 1943 until their divorce in 1960. Later that year, he married his second wife, Joan Drane. He and Drane divorced in 1974. Two years later, he married his third wife, Barbara Havelone, to whom he remained married until his death in 1989. He had four children.
Death.
Despite suffering from heart disease from the late 1970s and having a pacemaker installed in the early 1980s, Van Cleef continued to work in films until his death on December 16, 1989, at the age of 64. He collapsed in his home in Oxnard, California, from a heart attack. Throat cancer was listed as a secondary cause of death. Van Cleef is interred at Forest Lawn Memorial Park Cemetery, Hollywood Hills, California.
In popular culture.
Lee van Cleef's characters in the Sergio Leone movies were inspiration for the character Elliot Belt in the Lucky Luke comic album, "The Bounty Hunter".
The band Primus has a song about Lee Van Cleef on their album "Green Naugahyde".

</doc>
