<doc id="32881" url="http://en.wikipedia.org/wiki?curid=32881" title="Warez">
Warez

Warez are copyrighted works distributed without fees or royalties, and may be traded, in general violation of copyright law. Warez are generally unauthorized releases by organized groups, as opposed to file sharing between friends or large groups of people with similar interest using a darknet. Warez are not usually commercial software counterfeiting. "Warez" was initially coined by members of the various computer underground circles, but has since become commonplace among Internet users and the mass media.
The word "warez" is a leetspeak plural form of "ware", short for computer software. Thus it is intended to be pronounced like the word wares, , but people commonly mispronounce the "e", , as in the English pronunciation of Juárez.
Warez is used most commonly as a noun: "My neighbour downloaded 10 gigabytes of warez yesterday"; but has also been used as a verb: "The new Windows was warezed a month before the company officially released it". The collection of private warez groups is referred to globally as the "warez scene" or more ambiguously "The Scene".
Warez distribution.
Warez are often distributed outside of The Scene (a collection of warez groups) by torrents (files including tracker info, piece size, uncompressed file size, comments, and vary in size from 1 k, to 400 k.) uploaded to a popular P2P website by an associate or friend of the cracker or cracking crew. An nfo or FILE ID.DIZ is often made to promote who created the release. It is then leeched (downloaded) by users of the tracker and spread to other sharing sites using P2P, or other sources such as newsgroups. From there, it can be downloaded by millions of users all over the world. Often, one release is duplicated, renamed, then re-uploaded to different sites so that eventually, it can become impossible to trace the original file. Another increasingly popular method of distributing Warez is via one-click hosting websites. In the early 1990s, warez were often traded on cassette tapes with different groups and published on bulletin boards that had a warez section.
Rise of software piracy.
Piracy has been an ongoing phenomenon that started when high quality, commercially produced software was released for sale. Whether the medium was cassette tape or floppy disk, software pirates found a way to duplicate the software and spread it without the permission of the maker. Thriving pirate communities were built around the Apple II, Commodore 64, the Atari 400 and Atari 800 line, the ZX Spectrum, the Amiga, and the Atari ST, among other personal computers. Entire networks of BBSes sprang up to traffic illegal software from one user to the next. Machines like the Amiga and the Commodore 64 had an international pirate network, through which software not available on one continent would eventually make its way to every region via bulletin board systems.
It was also quite common in the 1980s to use physical floppy disks and the postal service for spreading software, in an activity known as "mail trading". Prior to the sale of software that came on CD-ROM discs and after hard drives had become available, the software did not require the floppy disc to be in the drive when starting and using the program. So, a user could install it onto his/her computer and mail the disk to the next person, who could do the same. Particularly widespread in continental Europe, mail trading was even used by many of the leading cracker groups as their primary channel of interaction. Software piracy via mail trading was also the most common means for many computer hobbyists in the Eastern bloc countries to receive new Western software for their computers.
Copy-protection schemes for the early systems were designed to defeat the casual pirate, as "crackers" would typically release a pirated game to the pirate "community" the day they were earmarked for market.
A famous event in the history of software piracy policy was an open letter written by Bill Gates of Microsoft, dated February 3, 1976, in which he argued that the quality of available software would increase if software piracy was less prevalent. However, until the early 1990s, software piracy was not yet considered a serious problem by most people. In 1992, the Software Publishers Association began to battle against software piracy, with its promotional video "Don't Copy That Floppy". It and the Business Software Alliance have remained the most active anti-piracy organizations worldwide, although to compensate for extensive growth in recent years, they have gained the assistance of the Recording Industry Association of America (RIAA), the Motion Picture Association of America (MPAA), as well as American Society of Composers, Authors, and Publishers (ASCAP) and Broadcast Music Incorporated (BMI).
Today most warez files are distributed to the public via bittorrent and One-click hosting sites. Some of the most popular software companies that are being targeted are Adobe, Microsoft, Nero, Apple, DreamWorks, and Autodesk, to name a few. To reduce the spread of pirating, some companies have hired people to release "fake" torrents (known as Torrent poisoning), which look real and are meant to be downloaded, but while downloading the individual does not realize that the company that owns the software has received his/her IP address. They will then contact his/her ISP, and further legal action may be taken by the company/ISP.
Causes that have accelerated its growth.
In the mid-1990s, computers became more popular. This was largely attributed to Microsoft and the release of Windows 95, which made using an IBM PC compatible computer much easier for home users. Windows 95 became so popular that in developed countries nearly every middle-class household had at least one computer. Similar to televisions and telephones, computers became a necessity to every person in the information age. As the use of computers increased, so had software and cyber crimes.
In the mid-1990s, the average Internet user was still on dial-up, with average speed ranging between 28.8 and 33.6 kbit/s. If one wished to download a piece of software, which could run about 200 MB, the download time could be longer than one day, depending on network traffic, the Internet Service Provider, and the server. Around 1997, broadband began to gain popularity due to its greatly increased network speeds. As "large-sized file transfer" problems became less severe, warez became more widespread and began to affect large software files like animations and movies.
In the past, files were distributed by point-to-point technology: with a central uploader distributing files to downloaders. With these systems, a large number of downloaders for a popular file uses an increasingly larger amount of bandwidth. If there are too many downloads, the server can become unavailable. The opposite is true for peer-to-peer networking; the "more" downloaders the "faster" the file distribution is. With swarming technology as implemented in file sharing systems like eDonkey2000 or BitTorrent, downloaders help the uploader by picking up some of its uploading responsibilities. There are many sites with links to One-click hosting websites and other sites where one can upload files that contribute to the growing amount of warez.
Distribution via compromised FTP servers.
Prior to the development of modern peer-to-peer sharing systems and home broadband service, sharing warez sometimes involved warez groups scanning the Internet for weakly secured computer systems with high-speed connections. These weakly secured systems would be compromised by exploiting the poor FTP security, creating a special directory on the server with an unassuming name to contain the illegal content.
A common mistake of early FTP administrators was to permit a directory named /incoming that allows full read and write access by external users, but the files themselves in /incoming were hidden. By creating a directory inside /incoming, this hidden directory would then allow normal file viewing. Users of the compromised site would be directed to login and go to a location such as /incoming/data/warez to find the warez content. Messages could be left for other warez users by uploading a plain text file with the message inside.
These hackers would also use known software bugs to illicitly gain full administrative remote control over a computer, and install a hidden FTP service to host their warez. This FTP service was usually running on an unusual port number, or with a non-anonymous login name like "login: warez / Password: warez" to help prevent discovery by legitimate users. Information about this compromised system would then be distributed to a select group of people who were part of the warez scene.
It was important for warez group members to regulate who had access to these compromised FTP servers, to keep the network bandwidth usage low. A site that suddenly became very popular would be noticed by the real owners of the equipment due to their business systems having become slow or low on disk space, resulting in an investigation of system usage which inevitably results in discovery and removal of the warez, and tightening of the site security.
Automated warez distribution via IRC robots.
As the ability to compromise and attain full remote control of business servers became more developed, the warez groups would hack a server and install an IRC robot on the compromised systems alongside the FTP service, or the IRC robot would provide file sharing directly by itself. This software would intelligently regulate access to the illicit data by using file queues to limit bandwidth usage, or by only running during off-hours overnight when the business owning the compromised hardware was closed for the day.
In order to advertise the existence of the compromised site, the IRC software would join public IRC "warez" channels as a "bot" and post into the channel with occasional status messages every few minutes, providing information about how many people are logged in to the warez host, how many files are currently being downloaded, what the upload/download ratio is (to force users into contributing data of their own before they can download), which warez distributor is running the bot, and other status information.
Note that this functionality still exists and can still be found on IRC "warez" channels, as an alternative to the modern and streamlined P2P distribution systems. The opportunity to find and compromise poorly secured systems on which to create an illicit warez distribution site has only increased, with the popular use of broadband service by home users who may not fully understand the security implications of having their home computer always turned on and connected to the Internet.
Types of warez.
There is generally a distinction made between different sub-types of warez. The unusual spellings shown here were commonly used as directory names within a compromised server, to organize the files rather than having them all thrown together in a single random collection.
Movie piracy.
Movie piracy was looked upon as impossible by the major studios. When dial-up was common in early and mid-1990s, movies distributed on the Internet tended to be small. The technique that was usually used to make them small was to use compression software, thus lowering the video quality significantly. At that time, the largest piracy threat was software.
However, along with the rise in broadband internet connections beginning around 1998, higher quality movies began to see widespread distribution – with the release of DeCSS, ISO images copied directly from the original DVDs were slowly becoming a feasible distribution method. Today, movie sharing has become so common that it has caused major concern amongst movie studios and their representative organizations. Because of this the MPAA is often running campaigns during movie trailers where it tries to discourage people from copying material without permission. Unlike the music industry, which has had online music stores available for several years, the movie industry only moved to online distribution in 2006, after the launch of Amazon Unbox.
Distribution of warez.
File formats of warez.
A CD software release can contain up to 700 megabytes of data, which presented challenges when sending over the Internet, particularly in the late 1990s when broadband was unavailable to most home consumers. These challenges apply to an even greater extent for a single-layer DVD release, which can contain up to 4.7 GB of data. The warez scene made it standard practice to split releases up into many separate pieces, called disks, using several file compression formats: (historical TAR, LZH, ACE, UHA, ARJ), ZIP, and most commonly RAR. The original purpose of these "disks" was so that each .rar file could fit on a single 1.44 MB 3½ inch floppy disk. With the growing size of games, this is no longer feasible, as hundreds of disks would need to be used. The average size of disks released by groups today are 50 megabytes or 100 megabytes, however it is common to find disks up to 200 megabytes.
This method has many advantages over sending a single large file:
Despite the fact that many modern ftp programs support segmented downloading, the compression via RAR, ZIP, and breaking up of files has not changed.
Releases of software titles often come in two forms. The full form is a full version of a game or application, generally released as CD or DVD-writable disk images (BIN or ISO files). A rip is a cut-down version of the title in which additions included on the legitimate DVD/CD (generally PDF manuals, help files, tutorials, and audio/video media) are omitted. In a game rip, generally all game video is removed, and the audio is compressed to MP3 or Vorbis, which must then be decoded to its original form before playing. These rips are very rare today, as most modern broadband connections can easily handle the full files, and the audio is usually already compressed by the original producer in some fashion.
Motivations and arguments.
There is a growing movement, exemplified by groups like The Pirate Party and scholars at the The Mises Institute, that the very idea of intellectual property is an anathema to free society. This is in contrast to some of the more traditional open source advocates such as Lawrence Lessig, who advocate for middle ground between freedom and intellectual property. For these people the argument is not about exploiting anything; it's about the freedom to control their own property such as their computers.
Software pirates generally exploit the international nature of the copyright issue to avoid law enforcement in specific countries.
The production and/or distribution of warez is illegal in most countries. However, it is typically overlooked in poorer third world countries with weak or non-existent protection for intellectual property. Additionally, some first world countries have loopholes in legislation that allow the warez to continue.
Legality.
Warez is often a form of copyright infringement punishable as either a civil wrong or a crime. The laws and their application to warez activities may vary greatly from country to country. Generally, however, there are four elements of criminal copyright infringement: the existence of a valid copyright, that copyright was infringed, the infringement was willful and the infringement was either for commercial gain or substantial (a level often set by statute). Often public sites such as pages hosting torrent files claim that they are not breaking any laws because they are not offering the actual data but link only to other places or peers that contain the infringing material.
In addition, nearly all Web providers do not permit the hosting of warez, and will delete any site found to be hosting them.
Depending on the country, in some cases, software piracy might become legal and encouraged. As a dispute between Iran and USA over membership in WTO, and subsequent blocking of Iran's attempts at full-membership in the organization by the USA, has led Iran to encourage US software piracy. Subsequently, there has been a surge in Iranian "warez" and "crackz" websites, as unlike other countries, the Iranian laws do not forbid hosting them inside Iran. See: Iran and copyright issues
Terminology.
Piracy like all other words has different shades of meaning. Some denotative, others connotative, some implying social acceptability, others pejorative. Whoever controls access to the discourse is able to pick the words with meanings that frame the reader's response. While the term 'piracy' is commonly used to describe a significant range of activities, most of which are unlawful, the relatively neutral meaning in this context is "...mak[ing] use of or reproduc[ing] the work of another without authorization". Some groups (including the Free Software Foundation) object to the use of this and other words such as "theft" because they represent a partisan attempt to create a prejudice that is used to gain political ground. ""Publishers often refer to prohibited copying as "piracy." In this way, they imply that illegal copying is ethically equivalent to attacking ships on the high seas, kidnapping and murdering the people on them"" (FSF). The FSF advocate the use of terms like "prohibited copying" or "unauthorized copying", or "sharing information with your neighbor."
On the other hand, many self-proclaimed "software pirates" take pride in the term. Although the use of this term is controversial, it is embraced by some groups such as Pirates With Attitude.
DDL Sites or Direct Download Sites are sites that index links to locations where files can be directly downloaded to the user's computer. Many such sites link to free file hosting services, for the hosting of materials. DDL sites do not directly host the material and can avoid the fees that normally accompany large file hosting.
Warez and malware.
There is a common perception that warez sites represent high risk in terms of malware. In addition, there are several papers showing there is indeed correlation between warez/pirate sites and malware. In particular, one study shows that out of all domains the study classified as "pirate", 7.1% are infected (while out of "random" domains only 0.4% were infected); another study maintains that '"maliciousness" of the content for sites they classified as "pirate" (which specifically included "warez" sites) is the highest among all the researched site categories. Domains related to anti-copy protection tools are among the most malicious sites. Another study specifically targeted anti-copy protection tools such as cracks and key generators. They conclude that the majority of these programs aim to infect the user's computer with one or more types of malware. The chance of the end-user being exposed to malicious code when dealing with cracked applications or games is more than 50%.
Infected warez directly from the warez scene on the other hand, is a very unusual occurrence. The malicious content is usually added at a later stage by third parties.
References.
</dl>

</doc>
<doc id="32905" url="http://en.wikipedia.org/wiki?curid=32905" title="West Virginia">
West Virginia

West Virginia is a state located in the Appalachian region of the Southern United States. It is bordered by Virginia to the southeast, Kentucky to the southwest, Ohio to the northwest, Pennsylvania to the north (and, slightly, east), and Maryland to the northeast. West Virginia is the 41st largest by area and the 38th most populous of the 50 United States. The capital and largest city is Charleston.
West Virginia became a state following the Wheeling Conventions of 1861, in which delegates from 50 northwestern counties of Virginia decided to break away from Virginia during the American Civil War. The new state was admitted to the Union on June 20, 1863, and was a key Civil War border state. West Virginia was the only state to form by separating from a Confederate state and was one of two states formed during the American Civil War (the other being Nevada, which separated from Utah Territory).
The Census Bureau and the Association of American Geographers classify West Virginia as part of the South. The northern panhandle extends adjacent to Pennsylvania and Ohio, with the West Virginia cities of Wheeling and Weirton just across the border from the Pittsburgh metropolitan area, while Bluefield is less than 70 mi from North Carolina. Huntington in the southwest is close to the states of Ohio and Kentucky, while Martinsburg and Harpers Ferry in the Eastern Panhandle region are considered part of the Washington metropolitan area, in between the states of Maryland and Virginia. The unique position of West Virginia means that it is often included in several geographical regions, including the Mid-Atlantic, the Upland South, and the Southeastern United States. It is the only state that is entirely within the area served by the Appalachian Regional Commission; the area is commonly defined as "Appalachia".
The state is noted for its mountains and rolling hills, its historically significant logging and coal mining industries, and its political and labor history. It is one of the most densely karstic areas in the world, making it a choice area for recreational caving and scientific research. The karst lands contribute to much of the state's cool trout waters. It is also known for a wide range of outdoor recreational opportunities, including skiing, whitewater rafting, fishing, hiking, backpacking, mountain biking, and hunting.
History.
Many ancient man-made earthen mounds from various prehistoric mound builder cultures survive, especially in the areas of Moundsville, South Charleston, and Romney. The artifacts uncovered in these give evidence of village societies. They had a tribal trade system culture that crafted cold worked copper pieces.
The Iroquois drove out other American Indian tribes from the region to reserve the upper Ohio Valley as a hunting ground in the 1670s, during the Beaver Wars. Siouan language tribes such as the Moneton had also been recorded in the area previously.
The area now occupied by West Virginia was contested territory among European Americans as well, with the colonies of Pennsylvania and Virginia claiming territorial rights before the American Revolutionary War. Some speculative land companies, such as the Vandalia Company, and later the Ohio Company and Indiana Company, tried to legitimize their claims to land in parts of West Virginia and Kentucky, but failed. With the settlement of the Pennsylvania and Virginia border dispute, which resulted in the creation of Kentucky, Kentuckians "were satisfied [...], and the inhabitants of a large part of West Virginia were grateful."
West Virginia was originally part of the British Virginia Colony from 1607 to 1776 and the western part of the state of Virginia (which was commonly referred as "Trans-Allegheny Virginia" prior to the formation of West Virginia) from 1776 to 1863. Long discontented with electoral malapportionment and underrepresentation in the state legislature, its residents became sharply divided over the issue of secession from the Union during the Civil War. Residents of the western and northern counties set up a separate government under Francis Pierpont in 1861, which they called the "restored" government. Most voted to separate from Virginia and the new state was admitted to the Union in 1863. In 1864 a state constitutional convention drafted a constitution, which was ratified by the legislature without putting it to popular vote. West Virginia abolished slavery and temporarily disfranchised men who had held Confederate office or fought for the Confederacy.
West Virginia's history has been profoundly affected by its mountainous terrain, numerous and vast river valleys, and rich natural resources. These were all factors driving its economy and the lifestyles of its residents, and remain so today.
Prehistory.
A 2010 analysis of a local stalagmite revealed that native Americans were burning forests to clear land as early as 100 BC. Some regional late-prehistoric Eastern Woodland tribes were more involved in hunting and fishing, practicing the slash and burn Eastern Agricultural Complex gardening method. Another group progressed to the more time-consuming, advanced companion crop fields method of gardening. Also continuing from ancient indigenous people of the state, field space and time was given to tobacco growing through to early historic.
"Maize (corn) did not make a substantial contribution to the diet until after 1150 B.P.", to quote Mills (OSU 2003). Eventually, tribal villages began relying heavily on corn to feed their turkey flocks, as Kanawha Fort Ancients practiced bird husbandry. The local Indians made corn bread and a flat rye bread called "banick" as they emerged from the protohistoric era. A horizon extending from a little before the early 18th century is sometimes called the acculturating "Fireside Cabin culture," when trading posts along the Potomac and James rivers spread across the state.
Robert F. Maslowski wrote, "The Adena Indians used pipes for ceremonies. They were carved of stone and they were exceptional works of art. Pipes and the smoking of tobacco became more common during the Late Prehistoric period. They were often made of clay and rather plain." "Nothing is known about Paleo-Indian and Archaic houses in the Kanawha Valley, but archeologists have found evidence of Woodland and Fort Ancient houses." "Woodland Indians lived in wigwams ... The Woodland Indians grew sunflowers, gourds, squash and several seeds such as lambsquarter, may grass, sumpweed, smartweed and little barley." "Fort Ancient Indians lived in much larger square or rectangular houses ... The Fort Ancient Indians can be considered true farmers. They cultivated large agricultural fields around their villages. They no longer grew such a variety of seeds but concentrated on growing corn, beans, sunflowers, gourds, and many types of squash including the pumpkin. They also grew domestic turkeys and kept dogs as pets." As of 2009, over 12,500 archaeological sites have been documented in West Virginia (Bryan Ward 2009:10).
European exploration and settlement.
In 1671, General Abraham Wood, at the direction of Royal Governor William Berkeley of the Virginia Colony, sent a party from Fort Henry led by Thomas Batts and Robert Fallam that discovered Kanawha Falls. Some sources state that Governor Alexander Spotswood's 1716 Knights of the Golden Horseshoe Expedition (for which the state's Golden Horseshoe Competition for 8th graders was named) had penetrated as far as Pendleton County, however original accounts of the excursion suggest to modern historians that none of his horsemen ventured much farther west of the Blue Ridge Mountains than Harrisonburg, Virginia. John Van Metre, an Indian trader, penetrated into the northern portion in 1725. The same year, German settlers from Pennsylvania founded New Mecklenburg, the present Shepherdstown, on the Potomac River, and others followed.
King Charles II of England, in 1661, granted to a company of gentlemen the land between the Potomac and Rappahannock rivers, known as the Northern Neck. The grant finally came into the possession of Thomas Fairfax, 6th Lord Fairfax of Cameron, and in 1746, a stone was erected at the source of the North Branch Potomac River to mark the western limit of the grant. A considerable part of this land was surveyed by George Washington between 1748 and 1751. The diary kept by the surveyor indicates that there were already many squatters, largely of German origin, along the South Branch Potomac River. Christopher Gist, a surveyor in the employ of the first Ohio Company, which was composed chiefly of Virginians, explored the country along the Ohio River north of the mouth of the Kanawha River between 1751 and 1752. The company sought to have a fourteenth colony established with the name "Vandalia". Many settlers crossed the mountains after 1750, though they were hindered by Native American resistance. Few Native Americans lived permanently within the present limits of the state, but the region was a common hunting ground, crossed by many trails. During the French and Indian War the scattered British settlements were almost destroyed.
In 1774, the Crown Governor of Virginia John Murray, 4th Earl of Dunmore, led a force over the mountains, and a body of militia under then-Colonel Andrew Lewis dealt the Shawnee Indians, under Hokoleskwa (or "Cornstalk"), a crushing blow during the Battle of Point Pleasant at the junction of the Kanawha and the Ohio rivers. At the Treaty of Camp Charlotte concluding Dunmore's War, Cornstalk agreed to recognize the Ohio as the new boundary with the "Long Knives". By 1776, however, the Shawnee had returned to war, joining the Chickamauga. Native American attacks continued until after the American Revolutionary War. During the war, the settlers in western Virginia were generally active Whigs and many served in the Continental Army. However, Claypool's Rebellion of 1780–1781, in which a group of men refused to pay Colonial taxes showed war-weariness in West Virginia.
Trans-Allegheny Virginia.
Social conditions in western Virginia were entirely unlike those in the eastern portion of the state. The population was not homogeneous, as a considerable part of the immigration came by way of Pennsylvania and included Germans, Protestant Scotch-Irish, and settlers from the states farther north. Counties in the east and south were settled mostly by east Virginians. During the American Revolution, the movement to create a state beyond the Alleghenies was revived and a petition for the establishment of "Westsylvania" was presented to Congress, on the grounds that the mountains made an almost impassable barrier on the east. The rugged nature of the country made slavery unprofitable, and time only increased the social, political, economic, and cultural differences ("see" Tuckahoe-Cohee) between the two sections of Virginia.
A convention that met in 1829 to form a new constitution for Virginia, against the protest of the counties beyond the mountains, required a property qualification for suffrage and gave the slave-holding counties the benefit of three-fifths of their slave population in apportioning the state's representation in the U.S. House of Representatives. As a result, every county beyond the Alleghenies except one voted to reject the constitution, which nevertheless passed because of eastern support.
The Virginia Constitutional Convention of 1850–51, the Reform Convention, addressed a number of issues important to western Virginians. The vote was extended to all white males of 21 years of age plus. The governor, lieutenant-governor, the judiciary, sheriffs, and other county officers were to be elected by public vote. The composition of the General Assembly was changed; representation in the house of delegates was apportioned on the white basis of the census of 1850, but the Senate was fixed arbitrarily, the west receiving twenty, and the east thirty, senators. This was made acceptable to the west by a provision that required the General Assembly to reapportion representation on the white basis in 1865, or else put the matter to a public referendum. But the east also gave itself a tax advantage in requiring a property tax at true and actual value, except for slaves. Slaves under the age of 12 years were not taxed and slaves over that age were taxed at only $300, a fraction of their true value. Small farmers, however, had all their assets, animals, and land taxed at full value. Despite this tax and the lack of internal improvements in the west, the vote was 75,748 for and 11,063 against the new Constitution, most of the latter being from eastern counties, which did not like the compromises made for the west.
Given these differences, many in the west had long contemplated a separate state. In particular, men like the lawyer Francis H. Pierpont from Fairmont, had long chafed under the political domination of the Tidewater and Piedmont slave-holders. In addition to differences over the abolition of slavery, it was felt the Virginia government ignored and refused to spend funds on needed internal improvements in the west, such as turnpikes and railroads.
Separation from Virginia.
West Virginia was the only state in the Union to separate from a Confederate state (Virginia) during the American Civil War. In Richmond on April 17, 1861, the 49 delegates from the future state of West Virginia voted 17 in favor of the Ordinance of Secession (of Virginia from the United States), 30 against, and 2 abstentions. Almost immediately after the vote to proceed with secession from the Union prevailed in the Virginia General Assembly, a mass meeting at Clarksburg recommended that each county in northwestern Virginia send delegates to a convention to meet in Wheeling on May 13, 1861. When this First Wheeling Convention met, 425 delegates from 25 counties were present, though more than one-third of the delegates were from the northern panhandle area, but soon there was a division of sentiment.
Some delegates favored the immediate formation of a new state, while others argued that, as Virginia's secession had not yet been passed by the required referendum, such action would constitute revolution against the United States. It was decided that if the ordinance were adopted (of which there was little doubt), another convention including the members-elect of the legislature would meet in Wheeling in June 1861. At the election on May 23, 1861, secession was ratified by a large majority in the state as a whole, but in the western counties 34,677 voted against and 19,121 voted for the Ordinance.
The Second Wheeling Convention met as agreed on June 11 and declared that, since the Secession Convention had been called without the consent of the people, all its acts were void and that all who adhered to it had vacated their offices. The Wheeling Conventions, and the delegates themselves, were never actually elected by public ballot to act on behalf of western Virginia. An act for the reorganization of the government was passed on June 19. The next day Francis H. Pierpont was chosen by other delegates at the convention to be governor of Virginia, other officers were elected, and the convention adjourned. The legislature was composed of 103 members, 33 of whom had been elected to the Virginia General Assembly on May 23.
This number included some hold-over Senators from 1859 and as such had vacated their offices to convene in Wheeling. The other members "were chosen even more irregularly—some in mass meetings, others by county committee, and still others were seemingly self-appointed" This irregular assembly met on June 20 and appointed Unionists to hold the remainder of the state offices, organized a rival state government, and elected two United States senators who were promptly recognized by the federal government in Washington, D.C. Thus, there were two state governments in Virginia, one pledging allegiance to the United States and one to the Confederacy.
The Wheeling Convention, which had taken a recess until August 6, reassembled on August 20 and called for a popular vote on the formation of a new state and for a convention to frame a constitution if the vote should be favorable. At the October 24, 1861 election, 18,408 votes were cast for the new state and only 781 against. The veracity of these election results has been questioned, since the Union army then occupied the area and Union troops were stationed at many of the polls to prevent Confederate sympathizers from voting. Most of the affirmative votes came from 16 counties around the Northern panhandle. Over 50,000 votes had been cast on the Ordinance of Secession, yet the vote on statehood garnered little more than 19,000. In Ohio County, home to Wheeling, only about one-quarter of the registered voters cast votes. At the Constitutional Convention in November 1861, a Mr. Lamb of Ohio County and a Mr. Carskadon said that in Hampshire County, out of 195 votes only 39 were cast by citizens of the state; the rest were cast illegally by Union soldiers. In most of what would become West Virginia, there was no vote at all as two-thirds of the territory of West Virginia had voted for secession and county officers were still loyal to Richmond. Votes recorded from pro-secession counties were mostly cast elsewhere by Unionist refugees from these counties. The convention began on November 26, 1861 and finished its work on February 18, 1862; the instrument was ratified (18,162 for and 514 against) on April 11, 1862.
On May 13 the state legislature of the reorganized government approved the formation of the new state. An application for admission to the Union was made to Congress, and on December 31, 1862, an enabling act was approved by President Abraham Lincoln admitting West Virginia, on the condition that a provision for the gradual abolition of slavery be inserted in its constitution. While many felt that West Virginia's admission as a state was both illegal and unconstitutional, Lincoln issued his "Opinion on the Admission of West Virginia" finding that "the body which consents to the admission of West Virginia is the Legislature of Virginia," and that its admission was therefore both constitutional and expedient.
The convention was reconvened on February 12, 1863, and the abolition demand of the federal enabling act was met. The revised constitution was adopted on March 26, 1863 and on April 20, 1863, President Lincoln issued a proclamation admitting the state 60 days later on June 20, 1863. Meanwhile, officers for the new state were chosen, while Gov. Pierpont moved his pro-Union Virginia capital to Union-occupied Alexandria, where he asserted and exercised jurisdiction over all of the remaining Virginia counties within the federal lines.
The question of the constitutionality of the formation of the new state was later brought before the Supreme Court of the United States in the following manner: Berkeley and Jefferson counties lying on the Potomac east of the mountains, in 1863, with the consent of the reorganized government of Virginia voted in favor of annexation to West Virginia. Many voters of the strongly pro-secessionist counties were absent in the Confederate Army when the vote was taken and refused to acknowledge the transfer upon their return. The Virginia General Assembly repealed the act of secession and in 1866 brought suit against West Virginia, asking the court to declare the counties a part of Virginia which would have declared West Virginia's admission as a state unconstitutional. Meanwhile, on March 10, 1866, Congress passed a joint resolution recognizing the transfer. The Supreme Court decided in favor of West Virginia in 1870.
During the Civil War, Union General George B. McClellan's forces gained possession of the greater part of the territory in the summer of 1861, culminating at the Battle of Rich Mountain, and Union control was never again seriously threatened, despite an attempt by Robert E. Lee in the same year. In 1863, General John D. Imboden, with 5,000 Confederates, overran a considerable portion of the state. Bands of guerrillas burned and plundered in some sections, and were not entirely suppressed until after the war ended. The Eastern Panhandle counties were more affected by the war, with military control of the area repeatedly changing hands.
The area which became West Virginia actually furnished about an equal number of soldiers to the federal and Confederate armies, approximately 22,000–25,000 each. The Wheeling government found it necessary in 1865 to strip voting rights from returning Confederates in order to retain control. James Ferguson, who proposed the law, said that if it was not enacted he would lose election by 500 votes. The property of Confederates might also be confiscated, and in 1866 a constitutional amendment disfranchising all who had given aid and comfort to the Confederacy was adopted. The addition of the Fourteenth and Fifteenth Amendments to the United States Constitution caused a reaction. The Democratic party secured control in 1870, and in 1871, the constitutional amendment of 1866 was abrogated. The first steps toward this change had been taken, however, by the Republicans in 1870. On August 22, 1872, an entirely new constitution was adopted.
Beginning in Reconstruction, and for several decades thereafter, the two states disputed the new state's share of the pre-war Virginia government's debts, which had mostly been incurred to finance public infrastructure improvements, such as canals, roads, and railroads under the Virginia Board of Public Works. Virginians, led by former Confederate General William Mahone, formed a political coalition which was based upon this, the Readjuster Party. Although West Virginia's first constitution provided for the assumption of a part of the Virginia debt, negotiations opened by Virginia in 1870 were fruitless, and in 1871, Virginia funded two-thirds of the debt and arbitrarily assigned the remainder to West Virginia. The issue was finally settled in 1915, when the Supreme Court of the United States ruled that West Virginia owed Virginia $12,393,929.50. The final installment of this sum was paid in 1939.
Development of natural resources.
After Reconstruction, the new 35th state benefited from the development of its mineral resources more than any other single economic activity.
Saltpeter caves had been employed throughout Appalachia for munitions; the border between West Virginia and Virginia includes the "Saltpeter Trail", a string of limestone caverns containing rich deposits of calcium nitrate that were rendered and sold to the government. The trail stretched from Pendleton County to the western terminus of the route in the town of Union, Monroe County. Nearly half of these caves are on the West Virginia side, including Organ Cave and Haynes Cave. In the late 18th-century, saltpeter miners in Haynes Cave found large animal bones in the deposits. These were sent by a local historian and frontier soldier Colonel John Stuart to Thomas Jefferson. The bones were named "Megalonyx jeffersonii", or great-claw, and became known as Jefferson's three-toed sloth. It was declared the official state fossil of West Virginia in 2008. The West Virginia official state rock is bituminous coal, and the official state gemstone is silicified Mississippian fossil Lithostrotionella coral.
The limestone also produced a useful quarry industry, usually small, and softer, high-calcium seams were burned to produce industrial lime. This lime was used for agricultural and construction purposes; for many years a specific portion of the C & O Railroad carried limestone rock to Clifton Forge, Virginia as an industrial flux.
Salt mining had been underway since the 18th century, though it had largely played out by the time of the American Civil War, when the red salt of Kanawha County was a valued commodity of first Confederate, and later Union forces. Later, more sophisticated mining methods would restore West Virginia's role as a major producer of salt.
However, in the second half of the 19th century, there was an even greater treasure not yet developed, bituminous coal. It would fuel much of the Industrial Revolution in the U.S. and the steamships of many of the world's navies.
The residents (both Native Americans and early European settlers) had long known of the underlying coal, and that it could be used for heating and fuel. However, for a long time, very small "personal" mines were the only practical development. After the War, with the new railroads came a practical method to transport large quantities of coal to expanding U.S. and export markets. As the anthracite mines of northwestern New Jersey and Pennsylvania began to play out during this same time period, investors and industrialists focused new interest in West Virginia. Geologists such as Dr. David T. Ansted surveyed potential coal fields and invested in land and early mining projects.
The completion of the Chesapeake and Ohio Railway (C&O) across the state to the new city of Huntington on the Ohio River in 1872 opened access to the New River Coal Field. Soon, the C&O was building its huge coal pier at Newport News, Virginia on the large harbor of Hampton Roads. In 1881, the new Philadelphia-based owners of the former Atlantic, Mississippi and Ohio Railroad (AM&O), which stretched across Virginia's southern tier from Norfolk, had sights clearly set on the Mountain State, where the owners had large land holdings. Their railroad was renamed Norfolk and Western (N&W), and a new railroad city was developed at Roanoke to handle planned expansion. After its new president Frederick J. Kimball and a small party journeyed by horseback and saw firsthand the rich bituminous coal seam which his wife named "Pocahontas", the N&W redirected its planned westward expansion to reach it. Soon, the N&W was also shipping from new coal piers at Hampton Roads.
In 1889, in the southern part of the state, along the Norfolk and Western rail lines, the important coal center of Bluefield, West Virginia was founded. The "capital" of the Pocahontas coalfield, this city would remain the largest city in the southern portion of the state for several decades. It shares a sister city with the same name, Bluefield, in Virginia.
In the northern portion of the state and elsewhere, the older Baltimore and Ohio Railroad (B&O) and other lines also expanded to take advantage of coal opportunities. The B&O developed coal piers in Baltimore and at several points on the Great Lakes. Other significant rail carriers of coal were the Western Maryland Railway (WM), Southern Railway (SOU), and the Louisville and Nashville Railroad (L&N).
Particularly notable was a latecomer, the Virginian Railway (VGN). By 1900, only a large area of the most rugged terrain of southern West Virginia was any distance from the existing railroads and mining activity. Within this area west of the New River Coalfield in Raleigh and Wyoming counties lay the Winding Gulf Coalfield, later promoted as the "Billion Dollar Coalfield."
A protégé of Dr. Ansted was William Nelson Page (1854–1932), a civil engineer and mining manager in Fayette County. Former West Virginia Governor William A. MacCorkle described him as a man who knew the land "as a farmer knows a field." Beginning in 1898, Page teamed with northern and European-based investors to take advantage of the undeveloped area. They acquired large tracts of land in the area, and Page began the Deepwater Railway, a short-line railroad which was chartered to stretch between the C&O at its line along the Kanawha River and the N&W at Matoaka, a distance of about 80 mi.
Although the Deepwater plan should have provided a competitive shipping market via either railroad, leaders of the two large railroads did not appreciate the scheme. In secret collusion, each declined to negotiate favorable rates with Page, nor did they offer to purchase his railroad, as they had many other short-lines. However, if the C&O and N&W presidents thought they could thus kill the Page project, they were to be proved mistaken. One of the silent partner investors Page had enlisted was millionaire industrialist Henry Huttleston Rogers, a principal in John D. Rockefeller's Standard Oil Trust and an old hand at developing natural resources and transportation. A master at competitive "warfare", Henry Rogers did not like to lose in his endeavors and also had "deep pockets".
Instead of giving up, Page (and Rogers) quietly planned and then built their tracks all the way east across Virginia, using Rogers' private fortune to finance the $40-million cost. When the renamed Virginian Railway (VGN) was completed in 1909, no fewer than three railroads were shipping ever-increasing volumes of coal to export from Hampton Roads. West Virginia coal was also under high demand at Great Lakes ports. The VGN and the N&W ultimately became parts of the modern Norfolk Southern system, and the VGN's well-engineered 21st-century tracks continue to offer a favorable gradient to Hampton Roads.
As coal mining and related work became major employment activities in the state, there was considerable labor strife as working conditions, safety issues and economic concerns arose. Even in the 21st century, mining safety and ecological concerns is still challenging to the state whose coal continues to power electrical generating plants in many other states.
Coal is not the only valuable mineral found in West Virginia, as the state was the site of the 1928 discovery of the 34.48 carat (6.896 g) Jones Diamond.
Geography.
Located in the Appalachian Mountain range, West Virginia covers an area of 24,229.76 sqmi, with 24,077.73 sqmi of land and 152.03 sqmi of water, making it the 41st-largest state in the United States. West Virginia borders Pennsylvania and Maryland in the northeast, Virginia in the southeast, Ohio in the northwest, and Kentucky in the southwest. Its longest border is with Virginia at 381 miles, followed by Ohio at 243 miles, Maryland at 174 miles, Pennsylvania at 118 miles, and Kentucky at 79 miles.
Geology and terrain.
West Virginia is located entirely within the Appalachian Region, and the state is almost entirely mountainous, giving reason to the nickname "The Mountain State" and the motto "Montani Semper Liberi" ("Mountaineers are always free"). The elevations and ruggedness drop near large rivers like the Ohio River or Shenandoah River. About 75% of the state is within the Cumberland Plateau and Allegheny Plateau regions. Though the relief is not high, the plateau region is extremely rugged in most areas. The average elevation of West Virginia is approximately 1500 ft above sea level, which is the highest of any U.S. state east of the Mississippi River.
On the eastern state line with Virginia, high peaks in the Monongahela National Forest region give rise to an island of colder climate and ecosystems similar to those of northern New England and eastern Canada. The highest point in the state is atop Spruce Knob, at 4863 ft, and is covered in a boreal forest of dense spruce trees at altitudes above 4000 ft. Spruce Knob lies within the Monongahela National Forest and is a part of the Spruce Knob-Seneca Rocks National Recreation Area. A total of six wilderness areas can also be found within the forest. Outside the forest to the south, the New River Gorge is a canyon 1000 ft deep, carved by the New River. The National Park Service manages a portion of the gorge and river that has been designated as the New River Gorge National River, one of only 15 rivers in the U.S. with this level of protection.
Other areas under protection and management include:
Most of West Virginia lies within the Appalachian mixed mesophytic forests ecoregion, while the higher elevations along the eastern border and in the panhandle lie within the Appalachian-Blue Ridge forests. The native vegetation for most of the state was originally mixed hardwood forest of oak, chestnut, maple, beech, and white pine, with willow and American sycamore along the state's waterways. Many of the areas are rich in biodiversity and scenic beauty, a fact that is appreciated by native West Virginians, who refer to their home as "Almost Heaven" (from the song, "Take Me Home, Country Roads" by John Denver). Before the song, it was known as "The Cog State" (Coal, Oil, and Gas) or "The Mountain State".
The underlying rock strata are sandstone, shale, bituminous coal beds, and limestone laid down in a near-shore environment from sediments derived from mountains to the east, in a shallow inland sea on the west. Some beds illustrate a coastal swamp environment, some river delta, some shallow water. Sea level rose and fell many times during the Mississippian and Pennsylvanian eras, giving a variety of rock strata. The Appalachian Mountains are some of the oldest on earth, having formed over 300 million years ago.
Climate.
The climate of West Virginia is generally a humid continental climate (Köppen climate classification "Dfa", except "Dfb" at the higher elevations) with warm to hot, humid summers and cool to cold winters, increasing in severity with elevation. Some southern highland areas also have a mountain temperate climate (Köppen "Cfb") where winter temperatures are more moderate and summer temperatures are somewhat cooler. However, the weather is subject in all parts of the state to change. The hardiness zones range from zone 5b in the central Appalachian mountains to zone 7a in the warmest parts of the lowest elevations. 
In the Eastern Panhandle and the Ohio River Valley, temperatures are warm enough to see and grow subtropical plants such as Southern magnolia ("Magnolia grandiflora"), Crepe Myrtle, Albizia julibrissin, American Sweetgum and even the occasional needle palm and sabal minor. These plants do not thrive as well in other parts of the state. The Eastern prickly pear grows well in many portions of the state.
Average January temperatures range from around 26 °F (−4 °C) near the Cheat River to 41 °F (5 °C) along sections of the border with Kentucky. July averages range from 67 °F (19 °C) along the North Branch Potomac River to 76 °F (24 °C) in the western part of the state. It is cooler in the mountains than in the lower sections of the state. The highest recorded temperature in the state is 112 F at Martinsburg on July 10, 1936 and the lowest recorded temperature in the state is -37 F at Lewisburg on December 30, 1917. 
Annual precipitation ranges from less than 32 in in the lower eastern section to more than 56 in in higher parts of the Allegheny Front. Slightly more than half the rainfall occurs from April to September. Dense fogs are common in many valleys of the Kanawha section, especially the Tygart Valley. West Virginia is also one of the cloudiest states in the nation, with the cities of Elkins and Beckley ranking 9th and 10th in the U.S. respectively for the number of cloudy days per year (over 210). In addition to persistent cloudy skies caused by the damming of moisture by the Alleghenies, West Virginia also experiences some of the most frequent precipitation in the nation, with Snowshoe averaging nearly 200 days a year with either rain or snow. Snow usually lasts only a few days in the lower sections but may persist for weeks in the higher mountain areas. An average of 34 in of snow falls annually in Charleston, although during the winter of 1995–1996 more than three times that amount fell as several cities in the state established new records for snowfall. Average snowfall in the Allegheny Highlands can range up to 180 in per year. Severe weather is somewhat less prevalent in West Virginia than in most other eastern states, and it ranks among the least tornado-prone states east of the Rockies.
Demographics.
The United States Census Bureau estimates that the population of West Virginia was 1,850,326 on July 1, 2014, a -0.14% increase since the 2010 United States Census. The center of population of West Virginia is located in Braxton County, in the town of Gassaway. The Gallup-Hathaway Well-being Index for 2012 rated West Virginia the most miserable state in the country, scoring 50th in categories such as emotional and physical health.
At the 2010 Census, the racial composition of the states population was:
In the same year, 1.2% of West Virginia's population was of Hispanic, Latino, or Spanish origin (they may be of any race).
As of 2012, West Virginia has an estimated population of 1,855,413, which is an increase of 49, or 0.0%, from the prior year and an increase of 2,414, or 0.13%, since the year 2000. This includes a natural decrease since the last census of 3,296 people (that is 108,292 births minus 111,588 deaths) and an increase from net migration of 14,209 people into the state. West Virginia is the least populous southeastern state. Immigration from outside the United States resulted in a net increase of 3,691 people, and migration within the country produced a net increase of 10,518 people.
Only 1.1% of the state's residents were foreign-born, placing West Virginia last among the 50 states in that statistic. It also has the lowest percentage of residents that speak a language other than English in the home (2.7%).
The five largest ancestry groups in West Virginia are: German (18.9%), Irish (15.1%) American (12.9%), English (11.8%) and Italian (4.7%) In the 2000 Census People who identified their ethnicity as simply American made up 18.7% of the population. the majority of these people are of Scots-Irish ancestry, or of English descent.
Large numbers of people of German ancestry are present in the northeastern counties of the state. People of English ancestry are present throughout the entire state. Many West Virginians who self-identify as Irish are actually Scots-Irish Protestants.
5.6% of West Virginia's population were reported as under 5, 22.3% under 18, and 15.3% were 65 or older. Females made up approximately 51.4% of the population.
There were 20,928 births in 2006. Of these, 19,757 (94.40% of the births, 95.19% of the population) were to Non-Hispanic Whites. There were 22 births to American Indians (0.11% of the births and 0.54% of the population), 177 births to Asians (0.85% of the births and 0.68% of the population), 219 births to Hispanics (1.05% of the births and 0.88% of the population) and 753 births to Blacks and others (3.60% of the births and 3.56% of the population).
The state's Northern Panhandle, and North-Central region feel an affinity for Pittsburgh, Pennsylvania. Also, those in the Eastern Panhandle feel a connection with the Washington, D.C. suburbs in Maryland and Virginia, and southern West Virginians often consider themselves Southerners. Finally, the towns and farms along the mid-Ohio River, which forms most the state's western border, have an appearance and culture somewhat resembling the Midwest.
Religion.
Several surveys have been made in recent years, in 2008 by the American Religion Identity Survey, in 2010 by the Pew Forum on Religion and Public Life. The Pew survey results admit to a 6.5% margin of error plus or minus, while the ARIS survey states that "estimates are subject to larger sampling errors in states with small populations." A characteristic of religion in Appalachian communities is the abundance of independent, non-affiliated churches, which "remain unnoted and uncounted in any census of church life in the United States." This sometimes leads to the belief that these communities are "unchurched".
The largest denomination as of 2010 was the United Methodist Church with 136,000 members in 1,200 congregations. The second largest Protestant Church was the American Baptist churches in the USA with 88,000 members and 381 congregations. The Southern Baptist church had 44,000 members and 287 congregations. The Churches of Christ had 22,000 members. The Presbyterian Church had 200 congregations and 20,000 members.
Economy.
Overview.
The economy of West Virginia nominally would be the 62nd largest economy globally behind Iraq and ahead of Croatia according to 2009 World Bank projections, and the 64th largest behind Iraq and ahead of Libya according to 2009 International Monetary Fund projections. The state has a projected nominal GSP of $63.34 billion in 2009 according to the Bureau of Economic Analysis report of November 2010, and a real GSP of $55.04 billion. The real GDP growth of the state in 2009 of .7% was the 7th best in the country. West Virginia was only one of ten states in 2009 which grew economically.
While per capita income fell 2.6% nationally in 2009, West Virginia's grew at 1.8%. Through the first half of 2010, exports from West Virginia topped $3 billion, growing 39.5% over the same period from the previous year and ahead of the national average by 15.7%.
Morgantown was ranked by Forbes as the #10 best small city in the nation to conduct business in 2010. The city is also home to West Virginia University, the 95th best public university according to U.S. News & World Report in 2011. The proportion of West Virginia's adult population with a bachelor's degree is the lowest in the U.S. at 17.3%.
The net corporate income tax rate is 8.5%, scheduled for reduction to 6.5% by 2014, while business costs are 13% below the national average.
Tourism.
The state has an environment reflecting its temperate topography. Tourist sites include the New River Gorge Bridge, Harpers Ferry National Historical Park and many state parks. The Greenbrier hotel and resort, originally built in 1778, has long been considered a premier hotel frequented by numerous world leaders and U.S. Presidents over the years. West Virginia is also home to the Green Bank Telescope at the National Radio Astronomy Observatory. The main building of Weston State Hospital, the largest hand-cut sandstone building in North America, second worldwide only to the Kremlin in Moscow, is also located in West Virginia. Tours of the building, which is a National Historic Landmark and member of the National Civil War Trail, are offered seasonally and by appointment year round.
Resources.
One of the major resources in West Virginia's economy is coal. According to the Energy Information Administration, West Virginia is a top coal-producer in the United States, second only to Wyoming. West Virginia also produces large amounts of natural gas and a fair amount of oil. West Virginia is located in the heart of the Marcellus Shale Natural Gas Bed, which stretches from Tennessee north to New York in the middle of Appalachia.
Nearly all of the electricity generated in West Virginia is from coal-fired power plants. West Virginia produces a surplus of electricity and leads the Nation in net interstate electricity exports. Farming is also practiced in West Virginia, but on a limited basis because of the mountainous terrain over much of the state.
Logging is also a big producer in West Virginia because of the overpopulation of deciduous forest all across the state. Oak, cherry, poplar, maple, and pine are just a few of the various trees often harvested in the state.
Green energy.
West Virginia has the potential to generate 4,952 GWh/year from 1,883 MW of wind power, using 80 meter high wind turbines, or 8,627 GWh/year from 2,772 MW of 100 meter wind turbines, and 60,000 GWh from 40,000 MW of photovoltaics, including 3,810 MW of rooftop photovoltaics.
Source:
Taxes.
West Virginia personal income tax is based on federal adjusted gross income (not taxable income), as modified by specific items in West Virginia law. Citizens are taxed within five income brackets, which range from 3.0 percent to 6.5 percent. The state's consumer sales tax is levied at 6 percent. Effective January 1, 2004, calculation of WV consumer sales tax has been converted to a calculated figure from the bracket system, and remains at 6 percent for most goods (non-prepared foods are not taxed).
West Virginia counties administer and collect property taxes, although property tax rates reflect levies for state government, county governments, county boards of education and municipalities. Counties may also impose a hotel occupancy tax on lodging places not located within the city limits of any municipality that levies such a tax. Municipalities may levy license and gross receipts taxes on businesses located within the city limits and a hotel occupancy tax on lodging places in the city. Although the Department of Tax and Revenue plays a major role in the administration of this tax, less than one-half of 1 percent of the property tax collected goes to state government.
The primary beneficiaries of the property tax are county boards of education. Property taxes are paid to the sheriff of each of the state's 55 counties. Each county and municipality can impose its own rates of property taxation within the limits set by the West Virginia Constitution. The West Virginia legislature sets the rate of tax of county boards of education. This rate is used by all county boards of education statewide. However, the total tax rate for county boards of education may differ from county to county because of excess levies. The Department of Tax and Revenue supervises and otherwise assists counties and municipalities in their work of assessment and tax rate determination. The total tax rate is a combination of the tax levies from four state taxing authorities: state, county, schools and municipal. This total tax rate varies for each of the four classes of property, which consists of personal, real and intangible properties. Property is assessed according to its use, location and value as of July 1. All property is reappraised every three years; annual adjustments are made to assessments for property with a change of value. West Virginia does not impose an inheritance tax. Because of the phase-out of the federal estate tax credit, West Virginia's estate tax is not imposed on estates of persons who died on or after January 1, 2005.
Largest private employers.
The largest private employers in West Virginia, as of March 2011, were:
Quality of life.
Economy.
West Virginia Governor Tomblin's proposed 2014-15 budget submitted in January 2014 had an estimated budget gap of $146–$265 million, and halfway through the 2013-14 fiscal year, tax revenues were $82 million short. The West Virginia Legislature in March 2014 passed its budget bill, taking $147 million from the Rainy Day Fund to balance the 2015 budget. Governor Tomblin&apos;s Deputy Chief of Staff Jason Pizatella, after the state legislature passed the budget, said that West Virginia is expecting another dismal budget in 2016 and could need $150–170 million to balance the next year's budget.
West Virginia coal exports declined 40% in 2013 - a loss of $2.9 billion and overall total exports declined 26%. West Virginia ranked last in the Gallup Economic Index for the fourth year running. West Virginia's score was -44, or a full 17 points lower than the average of -27 for the other states in the bottom ten. West Virginia ranked 48th in the CNBC "Top States for Business 2013" based on measures of competitiveness such as economy, workforce and cost of living - ranking among the bottom 5 states for the last six years running. West Virginia ranked 49th in the 2014 State New Economy Index, and has ranked in the bottom three states since 1999. West Virginia ranked last or next-to-last in critical indicators such as Workforce Education, Entrepreneurial Activity, High-Tech Jobs, and Scientists and Engineers.
On January 9, 2014 near Charleston, the state capital, a chemical spill occurred that contaminated the water supply of 300,000 people in nine West Virginia counties. According to Bloomberg News, lost wages, revenue, and other economic harm from the chemical spill could top $500 million. and West Virginia's Marshall University Center for Business and Economic Research estimated that about $61 million was lost by businesses in the first four days alone after the spill.
Employment.
In 2012, West Virginia's Gross Domestic Product (GDP) grew by 3.3%. The state issued a report highlighting the state's GDP as indicating a fast-growing economy, but did not address employment indicators. In 2009-2013, the U.S. real GDP increased 9.6% and total employment increased 3.9%. In West Virginia during the same time period, its real GDP increased about 11%, while total employment decreased by 1,000 jobs from 746,000 to 745,000.
In 2013, West Virginia ranked last in the nation with an employment-to-population ratio of only 50%, compared to the national average of 59%. The state lost 5,600 jobs in its labor force in four critical economic sectors: construction (1,900), manufacturing (1,100), retail (1,800), and education (800), while gaining just 400 in mining and logging. The state's Civilian Labor Force dropped by 15,100.
Wages.
Personal income growth in West Virginia during 2013 was only 1.5% - the lowest in the nation - and about half the national average of 2.6%. Overall income growth in West Virginia in the last 30 years has been only 13% (about one-third of the national average of 37%). Wages of the impoverished bottom 1% income earners decreased by 3%, compared to the national average which increased 19%.
Population.
United Van Lines 37th Annual Migration Study showed in 2013 that 60% more people moved out of the Mountain State than moved in. West Virginia's population is expected to decline by more than 19,000 residents by 2030, and West Virginia could lose one of its three seats in the United States House of Representatives. West Virginia is the only state where death rates exceeds birth rates. During 2010-2013, about 21,000 babies per year were born in West Virginia, but over 3 years West Virginia had 3,000 more deaths than births.
Family.
Gallup-Healthways annual "State of American Well-Being" rankings reports that 1,261 concerned West Virginians rated themselves as "suffering" in categories such as Quality of Life, Physical Health, and Access to Basic Needs. Overall, West Virginia citizens rated themselves as being more miserable than people in all other states - for 5 years running. In addition, the Gallup Well-Being Index for 2013 ranked Charleston, the state capital, and Huntington last and next-to-last out of 189 U.S. Metropolitan Statistical Areas.
The Annie E. Casey Foundation's National Index of Children's Progress ranked West Virginia 43rd in the nation for all kids, and last for white kids. The Annie E. Casey Foundation's 2013 KIDS COUNT Data Book also ranked West Virginia's education system 47th in the nation for the second straight year. Charleston, West Virginia has the worst divorce rate among 100 cities in the nation. Stephen Smith, the executive director of the West Virginia Healthy Kids and Families Coalition, said that poor employment prospects are to blame: "The pressure to make a good living puts strain on a marriage, and right now it is infinitely harder to make a living here than it was 40 years ago."
Health.
United Health Foundation's "America's Health Rankings" for 2013 found that Americans are making considerable progress in key health measures. West Virginia, however, ranked either last or second-to-last in 20 categories, including cancer, child immunization, diabetes, disabilities, drug deaths, teeth loss, low birth weight, missed work days due to health, prescription drug overdose, preventable hospitalizations, and senior clinical care. Wisconsin Population Health Institute annual "Health Rankings" for 2012 showed West Virginia spends $9,671 per capita on health care annually. El Salvador spends just $467, yet both have the same life expectancy. In 2012, according to the Census Bureau, West Virginia was the only state where death rates exceeds birth rates. During 2010-2013, about 21,000 babies per year were born in West Virginia, but there were 24,000 deaths. In demographics, this is called a "net mortality society."
The National Center for Health Statistics says that national birth rates for teenagers are at historic lows - during 2007-2010, teen birth rates fell 17% nationally;. West Virginia, however, ranked last with a 3% increase in birth rates for teenagers. A study by West Virginia's Marshall University showed that 19% of babies born in the state have evidence of drug or alcohol exposure. This is several times the national rate, where studies show that about 5.9% of pregnant women in the U.S. use illicit drugs, and about 8.5% consume any alcohol. An Institute for Health Policy Research study determined that mortality rates in Appalachia are correlated with coal production. In twenty West Virginia coal counties mining more than 1 million tons of coal per year and having a total population of 850,000, there are about 10,100 deaths per year, with 1,400 of those statistically attributed to deaths from heart, respiratory and kidney disease from living in an Appalachian coal county.
Governance.
West Virginia's capital and seat of government is the city of Charleston, located in the southwest area of the state.
Legislative branch.
The West Virginia Legislature is bicameral. It consists of the House of Delegates and the Senate. It is a citizen's legislature, meaning that legislative office is not a full-time occupation, but rather a part-time position. Consequently, the legislators often hold a full-time job in their community of residence.
Typically, the legislature is in session for 60 days between January and early April. The final day of the regular session ends in a bewildering fury of last-minute legislation in order to meet a constitutionally imposed deadline of midnight. During the remainder of the year, monthly interim sessions are held in preparation for the regular session. Legislators also gather periodically for 'special' sessions when called by the governor.
The title of Lieutenant Governor is assigned by statute to the Senate President.
Executive branch.
The governor, elected every four years on the same day as the U.S. Presidential election, is sworn in during the following January.
Governors of West Virginia can serve two consecutive terms but must sit out a term before serving a third term in office.
The title of Lieutenant Governor is assigned by statute to the Senate President.
Judicial branch.
West Virginia is one of nineteen states that do not have a death penalty, and it is the only state in the southeastern United States to have abolished it.
For the purpose of courts of general jurisdiction, the state is divided into 31 judicial circuits. Each circuit is made up of one or more counties. Circuit judges are elected in partisan elections to serve eight-year terms.
West Virginia's highest court is the Supreme Court of Appeals. The Supreme Court of Appeals of West Virginia is the busiest appellate court of its type in the United States. West Virginia is one of 11 states with a single appellate court. The state constitution allows for the creation of an intermediate court of appeals, but the Legislature has never created one. The Supreme Court is made up of five justices, elected in partisan elections to 12-year terms.
West Virginia is an alcoholic beverage control state. However, unlike most such states, it does not operate retail outlets, having exited that business in 1990. It retains a monopoly on wholesaling of distilled spirits only.
Politics.
At the state level, West Virginia's politics were largely dominated by the Democratic Party from the Great Depression through the 2000s. This was a legacy of West Virginia's very strong#redirect tradition of union membership. Since 2000, state elections have become more competitive at both the state and federal levels. While Democrats currently control the governorship, the majority of statewide offices, and one U.S. Senate seat, Republicans now hold one U.S. Senate seat, all three of the state's U.S. House seats, and a majority in both houses of the West Virginia Legislature.
Since 2000, West Virginians have supported the Republican candidate in every presidential election. The state is regarded as a "deep red" state at the federal level. In the 2012 presidential election Republican Mitt Romney won the state defeating Democrat Barack Obama with 62% of the vote to 35% for Obama. Before 2000, West Virginia had been reckoned as a Democratic stronghold at the national level. From 1932 to 1996, the state voted in favor of the Republican Party three times—in the national Republican landslides of 1956, 1972 and 1984.
Evangelical Christians comprised 52 percent of the state's voters in 2008. A poll in 2005 showed that 53 percent of West Virginia voters are pro-life, the seventh highest in the country. In 2006, just 16 percent favored same-sex marriage. In 2008, 58 percent favored troop withdrawal from Iraq while just 32 percent wanted troops to remain. On fiscal policy in 2008, 52 percent said raising taxes on the wealthier individuals would benefit the economy, while 45 percent disagreed.
Transportation.
Highways form the backbone of transportation systems in West Virginia, with over 37300 mi of public roads in the state. Airports, railroads, and rivers complete the commercial transportation modes for West Virginia. Commercial air travel is facilitated by airports in Charleston, Huntington, Morgantown, Beckley, Lewisburg, Clarksburg, and Parkersburg. All but Charleston and Huntington are subsidized by the US Department of Transportation's Essential Air Service program. The cities of Charleston, Huntington, Beckley, Wheeling, Morgantown, Clarksburg, Parkersburg and Fairmont have bus-based public transit systems, and the Huntington and Charleston systems jointly operate a twice per weekday interconnecting service.
West Virginia University in Morgantown boasts the PRT (personal rapid transit) system, the state's only single rail public transit system. Developed by Boeing, the WVU School of Engineering and the Department of Transportation, it was a model for low-capacity light transport designed for smaller cities. Recreational transportation opportunities abound in West Virginia, including hiking trails, rail trails, ATV off road trails, white water rafting rivers, and two tourist railroads (Cass Scenic Railroad, and the Potomac Eagle Scenic Railroad).
West Virginia is crossed by six interstate highways. I-64 enters the state near White Sulphur Springs in the mountainous east, and exits for Kentucky in the west, near Huntington. I-77 enters from Virginia in the south, near Bluefield. It runs north past Parkersburg before it crosses into Ohio. I-64 and I-77 between Charleston and Beckley are merged as toll road known as the West Virginia Turnpike, which continues as I-77 alone from Beckley to Princeton. It was constructed beginning in 1952 as a two lane road, but rebuilt beginning in 1974 to interstate standards. Today almost nothing of the original construction remains. I-68's western terminus is in Morgantown. From there it runs east into Maryland. At the I-68 terminus in Morgantown, it meets I-79, which enters from Pennsylvania and runs through the state to its southern terminus in Charleston. I-70 briefly runs through West Virginia, crossing the northern panhandle through Wheeling, while I-470 is a bypass of Wheeling (making Wheeling among the smallest cities with an interstate bypass). I-81 also briefly runs in West Virginia through the Eastern Panhandle where it goes through Martinsburg.
The interstates are supplemented by roads constructed under the Appalachian Corridor system. Four Corridors are complete. Corridor D, carrying US 50, runs from the Ohio River, and I-77, at Parkersburg to I-79 at Clarksburg. Corridor G, carrying US 119, runs from Charleston to the Kentucky border at Williamson. Corridor L, carrying US 19, runs from the Turnpike at Beckley to I-79 near Sutton (and provides a short cut of about 40 miles and bypasses Charleston's urban traffic for traveler heading to and from Florida). Corridor Q, carrying US 460, runs through Mercer County, entering the state from Giles County, Virginia and then reentering Virginia at Tazewell County.
Work continues on the long delayed Corridor H, which will carry US 48 from Weston to the Virginia line near Wardensville. As of 2013, a section from Weston to Elkins, and another section from Wardensville to near Scher are complete. Other projects under development are a four-lane upgrade of US 35 from Scott Depot to the Ohio River at Point Pleasant, which is about two-thirds complete; a four lane upgrade of WV 10 from Logan to Man and then of WV 80 from Man to Gilbert, which is about one-third complete; and four lane upgrades to US 52 from Bluefield to Williamson, known as the "King Coal Highway" and from Williamson to Huntington, known as the "Tolsia Highway" which are many years from completion. A project is also ongoing known as the "Coalfields Expressway" which will carry US 121 from Beckley west across Raleigh, Wyoming, and McDowell counties, entering Virginia near Bishop.
Rail lines in the state used to be more prevalent, but many lines have been discontinued because of increased automobile traffic. Many old tracks have been converted to rail trails for recreational use, although the coal producing areas still have railroads running at near capacity. Amtrak's Cardinal roughly parallels I-64's path through the state. MARC trains serve commuters in the eastern panhandle. In 2006 Norfolk Southern along with the West Virginia and U.S. Government approved a plan to modify many of the rail tunnels in West Virginia, especially in the southern half of the state, to allow for double stacked cars (see inter-modal freight). This is expected to also help bring economic growth to the southern half of the state. An Intermodal Freight Facility is underconstruction near Prichard, south of Huntington.
Because of the mountainous nature of the entire state, West Virginia has several notable tunnels and bridges. The most famous of these is the New River Gorge Bridge, which was at a time the longest steel single-arch bridge in the world with a 3,031-foot (924 m) span. The bridge is also pictured on the West Virginia state quarter. The Fort Steuben Bridge (Weirton-Steubenville Bridge) was at its time of construction one of only three cable-stayed steel girder trusses in the United States. "The Veterans Memorial Bridge was designed to handle traffic from the Fort Steuben Bridge as well as its own traffic load," to quote the "Weirton Daily Times" newspaper. The 80-year-old Fort Steuben Bridge (Weirton-Steubenville Bridge) was permanently closed on January 8, 2009. The Wheeling Suspension Bridge was the first bridge built across the Ohio River in 1849 and for a time was the longest suspension bridge in the world. It is still the oldest vehicular suspension bridge in the United States still in use.
Important cities and towns.
State capitals.
Originally, the state capital was in Wheeling (1863 to 1870). It was then moved to Charleston, a more central city (1870 to 1875). However it was returned to Wheeling in 1875, until the capitol burned down in 1885. It was moved back to Charleston in 1885, and it has been there since.
Culture.
Sports.
West Virginia is home to college sports teams from two schools — West Virginia and Marshall — that play in NCAA Division I. West Virginia is also home to several professional minor league teams from baseball, football, soccer, and other sports.
Music.
Appalachian music.
West Virginia's folk heritage is a part of the Appalachian folk music tradition, and includes styles of fiddling, ballad singing, and other styles that draw on Scots-Irish music. Camp Washington-Carver, a Mountain Cultural Arts Center located at Clifftop in Fayette County, hosts an annual Appalachian String Band Festival. The Capitol Complex in Charleston hosts The Vandalia Gathering, where traditional Appalachian musicians compete in contests and play in impromptu jam sessions and evening concerts over the course of the weekend. The Augusta Heritage Center sponsored by Davis & Elkins College in Elkins in Randolph County produces the annual Augusta Heritage Festival which includes intensive week-long workshops that are in the summer that help preserve Appalachian heritage and traditions.
Classical music.
The West Virginia Symphony Orchestra was founded in 1939, as the "Charleston Civic Orchestra", before becoming the "Charleston Symphony Orchestra" in 1943. The first conductor was William R. Wiant, followed by the conductor Antonio Modarelli, who was written about in the November 7, 1949 "Time Magazine" for his composition of the "River Saga", a six-section program piece about the Kanawha River according to the Charleston Gazette's November 6, 1999 photo essay, "Snapshots of the 20th Century". Prior to coming to Charleston, Modarelli had conducted the Wheeling Symphony Orchestra and the Philadelphia Orchestra, according to the orchestra's website.
The Pulitzer Prize-winning 20th-century composer George Crumb was born in Charleston and earned his Bachelor's Degree there before moving outside the state. There had also been a series of operatic style concerts performed in Wheeling during mid-century as well.
Musical innovation.
The West Virginia Cultural Center in Charleston is home to the West Virginia Division of Culture and History which helps underwrite and coordinate a large number of musical activities. The Center is also home to Mountain Stage, an internationally broadcast live-performance music radio program established in 1983 that is carried by many affiliates of National Public Radio. The program also travels to other venues in the state such as the West Virginia University Creative Arts Center in Morgantown.
The Center hosts concerts sponsored by the Friends of Old Time Music and Dance, which brings an assortment of acoustic roots music to West Virginians. The Center also hosts the West Virginia Dance Festival, which features classical and modern dance.
Huntington's historic Keith-Albee Theatre, built by brothers A.B. and S.J. Hyman, was originally opened to the public on May 7, 1928, and hosts a variety of performing arts and music attractions. The theatre was eventually gifted to Marshall University and is currently going through renovation to restore it to its original splendor.
Every summer Elkins hosts the Augusta Heritage Festival, which brings folk musicians from around the world. The town of Glenville has long been home to the annual West Virginia State Folk Festival.
The Mountaineer Opera House in Milton hosts a variety of musical acts.
John Denver's hit song "Take Me Home, Country Roads" describes the experience of driving through West Virginia. The song mentions the Shenandoah River and the Blue Ridge Mountains, both features traversing the easternmost extremity of the state's "eastern panhandle", in Jefferson County. On March 8, 2014, West Virginia Governor Earl Ray Tomblin signed House Concurrent Resolution 40 naming "Take Me Home, Country Roads" the fourth official state song of West Virginia.
Symphony Sunday is an annual event hosted by the West Virginia Symphony Orchestra held in June. It is a day full day of music by community groups, food, and family fun, culminating in a free performance by the West Virginia Symphony Orchestra with a fireworks display following. The event began in 1982 and is held on the front lawn of the University of Charleston.
The Daily Mail Kanawha County Majorette and Band Festival is West Virginia's longest running music festival. It is for the eight public high schools in Kanawha County. The festival began in 1947. It is held at the University of Charleston Stadium at Laidley Field in downtown Charleston.
Further reading.
Primary sources
</dl>

</doc>
<doc id="33105" url="http://en.wikipedia.org/wiki?curid=33105" title="Wargaming">
Wargaming

A wargame (also war game) is a strategy game that deals with military operations of various types, real or fictional. Wargaming is the hobby dedicated to the play of such games, which can also be called conflict simulations, or consims for short. When used professionally by the military to study warfare, "war game" may refer to a simple theoretical study or a full-scale military exercise. Hobby wargamers have traditionally used "wargame", while the military has generally used "war game"; this is not a hard and fast rule. Although there may be disagreements as to whether a particular game qualifies as a wargame or not, a general consensus exists that all such games must explore and represent some feature or aspect of human behaviour directly bearing on the conduct of war, even if the game subject itself does not concern organized violent conflict or warfare. The business wargames exists too, but in general they are only role playing games based on market situations.
Wargames are generally categorized as historical, hypothetical, fantasy, or science fiction. Historical games by far form the largest group. These games are based upon real events and attempt to represent a reasonable approximation of the actual forces, terrain, and other material factors faced by the actual participants. Hypothetical games are games grounded in historical fact but concern battles or conflicts that did not (or have yet to) actually happen. Fantasy and science fiction wargames either draw their inspiration from works of fiction or provide their own imaginary setting. Highly stylized conflict games such as chess are not generally considered wargames, although they are recognized as being related. Games involving conflict in other arenas than the battlefield, such as business, sports or natural environment are similarly usually excluded.
The modern wargaming hobby has its origins at the beginning of the 19th century, with von Reiswitz's Kriegsspiel rules. Later, H.G. Wells' book Little Wars ushered in the age of miniatures games in which two or more players simulated a battle as a pastime. During the 1950s the first large-scale, mass-produced board games depicting military conflicts were published. These games were at the height of their popularity during the 1970s, and became quite complex and technical in that time.
Wargaming has changed dramatically over the years, from its roots in miniatures and board wargaming, to contemporary computer and computer assisted wargames; however, both miniature and board wargames maintain a healthy, if small, hobby market with lighter games being popular with many 'non-wargamers'.
Overview.
Like all games, wargames exist in a range of different complexities. Some are fundamentally simple—often called "beer-and-pretzel" games—whereas others attempt to simulate a high level of historical realism. These latter games typically require extensive rulebooks that encompass a large variety of actions and details. These games often require a considerable study of the rules before they can be played. Wargames also feature a range of scales, from games that simulate individual soldiers, to ones that chart the course of an entire global or even galactic war.
Wargames are generally a representational art form. Usually, this is of a fairly concrete historical subject (such as the Battle of Gettysburg, one of several popular topics in the genre), but it can also be extended to non-historical ones as well. The Cold War provided fuel for many games that attempted to show what a non-nuclear (or, in a very few cases, nuclear) World War III would be like, moving from a re-creation to a predictive model in the process. Fantasy and science fiction subjects are sometimes not considered wargames because there is nothing in the real world to model, however, conflict in a self-consistent fictional world lends itself to exactly the same types of games and game designs as does military history.
Because of these attitudes, there are many games and types of games that may appear to be a wargame at first glance, but are not accepted as such by members of the hobby, and many that would be considered debatable. "Risk" could be considered a wargame; it uses an area map of the Earth and is unabashedly about sending out armies to conquer the world. However, it has no readily-discernible timeframe, and combat is extremely abstract, leading many to not consider it as an actual wargame, or only tangentially as one.
The highest percentage of war-themed games that are not wargames come from the video game industry. Most markedly real-time strategy games (such as "StarCraft") deal with combat nearly exclusively, but the gameplay-enhancing conventions of the genre also destroy realism. For example, in actual combat, vehicle armor is generally a binary proposition. Either the round penetrates and the vehicle is knocked out, or it does not and the vehicle is unaffected. RTS games make a habit of giving a vehicle a "health bar" that generally allows it to survive even powerful single shots, but each hit reduces its health by some amount, allowing a high volume of rifle fire to knock out a well armored tank. Other notable genre conventions include the construction of buildings and vehicles within the timeframe of a battle (i.e., hours, if not less) and a lack of any command and control, supply, or morale systems.
A major determinant of the complexity and size of a wargame is how realistic it is intended to be. Some games constitute a serious study of the subject at hand, whereas others are intended to be light entertainment. In general, a more serious study will have longer, more detailed rules, more complexity, and more record keeping. More casual games may only bear a passing resemblance to the subject, although many still try to encourage the same types of decision making as the player's historical counterparts, and thereby bring forth the "feel" of the conflict.
Wargames tend to have a few fundamental problems. Notably, both player knowledge, and player action are much less limited than what would be available to the player's real-life counterparts. Some games have rules for command and control and fog of war, using various methods. While results vary, many of these mechanisms can be cumbersome and onerous in traditional games. The "edge of world problem" raises the issue of what to do at the artificial boundary of the physical edge of a board game, in contrast to real life where there is no "edge" and units off-board can have a tangible effect on a scenario. Computer wargames can more easily incorporate these features because the computer can conceal information from players and act as an impartial judge (even while playing one side). However, due to interface issues, these can still be found to be as frustrating to the player as traditional methods.
History.
Drawing inspiration from chess, Hellwig, Master of Pages to the Duke of Brunswick, created a battle emulation game in 1780. According to Max Boot's book "War Made New" (2006, pg 122), sometime between 1803 and 1809, the Prussian General Staff developed war games, with staff officers moving metal pieces around on a game table (with blue pieces representing their forces and red pieces those of the enemy), using dice rolls to indicate random chance and with a referee scoring the results. Increasingly realistic variations became part of military training in the 19th century in many nations, and were called "kriegsspiel" or "wargames". Wargames or military exercises are still an important part of military training today.
Modern wargaming originated with the military need to study warfare and to 'reenact' old battles for instructional purposes. The stunning Prussian victory over the Second French Empire in the Franco-Prussian War of 1870-1871 is sometimes partly credited to the training of Prussian officers with the game "Kriegsspiel", which was invented around 1811 and gained popularity with many officers in the Prussian army. These first wargames were played with dice which represented "friction", or the intrusion of less than ideal circumstances during a real war (including morale, weather, the fog of war, etc.), though this was usually replaced by an umpire who used his own combat experience to determine the results.
"The first specific non-military wargame club was started in Oxford, England, in the 19th century." Naval enthusiast and analyst Fred T. Jane came up with a set of rules for depicting naval actions with the use of model ships, or miniatures around 1898 . The 1905/6 edition of "Jane's Fighting Ships" includes a revised edition for "The Naval War Game".
H.G. Wells' books "Floor Games" (1911) and "Little Wars" (1913) were attempts to codify rules for fighting battles with toy soldiers (miniatures), and make them available to the general public. They were very simple games, and in some ways just provide a context for shooting spring-loaded toy cannons at toy soldiers, but "in his Appendix to "Little Wars", Wells speaks of the changes required to convert his admittedly simplistic rules into a more rigorous "Kriegspiel"." However, Wells also states in his rules that combat "should be by actual gun and rifle fire and not by computation. Things should happen and not be decided," in opposition to the general nature of "Kriegspiel" play.
In 1940 "Fletcher Pratt's Naval War Game" was first published. The game started in New York, but other clubs formed around the USA. Jack Coggins was invited by Pratt to participate, and recalled that Pratt's game involved dozens of tiny wooden ships—built to a scale of about one inch to 50 feet—spread over the living room floor of his apartment. Their maneuvers and the results of their battles were calculated via a complex mathematical formula, with scale distances marked off with tape measures. The game's popularity grew and moved to using a ballroom for games with 60 or more players per side. The game was respected by the Naval War College and serving naval officers regularly participated in games For an evaluation of the Fletcher Pratt Game versus reality see Chapter 10 of "The Fletcher Pratt Naval Wargame" book. 
All of these games were meant to be accessible to the general public, but actual play was made difficult owing to the expense of purchasing an army or navy's worth of miniatures. As leisure time and disposable income generally rose through the 20th Century, miniatures games slowly gained a following. Most gaming groups informally wrote and/or revised their own rules, which were never published. In 1955 Jack Scruby started producing miniatures using RTV rubber molds, which greatly reduced their expense, and he turned this into a business (Scruby Miniatures) in 1957 and started publishing "War Game Digest". It, and its successors, put fellow miniatures enthusiasts in touch with each other, and provided a forum for ideas and locally produced rules to be shared with the rest of the hobby.
Around the same time in the UK Donald Featherstone began writing an influential series of books on wargaming, which represented the first mainstream published contribution to wargaming since H.G Wells. Titles included : "Wargames", "Advanced Wargames", "Solo Wargaming", "Wargame Campaigns", "Battles with Model Tanks", "Skirmish Wargaming". Such was the popularity of such titles that other authors were able to have published wargaming titles. This output of published wargaming titles from British authors coupled with the emergence at the same time of several manufacturers providing suitable wargame miniatures (e.g. Miniature Figurines, Hinchliffe, Peter Laing, Garrisson, Skytrex, Davco, Heroic & Ros) was responsible for the huge upsurge of popularity of the hobby in the late 1960s and into the 1970s.
Meanwhile, the first modern mass-market wargame, based on cardboard counters and maps, was designed and published by Charles S. Roberts in 1952. After nearly breaking even on "Tactics", he decided to found the Avalon Hill Game Company as a publisher of intelligent games for adults, and is called "The father of board wargaming". The modern commercial board wargaming industry is considered to have begun with the publication of "Tactics II" in 1958, and the founding of "The General Magazine" by Avalon Hill in 1964. In 1961, AH published Roberts' "Gettysburg", which is considered to be the first board wargame based entirely on a historical battle. "D-Day" and "Chancellorsville", the first commercial games to use a hexagonal mapboard, were also published that year.
Avalon Hill had a very conservative publishing schedule, typically about two titles a year, and wargames were only about half their line. During the late 1960s, a number of small magazines dedicated to the hobby appeared, along with new game companies. The most important of these were undoubtedly "Strategy & Tactics", and the company founded to save it from failing: Simulations Publications, Inc. (SPI). Under SPI, "S&T" started including a new game in every issue of the magazine, which along with the regular games SPI was publishing vastly increased the number of wargames available.
Coupled with an aggressive advertising campaign, this caused a tremendous rise in the popularity of wargaming in the early 1970s, with a large number of new companies starting up. Two of these would last for some years: Game Designers' Workshop (GDW), and Tactical Studies Rules (TSR). TSR's medieval era miniatures game, "Chainmail" (1971) included a fantasy supplement that led to a new phenomenon that would become much bigger than its parent hobby, role-playing games (RPGs). (For a better look at these developments see the history of role-playing games.)
The 1970s can be considered the 'Golden Age of Wargaming', with a large number of new companies publishing an even larger number of games throughout the decade, powered by an explosive rise in the number of people playing wargames. Avalon Hill's "PanzerBlitz" (1970), "Panzer Leader" (1974), and "Squad Leader" (1977) were particularly popular during this time, with their innovative geomorphic mapboard system. Wargames began to diversify in subject matter, with the first science-fiction wargame ("Galactic Warfare", published in the UK by Davco) appearing in 1973 and one of the longest lasting and most successful, "Star Fleet Battles", published by Task Force Games, appearing in 1979. Wargames also diversified in size during the decade with both microgames such as Steve Jackson's "Ogre "that had one small map, about 100 pieces and a complexity that permitted games to be completed in about an hour, and "monster games" such as "War in Europe" with over a dozen large maps and thousands of pieces, requiring dozens of hours to complete.
The boom came to an end, and was followed by the usual bust in the early 1980s, most markedly with the acquisition of SPI by TSR in 1982. The hobby never truly recovered from this, and is today much smaller than it was during the 1970s. Numerous factors have been implicated in the decline, including the rise of gaming alternatives (such as RPGs), the ever increasing complexity of wargames, and changing demographics and lifestyles.
During the 1980s, much of the market for wargames was dominated by roleplaying games. Then, when personal computers became available, gamers could simply "sit down and play" without learning masses of rules, clearing physical space, and finding and coordinating schedules with opponents. However, in 1983 Games Workshop published "Warhammer Fantasy Battle", initially as a "Mass-combat Role Playing Game", which quickly moved to dominate the fantasy wargaming market. When collectible card games arrived in the 1990s, the gaming market became even more competitive. By this time, many wargame publishers were already long gone.
Despite the decline, wargaming continues to survive in different forms. "Advanced Squad Leader" (1985) became a niche hobby in and of itself, and "Axis and Allies" (1984) was very popular with the mass market audience and Games Workshop's "Warhammer Fantasy Battle" (1983) spawned a long-lasting line popular miniatures games including the successive editions of "Warhammer" and the science-fantasy "Warhammer 40,000" game. The genre of 'card-driven games' emerged with the publication of "We the People" by AH in 1994, and continues in current releases from GMT. "Battle Cry" (2000) and "Memoir '44" (2004) proved that light wargames can still be commercially successful, as long as the rules are clear and accessible, and the components are high in quality. Block wargames, such as those published by Columbia Games remain quite popular. Companies like GMT Games and Multi-Man Publishing continue to survive and publish highly detailed hex and counter wargames.
Miniature versions.
Miniature wargaming typically involves the use of 6–54 mm painted metal or plastic miniatures for units, and model scenery placed on a tabletop or floor as a playing surface, although other open areas such as gardens and sandboxes are sometimes used. Games with miniatures are sometimes called tabletop games, tabletop wargames, miniature wargames, or simply wargames. Miniatures games generally measure distance for movement and range with a string or tape measure.
Miniature wargamers generally prefer rule sets that can be used for any battle in a particular era or war, instead of a specific event, as is common in board wargames. Because armies and terrain can be combined in all possible ways, miniatures wargaming is generally more varied and flexible than other forms of wargaming. The preparation also tends to be more time consuming and expensive. Miniature wargamers typically enjoy painting miniatures and constructing terrain, and this is an important part of the hobby for them.
Because information cannot be displayed on a miniature figure as conveniently as on a cardboard counter, miniature wargames often lack the complexity and detail of some of the heavier board wargames.
The popularity of miniatures wargaming stayed relatively stable during the boom and bust of board wargames. Today, games such as "Warhammer Fantasy Battle", "Dust Warfare", "Flames of War", Field of Glory and the newer collectible miniatures games continue to recruit new interest into the oldest form of the wargaming hobby.
Board versions.
In the United States, board wargames are often equated with the entire hobby of wargaming. In Europe, and especially Britain, they are a relatively minor part of the hobby. The genre is known for a number of common conventions that were developed early on, but these do not necessarily appear in all board wargames.
The early history of board wargaming was dominated by Avalon Hill, even though other companies, such as SPI, left their own permanent marks on the industry. With the purchase of Avalon Hill by Hasbro, no one company is identified with the hobby as a whole. GMT and Decision Games are two of the more influential board wargame companies in existence today.
Card versions.
Because of their nature, cards are well suited for abstract games, as opposed to the simulation aspects of wargames. Traditional card games are not considered wargames even when nominally about the same subject (such as the game "War").
An early card wargame would be "Nuclear War", a 'tongue-in-cheek game of the end of the world', first published in 1966 and still published today by Flying Buffalo. It does not simulate how any actual nuclear exchange would happen, but it is still structured unlike most card games because of the way it deals with its subject.
In the late 1970s Battleline Publications (a board wargame company) produced two card games, "Naval War" and "Armor Supremacy". The first was fairly popular in wargaming circles, and is a light system of naval combat, though again not depicting any 'real' situation (players may operate ships from opposing navies side-by-side). "Armor Supremacy" was not as successful, but is a look at the constant design and development of new types of tanks during World War II.
The most successful card wargame (as a card game and as a wargame) would almost certainly be "Up Front", a card game about tactical combat in World War II published by Avalon Hill in 1983. The abstractness is harnessed in the game by having the deck produce random terrain, and chances to fire, and the like, simulating uncertainty as to the local conditions (nature of the terrain, etc.).
 is a specialist designer and publisher of card games for several genres, including air combat and World War II and Modern land combat.
Also, card driven games (CDGs), first introduced in 1993, use a deck of (custom) cards to drive most elements of the game, such as unit movement (activation) and random events. These are, however, distinctly board games, the deck is merely one of the most important "elements" of the game.
Computer versions.
As in all aspects of modern life, personal computers have had a profound impact on wargaming. Computers allow gamers separated by many miles to play a game. They also handle many of the tedious aspects of wargaming, such as highly technical rules and record keeping. Finally, with the development of artificial intelligence, computers can actually serve as opponents, and thus provide opportunities for solitaire gaming.
In the video game industry, "wargames" are considered a subgenre of strategy game that emphasizes strategic or tactical warfare on a map. These wargames generally take one of four archetypal forms, depending on whether the game is turn-based or real-time and whether the game's focus is upon military strategy or tactics.
Comparison with traditional versions.
Many contemporary computer strategy games can be considered wargames, in the sense that they are a simulation of warfare on some level. The mechanics and language have little in common with board and miniature games, but the general subject matter is popular. That said, most war-themed computer and video games are generally not considered wargames by the wargaming hobby. This usually occurs because of the perception that slavish attention to 'realism' will cause a game to be rejected as 'uninteresting' or boring by the mass-market. Therefore the mass-market video games tend to be easier to get into, and quick to play. However, not all video games are equally unrealistic, as successful games such as the "Total War" and "Hearts of Iron" game series are historically based.
On the other hand, many video games include fog of war, meaning that what is visible on the map is limited to what is within a certain range of the player's units. This is a feature often talked about in traditional wargames, but traditionally impractical to implement outside of a computer.
Computers used in traditional versions.
Computer-assisted versions.
In the recent years, programs have been developed for computer-assisted gaming as regards to wargaming. Two different categories can be distinguished: local computer assisted wargames and remote computer assisted wargames.
Local computer assisted wargames are mostly not designed toward recreating the battlefield inside computer memory, but employing the computer to play the role of game master by storing game rules and unit characteristics, tracking unit status and positions or distances, animating the game with sounds and voice and resolving combat. Flow of play is simple: each turn, the units come up in a random order. Therefore the more units an opponent has, the more chance he will be selected for the next turn. When a unit comes up, the commander specifies an order and if offensive action is being taken, a target, along with details about distance. The results of the order, base move distance and effect to target, are reported, and the unit is moved on the tabletop. All distance relationships are tracked on the tabletop. All record-keeping is tracked by the computer.
Remote computer assisted wargames can be considered as extensions to the concept of PBEM gaming, however the presentation and actual capabilities are completely different. They have been designed to replicate the look and feel of existing board or miniatures wargames on the computer. The map and counters are presented to the user who can then manipulate these, more-or-less as if he were playing the physical game, and send a saved file off to his opponent, who can review what has been done without having to duplicate everything on his physical set-up of the game, and respond. Some allow for both players to get on-line and see each other's moves in real-time.
These systems are generally set up so that while one can play the game, the program has no knowledge of the rules, and cannot enforce them. The human players must have a knowledge of the rules themselves. The idea is to promote the playing of the games (by making play against a remote opponent easier), while supporting the industry (and reducing copyright issues) by ensuring that the players have access to the actual physical game.
The four main programs that can be used to play a number of games each are "Aide de Camp", "Cyberboard", "Vassal" and "ZunTzu". "Aide de Camp" is available for purchase, while the other three are offered free. "Vassal" is in turn an outgrowth of the "VASL" (Virtual "ASL") project, and uses Java, making it accessible to any computer that can run a modern JVM, while the other three are Microsoft Windows programs.
Play-by-Mail (PBM).
Wargames were played remotely through the mail, with players sending lists of moves, or orders, to each other through the mail.
In some early PBM systems, six sided dice rolling was simulated by designating a specific stock and a future date and once that date passed, the players would determine an actions outcome using the sales in hundreds value for specific stocks on a specific date and then dividing the NYSE published sales in hundreds by six, using the remainder as the dice result.
"Nuclear Destruction", by the Flying Buffalo, was an early PBM game in 1970. Origins Award Hall-of-Fame member "Middle-Earth Play-By-Mail" is still active today.
Reality Simulations, Inc. still runs a number of PBM games, such as Duel2 (formerly known as Duelmasters), Hyborian War, and Forgotten Realms: War of the Avatars.
E-mail and traditional versions.
Since e-mail is faster than the standard postal service, the rise of the Internet saw a shift of people playing board wargames from play-by-mail (PBM) to play-by-email (PBEM) or play-by-web (PBW). The mechanics were the same, merely the medium was faster.
At this time, turn-based strategy computer games still had a decent amount of popularity, and many started explicitly supporting the sending of saved-game files through email (instead of needing to find the file to send to the opponent by hand). As with all types of video games, the rise in home networking solutions and Internet access has also meant that networked games are now common and easy to set up.
Types.
While wargaming is a genre itself, it can be categorized into a number of subgenres. The most obvious division is by the categories given above. i.e., miniatures, board, computer, etc. This is so obvious, in fact, that most people verbally (and mentally) skip over it. A person might discuss (depending on context) 'board games' or 'wargames' and assume the other element without feeling any need to state 'board wargames'.
Beyond this, there are a few other characteristics that are used to define wargames. Another element that tends to be assumed is the "environment", or type of warfare (land, naval, air) depicted, at least if the subject matter is land warfare (a game on naval or air warfare will specify such if not immediately obvious). The most common genres that categories are explicitly based on is the period or era of the game, and then the scale of the game. Naturally, games concerned with a particular combination of period, scale and environment tend to emphasize similar features.
Environment.
The bulk of wargames concentrate on land warfare, the oldest of all types of warfare, and generally the easiest to simulate. Naval warfare and naval wargames are also popular, and go all the way back to the beginnings of the hobby. Aerial warfare is relatively recent, and wargames on the subject are usually tactical games simulating dogfights, there are relatively few dealing with "just" the air war of a larger conflict. Dealing with multiple elements complicates the model of the simulation side of a wargame, so games with a true combined arms approach tend to be strategic in nature, where all aspects are abstracted to a greater degree. While there are some near-future possibilities for space warfare, there are very few 'serious' games on the subject, and wargames set in space are almost purely in the genre of science fiction.
Historical period.
As wargames are generally historical, games are generally grouped into periods. These divisions mirror the scholarly divisions of history to some extent, but as certain subjects are very popular, certain "wars" are a category all by themselves. World War II, the American Civil War, and the Napoleonic Wars are the most popular historical categories, with other subjects generally being broken down as Ancients, Middle Ages, Early Gunpowder, Horse and Musket, and Modern. Note that much of history from 1800-1950 is often not reflected well in general parlance as they are overshadowed by the 'big three', games on other subjects in this era are often referred to by the actual war they deal with. Unsurprisingly the various periods and wars, especially the better known ones, are usually referred to as initials or acronyms. Thus 18th century is usually broken down into sub periods such as 'WSS' (War of Spanish Succession), 'WAS' (War of Austrian Succession), 'SYW' (Seven Years' War) and 'AWI' (American War of Independence). Some rules will take a broad brush approach, and cover a large period of time, such as 'Black Powder' which advertises as covering both 18th and 19th centuries, and have period specific sub-rules. Others concentrate purely on one war, such as 'Beneath the Lily Banners' (or BLB as it is often called) concentrates purely of the War of Spanish Succession.
Early 'modern' wargaming, as popularised by Grant and Featherstone, usually broke down the history into "Ancients", usually Biblical and Classical eras, "Horse and Musket", covering the 18th and 19th centuries, and "Modern", World War II onwards.
In the early days, wargames were either historical, or somewhat abstract. "Tactics II", the first general commercial board wargame, featured a fictional landscape with two made up countries but whose armies had capabilities based on contemporary conventional forces. Analogous to those, are the 'contemporary' games, ones that simulate current forces and postulate what an actual war involving them would be like. These were popular during the Cold War, but have faded with the fall of the Soviet Union. During the 1970s, fantasy and science fiction made themselves felt as genres that could work inside of wargames. These tend to be more varied, as different assumptions can lead to vastly different types of warfare, but there has been no real concern with subdividing the genres more closely.
Finally, wargames do not necessarily have to involve traditional concepts of warfare and battles and games can enact typical film genres such as gang battles, crime and law enforcement. Similarly martial arts or even non-combat situations and adventures can be gamed where there are other objectives that require strategy combined with the elements of chance (dice/cards etc.) to be achieved.
Notable examples.
Board versions.
While a comprehensive list will show the variety of titles, the following games are notable for the reasons indicated:
Miniature versions.
See also List of miniature wargames.

</doc>
<doc id="33109" url="http://en.wikipedia.org/wiki?curid=33109" title="Wearable computer">
Wearable computer

Wearable computers, also known as body-borne computers or wearables are miniature electronic devices that are worn by the bearer under, with or on top of clothing. This class of wearable technology has been developed for general or special purpose information technologies and media development. Wearable computers are especially useful for applications that require more complex computational support than just hardware coded logics.
If one is asked to give a simple, yet modern, example for wearable technology, that will be the Nike+ system which allows you to track your time, distance, pace and calories via a sensor in the shoe. Another example can be Google Glass, which combine innovative displays with some novel gestural movements for interaction.
One of the main features of a wearable computer is consistency. There is a constant interaction between the computer and user, i.e. there is no need to turn the device on or off. Another feature is the ability to multi-task. It is not necessary to stop what you are doing to use the device; it is augmented into all other actions. These devices can be incorporated by the user to act like a prosthetic. It can therefore be an extension of the user’s mind and/or body.
Many issues are common to the wearables as with mobile computing, ambient intelligence and ubiquitous computing research communities, including power management and heat dissipation, software architectures, wireless and personal area networks.
Areas of applications.
In many applications, user's skin, hands, voice, eyes, arms as well as motion or attention are actively engaged as the physical environment.
Wearable computer items have been initially developed for and applied with e.g.
and other usage also.
Today still "wearable computing" is a topic of active research, with areas of study including user interface design, augmented reality, pattern recognition. The use of wearables for specific applications or for compensating disabilities as well as supporting elderly people steadily increases.
The application of wearable computers into fashion design is evident through Microsoft's prototype of "The Printing Dress" at the International Symposium on Wearable Computers in June 2011.
History.
Due to the varied definitions of "wearable" and "computer", the first wearable computer could be as early as the first abacus on a necklace, a 16th-century abacus ring, the first wristwatch made by Breguet for the Queen of Naples in 1810, or the covert timing devices hidden in shoes to cheat at roulette by Thorp and Shannon in the 1960s and 1970s.
A computer is not merely a time-keeping or calculating device, but rather a user-programmable item for complex algorithms, interfacing, and data management. By this definition, the wearable computer was invented by Steve Mann, in the late 1970s:
 Steve Mann, a professor at the University of Toronto, was hailed as the father of the wearable computer and the ISSCC's first virtual panelist, by moderator Woodward Yang of Harvard University (Cambridge Mass.).
— IEEE ISSCC 8 Feb. 2000
The development of wearable items has taken several steps of miniaturization from discrete electronics over hybrid designs to fully integrated designs, where just one processor chip, a battery and some interface conditioning items make the whole unit.
1600s.
The Qing Dynasty saw the introduction of a fully functional abacus on a ring, which could be used while it was being worn.
1800s.
The first wearable timepiece was made by watchmaker Breguet for the Queen of Naples in 1810. It was a small ladies' pocket watch on a bracelet chain. Again, a wristwatch is a "wearable computer" in the sense that it can be worn, and that it also computes time. But it is not a general-purpose computer in the sense of the modern word.
Military use of wearables: In Girard-Perregaux made wristwatches for the German Imperial Navy after an artillery officer complained that it was not convenient to use both hands to operate a pocket watch while timing his bombardments. The officer had strapped a pocket watch onto his wrist and his superiors liked his solution, and thus asked La Chaux-de-Fonds to travel to Berlin to begin production of small pocket watches attached to wrist bracelets.
Early acceptance of wristlets by men serving in the military was not widespread, though:
 Wristlets, as they were called, were reserved for women, and considered more of a passing fad than a serious timepiece. In fact, they were held in such disdain that many a gentlemen were actually quoted to say they "would sooner wear a skirt as wear a wristwatch".
 — International Watch Magazine
1960s and 1970s.
In 1961 mathematicians Edward O. Thorp, and Claude Shannon built some computerized timing devices to help them cheat at the game of roulette. One such timer was concealed in a shoe, another in a pack of cigarettes. Various versions of this apparatus were built in the 1960s and 1970s. Detailed pictures of a shoe-based timing device can be viewed at .
Thorp refers to himself as the inventor of the first "wearable computer" In other variations, the system was a concealed cigarette-pack sized analog computer designed to predict the motion of roulette wheels. A data-taker would use microswitches hidden in his shoes to indicate the speed of the roulette wheel, and the computer would indicate an octant of the roulette wheel to bet on by sending musical tones via radio to a miniature speaker hidden in a collaborator's ear canal. The system was successfully tested in Las Vegas in June 1961, but hardware issues with the speaker wires prevented it from being used beyond test runs. This was not a wearable computer, because it could not be repurposed during use; rather it was an example of task-specific hardware. This work was kept secret until it was first mentioned in Thorp's book "Beat the Dealer" (revised ed.) in 1966 and later published in detail in 1969.
The 1970s saw the rise of similar special purpose hardware timing devices, such as roulette prediction devices using next-generation technology. In particular, a group known as Eudaemonic Enterprises used a CMOS 6502 microprocessor with 5K RAM to create a shoe computer with inductive radio communications between a data-taker and bettor.
Another early wearable system was a camera-to-tactile vest for the blind, published by C.C. Collins in 1977, that converted images into a 1024-point, 10-inch square tactile grid on a vest. On the consumer end, 1977 also saw the introduction of the HP-01 algebraic calculator watch by Hewlett-Packard.
1980s.
The 1980s saw the rise of more general-purpose wearable computers that fit the modern definition of "computer" by going beyond task-specific hardware to more general-purpose (e.g. reprogrammable by the user) devices. In 1981 Steve Mann designed and built a backpack-mounted 6502-based wearable multimedia computer with text, graphics, and multimedia capability, as well as video capability (cameras and other photographic systems). Mann went on to be an early and active researcher in the wearables field, especially known for his 1994 creation of the Wearable Wireless Webcam, the first example of Lifelogging.
Though perhaps not technically "wearable," in 1986 Steve Roberts built Winnebiko-II, a recumbent bicycle with on-board computer and chorded keyboard. Winnebiko II was the first of Steve Roberts' forays into nomadic computing that allowed him to type while riding.
1989-1999.
In 1989 Reflection Technology marketed the Private Eye head-mounted display, which scanned a vertical array of LEDs across the visual field using a vibrating mirror. This display gave rise to several hobbyist and research wearables, including Gerald "Chip" Maguire's IBM / Columbia University Student Electronic Notebook, Doug Platt's Hip-PC and Carnegie Mellon University's VuMan 1 in 1991. The Student Electronic Notebook consisted of the Private Eye, Toshiba diskless AIX notebook computers (prototypes) and a stylus based input system plus virtual keyboard, and used direct-sequence spread spectrum radio links to provide all the usual TCP/IP based services, including NFS mounted file systems and X11, all running in the Andrew Project environment. The Hip-PC included an Agenda palmtop used as a chording keyboard attached to the belt and a 1.44 megabyte floppy drive. Later versions incorporated additional equipment from Park Engineering. The system debuted at "The Lap and Palmtop Expo" on 16 April 1991. VuMan 1 was developed as part of a Summer-term course at Carnegie Mellon's Engineering Design Research Center, and was intended for viewing house blueprints. Input was through a three-button unit worn on the belt, and output was through Reflection Tech's Private Eye. The CPU was an 8 MHz 80188 processor with 0.5 MB ROM.
In 1993 the Private Eye was used in Thad Starner's wearable, based on Doug Platt's system and built from a kit from Park Enterprises, a Private Eye display on loan from Devon Sean McCullough, and the Twiddler chording keyboard made by Handykey. Many iterations later this system became the MIT "Tin Lizzy" wearable computer design, and Starner went on to become one of the founders of MIT's wearable computing project. 1993 also saw Columbia University's augmented-reality system known as KARMA: Knowledge-based Augmented Reality for Maintenance Assistance. Users would wear a Private Eye display over one eye, giving an overlay effect when the real world was viewed with both eyes open. KARMA would overlay wireframe schematics and maintenance instructions on top of whatever was being repaired. For example, graphical wireframes on top of a laser printer would explain how to change the paper tray. The system used sensors attached to objects in the physical world to determine their locations, and the entire system ran tethered from a desktop computer.
In 1994 Edgar Matias and Mike Ruicci of the University of Toronto, debuted a "wrist computer." Their system presented an alternative approach to the emerging head-up display plus chord keyboard wearable. The system was built from a modified HP 95LX palmtop computer and a Half-QWERTY one-handed keyboard. With the keyboard and display modules strapped to the operator's forearms, text could be entered by bringing the wrists together and typing. The same technology was used by IBM researchers to create the half-keyboard "belt computer. Also in 1994, Mik Lamming and Mike Flynn at Xerox EuroPARC demonstrated the Forget-Me-Not, a wearable device that would record interactions with people and devices and store this information in a database for later query. It interacted via wireless transmitters in rooms and with equipment in the area to remember who was there, who was being talked to on the telephone, and what objects were in the room, allowing queries like "Who came by my office while I was on the phone to Mark?" As with the Toronto system, Forget-Me-Not was not based on a head-mounted display.
Also in 1994, DARPA started the Smart Modules Program to develop a modular, "humionic" approach to wearable and carryable computers, with the goal of producing a variety of products including computers, radios, navigation systems and human-computer interfaces that have both military and commercial use. In July 1996 DARPA went on to host the "Wearables in 2005" workshop, bringing together industrial, university and military visionaries to work on the common theme of delivering computing to the individual. A follow-up conference was hosted by Boeing in August 1996, where plans were finalized to create a new academic conference on wearable computing. In October 1997, Carnegie Mellon University, MIT, and Georgia Tech co-hosted the IEEE International Symposium on Wearables Computers (ISWC) in Cambridge, Massachusetts. The symposium was a full academic conference with published proceedings and papers ranging from sensors and new hardware to new applications for wearable computers, with 382 people registered for the event.
2000s.
In 2002, as part of Kevin Warwick's Project Cyborg, Warwick's wife, Irena, wore a necklace which was electronically linked to Warwick's nervous system via an implanted electrode array. The color of the necklace changed between red and blue dependent on the signals on Warwick's nervous system. Dr. Bruce H Thomas and Dr. Wayne Piekarski developed the Tinmith wearable computer system to support augmented reality. This work was first published internationally in 2000 in the ISWC conference. The work was carried out at the Wearable Computer Lab in the University of South Australia.
In the late 2000s, various Chinese companies began producing mobile phones in the form of wristwatches, the descendants of which as of 2013 include the i5 and i6, which are GSM phones with 1.8 inch displays, and the ZGPAX s5 Android wristwatch phone.
2010s.
The current moves in standardization with IEEE, IETF and several industry groups (e.g. Bluetooth) leads to more various interfacing under the WPAN (wireless personal area network) and the WBAN (Wireless body area network) offer new classification of designs for interfacing and networking.
Also, the 6th-generation iPod Nano has a wristwatch attachment available to convert it to a wearable wristwatch computer.
The developments of wearable computing now encompasses Rehabilitation Engineering, Ambulatory intervention treatment, life guard systems, Defense wearable systems.
Sony is now selling an Android compatible wrist watch called Sony SmartWatch. It must be paired with an Android phone as an additional, remote display and notification tool.
Google Glass launched their optical head-mounted display (OHMD) to a test group of users in 2013, and plan on launching it to consumers sometime in 2014. Google's mission is to produce a mass-market ubiquitous computer that displays information in a smartphone-like hands-free format, that can interact with the Internet via natural language voice commands.
In September 2014, Apple announced that the company is working on a smartwatch called Apple Watch. According to the New York Times, Apple has been testing both solar and wireless charging for the upcoming product.
Commercialization.
The commercialization of general-purpose wearable computers, as led by companies such as Xybernaut, CDI and ViA, Inc. has thus far met with limited success. Publicly traded Xybernaut tried forging alliances with companies such as IBM and Sony in order to make wearable computing widely available, but in 2005 their stock was delisted and the company filed for Chapter 11 bankruptcy protection amid financial scandal and federal investigation. Xybernaut emerged from bankruptcy protection in January, 2007. ViA, Inc. filed for bankruptcy in 2001 and subsequently ceased operations. 1998 Seiko marketed the Ruputer, a computer in a (fairly large) wristwatch, to mediocre returns. In 2001 IBM developed and publicly displayed two prototypes for a wristwatch computer running Linux. The last message about them dates to , saying the device would cost about $250 but it is still under development. In 2002 Fossil, Inc. announced the Fossil Wrist PDA, which ran the Palm OS. Its release date was set for summer of 2003, but was delayed several times and was finally made available on 5 January 2005. Timex Datalink is another example of a practical wearable computer. Hitachi launched a wearable computer called Poma in 2002. Eurotech offers the ZYPAD, a wrist wearable touch screen computer with GPS, Wi-Fi and Bluetooth connectivity and which can run a number of custom applications. In 2013, a wearable computing device on the wrist to control body temperature was developed at MIT.
Evidence of weak market acceptance was demonstrated when Panasonic Computer Solutions Company's product failed. Panasonic has specialized in mobile computing with their Toughbook line for over 10 years and has extensive market research into the field of portable, wearable computing products. In 2002, Panasonic introduced a wearable brick computer coupled with a handheld or armworn touchscreen. The brick would communicate wirelessly to the screen, and concurrently the brick would communicate wirelessly out to the internet or other networks. The wearable brick was quietly pulled from the market in 2005, while the screen evolved to a thin client touchscreen used with a handstrap. (The "Brick" Computer is the CF-07 Toughbook, dual batteries, screen used same batteries as the base, 800 x 600 resolution, optional GPS and WWAN. Has one M-PCI slot and one PCMCIA slot for expansion. CPU used is a 600 MHz Pentium 3 factory under clocked to 300 MHz so it can stay cool passively as it has no fan. Micro dim ram is upgradable. The screen can be used wirelessly on other computers.)
Google has announced that it has been working on a head-mounted display-based wearable "augmented reality" device called Google Glass. An early version of the device was available to the US public from April 2013 until January 2015. Despite ending sales of the device through their Explorer Program, Google has stated that they plan to continue developing the technology.
LG and iriver produce earbud wearables measuring heart rate and other biometrics, as well as various activity metrics.
Military use.
The wearable computer was introduced to the US Army in 1989 as a small computer that was meant to assist soldiers in battle. Since then, the concept has grown to include the = Land Warrior program and proposal for future systems.<ref name=http://scholar.google.com.au/scholar?client=safari&rls=en&q=army+use+wearable+computer&oe=UTF-8&redir_esc&um=1&ie=UTF-8&lr&cites=8453139059098922145></ref> The most extensive military program in the wearables arena is the US Army's Land Warrior system, which will eventually be merged into the Future Force Warrior system.
F-INSAS is an Indian Military Project, designed largely with wearable computing.

</doc>
<doc id="33110" url="http://en.wikipedia.org/wiki?curid=33110" title="Wilhelm von Humboldt">
Wilhelm von Humboldt

Friedrich Wilhelm Christian Karl Ferdinand von Humboldt (22 June 1767 – 8 April 1835) was a Prussian philosopher, government functionary, diplomat, and founder of the Humboldt University of Berlin, which was named after him in 1949 (and also after his brother, Alexander von Humboldt, a naturalist).
He is especially remembered as a linguist who made important contributions to the philosophy of language and to the theory and practice of education. In particular, he is widely recognized as having been the architect of the Humboldtian education ideal, which was used from the beginning in Prussia as a model for its system of education and later in countries such as the United States and Japan.
His younger brother, Alexander von Humboldt, was famous as a geographer, naturalist, and explorer.
Biography.
Humboldt was born in Potsdam, Margraviate of Brandenburg, and died in Tegel, Province of Brandenburg.
In June 1791, he married Karoline von Dacheröden. They had eight children, of whom five survived to adulthood.
Philosopher.
Humboldt was a philosopher; he wrote "On the Limits of State Action" in 1791–1792 (though it was not published until 1850, after Humboldt's death), one of the boldest defences of the liberties of the Enlightenment. It influenced John Stuart Mill's essay "On Liberty" through which von Humboldt's ideas became known in the English-speaking world. Humboldt outlined an early version of what Mill would later call the "harm principle".
The section dealing with education was published in the December 1792 issue of the "Berlinische Monatsschrift" under the title "On public state education". With this publication, Humboldt took part in the philosophical debate regarding the direction of national education that was in progress in Germany, as elsewhere, after the French Revolution.
Educational reforms.
Humboldt had been home schooled and never finished his comparably short university studies.
Nevertheless, he became one of the most influential officials in German education. Actually, Humboldt had intended to become Minister of education, but failed to attain that position. The Prussian King asked him to leave Rome in 1809 and to lead the directorate of education under Friedrich Ferdinand Alexander zu Dohna-Schlobitten. Humboldt did not reply to the appointment for several weeks and would have preferred to stay on at the embassy in Rome. His wife did not return with him to Prussia; the couple met again when Humboldt stepped down from the educational post and was appointed head of the Embassy in Vienna.
Humboldt installed a standardized system of public instruction, from basic schools till secondary education, and founded Berlin University. He imposed a standardization of state examinations and inspections and created a special department within the ministry to oversee and design curricula, textbooks and learning aids.
Humboldt's plans for reforming the Prussian school system were not published until long after his death, together with his fragment of a treatise on the 'Theory of Human Education', which he had written in about 1793. Here, Humboldt states that 'the ultimate task of our existence is to give the fullest possible content to the concept of humanity in our own person […] through the impact of actions in our own lives.' This task 'can only be implemented through the links established between ourselves as individuals and the world around us' (GS, I, p. 283).
Humboldt's concept of education does not lend itself solely to individualistic interpretation. It is true that he always recognized the importance of the organization of individual life and the 'development of a wealth of individual forms' (GS, III, p. 358), but he stressed the fact that 'self-education can only be continued […] in the wider context of development of the world' (GS, VII, p. 33). In other words, the individual is not only entitled, but also obliged, to play his part in shaping the world around him.
Humboldt's educational ideal was entirely coloured by social considerations. He never believed that the 'human race could culminate in the attainment of a general perfection conceived in abstract terms'. In 1789, he wrote in his diary that 'the education of the individual requires his incorporation into society and involves his links with society at large' (GS, XIV, p. 155). In his essay on the 'Theory of Human Education', he answered the question as to the 'demands which must be made of a nation, of an age and of the human race'. 'Education, truth and virtue' must be disseminated to such an extent that the 'concept of mankind' takes on a great and dignified form in each individual (GS, I, p. 284). However, this shall be achieved personally by each individual, who must 'absorb the great mass of material offered to him by the world around him and by his inner existence, using all the possibilities of his receptiveness; he must then reshape that material with all the energies of his own activity and appropriate it to himself so as to create an interaction between his own personality and nature in a most general, active and harmonious form' (GS, II, p. 117).
Diplomat.
As a successful diplomat between 1802 and 1819, Humboldt was plenipotentiary Prussian minister at Rome from 1802, ambassador at Vienna from 1812 during the closing struggles of the Napoleonic Wars, at the congress of Prague (1813) where he was instrumental in drawing Austria to ally with Prussia and Russia against France, a signer of the peace treaty at Paris and the treaty between Prussia and defeated Saxony (1815), at Frankfurt settling post-Napoleonic Germany, and at the congress at Aachen in 1818. However, the increasingly reactionary policy of the Prussian government made him give up political life in 1819; and from that time forward he devoted himself solely to literature and study.
Linguist.
Wilhelm von Humboldt was an adept linguist and studied the Basque language. He translated Pindar and Aeschylus into German.
Humboldt's work as a philologist in Basque has had more extensive impact than his other work. His visit to the Basque country resulted in "Researches into the Early Inhabitants of Spain by the help of the Basque language" (1821). In this work, Humboldt endeavored to show by examining geographical placenames, that at one time a race or races speaking dialects allied to modern Basque extended throughout Spain, southern France and the Balearic Islands; he identified these people with the "Iberians" of classical writers, and further surmised that they had been allied with the Berbers of northern Africa. Humboldt's pioneering work has been superseded in its details by modern linguistics and archaeology, but is sometimes still uncritically followed even today. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.
Humboldt died while preparing his greatest work, on the ancient Kawi language of Java, but its introduction was published in 1836 as "The Heterogeneity of Language and its Influence on the Intellectual Development of Mankind". This essay on the philosophy of speech:
He is credited with being the first European linguist to identify human language as a rule-governed system, rather than just a collection of words and phrases paired with meanings. This idea is one of the foundations of Noam Chomsky's theory of language. Chomsky frequently quotes Humboldt's description of language as a system which "makes infinite use of finite means", meaning that an infinite number of sentences can be created using a finite number of grammatical rules. Humboldt scholar Tilman Borsche, however, notes profound differences between von Humboldt's view of language and Chomsky's.
More recently, Humboldt has also been credited as an originator of the linguistic relativity hypothesis (more commonly known as the Sapir–Whorf hypothesis), developed by linguists Edward Sapir or Benjamin Whorf a century later.
The reception of Humboldt's work remains problematic in English-speaking countries, despite the work of Langham Brown, Manchester and Underhill (Humboldt, Worldview & Language, 2009), on account of his concept of what he called "Weltansicht", the linguistic worldview, with "Weltanschauung" being translated simply as 'worldview' a term associated with ideologies and cultural mindsets in both German and English. The centrality of distinction in understanding Huimbolt's work was set out by one of the leading contemporary German Humboldt scholars, Jürgen Trabant, in his works in both German and French. Polish linguists, at the Lublin School (see Jerzy Bartmiński) in their research of Humboldt, also stress this distinction between the worldviews of a personal or political kind and the worldview that is implicit in language as a conceptual system.
However, little rigorous research in English has gone into exploring the relationship between the linguistic worldview and the transformation and maintenance of this worldview by individual speakers. One notable exception is the work of Underhill, who explores comparative linguistic studies in both "Creating Worldviews: Language", "Ideology & Metaphor" (2011) and in "Ethnolinguistics and Cultural Concepts: Truth, Love, Hate & War". In Underhill's work, a distinction is made between five forms of worldview: world-perceiving, world-conceiving, cultural mindset, personal world and perspective, in order to convey the distinctions Humboldt was concerned with preserving in his ethnolinguistics. Probably the most well-known linguist working with a truly Humboldtian perspective writing in English today is Anna Wierzbicka, who has published a number of comparative works on semantic universals and conceptual distinctions in language.
The Rouen Ethnolinguistics Project, in France, has recently put online a 7-hour series of lectures on Humblodt's thought on language, with the Berlin specialist, Prof. Trabant
THE JURGEN TRABANT WILHELM VON HUMBOLDT LECTURES
https://webtv.univ-rouen.fr/permalink/c1253a18f7e5ecnge8dp/ 
Trabant's lectures take viewers into Humboldt with a very gentle introduction, and then get down to the deeper questions of worldview, concept-formation, translation, and the work of the mind.

</doc>
<doc id="33120" url="http://en.wikipedia.org/wiki?curid=33120" title="Web crawler">
Web crawler

A Web crawler is an Internet bot which systematically browses the World Wide Web, typically for the purpose of Web indexing. A Web crawler may also be called a Web spider, an ant, an automatic indexer, or (in the FOAF software context) a Web scutter.
Web search engines and some other sites use Web crawling or spidering software to update their web content or indexes of others sites' web content. Web crawlers can copy all the pages they visit for later processing by a search engine which indexes the downloaded pages so the users can search much more efficiently.
Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping (see also data-driven programming).
Overview.
A Web crawler starts with a list of URLs to visit, called the "seeds". As the crawler visits these URLs, it identifies all the hyperlinks in the page and adds them to the list of URLs to visit, called the "crawl frontier". URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as they were on the live web, but are preserved as ‘snapshots'.
The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.
The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content. Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.
As Edwards "et al." noted, "Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained." A crawler must carefully choose at each step which pages to visit next.
Crawling policy.
The behavior of a Web crawler is the outcome of a combination of policies:
Selection policy.
Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40-70% of the indexable Web; a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. As a crawler always downloads just a fraction of the Web pages, it is highly desirable for the downloaded fraction contains the most relevant pages and not just a random sample of the Web.
This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain, or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling.
Cho "et al." made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. The ordering metrics tested were breadth-first, backlink count and partial Pagerank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his Ph.D. dissertation at Stanford on web crawling.
Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that "the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates."
Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). In OPIC, each page is given an initial sum of "cash" that is distributed equally among the pages it points to. It is similar to a Pagerank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of "cash". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web.
Boldi "et al." used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Surprisingly, some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations.
Baeza-Yates "et al." used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one.
Daneshpajouh "et al." designed a community based algorithm for discovering good seeds. Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds a new crawl can be very effective.
Restricting followed links.
A crawler may only want to seek out HTML pages and avoid all other MIME types. In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped.
Some crawlers may also avoid requesting any resources that have a "?" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses a rewrite engine to simplify its URLs.
URL normalization.
Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term "URL normalization", also called "URL canonicalization", refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of "." and ".." segments, and adding trailing slashes to the non-empty path component.
Path-ascending crawling.
Some crawlers intend to download as many resources as possible from a particular web site. So "path-ascending crawler" was introduced that would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling.
Focused crawling.
The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers. The concepts of topical and focused crawling were first introduced by Filippo Menczer and by Soumen Chakrabarti "et al."
The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton in the first web crawler of the early days of the Web. Diligenti "et al." propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points.
Academic-focused crawler.
An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the "citeseerxbot", which is the crawler of CiteSeerX search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open source crawlers, such as Heritrix, must be customized to filter out other MIME types, or a middleware is used to extract these documents out and import them to the focused crawl database and repository. Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents takes only a small fraction in the entire web pages, a good seed selection are important in boosting the efficiencies of these web crawlers. Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads.
Re-visit policy.
The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates and deletions.
From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age.
Freshness: This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page "p" in the repository at time "t" is defined as:
Age: This is a measure that indicates how outdated the local copy is. The age of a page "p" in the repository, at time "t" is defined as:
Coffman "et al." worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler.
The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are out-dated, while in the second case, the crawler is concerned with how old the local copies of pages are.
Two simple re-visiting policies were studied by Cho and Garcia-Molina:
Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change.
Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency.
Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them.
To improve freshness, the crawler should penalize the elements that change too often. The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman "et al." note, "in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible". Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, while Ipeirotis "et al." show how to use statistical tools to discover parameters that affect this distribution. Note that the re-visiting policies considered here regard all pages as homogeneous in terms of quality ("all pages on the Web are worth the same"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy.
Politeness policy.
Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. Needless to say, if a single crawler is performing multiple requests per second and/or downloading large files, a server would have a hard time keeping up with requests from multiple crawlers.
As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. The costs of using Web crawlers include:
A partial solution to these problems is the robots exclusion protocol, also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google, Ask Jeeves, MSN and Yahoo! Search are able to use an extra "Crawl-delay:" parameter in the robots.txt file to indicate the number of seconds to delay between requests.
The first proposed interval between successive pageloads was 60 seconds. However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. This does not seem acceptable.
Cho uses 10 seconds as an interval for accesses, and the WIRE crawler uses 15 seconds as the default. The MercatorWeb crawler follows an adaptive politeness policy: if it took "t" seconds to download a document from a given server, the crawler waits for 10"t" seconds before downloading the next page. Dill "et al." use 1 second.
For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl.
Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3–4 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Brin and Page note that: "... running a crawler which connects to more than half a million servers (...) generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen."
Parallelization policy.
A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes.
Architectures.
A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture.
Shkapenyuk and Suel noted that:
While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability.
Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about "search engine spamming", which prevent major search engines from publishing their ranking algorithms.
Security.
While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presense in search engines, web crawling can also have unintended consequences and lead to a compromise or data breach if search engine indexes resources that shouldn't be publicly available or pages revealing potentially vulnerable versions of software. In a study from 2013 majority of websites that were victims of opportunistic hacking (mostly website defacements) were pretty well indexed by search engines, which was the main factor to allowed the attackers to find potential victims using specific search engine queries.
Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing (with robots.txt) search engines to index the public parts of their websites and explicitly blocking indexing of transactional parts (login pages, private pages etc).
Crawler identification.
Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.
It is important for Web crawlers to identify themselves so that Web site administrators can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine.
Crawling the deep web.
A vast amount of web pages lie in the deep or invisible web. These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai are intended to allow discovery of these deep-Web resources.
Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in codice_1 form. In some cases, such as the Googlebot, Web crawling is done on all text contained inside the hypertext content, tags, or text.
Strategic approaches may be taken to target deep Web content. With a technique called screen scraping, specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers.
Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index.
Web crawler bias.
A recent study based on a large scale analysis of robots.txt files showed that certain web crawlers were preferred over others, with Googlebot being the most preferred web crawler.
Visual vs programmatic crawlers.
There are a number of "visual web scraper/crawler" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of "visual scrapers" like outwithub and import.io remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data.
The visual scraping/crawling methodology relies on the user "teaching" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs), there is continued growth and investment in this area by investors and end-users.
Examples.
The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features:
In addition to the specific crawler architectures listed above, there are general crawler architectures published by Cho
and Chakrabarti.

</doc>
<doc id="33123" url="http://en.wikipedia.org/wiki?curid=33123" title="Wireless Valley">
Wireless Valley

Wireless Valley is a term that was coined by Professor Ted Rappaport in 1990 when he was a faculty member at Virginia Tech, and was used to describe the Roanoke/Blacksburg, Virginia region and the potential of research to create spin-out companies. He and his students founded TSR Technologies in 1990, a company that made software-defined cellular and paging intercept and drive test equipment that was sold to Allen Telecom in 1993, and Wireless Valley Communications in 1995, a company that pioneered the creation of computer-aided wireless network prediction and management software that was sold to Motorola in late 2005. This term was later used as nickname to describe several regional clusters of companies in the information technology sector, in analogy to California's Silicon Valley:

</doc>
<doc id="33124" url="http://en.wikipedia.org/wiki?curid=33124" title="Wild Strawberries (film)">
Wild Strawberries (film)

 
Wild Strawberries is a 1957 Swedish film written and directed by Ingmar Bergman about an old man recalling his past. The original Swedish title is Smultronstället, which literally means "The wild strawberry patch" but idiomatically signifies an underrated gem of a place, often with personal or sentimental value. The cast includes Victor Sjöström in his final screen performance, as well as Bergman regulars Bibi Andersson, Ingrid Thulin and Gunnar Björnstrand. Max von Sydow also appears in a small role. Bergman wrote the screenplay while hospitalized. Exploring philosophical themes such as introspection and human existence, "Wild Strawberries" is often considered to be one of Bergman's greatest and most moving films.
Plot.
Grouchy, stubborn and egotistical Professor Isak Borg is a widowed 78-year-old physician who specialized in bacteriology. Before specializing he served as general practitioner in rural Sweden. He sets out on a long car ride from Stockholm to Lund to be awarded the degree of Doctor Jubilaris 50 years after he received his doctorate from Lund University. He is accompanied by his pregnant daughter-in-law Marianne who does not much like her father-in-law and is planning to separate from her husband, Evald, Isak's only son, who does not want her to have the baby, their first.
During the trip, Isak is forced by nightmares, daydreams, old age and impending death to reevaluate his life. He meets a series of hitchhikers, each of whom sets off dreams or reveries into Borg's troubled past. The first group consists of two young men and their companion, a woman named Sara who is adored by both men. Sara, a double for Isak's love of his youth, is played by the same actress. The first group remains with him throughout his journey. Next Isak and Marianne pick up an embittered middle-aged couple, the Almans, whose vehicle has nearly collided with theirs. The pair exchanges such terrible vitriol and venom that Marianne stops the car and demands that they leave. The couple reminds Isak of his own unhappy marriage. In a dream sequence, Isak is asked by Sten Alman, now the examiner, to read “foreign” letters on the blackboard. He cannot. So, Alman reads it for him: "A doctor's first duty is to ask forgiveness," from which he concludes, "You are guilty of guilt." 
He reminisces about his childhood at the seaside and his sweetheart Sara, with whom he remembered gathering strawberries, but who instead married his brother. He is confronted by his loneliness and aloofness, recognizing these traits in both his ancient mother (whom they stop to visit) and in his middle-aged physician son, and he gradually begins to accept himself, his past, his present and his approaching death. In one dream, he is quizzed by a very judgmental medical professor; he is also praised by a small-town merchant who remembers him.
Borg finally arrives at his destination and is promoted to Doctor Jubilaris, but this proves to be an empty ritual. That night, he bids a loving goodbye to his young friends, to whom the once bitter old man whispers in response to a playful declaration of the young girl's love, "I'll remember." As he goes to his bed in his son's home, he is overcome by a sense of peace, and dreams of a family picnic by a lake. Closure and affirmation of life have finally come, and Borg's face radiates joy.
Production.
Origins.
Bergman's idea for the film originated on a drive from Stockholm to Dalarna during which he stopped in Uppsala, his hometown. Driving by his grandmother's house, he suddenly imagined how it would be if he could open the door and inside find everything just as it was during his childhood. "So it struck me — what if you could make a film about this; that you just walk up in a realistic way and open a door, and then you walk into your childhood, and then you open another door and come back to reality, and then you make a turn around a street corner and arrive in some other period of your existence, and everything goes on, lives. That was actually the idea behind "Wild Strawberries"". Later, he would revise the story of the film's genesis. In "Images: My Life in Film," he comments on his own earlier statement: "That's a lie. The truth is that I am forever living in my childhood."
Development.
Bergman wrote the screenplay of "Wild Strawberries" in Stockholm's Karolinska Hospital (the workplace of Isak Borg) in the late spring of 1957; he'd recently been given permission to proceed by producer Carl Anders Dymling on the basis of a short synopsis. He was in the hospital for two months, being treated for recurrent gastric problems and general stress. Bergman's doctor at Karolinska was his good friend Sture Helander who invited him to attend his lectures on psychosomatics. Helander was married to Gunnel Lindblom who was to play Isak's sister Charlotta in the film. Bergman was at a high point of his professional career after a triumphant season at the Malmö City Theatre, where he had been artistic director since 1952, and the success of both "Smiles of a Summer Night" and "The Seventh Seal". However his private life was in disarray; his third marriage was on the rocks; his affair with Bibi Andersson, which had begun in 1954, was coming to an end; his relationship with his parents was, after an attempted reconciliation with his mother, at a desperately low ebb.
Casting and preproduction progressed rapidly. The completed screenplay is dated 31 May and shooting took place between 2 July 1957 and 27 August 1957. The scenes at the summer house were filmed in Saltsjöbaden, a fashionable resort in the Stockholm archipelago. Part of the nightmare sequence was shot with predawn summer light in Gamla stan, the old part of central Stockholm. Most of the movie was made at SF’s studio and on its back lot at Råsunda in northern Stockholm.
Casting.
The director's immediate choice for the leading role of the old professor was Victor Sjöström, Bergman's silent film idol and early counselor at Svensk Filmindustri, whom he had directed in "To Joy" eight years earlier. "Victor," Bergman remarked, "was feeling wretched and didn’t want to [do it];... he must have been seventy eight. He was misanthropic and tired and felt old. I had to use all my powers of persuasion to get him to play the part."
In "Bergman on Bergman," he has stated that he only thought of Sjöström when the screenplay was complete, and that he asked Dymling to contact the famous actor and film director. Yet in "Images: My Life in Film," he claims, "It is probably worth noting that I never for a moment thought of Sjöström when I was writing the screenplay. The suggestion came from the film's producer, Carl Anders Dymling. As I recall, I thought long and hard before I agreed to let him have the part."
During the shooting, the health of the 79-year-old Sjöström gave cause for concern. Dymling had persuaded him to take on the role with the words: "All you have to do is lie under a tree, eat wild strawberries and think about your past, so it's nothing too arduous." This was inaccurate and the burden of the film was completely on Sjöström who is in all but one scene of the film. Initially, Sjöström had problems with his lines which made him frustrated and angry. He would go off into a corner and beat his head against the wall in frustration, even to the point of drawing blood and producing bruises. He sometimes quibbled over details in the script. To unburden his revered mentor, Bergman made a pact with Ingrid Thulin that if anything went wrong during a scene, she would take the blame on herself. Things improved when they changed filming times so that Sjöström could get home in time for his customary late afternoon whisky at 5:00. Sjöström got along particularly well with Bibi Andersson.
As usual, Bergman chose his collaborators from a team of actors and technicians with whom he had worked before in the cinema and the theater. As Sara, Bibi Andersson plays both Borg’s childhood sweetheart who left him to marry his brother and a charming, energetic young woman who reminds him of that lost love. Andersson, then twenty one years old, was a member in Bergman's famed repertory company. He gave her a small part in his films "Smiles of a Summer Night" (1955) and as the jester’s wife in "The Seventh Seal" (1957). She would continue to work for him in many more films, most notably in "Persona" (1966). Ingrid Thulin plays Marianne, the sad, gentle and warm daughter-in-law of Borg. She appeared in other Bergman films as the mistress in "Winter Light" (1963) and as one of three sisters in "Cries and Whispers" (1972). Bergman’s first wife, Else Fisher, made a brief unaccredited appearance as Borg’s mother in the final flashback; their daughter, Lena, played one of Isak’s twin sisters.
Awards and honors.
The film won the Golden Bear for Best Film at the 8th Berlin International Film Festival, "Best Film" and "Best Actor" at the Mar del Plata Film Festival and won the Golden Globe for Best Foreign Film in 1960. It was also nominated for an Academy Award for Original Screenplay.
The film is included on the Vatican Best Films List, recommended for its portrayal of a man's "interior journey from pangs of regret and anxiety to a refreshing sense of peace and reconciliation".
The film also influenced the Woody Allen films "Stardust Memories", "Another Woman", "Crimes and Misdemeanors", and "Deconstructing Harry". In "Stardust Memories", the film's plot is similar in that the protagonist, filmmaker Sandy Bates (Woody Allen), is attending a viewing of his films, while reminiscing about and reflecting on his life and past relationships and trying to fix and stabilize his current ones, which are infused with flashbacks and dream sequences. In "Another Woman", the film’s main character, Marion Post (Gena Rowlands), is also accused by friends and relatives of being cold and unfeeling, which forces her to reexamine her life. Allen also borrows several tropes from Bergman’s film, such as having Lynn (Frances Conroy), Post’s sister-in-law, tell her that her brother Paul (Harris Yulin) hates her and having a former student tell Post that her class changed her life. Allen has Post confront the demons of her past via several dream sequences and flashbacks that reveal important information to a viewer, as in "Wild Strawberries". In "Crimes and Misdemeanors", Allen made reference to the scene in which Isak watches his family have dinner. In "Deconstructing Harry", the plot (that of an academic on a long drive to receive an honorary award from his old university, as well as the people whom he picks up, while reflecting upon his life's experiences, with dream sequences) essentially mirrors that of "Wild Strawberries".
In a 1963 interview with "Cinema" magazine, Stanley Kubrick listed the film as his second favourite of all time.
In 2009, Roger Scruton wrote, "The camera stalks the unfolding story like a hunter, pausing to take aim at the present only to bring it into chaﬁng proximity with the past. And the images, often grainy, with sharply foregrounded details, leave many objects lingering like ghosts in the out-of-focus hinterland. In "Wild Strawberries", things, like people, are saturated with the psychic states of their observers, drawn into the drama by a camera which endows each detail with a consciousness of its own. The result is not whimsical or arbitrary, but on the contrary, entirely objective, turning to realities at every point where the camera might otherwise be tempted to escape from them. "Wild Strawberries" is one of many examples of true cinematic art".
In 2012, "Wild Strawberries" was ranked 63rd in the "Sight & Sound" critics' poll of the greatest films ever made.

</doc>
<doc id="33130" url="http://en.wikipedia.org/wiki?curid=33130" title="Werner Heisenberg">
Werner Heisenberg

Werner Karl Heisenberg (]; 5 December 1901 – 1 February 1976) was a German theoretical physicist and one of the key pioneers of quantum mechanics. He published his work in 1925 in a breakthrough paper. In the subsequent series of papers with Max Born and Pascual Jordan, during the same year, this matrix formulation of quantum mechanics was substantially elaborated. In 1927 he published his uncertainty principle, upon which he built his philosophy and for which he is best known. Heisenberg was awarded the Nobel Prize in Physics for 1932 "for the creation of quantum mechanics". He also made important contributions to the theories of the hydrodynamics of turbulent flows, the atomic nucleus, ferromagnetism, cosmic rays, and subatomic particles, and he was instrumental in planning the first West German nuclear reactor at Karlsruhe, together with a research reactor in Munich, in 1957. Considerable controversy surrounds his work on atomic research during World War II.
Following World War II, he was appointed director of the Kaiser Wilhelm Institute for Physics, which soon thereafter was renamed the Max Planck Institute for Physics. He was director of the institute until it was moved to Munich in 1958, when it was expanded and renamed the Max Planck Institute for Physics and Astrophysics.
Heisenberg was also president of the German Research Council, chairman of the Commission for Atomic Physics, chairman of the Nuclear Physics Working Group, and president of the Alexander von Humboldt Foundation.
Life and career.
Early years.
Heisenberg was born in Würzburg, Germany, to Kaspar Earnesta August Heisenberg, a secondary school teacher of classical languages who became Germany's only "ordentlicher Professor" (ordinarius professor) of medieval and modern Greek studies in the university system, and his wife, Annie Wecklein.
He studied physics and mathematics from 1920 to 1923 at the "Ludwig-Maximilians-Universität München" and the "Georg-August-Universität Göttingen". At Munich, he studied under Arnold Sommerfeld and Wilhelm Wien. At Göttingen, he studied physics with Max Born and James Franck, and he studied mathematics with David Hilbert. He received his doctorate in 1923, at Munich under Sommerfeld. He completed his Habilitation in 1924, at Göttingen under Born.
Because Sommerfeld had a sincere interest in his students and knew of Heisenberg's interest in Niels Bohr's theories on atomic physics, Sommerfeld took Heisenberg to Göttingen to the "Bohr-Festspiele" (Bohr Festival) in June 1922. At the event, Bohr was a guest lecturer and gave a series of comprehensive lectures on quantum atomic physics. There, Heisenberg met Bohr for the first time, and it had a significant and continuing effect on him.
Heisenberg's doctoral thesis, the topic of which was suggested by Sommerfeld, was on turbulence; the thesis discussed both the stability of laminar flow and the nature of turbulent flow. The problem of stability was investigated by the use of the Orr–Sommerfeld equation, a fourth order linear differential equation for small disturbances from laminar flow. He briefly returned to this topic after World War II.
Heisenberg's paper on the anomalous Zeeman effect was accepted as his "Habilitationsschrift" (Habilitation thesis) under Max Born at Göttingen.
In his youth he was a member and Scoutleader of the "Neupfadfinder", a German Scout association and part of the German Youth Movement. In August 1923 Robert Honsell and Heisenberg organized a trip ("Großfahrt") to Finland with a Scout group of this association from Munich.
Heisenberg arrived to Munich in 1919 as a member of Freikorps to fight the Bavarian Soviet Republic established a year earlier. Five decades later he recalled those days as youthful fun, like "playing cops and robbers and so on; it was nothing serious at all."
Career.
Göttingen, Copenhagen, and Leipzig.
From 1924 to 1927, Heisenberg was a Privatdozent at Göttingen. From 17 September 1924 to 1 May 1925, under an International Education Board Rockefeller Foundation fellowship, Heisenberg went to do research with Niels Bohr, director of the Institute of Theoretical Physics at the University of Copenhagen. His seminal paper, "Über quantentheoretischer Umdeutung" was published in September 1925. He returned to Göttingen and with Max Born and Pascual Jordan, over a period of about six months, developed the matrix mechanics formulation of quantum mechanics. On 1 May 1926, Heisenberg began his appointment as a university lecturer and assistant to Bohr in Copenhagen. It was in Copenhagen, in 1927, that Heisenberg developed his uncertainty principle, while working on the mathematical foundations of quantum mechanics. On 23 February, Heisenberg wrote a letter to fellow physicist Wolfgang Pauli, in which he first described his new principle. In his paper on the uncertainty principle, Heisenberg used the word "Ungenauigkeit" (imprecision).
In 1927, Heisenberg was appointed "ordentlicher Professor" (ordinarius professor) of theoretical physics and head of the department of physics at the Universität Leipzig; he gave his inaugural lecture on 1 February 1928. In his first paper published from Leipzig, Heisenberg used the Pauli exclusion principle to solve the mystery of ferromagnetism.
In Heisenberg's tenure at Leipzig, the quality of doctoral students, post-graduate and research associates who studied and worked with Heisenberg there is attested to by the acclaim later earned by these people; at various times, they included: Erich Bagge, Felix Bloch, Ugo Fano, Siegfried Flügge, William Vermillion Houston, Friedrich Hund, Robert S. Mulliken, Rudolf Peierls, George Placzek, Isidor Isaac Rabi, Fritz Sauter, John C. Slater, Edward Teller, John Hasbrouck van Vleck, Victor Frederick Weisskopf, Carl Friedrich von Weizsäcker, Gregor Wentzel and Clarence Zener.
In early 1929, Heisenberg and Pauli submitted the first of two papers laying the foundation for relativistic quantum field theory. Also in 1929, Heisenberg went on a lecture tour in China, Japan, India, and the United States.
Shortly after the discovery of the neutron by James Chadwick in 1932, Heisenberg submitted the first of three papers on his neutron-proton model of the nucleus. He was awarded the 1932 Nobel Prize in Physics.
In 1928, the British mathematical physicist P. A. M. Dirac had derived the relativistic wave equation of quantum mechanics, which implied the existence of positive electrons, later to be named positrons. In 1932, from a cloud chamber photograph of cosmic rays, the American physicist Carl David Anderson identified a track as having been made by a positron. In mid-1933, Heisenberg presented his theory of the positron. His thinking on Dirac's theory and further development of the theory were set forth in two papers. The first, "Bemerkungen zur Diracschen Theorie des Positrons (Remarks on Dirac's theory of the positron)" was published in 1934, and the second, "Folgerungen aus der Diracschen Theorie des Positrons (Consequences of Dirac's Theory of the Positron)", was published in 1936. In these papers Heisenberg was the first to reinterpret the Dirac equation as a "classical" field equation for any point particle of spin ħ/2, itself subject to quantization conditions involving anti-commutators. Thus reinterpreting it as a (quantum) field equation accurately describing electrons, Heisenberg put matter on the same footing as electromagnetism: as being described by relativistic quantum field equations which allowed the possibility of particle creation and destruction. (Hermann Weyl had already described this in a 1929 letter to Einstein.)
In the early 1930s in Germany, the "Deutsche Physik" movement was anti-Semitic and anti-theoretical physics, especially including quantum mechanics and the theory of relativity. As applied in the university environment, political factors took priority over the historically applied concept of scholarly ability, even though its two most prominent supporters were the Nobel Laureates in Physics Philipp Lenard and Johannes Stark.
After Adolf Hitler came to power in 1933, Heisenberg was attacked in the press as a "White Jew" by elements of the "Deutsche Physik" (German Physics) movement for his insistence on teaching about the roles of Jewish scientists. As a result, he came under investigation by the SS. This was over an attempt to appoint Heisenberg as successor to Arnold Sommerfeld at the University of Munich. The issue was resolved in 1938 by Heinrich Himmler, head of the SS. While Heisenberg was not chosen as Sommerfeld's successor, he was rehabilitated to the physics community during the Third Reich. Nevertheless, supporters of "Deutsche Physik" launched vicious attacks against leading theoretical physicists, including Arnold Sommerfeld and Heisenberg. On 29 June 1936, a National Socialist Party newspaper published a column attacking Heisenberg. On 15 July 1937, he was attacked in a journal of the SS. This was the beginning of what is called the Heisenberg Affair.
In mid-1936, Heisenberg presented his theory of cosmic-ray showers in two papers. Four more papers appeared in the next two years.
In June 1939, Heisenberg bought a summer home for his family in Urfeld am Walchensee, in southern Germany. He also traveled to the United States in June and July, visiting Samuel Abraham Goudsmit, at the University of Michigan in Ann Arbor. However, Heisenberg refused an invitation to emigrate to the United States. He did not see Goudsmit again until six years later, when Goudsmit was the chief scientific advisor to the American Operation Alsos at the close of World War II. Ironically, Heisenberg was arrested under Operation Alsos and detained in England under Operation Epsilon.
Matrix mechanics and the Nobel Prize.
Heisenberg’s paper establishing quantum mechanics has puzzled physicists and historians. His methods assume that the reader is familiar with Kramers-Heisenberg transition probability calculations. The main new idea, noncommuting matrices, is justified only by a rejection of unobservable quantities. It introduces the non-commutative multiplication of matrices by physical reasoning, based on the correspondence principle, despite the fact that Heisenberg was not then familiar with the mathematical theory of matrices. The path leading to these results has been reconstructed in MacKinnon, 1977, and the detailed calculations are worked out in Aitchison et al.
In Copenhagen, Heisenberg and Hans Kramers collaborated on a paper on dispersion, or the scattering from atoms of radiation whose wavelength is larger than the atoms. They showed that the successful formula Kramers had developed earlier could not be based on Bohr orbits, because the transition frequencies are based on level spacings which are not constant. The frequencies which occur in the Fourier transform of sharp classical orbits, by contrast, are equally spaced. But these results could be explained by a semi-classical Virtual State model: the incoming radiation excites the valence, or outer, electron to a virtual state from which it decays. In a subsequent paper Heisenberg showed that this virtual oscillator model could also explain the polarization of fluorescent radiation.
These two successes, and the continuing failure of the Bohr-Sommerfeld model to explain the outstanding problem of the anomalous Zeeman effect, led Heisenberg to use the virtual oscillator model to try to calculate spectral frequencies. The method proved too difficult to immediately apply to realistic problems, so Heisenberg turned to a simpler example, the anharmonic oscillator.
The dipole oscillator consists of a simple harmonic oscillator, which is thought of as a charged particle on a spring, perturbed by an external force, like an external charge. The motion of the oscillating charge can be expressed as a Fourier series in the frequency of the oscillator. Heisenberg solved for the quantum behavior by two different methods. First, he treated the system with the virtual oscillator method, calculating the transitions between the levels that would be produced by the external source.
He then solved the same problem by treating the anharmonic potential term as a perturbation to the harmonic oscillator and using the perturbation methods that he and Born had developed. Both methods led to the same results for the first and the very complicated second order correction terms. This suggested that behind the very complicated calculations lay a consistent scheme.
So Heisenberg set out to formulate these results without any explicit dependence on the virtual oscillator model. To do this, he replaced the Fourier expansions for the spatial coordinates by matrices, matrices which corresponded to the transition coefficients in the virtual oscillator method. He justified this replacement by an appeal to Bohr’s correspondence principle and the Pauli doctrine that quantum mechanics must be limited to observables.
On 9 July, Heisenberg gave Born this paper to review and submit for publication. When Born read the paper, he recognized the formulation as one which could be transcribed and extended to the systematic language of matrices, which he had learned from his study under Jakob Rosanes at Breslau University. Born, with the help of his assistant and former student Pascual Jordan, began immediately to make the transcription and extension, and they submitted their results for publication; the paper was received for publication just 60 days after Heisenberg's paper. A follow-on paper was submitted for publication before the end of the year by all three authors. (A brief review of Born's role in the development of the matrix mechanics formulation of quantum mechanics along with a discussion of the key formula involving the non-commutivity of the probability amplitudes can be found in an article by Jeremy Bernstein, "Max Born and the Quantum Theory". A detailed historical and technical account can be found in Mehra and Rechenberg's book "The Historical Development of Quantum Theory. Volume 3. The Formulation of Matrix Mechanics and Its Modifications 1925–1926.")
Up until this time, matrices were seldom used by physicists; they were considered to belong to the realm of pure mathematics. Gustav Mie had used them in a paper on electrodynamics in 1912 and Born had used them in his work on the lattices theory of crystals in 1921. While matrices were used in these cases, the algebra of matrices with their multiplication did not enter the picture as they did in the matrix formulation of quantum mechanics.
Born had learned matrix algebra from Rosanes, as already noted, but Born had also learned Hilbert's theory of integral equations and quadratic forms for an infinite number of variables as was apparent from a citation by Born of Hilbert's work "Grundzüge einer allgemeinen Theorie der Linearen Integralgleichungen" published in 1912. Jordan, too was well equipped for the task. For a number of years, he had been an assistant to Richard Courant at Göttingen in the preparation of Courant and David Hilbert's book "Methoden der mathematischen Physik I", which was published in 1924. This book, fortuitously, contained a great many of the mathematical tools necessary for the continued development of quantum mechanics. In 1926, John von Neumann became assistant to David Hilbert, and he coined the term Hilbert space to describe the algebra and analysis which were used in the development of quantum mechanics.
In 1928, Albert Einstein nominated Heisenberg, Born, and Jordan for the Nobel Prize in Physics, but it was not to be. The announcement of the Nobel Prize in Physics for 1932 was delayed until November 1933. It was at that time that it was announced Heisenberg had won the Prize for 1932 "for the creation of quantum mechanics, the application of which has, , led to the discovery of the allotropic forms of hydrogen" and Erwin Schrödinger and Paul Adrien Maurice Dirac shared the 1933 Prize "for the discovery of new productive forms of atomic theory". One can rightly ask why Born was not awarded the Prize in 1932 along with Heisenberg – Bernstein gives some speculations on this matter. One of them is related to Jordan joining the Nazi Party on 1 May 1933 and becoming a Storm Trooper. Hence, Jordan's Party affiliations and Jordan's links to Born may have affected Born's chance at the Prize at that time. Bernstein also notes that when Born won the Prize in 1954, Jordan was still alive, and the Prize was awarded for the statistical interpretation of quantum mechanics, attributable alone to Born.
Heisenberg's reaction to Born for Heisenberg receiving the Prize for 1932 and to Born for Born receiving the Prize in 1954 are also instructive in evaluating whether Born should have shared the Prize with Heisenberg. On 25 November 1933, Born received a letter from Heisenberg in which he said he had been delayed in writing due to a "bad conscience" that he alone had received the Prize "for work done in Göttingen in collaboration – you, Jordan and I." Heisenberg went on to say that Born and Jordan's contribution to quantum mechanics cannot be changed by "a wrong decision from the outside." In 1954, Heisenberg wrote an article honoring Max Planck for his insight in 1900. In the article, Heisenberg credited Born and Jordan for the final mathematical formulation of matrix mechanics and Heisenberg went on to stress how great their contributions were to quantum mechanics, which were not "adequately acknowledged in the public eye."
The "Deutsche Physik" movement.
On 1 April 1935, the eminent theoretical physicist Arnold Sommerfeld, Heisenberg's doctoral advisor at the University of Munich, achieved emeritus status. However, Sommerfeld stayed in his chair during the selection process for his successor, which took until 1 December 1939. The process was lengthy due to academic and political differences between the Munich Faculty's selection and that of the Reichserziehungsministerium (REM, Reich Education Ministry) and the supporters of "Deutsche Physik", which was anti-Semitic and had a bias against theoretical physics, especially quantum mechanics and the theory of relativity.
In 1935, the Munich Faculty drew up a list of candidates to replace Sommerfeld as ordinarius professor of theoretical physics and head of the Institute for Theoretical Physics at the University of Munich. There were three names on the list: Werner Heisenberg, who received the Nobel Prize in Physics for 1932, Peter Debye, who received the Nobel Prize in Chemistry in 1936, and Richard Becker – all former students of Sommerfeld. The Munich Faculty was firmly behind these candidates, with Heisenberg as their first choice. However, supporters of "Deutsche Physik" and elements in the REM had their own list of candidates, and the battle dragged on for over four years. During this time, Heisenberg came under vicious attack by the "Deutsche Physik" supporters. One attack was published in "Das Schwarze Korps", the newspaper of the Schutzstaffel (SS), headed by Heinrich Himmler. In this, Heisenberg was called a "White Jew" (i.e. an Aryan who acts like a Jew) who should be made to "disappear". These attacks were taken seriously, as Jews were violently attacked and incarcerated. Heisenberg fought back with an editorial and a letter to Himmler, in an attempt to resolve this matter and regain his honour.
At one point, Heisenberg's mother visited Himmler's mother. The two women knew each other, as Heisenberg's maternal grandfather and Himmler's father were rectors and members of a Bavarian hiking club. Eventually, Himmler settled the Heisenberg affair by sending two letters, one to SS Gruppenführer Reinhard Heydrich and one to Heisenberg, both on 21 July 1938. In the letter to Heydrich, Himmler said Germany could not afford to lose or silence Heisenberg, as he would be useful for teaching a generation of scientists. To Heisenberg, Himmler said the letter came on recommendation of his family and he cautioned Heisenberg to make a distinction between professional physics research results and the personal and political attitudes of the involved scientists. The letter to Heisenberg was signed under the closing "Mit freundlichem Gruß und, Heil Hitler!" (With friendly greetings, Heil Hitler!") Overall, the Heisenberg affair was a victory for academic standards and professionalism. However, the appointment of Wilhelm Müller to replace Sommerfeld was a political victory over academic standards. Müller was not a theoretical physicist, had not published in a physics journal, and was not a member of the Deutsche Physikalische Gesellschaft; his appointment was considered a travesty and detrimental to educating theoretical physicists.
During the SS investigation of Heisenberg, the three investigators had training in physics. Heisenberg had participated in the doctoral examination of one of them at the "Universität Leipzig". The most influential of the three, however, was Johannes Juilfs. During their investigation, they had become supporters of Heisenberg as well as his position against the ideological policies of the "Deutsche Physik" movement in theoretical physics and academia.
World War II.
In 1939, shortly after the discovery of nuclear fission, the German nuclear energy project, also known as the "Uranverein" (Uranium Club), had begun. Heisenberg was one of the principal scientists leading research and development in the project.
From 15 to 22 September 1941, Heisenberg traveled to German-occupied Copenhagen to lecture and discuss nuclear research and theoretical physics with Niels Bohr. The meeting, and specifically what it might reveal about Heisenberg's intentions concerning developing nuclear weapons for the Nazi regime, is the subject of the award-winning Michael Frayn play titled "Copenhagen". A television film version of the play was made by the BBC in 2002, with Stephen Rea as Bohr, and Daniel Craig as Heisenberg. The same meeting had previously been dramatised by the BBC's "Horizon" science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg. Documents relating to the Bohr-Heisenberg meeting were released in 2002 by the Niels Bohr Archive and by the Heisenberg family. A dramatized account of Heisenberg's work during the war is featured in the 2015 TV series "The Heavy Water War".
On 26 February 1942, Heisenberg presented a lecture to Reich officials on energy acquisition from nuclear fission, after the Army withdrew most of its funding. The Uranium Club was transferred to the Reich Research Council (RFR) in July 1942. On 4 June 1942, Heisenberg was summoned to report to Albert Speer, Germany's Minister of Armaments, on the prospects for converting the Uranium Club's research toward developing nuclear weapons. During the meeting, Heisenberg told Speer that a bomb could not be built before 1945, and would require significant monetary and manpower resources. Five days later, on 9 June 1942, Adolf Hitler issued a decree for the reorganization of the RFR as a separate legal entity under the Reich Ministry for Armament and Ammunition; the decree appointed Reich Marshall Hermann Göring as the president.
In September 1942, Heisenberg submitted his first paper of a three-part series on the scattering matrix, or S-matrix, in elementary particle physics. The first two papers were published in 1943 and the third in 1944. The S-matrix described only observables, i.e., the states of incident particles in a collision process, the states of those emerging from the collision, and stable bound states; there would be no reference to the intervening states. This was the same precedent as he followed in 1925 in what turned out to be the foundation of the matrix formulation of quantum mechanics through only the use of observables.
In February 1943, Heisenberg was appointed to the Chair for Theoretical Physics at the "Friedrich-Wilhelms-Universität" (today, the Humboldt-Universität zu Berlin). In April, his election to the "Preußische Akademie der Wissenschaften" (Prussian Academy of Sciences) was approved. That same month, he moved his family to their retreat in as Allied bombing increased in Berlin. In the summer, he dispatched the first of his staff at the "Kaiser-Wilhelm Institut für Physik" to Hechingen and its neighboring town of Haigerloch, on the edge of the Black Forest, for the same reasons. From 18–26 October, he traveled to German-occupied Netherlands. In December 1943, Heisenberg visited German-occupied Poland.
From 24 January to 4 February 1944, Heisenberg traveled to occupied Copenhagen, after the German Army confiscated Bohr's Institute of Theoretical Physics. He made a short return trip in April. In December, Heisenberg lectured in neutral Switzerland. The United States Office of Strategic Services sent former major league baseball catcher and OSS agent Moe Berg to attend the lecture carrying a pistol, with orders to shoot Heisenberg if his lecture indicated that Germany was close to completing an atomic bomb. Heisenberg did not give such an indication, so Berg decided not to shoot him, a decision Berg later described as his own "uncertainty principle".
In January 1945, Heisenberg, with most of the rest of his staff, moved from the "Kaiser-Wilhelm Institut für Physik" to the facilities in the Black Forest.
Uranium Club.
In December 1938, the German chemists Otto Hahn and Fritz Strassmann sent a manuscript to "Naturwissenschaften" reporting they had detected the element barium after bombarding uranium with neutrons; simultaneously, they communicated these results to Lise Meitner, who had in July of that year fled to the Netherlands and then went to Sweden.
Meitner, and her nephew Otto Robert Frisch, correctly interpreted these results as being nuclear fission. Frisch confirmed this experimentally on 13 January 1939.
Paul Harteck was director of the physical chemistry department at the University of Hamburg and an advisor to the "Heereswaffenamt" (HWA, Army Ordnance Office). On 24 April 1939, along with his teaching assistant Wilhelm Groth, Harteck made contact with the "Reichskriegsministerium" (RKM, Reich Ministry of War) to alert them to the potential of military applications of nuclear chain reactions. Two days earlier, on 22 April 1939, after hearing a colloquium paper by Wilhelm Hanle on the use of uranium fission in a "Uranmaschine" (uranium machine, i.e., nuclear reactor), Georg Joos, along with Hanle, notified Wilhelm Dames, at the "Reichserziehungsministerium" (REM, Reich Ministry of Education), of potential military applications of nuclear energy. The communication was given to Abraham Esau, head of the physics section of the "Reichsforschungsrat" (RFR, Reich Research Council) at the REM. On 29 April, a group, organized by Esau, met at the REM to discuss the potential of a sustained nuclear chain reaction.
The group included the physicists Walther Bothe, Robert Döpel, Hans Geiger, Wolfgang Gentner (probably sent by Walther Bothe), Wilhelm Hanle, Gerhard Hoffmann and Georg Joos; Peter Debye was invited, but he did not attend. After this, informal work began at the Georg-August University of Göttingen by Joos, Hanle and their colleague Reinhold Mannfopff; the group of physicists was known informally as the first "Uranverein" (Uranium Club) and formally as "Arbeitsgemeinschaft für Kernphysik". The group's work was discontinued in August 1939, when the three were called to military training.
The second "Uranverein" began after the "Heereswaffenamt" (HWA, Army Ordnance Office) squeezed the "Reichsforschungsrat" (RFR, Reich Research Council) out of the "Reichserziehungsministerium" (REM, Reich Ministry of Education) and started the formal German nuclear energy project under military auspices. The second "Uranverein" was formed on 1 September 1939, the day World War II began, and it had its first meeting on 16 September 1939. The meeting was organized by Kurt Diebner, advisor to the HWA, and held in Berlin. The invitees included Walther Bothe, Siegfried Flügge, Hans Geiger, Otto Hahn, Paul Harteck, Gerhard Hoffmann, Josef Mattauch and Georg Stetter. A second meeting was held soon thereafter and included Klaus Clusius, Robert Döpel, Werner Heisenberg and Carl Friedrich von Weizsäcker. Also at this time, the "Kaiser-Wilhelm Institut für Physik" (KWIP, Kaiser Wilhelm Institute for Physics, after World War II the Max Planck Institute for Physics), in Berlin-Dahlem, was placed under HWA authority, with Diebner as the administrative director, and the military control of the nuclear research commenced.
When it was apparent that the nuclear energy project would not make a decisive contribution to ending the war effort in the near term, control of the KWIP was returned in January 1942 to its umbrella organization, the "Kaiser-Wilhelm Gesellschaft" (KWG, Kaiser Wilhelm Society, after World War II the Max-Planck Gesellschaft), and HWA control of the project was relinquished to the RFR in July 1942. The nuclear energy project thereafter maintained its "kriegswichtig" (important for the war) designation and funding continued from the military. However, the German nuclear power project was then broken down into the following main areas: uranium and heavy water production, uranium isotope separation and the "Uranmaschine" (uranium machine, i.e., nuclear reactor). Also, the project was then essentially split up between a number of institutes, where the directors dominated the research and set their own research agendas. The dominant personnel and facilities were the following:
Heisenberg was appointed director-in-residence of the KWIP on 1 July 1942, as Peter Debye was still officially the director and on leave in the United States; Debye had gone on leave as he was a citizen of The Netherlands and had refused to become a German citizen when the HWA took administrative control of the KWIP. Heisenberg still also had his department of physics at the University of Leipzig where work was done for the "Uranverein" by Robert Döpel and his wife Klara Döpel. During the period Kurt Diebner administered the KWIP under the HWA program, considerable personal and professional animosity developed between Diebner and the Heisenberg inner circle – Heisenberg, Karl Wirtz, and Carl Friedrich von Weizsäcker.
The point in 1942, when the army relinquished its control of the German nuclear energy project, was the zenith of the project relative to the number of personnel devoting time to the effort. There were only about 70 scientists working on the project, with about 40 devoting more than half their time to nuclear fission research. After this, the number of scientists working on applied nuclear fission diminished dramatically. Many of the scientists not working with the main institutes stopped working on nuclear fission and devoted their efforts to more pressing war related work.
Over time, the HWA and then the RFR controlled the German nuclear energy project. The most influential people in the project were Kurt Diebner, Abraham Esau, Walther Gerlach and Erich Schumann. Schumann was one of the most powerful and influential physicists in Germany. Schumann was director of the Physics Department II at the Frederick William University (later, University of Berlin), which was commissioned and funded by the "Oberkommando des Heeres" (OKH, Army High Command) to conduct physics research projects. He was also head of the research department of the HWA, assistant secretary of the Science Department of the OKH and "Bevollmächtiger" (plenipotentiary) for high explosives. Diebner, throughout the life of the nuclear energy project, had more control over nuclear fission research than did Walther Bothe, Klaus Clusius, Otto Hahn, Paul Harteck or Werner Heisenberg.
1945: Operation Alsos and Operation Epsilon.
Operation Alsos was an Allied effort commanded by the Russian-American Colonel Boris T. Pash. He reported directly to General Leslie Groves, commander of the Manhattan Engineer District, which was developing atomic weapons for the United States. The chief scientific advisor to Operation Alsos was the physicist Samuel Abraham Goudsmit. Goudsmit was selected for this task because of his knowledge of physics, he spoke German, and he personally knew a number of the German scientists working on the German nuclear energy project. He also knew little of the Manhattan Project, so, if he were captured, he would have little intelligence value to the Germans.
The objectives of Operation Alsos were to determine if the Germans had an atomic bomb program and to exploit German atomic related facilities, intellectual materials, materiel resources, and scientific personnel for the benefit of the US. Personnel on this operation generally swept into areas which had just come under control of the Allied military forces, but sometimes they operated in areas still under control by German forces.
Berlin had been a location of many German scientific research facilities. To limit casualties and loss of equipment, many of these facilities were dispersed to other locations in the latter years of the war. The "Kaiser-Wilhelm-Institut für Physik" (KWIP, Kaiser Wilhelm Institute for Physics) had mostly been moved in 1943 and 1944 to Hechingen and its neighboring town of Haigerloch, on the edge of the Black Forest, which eventually became the French occupation zone. This move and a little luck allowed the Americans to take into custody a large number of German scientists associated with nuclear research. The only section of the institute which remained in Berlin was the low-temperature physics section, headed by Ludwig Bewilogua (1906–83), who was in charge of the exponential uranium pile.
Nine of the prominent German scientists who published reports in "Kernphysikalische Forschungsberichte" as members of the "Uranverein" were picked up by Operation Alsos and incarcerated in England under Operation Epsilon: Erich Bagge, Kurt Diebner, Walther Gerlach, Otto Hahn, Paul Harteck, Werner Heisenberg, Horst Korsching, Carl Friedrich von Weizsäcker and Karl Wirtz. Also, incarcerated was Max von Laue, although he had nothing to do with the nuclear energy project. Goudsmit, the chief scientific advisor to Operation Alsos, thought von Laue might be beneficial to the postwar rebuilding of Germany and would benefit from the high level contacts he would have in England.
Heisenberg had been captured and arrested by Colonel Pash at Heisenberg's retreat in Urfeld, on 3 May 1945, in what was a true alpine-type operation in territory still under control by German forces. He was taken to Heidelberg, where, on 5 May, he met Goudsmit for the first time since the Ann Arbor visit in 1939. Germany surrendered just two days later. Heisenberg did not see his family again for eight months. Heisenberg was moved across France and Belgium and flown to England on 3 July 1945.
The 10 German scientists were held at Farm Hall in England. The facility had been a safe house of the British foreign intelligence MI6. During their detention, their conversations were recorded. Conversations thought to be of intelligence value were transcribed and translated into English. The transcripts were released in 1992. Bernstein has published an annotated version of the transcripts in his book "Hitler's Uranium Club: The Secret Recordings at Farm Hall", along with an introduction to put them in perspective. A complete, unedited publication of the British version of the reports appeared as "Operation Epsilon: The Farm Hall Transcripts", which was published in 1993 by the Institute of Physics in Bristol and by the University of California Press in the US.
Post 1945.
On 3 January 1946, the 10 Operation Epsilon detainees were transported to Alswede in Germany, which was in the British occupation zone. Heisenberg settled in Göttingen, also in the British zone. In July, he was named director of the "Kaiser-Wilhelm-Institut für Physik" (KWIP, Kaiser Wilhelm Institute for Physics), then located in Göttingen. Shortly thereafter, it was renamed the "Max-Planck-Institut für Physik", in honor of Max Planck and to assuage political objections to the continuation of the institute. Heisenberg was its director until 1958. In 1958, the institute was moved to Munich, expanded, and renamed "Max-Planck-Institut für Physik und Astrophysik" (MPIFA). Heisenberg was its director from 1960 to 1970; in the interim, Heisenberg and the astrophysicist Ludwig Biermann were co-directors. Heisenberg resigned his directorship of the MPIFA on 31 December 1970. Upon the move to Munich, Heisenberg also became an "ordentlicher Professor" (ordinarius professor) at the University of Munich.
Just as the Americans did with Operation Alsos, the Soviets inserted special search teams into Germany and Austria in the wake of their troops. Their objective, under the Russian Alsos, was also the exploitation of German atomic related facilities, intellectual materials, materiel resources and scientific personnel for the benefit of the Soviet Union. One of the German scientists recruited under this Soviet operation was the nuclear physicist Heinz Pose, who was made head of Laboratory V in Obninsk. When he returned to Germany on a recruiting trip for his laboratory, Pose wrote a letter to Werner Heisenberg inviting him to work in the USSR. The letter lauded the working conditions in the USSR and the available resources, as well as the favorable attitude of the Soviets towards German scientists. A courier hand delivered the recruitment letter, dated 18 July 1946, to Heisenberg; Heisenberg politely declined in a return letter to Pose.
In 1947, Heisenberg presented lectures in Cambridge, Edinburgh and Bristol. Heisenberg also contributed to the understanding of the phenomenon of superconductivity with a paper in 1947 and two papers in 1948, one of them with Max von Laue.
In the period shortly after World War II, Heisenberg briefly returned to the subject of his doctoral thesis, turbulence. Three papers were published in 1948 and one in 1950.
In the post-war period, Heisenberg continued his interests in cosmic-ray showers with considerations on multiple production of mesons. He published three papers in 1949, two in 1952, and one in 1955.
On 9 March 1949, the "Deutsche Forschungsrat" (German Research Council) was established by the "Max-Planck Gesellschaft" (MPG, Max Planck Society, successor organization to the "Kaiser-Wilhelm Gesellschaft"). Heisenberg was appointed president of the "Deutsche Forschungsrat". In 1951, the organization was fused with the Notgemeinschaft der Deutschen Wissenschaft (NG, Emergency Association of German Science) and that same year renamed the "Deutsche Forschungsgemeinschaft" (DFG, German Research Foundation). With the merger, Heisenberg was appointed to the presidium.
In 1952, Heisenberg served as the chairman of the Commission for Atomic Physics of the DFG. Also that year, he headed the German delegation to the European Council for Nuclear Research.
In 1953, Heisenberg was appointed president of the "Alexander von Humboldt-Stiftung" by Konrad Adenauer. Heisenberg served until 1975. Also, from 1953, Heisenberg's theoretical work concentrated on the unified field theory of elementary particles.
In late 1955 to early 1956, Heisenberg gave the Gifford Lectures at St Andrews University, in Scotland, on the intellectual history of physics. The lectures were later published as "Physics and Philosophy: The Revolution in Modern Science".
During 1956 and 1957, Heisenberg was the chairman of the "Arbeitskreis Kernphysik" (Nuclear Physics Working Group) of the "Fachkommission II "Forschung und Nachwuchs"" (Commission II "Research and Growth") of the "Deutschen Atomkommission" (DAtK, German Atomic Energy Commission). Other members of the Nuclear Physics Working Group in both 1956 and 1957 were: Walther Bothe, Hans Kopfermann (vice-chairman), Fritz Bopp, Wolfgang Gentner, Otto Haxel, Willibald Jentschke, Heinz Maier-Leibnitz, Josef Mattauch, Wolfgang Riezler, Wilhelm Walcher and Carl Friedrich von Weizsäcker. Wolfgang Paul was also a member of the group during 1957.
In 1957, Heisenberg was a signatory of the manifesto of the "Göttinger Achtzehn" (Göttingen Eighteen).
From 1957, Heisenberg was interested in plasma physics and the process of nuclear fusion. He also collaborated with the International Institute of Atomic Physics in Geneva. He was a member of the Institute's Scientific Policy Committee, and for several years was the Committee's chairman.
In 1973, Heisenberg gave a lecture at Harvard University on the historical development of the concepts of quantum theory.
On 24 March 1973, Heisenberg gave a speech before the Catholic Academy of Bavaria, accepting the Romano Guardini Prize. An English translation of its title is "Scientific and Religious Truth." And its stated goal was "In what follows, then, we shall first of all deal with the unassailability and value of scientific truth, and then with the much wider field of religion, of which – so far as the Christian religion is concerned – Guardini himself has so persuasively written; finally – and this will be the hardest part to formulate – we shall speak of the relationship of the two truths." A more detailed insight into Heisenberg's view on religion has been discussed by Wilfried Schröder in "Natural science and religion" (Bremen 1999, Science edition) and Wilfried Schröder "Naturerkenntnis und Religion" (Bremen, science edition 2008).
Personal life.
In January 1937 Heisenberg met Elisabeth Schumacher (1914-1998) at a private music recital. Elisabeth was the daughter of a well-known Berlin economics professor, and her brother was the economist E. F. Schumacher, author of "Small is Beautiful". Heisenberg married her on 29 April. Fraternal twins Maria and Wolfgang were born in January 1938, whereupon Wolfgang Pauli congratulated Heisenberg on his "pair creation" – a word play on a process from elementary particle physics, pair production. They had five more children over the next 12 years: Barbara, Christine, Jochen, Martin and Verena. Jochen became a physics professor at the University of New Hampshire.
Heisenberg enjoyed classical music and was an accomplished pianist.
Heisenberg was raised and lived as a Lutheran Christian, publishing and giving several talks reconciling science with his faith.
In his speech Scientific and Religious Truth (1974) while accepting the Romano Guardini Prize, Heisenberg affirmed:
“In the history of science, ever since the famous trial of Galileo, it has repeatedly been claimed that scientific truth cannot be reconciled with the religious interpretation of the world. Although I am now convinced that scientific truth is unassailable in its own field, I have never found it possible to dismiss the content of religious thinking as simply part of an outmoded phase in the consciousness of mankind, a part we shall have to give up from now on. Thus in the course of my life I have repeatedly been compelled to ponder on the relationship of these two regions of thought, for I have never been able to doubt the reality of that to which they point.” (Heisenberg 1974, 213)
“Where no guiding ideals are left to point the way, the scale of values disappears and with it the meaning of our deeds and sufferings, and at the end can lie only negation and despair.
Religion is therefore the foundation of ethics, and ethics the presupposition of life.” (Heisenberg 1974, 219).
“The first gulp from the glass of natural sciences will turn you into an atheist, but at the bottom of the glass God is waiting for you.” -W.Heisenberg 
In his autobiographical article in the journal "Truth", Henry Margenau (Professor Emeritus of Physics and Natural Philosophy at Yale University) pointed out: “I have said nothing about the years between 1936 and 1950. There were, however, a few experiences I cannot forget. One was my first meeting with Heisenberg, who came to America soon after the end of the Second World War. Our conversation was intimate and he impressed me by his deep religious conviction. He was a true Christian in every sense of that word.” 
Heisenberg also enjoyed mountaineering. In his autobiography, he included photographs from this activity.
Heisenberg died of cancer of the kidneys and gall bladder at his home, on 1 February 1976. The next evening, his colleagues and friends walked in remembrance from the Institute of Physics to his home and each put a candle near the front door. He is buried at Munich Waldfriedhof.
Honors and awards.
Heisenberg was awarded a number of honors:
Research Reports in Nuclear Physics.
The following reports were published in "Kernphysikalische Forschungsberichte" ("Research Reports in Nuclear Physics"), an internal publication of the German "Uranverein". The reports were classified Top Secret, they had very limited distribution, and the authors were not allowed to keep copies. The reports were confiscated under the Allied Operation Alsos and sent to the United States Atomic Energy Commission for evaluation. In 1971, the reports were declassified and returned to Germany. The reports are available at the Karlsruhe Nuclear Research Center and the American Institute of Physics.
Publications.
</dl>
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="33153" url="http://en.wikipedia.org/wiki?curid=33153" title="Whisky">
Whisky

Whisky or whiskey is a type of distilled alcoholic beverage made from fermented grain mash. Various grains (which may be malted) are used for different varieties, including barley, corn (maize), rye, and wheat. Whisky is typically aged in wooden casks, generally made of charred white oak.
Whisky is a strictly regulated spirit worldwide with many classes and types. The typical unifying characteristics of the different classes and types are the fermentation of grains, distillation, and aging in wooden barrels.
Etymology.
The word "whisky" (or "whiskey") is an anglicization of the Gaelic word "uisce"/"uisge" meaning water. Distilled alcohol was known in Latin as "aqua vitae" ("water of life"). This was translated to Gaelic as Irish: "uisce beatha" and Scottish Gaelic: "uisge beatha"="lively water" or "water of life". Early forms of the word in English included "uskebeaghe" (1581), "usquebaugh" (1610), "usquebath" (1621), "usquebae" (1715).
Names and spellings.
Much is made of the word's two spellings: "whisky" and "whiskey". There are two schools of thought on the issue. One is that the spelling difference is simply a matter of regional language convention for the spelling of a word, indicating that the spelling varies depending on the intended audience or the background or personal preferences of the writer (like the difference between "color" and "colour"; "tire" and "tyre"; or "recognize" and "recognise"), and the other is that the spelling should depend on the style or origin of the spirit being described. There is general agreement that when quoting the proper name printed on a label, the spelling on the label should not be altered. Some writers refer to "whisk(e)y" or "whisky/whiskey" to acknowledge the variation.
The spelling "whiskey" is common in Ireland and the United States while "whisky" is used in every other whisky producing country in the world. In the US, the usage has not always been consistent. From the late eighteenth century to the mid twentieth century, American writers used both spellings interchangeably until the introduction of newspaper style guides. Since the 1960s, American writers have increasingly used "whiskey" as the accepted spelling for aged grain spirits made in the US and "whisky" for aged grain spirits made outside the US. However, some prominent American brands, such as George Dickel, Maker's Mark, and Old Forester (all made by different companies), use the 'whisky' spelling on their labels, and the "Standards of Identity for Distilled Spirits", the legal regulations for spirit in the US, also use the 'whisky' spelling throughout.
"Scotch" is the internationally recognized term for "Scotch whisky".
History.
It is possible that distillation was practised by the Babylonians in Mesopotamia in the 2nd millennium BC, with perfumes and aromatics being distilled, but this is subject to uncertain and disputed interpretation of evidence. The earliest certain chemical distillations were by Greeks in Alexandria in the 1st century AD, but these were not distillations of alcohol. The medieval Arabs adopted the distillation technique of the Alexandrian Greeks, and written records in Arabic begin in the 9th century, but again these were not distillations of alcohol. Distilling technology passed from the medieval Arabs to the medieval Latins, with the earliest records in Latin in the early 12th century. The earliest records of the distillation of alcohol are in Italy in the 13th century, where alcohol was distilled from wine. An early description of the technique was given by Ramon Llull (1232 – 1315). Its use spread through medieval monasteries, largely for medicinal purposes, such as the treatment of colic and smallpox.
The art of distillation spread to Ireland and Scotland no later than the 15th century, as did the common European practice of distilling 'Aqua Vitae' or spirit alcohol primarily for medicinal purposes. The practice of medicinal distillation eventually passed from a monastic setting to the secular via professional medical practitioners of the time, The Guild of Surgeon Barbers. The first confirmed written record of whisky in Ireland comes from 1405, in the Irish "Annals of Clonmacnoise", which attributes the death of a chieftain to "taking a surfeit of aqua vitae" at Christmas. In Scotland, the first evidence of whisky production comes from an entry in the "Exchequer Rolls" for 1494 where malt is sent "To Friar John Cor, by order of the king, to make aquavitae", enough to make about 500 bottles.
James IV of Scotland (r. 1488–1513) reportedly had a great liking for Scotch whisky, and in 1506 the town of Dundee purchased a large amount of whisky from the Guild of Surgeon Barbers, which held the monopoly on production at the time. Between 1536 and 1541, King Henry VIII of England dissolved the monasteries, sending their monks out into the general public. Whisky production moved out of a monastic setting and into personal homes and farms as newly independent monks needed to find a way to earn money for themselves.
The distillation process was still in its infancy; whisky itself was not allowed to age, and as a result tasted very raw and brutal compared to today's versions. Renaissance-era whisky was also very potent and not diluted. Over time whisky evolved into a much smoother drink.
With a licence to distil Irish whiskey from 1608, the Old Bushmills Distillery in Northern Ireland is the oldest licenced whiskey distillery in the world.
In 1707, the Acts of Union merged England and Scotland, and thereafter taxes on it rose dramatically.
After the English Malt Tax of 1725, most of Scotland's distillation was either shut down or forced underground. Scotch whisky was hidden under altars, in coffins, and in any available space to avoid the governmental Excisemen. Scottish distillers, operating out of homemade stills, took to distilling whisky at night when the darkness hid the smoke from the stills. For this reason, the drink became known as moonshine. At one point, it was estimated that over half of Scotland's whisky output was illegal.
In America, whisky was used as currency during the American Revolution; George Washington operated a large distillery at Mount Vernon. Given the distances and primitive transportation network of colonial America, farmers often found it easier and more profitable to convert corn to whisky and transport it to market in that form. It also was a highly coveted sundry and when an additional excise tax was levied against it, the Whiskey Rebellion erupted in 1791.
The drinking of Scotch whisky was introduced to India in the nineteenth century. The first distillery in India was built by Edward Dyer at Kasauli in the late 1820s. The operation was soon shifted to nearby Solan (close to the British summer capital Shimla), as there was an abundant supply of fresh spring water there.
In 1823, the UK passed the Excise Act, legalizing the distillation (for a fee), and this put a practical end to the large-scale production of Scottish moonshine.
In 1831, Aeneas Coffey patented the Coffey still, allowing for cheaper and more efficient distillation of whisky. In 1850, Andrew Usher began producing a blended whisky that mixed traditional pot still whisky with that from the new Coffey still. The new distillation method was scoffed at by some Irish distillers, who clung to their traditional pot stills. Many Irish contended that the new product was, in fact, not whisky at all.
By the 1880s, the French brandy industry was devastated by the phylloxera pest that ruined much of the grape crop; as a result, whisky became the primary liquor in many markets.
During the Prohibition era in the United States lasting from 1920 to 1933, all alcohol sales were banned in the country. The federal government made an exemption for whisky prescribed by a doctor and sold through licensed pharmacies. During this time, the Walgreens pharmacy chain grew from 20 retail stores to almost 400.
Production.
Distillation.
A still for making whisky is usually made of copper, since it removes sulfur-based compounds from the alcohol that would make it unpleasant to drink. Modern stills are made of stainless steel with copper innards (piping, for example, will be lined with copper along with copper plate inlays along still walls). The simplest standard distillation apparatus is commonly known as a pot still, consisting of a single heated chamber and a vessel to collect purified alcohol.
Column stills are frequently used in the production of grain whisky and are the most commonly used type of still in the production of Bourbon and other American whiskeys. Column stills behave like a series of single pot stills, formed in a long vertical tube. Whereas a single pot still charged with wine might yield a vapour enriched to 40–50% alcohol, a column still can achieve a vapour alcohol content of 95.6%; an azeotropic mixture of alcohol and water.
Aging.
Whiskies do not mature in the bottle, only in the cask, so the "age" of a whisky is only the time between distillation and bottling. This reflects how much the cask has interacted with the whisky, changing its chemical makeup and taste. Whiskies that have been bottled for many years may have a rarity value, but are not "older" and not necessarily "better" than a more recent whisky that matured in wood for a similar time. After a decade or two, additional aging in a barrel does not necessarily improve a whisky.
Packaging.
Most whiskies are sold at or near an alcoholic strength of 40% abv, which is the statutory minimum in some countries – although the strength can vary, and cask-strength whisky may have as much as twice that alcohol percentage.
Exports.
Whisky is probably the best known of Scotland's manufactured products. Exports have increased by 87% in the past decade and it contributes over £4.25 billion to the UK economy, making up a quarter of all its food and drink revenues. In 2012, the US was the largest market for Scotch whisky (£655 million), followed by France (£535 million). It is also one of the UK's overall top five manufacturing export earners and it supports around 35,000 jobs. Principal whisky producing areas include Speyside and the Isle of Islay, where there are eight distilleries providing a major source of employment. In many places, the industry is closely linked to tourism, with many distilleries also functioning as attractions worth £30 million GVA each year.
In 2011, 70 per cent of Canadian whisky was exported, with about 60 per cent going to the US, and the rest to Europe and Asia. 15 million cases of Canadian whisky were sold in the US in 2011.
Types.
Whisky or whisky-like products are produced in most grain-growing areas. They differ in base product, alcoholic content, and quality.
Malts and grains are combined in various ways:
American.
American whiskey is distilled from a fermented mash of cereal grain. It must have the taste, aroma, and other characteristics commonly attributed to whiskey.
Some types of whiskey listed in the United States federal regulations are:
These types of American whiskey must be distilled to no more than 80% alcohol by volume, and barrelled at no more than 125 proof. Only water may be added to the final product; the addition of colouring or flavouring is prohibited. These whiskeys must be aged in new charred-oak containers, except for corn whiskey which does not have to be aged. If it is aged, it must be in uncharred oak barrels or in used barrels. Corn whiskey is usually unaged and sold as a legal version of moonshine.
If one of these whiskey types reaches two years aging or beyond, it is additionally designated as "straight", e.g., "straight rye whiskey". A whiskey that fulfils all above requirements but derives from less than 51% of any one specific grain can be called simply a "straight whiskey" without naming a grain.
US regulations recognize other whiskey categories, including:
Another important labelling in the marketplace is Tennessee whiskey, of which Jack Daniel's, George Dickel, Collier and McKeel, and Benjamin Prichard's are the only brands currently bottled. The main difference defining a Tennessee whiskey is its use of the Lincoln County Process, which involves filtration of the whiskey through charcoal. The rest of the distillation process is identical to bourbon whiskey. Whiskey sold as "Tennessee whiskey" is defined as Bourbon under NAFTA and at least one other international trade agreement, and is similarly required to meet the legal definition of Bourbon under Canadian law.
Australian.
Australian whiskies have won global whisky awards and medals, including the World Whiskies Awards and Jim Murray's Whisky Bible "Liquid Gold Awards".
Canadian.
By Canadian law Canadian whiskies must be produced and aged in Canada, be distilled from a fermented mash of cereal grain, be aged in wood barrels with a capacity limit of 700 L for not less than three years, and "possess the aroma, taste and character generally attributed to Canadian whisky". The terms "Canadian Whisky", "Rye Whisky", and "Canadian Rye Whisky" are legally indistinguishable in Canada and do not require any specific grain in their production. Canadian whiskies may contain caramel and flavouring in addition to the distilled mash spirits, and there is no maximum limit on the alcohol level of the distillation. To be exported under one of the "Canadian Whisky" designations, a whisky cannot contain more than 9.09% imported spirits.
Canadian whiskies are available throughout the world and are a culturally significant export. Well known brands include Crown Royal, Canadian Club, Seagram's, and Wiser's among others. The historic popularity of Canadian whisky in the United States is partly a result of rum runners illegally importing it into the country during the period of American Prohibition.
Danish.
Denmark began producing whisky early in 1974. The first Danish single malt to go on sale was Lille Gadegård from Bornholm, in 2005. Lille Gadegård is a winery as well, and uses its own wine casks to mature whisky.
The second Danish distilled single malt whisky for sale was Edition No.1 from the Braunstein microbrewery and distillery. It was distilled in 2007, using water from the Greenlandic ice sheet, and entered the market in March 2010.
English.
There are currently at least six distilleries producing English whisky. Though England is not very well known for making whisky, there were distillers previously operating in London, Liverpool and Bristol until the late 19th century, after which production of English single malt whisky ceased until 2003.
Finnish.
There are two working distilleries in Finland and a third one is under construction. Whisky retail sales in Finland are controlled solely by the state alcohol monopoly Alko and advertisement of strong alcoholic beverages is banned.
German.
German whisky production is a relatively recent phenomenon having only started in the last 30 years. The styles produced resemble those made in Ireland, Scotland and the United States: single malts, blends, wheat, and bourbon-like styles. There is no standard spelling of German whiskies with distilleries using both "whisky" and "whiskey". In 2008 there were 23 distilleries in Germany producing whisky.
Indian.
India consumes almost as much whisky as the rest of the world put together. Distilled alcoholic beverages that are labelled as "whisky" in India are commonly blends based on neutral spirits that are distilled from fermented molasses with only a small portion consisting of traditional malt whisky, usually about 10 to 12 percent. Outside India, such a drink would more likely be labelled a rum. According to the Scotch Whisky Association's 2013 annual report, "there is no compulsory definition of whisky in India, and the Indian voluntary standard does not require whisky to be distilled from cereals or to be matured." Ninety percent of the whisky consumed in India is molasses-based, although whisky wholly distilled from malt and other grains, is also manufactured and sold. Amrut, the first single malt whisky produced in India, was launched on 24 August 2004.
Irish.
Irish whiskeys are normally distilled thrice, Cooley Distillery being the exception as they also double distill. Though traditionally distilled using pot stills, the column still is now used to produce grain whiskey for blends. By law, Irish whiskey must be produced in Ireland and aged in wooden casks for a period of no less than three years, although in practice it is usually three or four times that period. Unpeated malt is almost always used, the main exception being Connemara Peated Malt whiskey.
There are several types of whiskey common to Ireland: single malt, single grain, blended whiskey and pure pot still whiskey.
Japanese.
The model for Japanese whiskies is the single malt Scotch, although there are also examples of Japanese blended whiskies. The base is a mash of malted barley, dried in kilns fired with a little peat (although considerably less than in Scotland), and distilled using the pot still method. Before 2000, Japanese whisky was primarily for the domestic market and exports were limited. Japanese whiskies such as Suntory and Nikka won many prestigious international awards between 2007 and 2014. Japanese whisky has earned a reputation for quality.
Scotch.
Scotch whiskies are generally distilled twice, although some are distilled a third time and others even up to twenty times. Scotch Whisky Regulations require anything bearing the label "Scotch" to be distilled in Scotland and matured for a minimum of three years in oak casks, among other, more specific criteria. Any age statement on the bottle, in the form of a number, must reflect the age of the youngest Scotch whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old.
The basic types of Scotch are malt and grain, which are combined to create blends. Scotch malt whiskies are divided into five main regions: Highland, Lowland, Islay, Speyside and Campbeltown.
Swedish.
Whisky started being produced in Sweden in 1955 by the now defunct "Skeppets whisky" brand. Their last bottle was sold in 1971. In 1999 Mackmyra Whisky was founded and is today the largest producer and has won several awards including European Whisky of the Year in Jim Murray's 2011 Whisky Bible and the International Wine & Spirits Competition (IWSC) 2012 Award for Best European Spirits Producer of 2012.
Welsh.
Although distillation of whisky in Wales began in Middle Ages there were no commercially operated distilleries during the 20th century. The rise of the temperance movement saw the decline the commercial production of liquor during the 19th century and in 1894 Welsh whisky production ceased. Recently, however, there has been a revival of Welsh whisky.
The revival of Welsh whisky began in the 1990s. Initially a "Prince of Wales" malt whisky was sold as Welsh whisky but was simply blended scotch bottled in Wales. A lawsuit by Scotch distillers ended this enterprise. In 2000, Penderyn Distillery started production of Penderyn single malt whisky. The first bottles went on sale on 1 March 2004, Saint David's Day, and it is now sold worldwide. Penderyn Distillery is located in the Brecon Beacons National Park and is considered to be the smallest distillery in the world.
Other.
ManX Spirit from the Isle of Man is distilled elsewhere and re-distilled in the country of its nominal "origin". The ManX distillery takes a previously matured Scotch malt whisky and re-distills it.
In 2010 a Czech whisky was released, the 21-year-old "Hammer Head".
In 2008 at least two distilleries in the traditionally brandy-producing Caucasus region announced their plans to enter the Russian domestic market with whiskies. The Stavropol-based Praskoveysky distillery bases its product on Irish whiskey, while in Kizlyar, Dagestan's "Russian Whisky" announced a Scotch-inspired drink in single malt, blended and wheat varieties.
Destilerías y Crianza del Whisky S.A. is a whisky distillery in Spain. Its eight-year-old Whisky DYC is a combination of malts and spirits distilled from barley aged separately a minimum of eight years in American oak barrels.
Frysk Hynder is a Dutch single malt, distilled and bottled in the Frisian "Us Heit Distillery". It is the first single malt produced in the Netherlands.
Chemistry.
Overview.
Whiskies and other distilled beverages, such as cognac, and rum are complex beverages that contain a vast range of flavouring compounds, of which some 200 to 300 are easily detected by chemical analysis. The flavouring chemicals include "carbonyl compounds, alcohols, carboxylic acids and their esters, nitrogen- and sulphur-containing compounds, tannins and other polyphenolic compounds, terpenes, and oxygen-containing heterocyclic compounds" and esters of fatty acids. The nitrogen compounds include pyridines, picolines and pyrazines.
Flavours from treating the malt.
The distinctive smoky flavour found in various types of whisky, especially Scotch, is due to the use of peat smoke to treat the malt. 
Flavours from distillation.
The flavouring of whisky is partially determined by the presence of congeners and fusel oils. Fusel oils are higher alcohols than ethanol, are mildly toxic, and have a strong, disagreeable smell and taste. An excess of fusel oils in whisky is considered a defect. A variety of methods are employed in the distillation process to remove unwanted fusel oils. Traditionally, American distillers focused on secondary filtration using charcoal, gravel, sand, or linen to remove undesired distillates.
Acetals are rapidly formed in distillates and a great many are found in distilled beverages, the most prominent being acetaldehyde diethyl acetal (1,1-diethoxyethane). Among whiskies the highest levels are associated with malt whisky. This acetal is a principal flavour compound in sherry, and contributes fruitiness to the aroma.
The diketone diacetyl (2,3-butanedione) has a buttery aroma and is present in almost all distilled beverages. Whiskies and cognacs typically contain more of this than vodkas, but significantly less than rums or brandies.
Flavours from oak.
Whisky that has been aged in oak barrels absorbs substances from the wood. One of these is cis-3-methyl-4-octanolide, known as the "whisky lactone" or "quercus lactone", a compound with a strong coconut aroma.
Commercially charred oaks are rich in phenolic compounds. One study identified 40 different phenolic compounds. The coumarin scopoletin is present in whisky, with the highest level reported in Bourbon whiskey.
Flavours and colouring from additives.
Depending on the local regulations, additional flavourings and colouring compounds may be added to the whisky. Canadian whisky may contain caramel and flavouring in addition to the distilled mash spirits. Scotch whisky may contain added (E150A) caramel colouring, but no other additives. The addition of flavourings is not allowed in American "straight" whiskey, but is allowed in American blends.
Chill filtration.
Whisky is often "chill filtered": chilled to precipitate out fatty acid esters and then filtered to remove them. Most whiskies are bottled this way, unless specified as "unchillfiltered" or "non chill filtered". This is done primarily for cosmetic reasons. Unchillfiltered whiskeys often turn cloudy when stored at cool temperatures or when cool water is added to them, and this is perfectly normal.

</doc>
<doc id="33174" url="http://en.wikipedia.org/wiki?curid=33174" title="Wharf">
Wharf

A wharf, quay (, also or ), staith or staithe is a structure on the shore of a harbour or on the bank of a river or canal where ships may dock to load and unload cargo or passengers.
Such a structure includes one or more berths (mooring locations), and may also include piers, warehouses, or other facilities necessary for handling the ships.
Overview.
A wharf commonly comprises a fixed platform, often on pilings. Commercial ports may have warehouses that serve as interim storage areas, since the typical objective is to unload and reload vessels as quickly as possible. Where capacity is sufficient a single wharf with a single berth constructed along the land adjacent to the water is normally used; where there is a need for more capacity multiple wharves, or perhaps a single large wharf with multiple berths, will instead be constructed, sometimes projecting into the water. A pier, raised over the water rather than within it, is commonly used for cases where the weight or volume of cargos will be low.
Smaller and more modern wharves are sometimes built on flotation devices (pontoons) to keep them at the same level as the ship, even during changing tides.
In everyday parlance the term "quay" is common in the United Kingdom, Canada, Australia, and many other Commonwealth countries, and the Republic of Ireland, whereas the term "wharf" is more common in the United States. In some contexts "wharf" and "quay" may be used to mean pier, berth, or jetty.
In old ports such as London (which once had around 1700 wharves ) many old wharves have been converted to residential or office use.
Etymology.
The word "wharf" comes from the Old English "hwearf", meaning "bank" or "shore", and its plural is either "wharfs" or "wharves"; collectively a group of these is referred to as a "wharfing" or "wharfage". "Wharfage" also refers to a fee charged by ports for the cargo handled there.
In the northeast and east of England the term "staithe" or "staith" (from the Norse for landing stage) is also used. For example Dunston Staiths in Gateshead and Brancaster Staithe in Norfolk. However, the term "staithe" may also be used to refer only to loading chutes or ramps used for bulk commodities like coal in loading ships and barges. It has been suggested that wharf actually is an acronym for "ware-house at river front", but it is actually a backronym created by Thames river boat guides.
Another explanation may be that the word "wharf" comes from the Saxon word "warft" or the Dutch word "werf" which both mean "yard", an outdoor place where work is done, like a shipyard (Dutch: scheepswerf) or a lumberyard (Dutch: houtwerf). This could explain the name Ministry Wharf located at Saunderton, just outside High Wycombe, which is nowhere near any body of water. In support of this explanation is the fact that many places in England with "wharf" in their names are in areas with a high Dutch influence, for example the Norfolk broads. 

</doc>
<doc id="33176" url="http://en.wikipedia.org/wiki?curid=33176" title="Demon dialing">
Demon dialing

In the computer hacking scene of the 1980s, demon dialing was a technique by which a computer is used to repeatedly dial a number (usually to a crowded modem pool) in an attempt to gain access immediately after another user had hung up. The expansion of accessible internet service provider connectivity since that time more or less rendered the practice obsolete.
A similar technique was sometimes used to get the first call for prizes in radio "call-in" shows, thus leading to the adoption of random "fifth caller," "seventeenth caller" etc. by radio stations to circumvent this practice.
The term "demon dialing" derives from the Demon Dialer product from Zoom Telephonics, Inc., a telephone device produced in the 1980s which repeatedly dialed busy telephone numbers under control of an extension phone.
"Demon dialing" was popularized by the movie WarGames, which showed a demon dialer program in action. After giving the program an area code and 3 digit prefix, the program then serially dialed every phone number in that prefix in order, recording which phone numbers were answered by a computer modem. Soon after, most likely because it was popularized by the movie, serial dialing was outlawed. Hackers got around this by simply randomizing the order the program dialed all the numbers. Later the terms "demon dialing" and "war dialing" became synonymous.
See also.
Automatic redial, which is essentially exactly the same thing, but for modems, and included in most telecommunications programs that use modems, including all variants of Dial-up networking ever included in the Windows operating system.

</doc>
<doc id="33178" url="http://en.wikipedia.org/wiki?curid=33178" title="White supremacy">
White supremacy

White supremacy or white supremacism is a form of racism centered upon the belief, and promotion of the belief, that white people are superior in certain characteristics, traits, and attributes to people of other racial backgrounds and that therefore whites should politically, economically and socially rule non-whites. The term is also typically used to describe a political ideology that perpetuates and maintains the social, political, historical and/or industrial domination by white people (as evidenced by historical and contemporary sociopolitical structures like the Atlantic Slave Trade, colonization of the Global South, Jim Crow laws in the United States, and miscegenation laws in settler colonies and former settler colonies like the United States, South Africa, Australia, and Madagascar, for example). Different forms of white supremacism put forth different conceptions of who is considered white, and different white supremacists identify various racial and cultural groups as their primary enemy.
In academic usage, the term "white supremacy" can also refer to a system where whites enjoy a structural advantage (privilege) over other ethnic groups, both at a collective and an individual level ("ceteris paribus", i. e., when individuals are compared that do not relevantly differ except in ethnicity).
History of white supremacy.
White supremacy has ideological foundations that at least date back to 17th-century scientific racism, the predominant paradigm of human variation that helped shape international and intra-national relations from the latter part of the Age of Enlightenment (in European history) through the late 20th century (marked by the end of Apartheid in South Africa in 1994, one of the last explicitly White supremacist sociopolitical structures to exist).
United States.
See also: .
White supremacy was dominant in the United States before the American Civil War and for decades after Reconstruction. In large areas of the United States, this included the holding of non-whites (specifically African Americans) in chattel slavery. The outbreak of the Civil War saw the desire to uphold white supremacy cited as a cause for state secession and the formation of the Confederate States of America.
In some parts of the United States, many people who were considered non-white were disenfranchised, barred from government office, and prevented from holding most government jobs well into the second half of the 20th century. Many U.S. states banned interracial marriage through anti-miscegenation laws until 1967, when these laws were declared unconstitutional. Additionally, white leaders often viewed Native Americans as obstacles to economic and political progress with respect to the natives' claims to land and rights.
Germany.
Nazism promoted the idea of a superior Aryan race in Germany during the early 20th century. Notions of white supremacy and Aryan racial superiority combined in the 19th century, with white supremacists maintaining that white people were members of an Aryan "master race" which is superior to other non-Aryan races, and particularly the Jews described as the "Semitic race", Slavs and Gypsies, which they associated with "cultural sterility". Arthur de Gobineau, a French racial theorist and aristocrat, blamed the fall of the ancient régime in France on racial degeneracy caused by racial intermixing, which he argued destroyed the purity of the Aryan race. Gobineau's theories, which attracted a strong following in Germany, emphasized the existence of an irreconcilable polarity between Aryan and Jewish cultures.
In order to preserve the Aryan race, the Nazis introduced in 1935 the Nuremberg racial laws which forbade sexual relations and marriages between Aryans and non-Aryans.
Nazis used the Mendelian inheritance theory to demonstrate the inheritance of social traits, claiming a racial nature of certain general traits such as inventiveness or criminal behaviour.
Many of the modern-day white supremacist groups around the world re-use German Nazi symbolism, including the swastika, to represent their beliefs.
According to the annual report of Germany's interior intelligence service (Verfassungsschutz) for 2012, at the time there were 26,000 right-wing extremists living in Germany, including 6000 neo-Nazis.
South Africa.
White supremacy was also dominant in South Africa under apartheid, which it maintained until 1994.
Academic use of the term.
The term "white supremacy" is used in academic studies of racial power to denote a system of structural or societal racism which privileges white people over others, regardless of the presence or absence of racial hatred. Legal scholar Frances Lee Ansley explains this definition as follows:
This and similar definitions are adopted or proposed by Charles Mills, bell hooks, David Gillborn, and Neely Fuller Jr. Some anti-racist educators, such as Betita Martinez and the Challenging White Supremacy workshop, also use the term in this way. The term expresses historic continuities between a pre-Civil Rights era of open white supremacism and the current racial power structure of the United States. It also expresses the visceral impact of structural racism through "provocative and brutal" language that characterizes racism as "nefarious, global, systemic, and constant." Academic users of the term sometimes prefer it to "racism" because it allows for a disconnection between racist feelings and white racial advantage or privilege.
Ideologies and movements.
Supporters of Nordicism consider the Nordic peoples to be a superior race considering all non-Nordic people, in particular Jews, Gypsies, Russians and other slavs. By the early-19th century white supremacy was attached to emerging theories of racial hierarchy. The German philosopher Arthur Schopenhauer attributed civilisational primacy to the White race:
The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate.
The eugenicist Madison Grant argued that the Nordic race had been responsible for most of humanity's great achievements, and that admixture was "race suicide". In Grant's 1916 book, "The Passing of the Great Race", Europeans who are not of Germanic origin, but have Nordic characteristics such as blonde/red hair and blue/green/gray eyes, were considered to be a Nordic admixture and suitable for Aryanization.
In the United States, the Ku Klux Klan (KKK) is the group most associated with the white supremacist movement. Many white supremacist groups are based on the concept of preserving genetic purity, and they do not focus solely on discrimination by skin color. The KKK's reasons for supporting racial segregation are not primarily based on religious ideals, but some Klan groups are openly Protestant. The KKK and other white supremacist groups like Aryan Nations, The Order and the White Patriot Party are considered Anti-Semitic.
Nazi Germany promulgated white supremacy in the belief that the Aryan race was the "master race". It was combined with a eugenics programme that aimed for racial hygiene by using compulsory sterilizations and extermination of the Untermenschen (or "sub-humans"), which eventually culminated in the Holocaust.
Christian Identity is another movement closely tied to white supremacy. Some white supremacists identify themselves as Odinists, although many Odinists reject white supremacy. Some white supremacist groups, such as the South African Boeremag, conflate elements of Christianity and Odinism. The World Church of the Creator (now called the Creativity Movement) is atheistic and it denounces the Christian religion and other theistic religions. Aside from this, its ideology is similar to many Christian Identity groups, in their belief that there is a Jewish conspiracy in control of governments, the banking industry and the media. Matthew F. Hale, founder of the World Church of the Creator has published articles stating that all races other than white are "mud races," which the religion teaches.
The white supremacist ideology has become associated with a racist faction of the skinhead subculture, despite the fact that when the skinhead culture first developed in the United Kingdom in the late 1960s, it was heavily influenced by black fashions and music, especially Jamaican reggae and ska, and African American soul music By the 1980s, a sizable and vocal white power skinhead faction had formed.
White supremacist recruitment activities are conducted primarily at a grassroots level and on the Internet. Widespread access to the Internet has led to a dramatic increase in white supremacist websites. The Internet provides a venue to openly express white supremacist ideas at little social cost, because people who post the information are able to remain anonymous.
Alliances with black supremacist groups.
In February 1962 George Lincoln Rockwell, the leader of the American Nazi Party, spoke at a Nation of Islam rally in Chicago, where he was applauded by Elijah Muhammed as he pronounced: "I am proud to stand here before black men. I believe Elijah Muhammed is the Adolf Hitler of the black man!" Rockwell had attended, but did not speak at, an earlier NOI rally in June 1961 in Washington, D.C. and even once donated $20 to the NOI. In 1965, after breaking with the Nation of Islam and denouncing its separatist doctrine, Malcolm X told his followers that the Nation of Islam under Elijah Muhammad had made secret agreements with the American Nazi Party and the Ku Klux Klan.
Rockwell and other white supremacists (e.g. Willis Carto) also supported less well-known black supremacist groups, such as Hassan Jeru-Ahmed's Blackman's Army of Liberation, in reference to which Rockwell told "Los Angeles Times" reporter Michael Drosnin in 1967 that "Any Negro wants to go back to Africa, I'll carry him piggy-back."
More recently, Tom Metzger, erstwhile Ku Klux Klan leader from California, spoke at an NOI rally in Los Angeles in September 1985 and donated $100 to the group. In October of that same year a collection of over 200 prominent white supremacists met at former Klan leader Robert E. Miles's farm to discuss an alliance with Louis Farrakhan, head of the NOI. In attendance were Edward Reed Fields of the National States' Rights Party, Richard Girnt Butler of Aryan Nations, Don Black, Roy Frankhouser, and Metzger, who said that "America is like a rotting carcass. The Jews are living off the carcass like the parasites they are. Farrakhan understands this."

</doc>
<doc id="33186" url="http://en.wikipedia.org/wiki?curid=33186" title="Waffen-SS">
Waffen-SS

The Waffen-SS (], "Armed SS") was created as the armed wing of the Nazi Party's "Schutzstaffel" (SS, "Protective Squadron"), and gradually developed into a multi-ethnic and multi-national military force of Nazi Germany.
The Waffen-SS grew from three regiments to over 38 divisions during World War II, and served alongside the "Heer" (regular army) but was never formally part of it. Adolf Hitler resisted integrating the Waffen-SS into the army, as it was to remain the armed wing of the Party and to become an elite police force once the war was won. Prior to the war, it was under the control of the "SS Führungshauptamt" (SS operational command office) beneath "Reichsführer-SS" Heinrich Himmler. Upon mobilization its tactical control was given to the High Command of the Armed Forces ("Oberkommando der Wehrmacht").
Initially membership was only open to people of Germanic "Aryan" origin, who were said to be the "Herrenvolk" (master race), according to Nazi racial ideology. The rules were partially relaxed in 1940, although groups considered by Nazis to be "sub-human" like ethnic Poles or Jews remained excluded. Hitler authorized the formation of units composed largely or solely of foreign volunteers and conscripts. Foreign SS units were made up from recruits in Albania, Armenia, Azerbaijan, Belarus, Belgium (both Wallonia and Flanders), Bulgaria, Denmark, Estonia, Finland, France, Galicia, Georgia, Hungary, India, Ireland, Italy, Latvia, Lithuania, Luxembourg, Netherlands, Norway, Poland, Romania, Russia (including Cossack and Tatar, Turkic SSR Republics), Slovakia, Slovenia, Spain, Sweden, Ukraine, Yugoslavia, Asian Regiment, Arab Regiment, USA (15-20 volunteers) and a small number of British troops, with the latter unit being a significant propaganda tool.
At the post-war Nuremberg Trials the Waffen-SS was condemned as a criminal organization due to its connection to the Nazi Party and involvement in numerous war crimes. Waffen-SS veterans were denied many of the rights afforded to veterans who had served in the "Heer" (army), "Luftwaffe" (air force), or "Kriegsmarine" (navy). An exception was made for Waffen-SS conscripts sworn in after 1943, who were exempted because of their involuntary servitude.
Origins (1929–39).
The origins of the Waffen-SS can be traced back to the selection of a group of 120 SS men in March 1933 by Josef "Sepp" Dietrich to form the "Sonderkommando" Berlin. By November 1933 the formation was 800 men strong, and at a remembrance ceremony in Munich for the tenth anniversary of the failed Munich Putsch the regiment swore allegiance to Hitler. The oaths pledged were "Pledging loyalty to him alone" and "Obedience unto death". The formation was given the title "Leibstandarte" (Bodyguard Regiment) "Adolf Hitler" (LAH). On 13 April 1934, by order of Himmler, the regiment became known as the "Leibstandarte SS Adolf Hitler" (LSSAH).
The "Leibstandarte" demonstrated their loyalty in June 1934 during the Night of the Long Knives, the purge of the "Sturmabteilung" (SA). The SA had over two million members at the end of 1933. Led by one of Hitler's oldest comrades, Ernst Röhm, the SA represented a threat to Hitler's relationship with the German Army and threatened to sour his relations with the conservatives of the country, people whose support Hitler needed to solidify his position in the German government. Hitler decided to act against the SA. The SS was put in charge of eliminating Röhm and the other high-ranking officers of the SA.
The Night of the Long Knives between 30 June and 2 July 1934 saw the killing of approximately 82 SA men, including almost its entire leadership, effectively ending the power of the SA. This action was largely carried out by the "Leibstandarte SS Adolf Hitler". In September 1934, Hitler authorized the formation of the military wing of the Nazi Party and approved the formation of the "SS-Verfügungstruppe" (SS-VT), a special service troop under Hitler's command. The SS-VT had to depend on the German Army for its supply of weapons and military training, and they had control of the recruiting system through local draft boards responsible for assigning conscripts to the different branches of the Wehrmacht to meet quotas set by the German High Command (Oberkommando der Wehrmacht or OKW in German). The SS was given the lowest priority for recruits.
Even with the difficulties presented by the quota system, Heinrich Himmler formed two new SS regiments, the "SS Germania" and "SS Deutschland", which together with the "Leibstandarte" and a communications unit made up the SS-VT. At the same time Himmler established the SS-Junkerschule Bad Tölz and SS-Junkerschule Braunschweig for training officers to lead the new regiments. Both schools used the regular army training methods and used former Army officers to train potential officers. 
Himmler initially in 1934 set stringent requirements for Waffen-SS recruits. They were to be German nationals who could prove their Aryan ancestry back to 1800, unmarried, and without a criminal record. A four-year commitment was required. Recruits had to be between the ages of 17 and 23, at least 1.74 m tall (1.78 m for the "Leibstandarte"). Concentration camp guards had to make a one-year commitment, be between the ages of 16 and 23, and at least 1.72 m tall. All recruits were required to have perfect teeth and eyesight and provide a medical certificate. By 1938 the height restrictions were relaxed, up to six dental fillings were permitted, and eyeglasses for astigmatism and mild vision correction were allowed. Once the war commenced, the physical requirements were no longer strictly enforced, and essentially any recruit who could pass a basic medical exam was considered for Waffen-SS service. Recruiting of ethnic Germans from other countries began in April 1940, and units consisting of non-Germanic recruits were formed beginning in 1942. Non-Germanic units were not considered to be part of the SS, which still maintained its strict racial criteria, but rather were considered to be foreign nationals serving under the command of the SS.
Members of the SS could be of any religion except Jewish, but atheists were not allowed. "Atheism is the only world-view or religious view that is not tolerated within the SS", Himmler wrote in 1937.
In 1936, Himmler selected former Lieutenant General Paul Hausser to be Inspector of the SS-VT with the rank of "Brigadefuhrer". Hausser transformed the SS-VT into a credible military force that was a match for the regular army.
On 17 August 1938, Hitler declared that the SS-VT would have a role in domestic as well as foreign affairs, which transformed this growing armed force into the rival that the army had feared. He decreed that service in the SS-VT qualified to fulfill military service obligations, although service in the "SS-Totenkopfverbände" or SS-TV would not. Some units of the SS-TV would, in the case of war, be used as reserves for the SS-VT, which did not have its own reserves. For all its training, the SS-VT was untested in combat. This changed in 1938, when two opportunities arose with the Anschluss of Austria in March and the occupation of the Sudetenland in October. A battalion of the "Leibstandarte" was chosen to accompany the Army troops in occupying Austria, and the three regiments of the SS-VT participated in the occupation of the Sudetenland. In both actions no resistance was met.
World War II.
1939.
Invasion of Poland.
Himmler's military formations at the outbreak of the war comprised several subgroups which would become the basis of the Waffen-SS.
In August 1939, Hitler placed the "Leibstandarte" and the SS-VT under the operational control of the Army High Command (OKH). Himmler retained command of the "Totenkopfstandarten," for employment behind the advancing combat units in what were euphemistically called "police and security duties".
Events during the Invasion of Poland raised doubts over the combat effectiveness of the SS-VT. Their willingness to fight was never in doubt; at times they were almost too eager. The OKW reported that the SS-VT had unnecessarily exposed themselves to risks and acted recklessly, incurring heavier losses than army troops. They also stated that the SS-VT was poorly trained and its officers unsuitable for command. As an example, OKW noted that the "Leibstandarte" had to be rescued by an army regiment after becoming surrounded at Pabianice by the Poles. In its defence, the SS-VT insisted that it had been hampered by having to fight piecemeal instead of as one formation, and was improperly equipped to carry out its objectives. During the invasion the "Leibstandarte" (LSSAH) became notorious for torching villages; members of the LSSAH murdered 50 Jews in the town of Błonie.
Himmler insisted that the SS-VT should be allowed to fight in its own formations under its own commanders, while the OKW tried to have the SS-VT disbanded altogether. Hitler was unwilling to upset either the army or Himmler, and chose a third path. He ordered that the SS-VT form its own divisions but that the divisions would be under army command.
First Divisions.
In October 1939, "Deutschland", "Germania", and "Der Führer" were reorganized into the SS-Verfügungs Division. The "Leibstandarte" remained independent and was increased in strength to a reinforced motorized regiment. Hitler authorized the creation of two new divisions: the SS "Totenkopf" Division, formed from militarized "Standarten" of the "SS-Totenkopfverbände", and the Polizei Division, formed from members of the national police force. Almost overnight the force that the OKW had tried to disband had increased from 18,000 to over 100,000 men. Hitler next authorized the creation in March 1940 of four Motorized Artillery battalions, one for each division and the "Leibstandarte". The OKW was supposed to supply these new battalions with weapons, but was reluctant to hand over guns from its own arsenal. The weapons arrived only slowly, and by the time of the Battle of France only the "Leibstandarte" battalion was up to strength.
1940.
France and the Netherlands.
The three SS divisions and the "Leibstandarte" spent the winter of 1939 and the spring of 1940 training and preparing for the coming war in the west. In May they moved to the front, and the "Leibstandarte" became part of the Army's 227th Infantry Division. The "Der Führer" Regiment was detached from the SS-VT Division and relocated near the Dutch border, with the remainder of the division behind the line in Münster, awaiting the order to invade the Netherlands. The SS "Totenkopf" and Polizei Divisions were held in reserve.
On 10 May the "Leibstandarte", wearing Dutch uniforms, overcame Dutch border guards to spearhead the German advance into the Netherlands, and the "Der Führer" advanced towards Utrecht. The following day the rest of the SS-VT Division crossed into the Netherlands and headed towards Rotterdam, which they reached on 12 May. After the surrender of Rotterdam, the "Leibstandarte" left for the Hague, which they reached on 15 May, capturing 3,500 Dutch as prisoners of war.
In France the SS "Totenkopf" was involved in the only Allied tank attack in the Battle of France. On 21 May units of the 1st Army Tank Brigade, supported by the 50th (Northumbrian) Infantry Division, took part in the Battle of Arras. The SS "Totenkopf" was overrun, finding their standard anti-tank gun, the 3.7 cm PaK 36, was no match for the British Matilda tank.
After the Dutch surrender, the "Leibstandarte" moved south to France on 24 May. Becoming part of the XIX Panzer Corps under the command of General Heinz Guderian, they took up a position 15 miles south west of Dunkirk along the line of the Aa Canal, with a bridgehead at Saint-Venant. That night the OKW ordered the advance to halt, with the British Expeditionary Force trapped. The "Leibstandarte" paused for the night, but the following day, in defiance of Hitler's orders, continued the advance. Dietrich ordered his III Battalion to cross the canal and take the heights beyond, where British artillery observers were putting the regiment at risk. They assaulted the heights and drove the observers off. Instead of being censured for his act of defiance, Dietrich was awarded the Knight's Cross of the Iron Cross.
The same day the British attacked Saint-Venant, forcing the SS-VT Division to retreat, the first time an SS unit had been forced to withdraw and relinquish ground. On 26 May the German advance resumed. On 27 May the "Deutschland" regiment reached the allied defensive line on the Leie River at Merville. They forced a bridgehead across the river and waited for the SS "Totenkopf" Division to arrive to cover their flank. What arrived first was a unit of British tanks, which penetrated their positions. The SS-VT managed to hold on against the British tank force, which got to within 15 feet of commander Felix Steiner's position. Only the arrival of the "Totenkopf" Panzerjäger platoon saved the "Deutschland" from being destroyed.
At the same time, on 27 May another unit from the "Totenkopf", the 14 Company, was involved in the Le Paradis massacre, where 99 men of the 2nd Battalion, Royal Norfolk Regiment were machine gunned, with survivors finished off with bayonets.
By 28 May the "Leibstandarte" had taken Wormhout, only ten miles from Dunkirk. Soldiers of the 2nd Battalion were responsible for the Wormhoudt massacre, where 80 British and French prisoners of war were killed.
By 30 May the British were cornered at Dunkirk, and the SS divisions continued the advance into France. The "Leibstandarte" reached Saint-Étienne, 250 miles south of Paris, and had advanced further into France than any other unit. The next day the French surrendered. Hitler expressed his pleasure with the performance of the "Leibstandarte" in the Netherlands and France, telling them, "Henceforth it will be an honour for you, who bear my name, to lead every German attack."
1940 expansion.
Himmler gained approval for the Waffen-SS to form its own high command, the "Kommandoamt der Waffen-SS" within the "SS-Führungshauptamt", which was created in August 1940. It received command of the SS-VT (the "Leibstandarte" and the "Verfügungs-Division", renamed "Reich") and the armed SS-TV regiments (the "Totenkopf-Division" together with several independent "Totenkopf-Standarten").
In August 1940 SS chief of staff Gottlob Berger approached Himmler with a plan to recruit volunteers in the conquered territories from the ethnic German and Germanic populations. At first Hitler had doubts about recruiting foreigners, but he was persuaded by Himmler and Berger. He gave approval for a new division to be formed from foreign nationals with German officers, and by June 1941 Danish and Norwegian volunteers had formed the SS Regiment "Nordland", with Dutch and Flemish volunteers forming the SS Regiment "Westland". The two regiments, together with "Germania" (transferred from the "Reich" Division), formed the SS Division 'Wiking". Volunteers came forward in such numbers that the SS was forced to open a new training camp just for foreign volunteers at Sennheim in Alsace-Lorraine.
1941.
At the beginning of the new year the "Polizei-Division" was brought under FHA administration, although it would not be formally merged into the Waffen-SS until 1942. At the same time the "Totenkopf-Standarten", aside from the three constituting the TK-Division, lost their Death's Head designation and insignia and were reclassified "SS-Infanterie-" (or "Kavallerie-") "Regimente". The 11th Rgt. was transferred into the "Reich" Division to replace "Germania"; the remainder were grouped into three independent brigades and a battle group in Norway.
By the spring of 1941 the Waffen-SS consisted of the equivalent of six or seven divisions: the "Reich", "Totenkopf", "Polizei", and "Wiking" Divisions and "Kampfgruppe" (later Division) "Nord", and the "Leibstandarte", 1 SS Infantry, 2 SS Infantry, and SS Cavalry Brigades.
Balkans.
In March 1941, a major Italian counterattack against Greek forces failed, and Germany was forced to come to the aid of its ally. Operation Marita began on 6 April 1941, with German troops invading Greece through Bulgaria and Yugoslavia in an effort to secure its southern flank.
"Reich" was ordered to leave France and head for Romania, and the "Leibstandarte" was ordered to Bulgaria. The "Leibstandarte", attached to the XL Panzer Corps, advanced west then south from Bulgaria into the mountains, and by 9 April had reached Prilep in Yugoslavia, 30 miles from the Greek border. Further north the SS "Reich", with the XLI Panzer Corps, crossed the Romanian border and advanced on Belgrade, the Yugoslav capital, arriving on 12 April to accept the city's surrender. The Royal Yugoslav Army surrendered a few days later.
The "Leibstandarte" had now crossed into Greece, and on 10 April engaged the 6th Australian Division in the Battle of the Klidi Pass. For 48 hours they fought for control of the heights, often engaging in hand-to-hand combat, eventually gaining control with the capture of Height 997, which opened the pass and allowed the German Army to advance into the Greek interior. This victory finally gained praise from the OKW: in the order of the day they were commended for their "unshakable offensive spirit" and told that "The present victory signifies for the "Leibstandarte" a new and imperishable page of honour in its history."
The "Leibstandarte" continued the advance on 13 May. When the Reconnaissance Battalion under the command of Kurt Meyer came under heavy fire from the Greek Army defending the Klisura Pass, they routed the defenders and captured 1,000 prisoners of war at the cost of six dead and nine wounded. The next day, Meyer captured Kastoria and took another 11,000 prisoners of war. By 20 May the "Leibstandarte" had cut off the retreating Greek Army at Metsovon and accepted the surrender of the Greek Epirus-Macedonian Army. As a reward, the "Leibstandarte" was nominally promoted to a full motorized division, although few additional elements had been added by the start of the Russian campaign and the "Division" remained effectively a reinforced brigade.
Soviet Union.
Operation Barbarossa, the German invasion of the Soviet Union, started on 22 June 1941, and all the Waffen-SS formations participated (including the SS "Reich", which was formally renamed to SS "Das Reich" by the Fall of 1941).
SS Division "Nord" in northern Finland took part in Operation Arctic Fox with the Finnish Army and fought at the disastrous battle of Salla, where against strong Soviet forces they suffered 300 killed and 400 wounded in the first two days of the invasion. Thick forests and heavy smoke from forest fires disoriented the troops and the division's units completely fell apart. By the end of 1941, "Nord" had suffered severe casualties. Over the winter of 1941–42 it received replacements from the general pool of Waffen-SS recruits, who were supposedly younger and better trained than the SS men of the original formation, which had been drawn largely from "Totenkopfstandarten" of Nazi concentration camp guards.
The rest of the Waffen-SS divisions and brigades fared better. The SS "Totenkopf" and Polizei divisions were attached to Army Group North, with the mission to advance through the Baltic states and on to Leningrad. The SS Division "Das Reich" was with Army Group Centre and headed towards Moscow. The SS Division "Wiking" and the "Leibstandarte" were with Army Group South, heading for the Ukraine and the city of Kiev.
The war in the Soviet Union proceeded well at first, but the cost to the Waffen-SS was extreme: by late October the "Leibstandarte" was at half strength due to enemy action and dysentery that swept through the ranks. "Das Reich" lost 60% of its strength and was still to take part in the Battle of Moscow. The unit was decimated in the following Soviet offensive. The "Der Führer" Regiment was reduced to 35 men out of the 2,000 that had started the campaign in June. Altogether, the Waffen-SS had suffered 43,000 casualties.
While the "Leibstandarte" and the SS divisions were fighting in the front line, behind the lines it was a different story. The 1 SS Infantry and 2 SS Infantry Brigades, which had been formed from surplus concentration camp guards of the SS-TV, and the SS Cavalry Brigade moved into the Soviet Union behind the advancing armies. At first they fought Soviet partisans and cut off units of the Red Army in the rear of Army Group South, capturing 7,000 prisoners of war, but from mid-August 1941 until late 1942 they were assigned to the Reich Main Security Office headed by Reinhard Heydrich. The brigades were now used for rear area security and policing, and were no longer under Army or Waffen-SS command. In the autumn of 1941, they left the anti-partisan role to other units and actively took part in the Holocaust. While assisting the "Einsatzgruppen", they participated in the liquidation of the Jewish population of the Soviet Union, forming firing parties when required. The three brigades were responsible for the murder of tens of thousands by the end of 1941.
Because it was more mobile and better able to carry out large-scale operations, the SS Cavalry Brigade played a pivotal role in the transition to the wholesale extermination of the Jewish population. On 27 July, the Brigade was ordered into action, and by 1 August the SS Cavalry Regiment was responsible for the death of 800 people; by 6 August, this total had reached 3,000 "Jews and partisans". On 1 August, after a meeting between Himmler, Erich von Bach-Zelewski and Hinrich Lohse, the brigades received the following order: "Explicit order by RFSS: All Jews must be shot. Drive the female Jews into the swamps."
Gustav Lombard, on receiving the order, advised his Battalion that "In future not one male Jew is to remain alive, not one family in the villages." Throughout the next weeks, soldiers of SS Cavalry Regiment 1 under Lombard's command murdered an estimated 11,000 Jews and more than 400 dispersed soldiers of the Red Army.
1942.
1942 expansion.
In 1942, the Waffen-SS was further expanded and a new division was entered on the rolls in March. By the second half of 1942 an increasing number of foreigners, many of whom were not volunteers, began entering the ranks. The 7th SS Volunteer Mountain Division "Prinz Eugen" was recruited from Volksdeutsche (ethnic Germans) drafted under threat of punishment by the local German leadership from Croatia, Serbia, Hungary, and Romania and used for anti-partisan operations in the Balkans. Himmler approved the introduction of formal compulsory service for the Volksdeutsche in German occupied Serbia. Another new division was formed at the same time, when the SS Cavalry Brigade was used as the cadre in the formation of the 8th SS Cavalry Division "Florian Geyer".
Panzergrenadier divisions.
The front line divisions of the Waffen-SS that had suffered through the Russian winter of 1941–1942 and the Soviet counter-offensive were withdrawn to France to recover and be reformed as Panzergrenadier divisions. Thanks to the efforts of Himmler and "Obergruppenführer" Paul Hausser, the new commander of the SS Panzer Corps, the three SS Panzergrenadier divisions "Leibstandarte", "Das Reich", and "Totenkopf" were to be formed with a full regiment of tanks rather than only a battalion. This meant that the SS Panzergrenadier divisions were full-strength Panzer divisions in all but name. They each received nine Tiger tanks, which were formed into the heavy panzer companies.
Demyansk Pocket.
The Soviet offensive of January 1942 trapped a number of German divisions in the Demyansk Pocket between February and April 1942; the 3rd SS "Totenkopf" was one of the divisions encircled by the Red Army. The Red Army liberated Demyansk on 1 March 1943 with the retreat of the German troops. "For his excellence in command and the particularly fierce fighting of the "Totenkopf"", Obergruppenführer Theodor Eicke was awarded the Oak Leaves to the Knight's Cross on 20 May 1942.
1943.
1943 expansion.
The Waffen-SS expanded further in 1943: in February the 9th SS Panzer Division "Hohenstaufen" and its sister division, the 10th SS Panzer Division "Frundsberg", were formed in France. They were followed in July by the 11th SS Volunteer Panzergrenadier Division "Nordland" created from Norwegian and Danish volunteers. September saw the formation of the 12th SS Panzer Division "Hitlerjugend" using volunteers from the Hitler Youth. Himmler and Berger successfully appealed to Hitler to form a Bosnian Muslim division, and the 13th Waffen Mountain Division of the SS Handschar (1st Croatian), the first non-Germanic division, was formed, to fight Josip Broz Tito's Yugoslav Partisans. This was followed by the 14th Waffen Grenadier Division of the SS (1st Galician) formed from volunteers from Galicia in western Ukraine. The 15th Waffen Grenadier Division of the SS (1st Latvian) was created in 1943, using compulsory military service in the Ostland. The final new 1943 division was the 16th SS Panzergrenadier Division "Reichsführer-SS", which was created using the Sturmbrigade Reichsführer SS as a cadre. By the end of the year, the Waffen-SS had increased in size from eight divisions and some brigades to 16 divisions.
Kharkov.
On the Eastern Front, the Germans suffered a devastating defeat when the 6th Army was defeated during the Battle of Stalingrad. Hitler ordered the SS Panzer Corps back to the Eastern Front for a counter-attack with the city of Kharkiv as its objective. The SS Panzer Corps was in full retreat on 19 February, having been attacked by the Soviet 6th Army, when they received the order to attack. In an example of an SS Commander disobeying Hitler's order to "stand fast and fight to the death", Hausser withdrew in front of the Red Army. During Manstein's counteroffensive, the SS Panzer Corps, without support from the Luftwaffe or neighbouring German formations, broke through the Soviet line and advanced on Kharkiv. Despite orders to encircle Kharkiv from the north, the SS Panzer Corps directly attacked in the Third Battle of Kharkov on 11 March. This led to four days of house-to-house fighting before Kharkov was recaptured by the 1 SS "Leibstandarte" on 15 March. Two days later the Germans recaptured Belgorod, creating the salient that in July 1943 led to the Battle of Kursk. The German offensive cost the Red Army an estimated 70,000 casualties but the house-to-house fighting in Kharkiv was particularly bloody for the SS Panzer Corps, which lost approximately 44% of its strength by the time operations ended in late March.
Warsaw Ghetto uprising.
The Warsaw Ghetto Uprising was a Jewish insurgency that arose within the Warsaw Ghetto from 19 April to 16 May, an effort to prevent the transportation of the remaining population of the ghetto to Treblinka extermination camp. Units involved from the Waffen-SS were 821 Waffen-SS Panzergrenadiers from five reserve and training battalions and one cavalry reserve and training battalion.
Kursk.
The next test for the Waffen-SS was the Battle of Prokhorovka, which was part of the Battle of Kursk. The SS Panzer Corps had been renamed the II SS Panzer Corps and was part of the 4th Panzer Army, which was chosen to spearhead the attack through the Soviet defenses. The attack penetrated to a depth of 35 km and was then stopped by the Soviet 1st Tank Army.
During the fighting over the next few days, the II SS Panzer Corps thought they were close to driving a wedge between the 1st Tank Army and Soviet 69th Army, and had even broken through the third line of Soviet defenses at Prokhorovka. Wrongly believing they had made a breakthrough, they were prepared to exploit the opportunity the next day. The Soviet reserves had been sent south to defend against a German attack by the III Panzer Corps. With the loss of their reserves, any hope they may have had of dealing a major defeat to the SS Panzer Corps ended. But the German advances now failed – despite appalling losses, the Soviet tank armies held the line and prevented the II SS Panzer Corps from making the expected breakthrough.
While the exact losses on each side cannot be established precisely, the outcome is clearer. Neither the Fifth Guards Tank Army nor the II SS Panzer Corps accomplished their objectives that day. The sudden and violent attack by strong Soviet reserves and the need to break off the assault by the German 9th Army on the northern shoulder of the Kursk salient due to Operation Kutuzov contributed to Hitler's decision to discontinue the attack, the implications of which made him 'sick to his stomach'. A parallel attack by the Red Army against the new 6th Army on the Mius river south of Kharkov necessitated the withdrawal of reserve forces held to exploit any success on the southern shoulder of Kursk, and the OKW also had to draw on some German troops from the Eastern Front to bolster the Mediterranean theatre following the Anglo-American Invasion of Sicily on the night of 9–10 July 1943. Regardless of the tactical outcome, the Battle of Prokhorovka was an operational victory for the Red Army. The Soviets were not beaten, and the strategic initiative had swung to the Red Army.
Italy.
After the Allied invasion of Italy in September 1943, Hitler ordered the II SS Panzer Corps to move to Italy, but in the end only the "Leibstandarte" was sent, where the only other Waffen-SS unit was the 16 SS Panzergrenadier Division "Reichsführer"-SS.
After the Italian surrender and collapse of 8 September 1943, the "Leibstandarte" was ordered to begin disarming nearby Italian units. It also had the task of guarding vital road and rail junctions in the north of Italy and was involved in several skirmishes with partisans. This went smoothly, with the exception of a brief skirmish with Italian troops stationed in Parma on 9 September. By 19 September all Italian forces in the Po River plain had been disarmed, but the OKW received reports that elements of the Italian Fourth Army were regrouping in Piedmont, near the French border. Joachim Peiper's mechanised III Battalion, SS Panzergrenadier Regiment 2, was sent to disarm these units. On arriving in the province of Cuneo, Peiper was met by an Italian officer who warned that his forces would attack unless Peiper's unit vacated the province immediately. After Peiper refused, the Italians attacked. Peiper's battalion defeated the Italians in a fierce battle, and then disarmed the remaining Italian forces in the area.
While the "Leibstandarte" was operating in the north, the 16 SS "Reichsführer"-SS sent a Kampfgruppe to contain the Anzio landings in January 1944. In March, the bulk of the 1st Italienische Freiwilligen Sturmbrigade (or "Brigata d'Assalto, Volontari" in Italian) was sent to the Anzio beachhead, where they fought alongside their German allies, receiving favourable reports and taking heavy losses. In recognition of their performance, Himmler declared the unit to be fully integrated into the Waffen-SS.
1944.
1944 expansion.
The Waffen-SS expanded again during 1944. January saw the formation of the 19th Waffen Grenadier Division of the SS (2nd Latvian), formed from the two SS Infantry Brigades as cadre with Latvian conscripts. The 20th Waffen Grenadier Division of the SS (1st Estonian) was formed via general conscription in February 1944, around a cadre from the 3 Estonian SS Volunteer Brigade. The 21st Waffen Mountain Division of the SS Skanderbeg (1st Albanian) was formed in March 1944 from Albanian and Kosovan volunteers, for anti-partisan duties in Albania and Kosovo. A second Waffen-SS cavalry division followed in April 1944, the 22nd SS Volunteer Cavalry Division "Maria Theresia". The bulk of the soldiers were Hungarian Army Volksdeutsche conscripts transferred to the Waffen-SS following an agreement between Germany and Hungary. The 23rd SS Volunteer Panzer Grenadier Division "Nederland" followed, formed from the 4th SS Volunteer Panzergrenadier Brigade Nederland, but it was never more than a large brigade. The 24th Waffen Gebirgs Division der SS was another division that was never more than brigade size, consisting mainly of ethnic German volunteers from Italy and volunteers from Slovenia, Croatia, Serbia, and Ukraine. They were primarily involved in fighting partisans in the Kras region of the Alps on the frontiers of Slovenia, Italy, and Austria, the mountainous terrain requiring specialized mountain troops and equipment. Two Hungarian divisions followed: the 25th Waffen Grenadier Division of the SS Hunyadi (1st Hungarian) and the 26th Waffen Grenadier Division of the SS (2nd Hungarian). These were formed under the authority of the Hungarian defense minister, at the request of Himmler. One regiment from the Hungarian Army was ordered to join, but they mostly consisted of Hungarian and Rumanian volunteers.
The 27th SS Volunteer Division "Langemarck" was formed next in October 1944, from Flemish volunteers added to the 6th SS Volunteer Sturmbrigade Langemarck, but again it was nothing more than a large brigade. The 5th SS Volunteer Sturmbrigade Wallonien was also upgraded to the 28th SS Volunteer Grenadier Division "Wallonien", but it too was never more than a large brigade. Plans to convert the Kaminnski Brigade into the 29th Waffen Grenadier Division of the SS RONA (1st Russian) were dropped after the execution of their commander, Bronislav Kaminski; instead the Waffen Grenadier Brigade of SS (Italian no. 1) became the 29th Waffen Grenadier Division of the SS (1st Italian). The 30th Waffen Grenadier Division of the SS (2nd Russian) was formed from the Schutzmannschaft-Brigade Siegling. The final new division of 1944, was the 31st SS Volunteer Grenadier Division, formed from conscripted Volksdeutsche, mainly from the Batschka region of Hungary.
Korsun-Cherkassy Pocket.
The Korsun-Cherkassy Pocket was formed in January 1944 when units of the 8th Army withdrew to the Panther-Wotan Line, a defensive position along the Dnieper River in Ukraine. Two army corps were left holding a salient into the Soviet lines extending some 100 km. The Red Army deployed the 1st and 2nd Ukrainian Fronts to form two armoured rings around the pocket, with an inner ring and an external ring to prevent relief formations from reaching the trapped units. Trapped in the pocket were a total of six German divisions, including the 5 SS "Wiking", with the attached 5th SS Volunteer Sturmbrigade Wallonien, and the Estonian SS Battalion "Narwa". The Germans broke out in coordination with other German forces from the outside, including the 1 SS "Leibstandarte". Roughly two out of three encircled men successfully escaped the pocket.
Raid on Drvar.
The Raid on Drvar, codenamed "Operation Rösselsprung", was an attack by the Waffen-SS and Luftwaffe on the command structure of the Yugoslav partisans. Their objective was the elimination of the partisan-controlled Supreme Headquarters and the capture of Tito. The offensive took place in April and May 1944. The Waffen-SS units involved were the 500th SS Parachute Battalion and the 7 SS "Prinz Eugen".
The assault started when a small group parachuted into Drvar to secure landing grounds for the following glider force. The 500th SS Parachute Battalion fought their way to Tito's cave headquarters and exchanged heavy gunfire resulting in numerous casualties on both sides. By the time German forces had penetrated into the cave, Tito had already escaped. At the end of the battle only 200 men of the 500th SS Parachute Battalion remained unwounded.
Baltic states.
In the Baltic states the Battle of Narva started in February. The battle can be divided into two phases: the Battle for Narva Bridgehead from February to July and the Battle of Tannenberg Line from July to September. A number of volunteer and conscript Waffen-SS units from Norway, Denmark, the Netherlands, Belgium, and Estonia fought in Narva, in what has been called by several authors the "Battle of the European SS". The units were all part of the III SS (Germanic) Panzer Corps in Army Group North, which consisted of the 11th SS Panzergrenadier Division "Nordland", the 4th SS Volunteer Panzergrenadier Brigade "Nederland", the 5th SS Volunteer Sturmbrigade "Wallonien", the 6th SS Volunteer Sturmbrigade "Langemarck", and the conscript 20th Waffen Grenadier Division of the SS (1st Estonian), under the command of Obergruppenführer Felix Steiner.
Also in Army Group North was the VI SS Corps, which consisted of the 15th Waffen Grenadier Division of the SS (1st Latvian) and the 19th Waffen Grenadier Division of the SS (2nd Latvian). Latvian Waffen SS and German army units held out in the Courland Pocket until the end of the war.
Normandy.
Operation Overlord, the Allied "D-Day" landings in Normandy, took place on 6 June 1944. In preparation for the expected landings the I SS Panzer Corps "Leibstandarte SS Adolf Hitler" was moved to Septeuil to the west of Paris in April 1944. The Corps had the 1 SS "Leibstandarte SS Adolf Hitler", 12 SS "Hitlerjugend", the 17 SS "Götz von Berlichingen" and the Army's Panzer-Lehr-Division divisions assigned to it. The corps was to form a part of General Leo Geyr von Schweppenburg's Panzer Group West, the Western theatre's armoured reserve. The Corps was restructured on 4 July 1944 and only the 1 SS "Leibstandarte" and the 12 SS "Hitlerjugend" remained at strength.
After the landings, the first Waffen-SS unit in action was the 12 SS "Hitlerjugend", which arrived at the invasion front on 7 June, in the Caen area. The same day they were involved in the Ardenne Abbey massacre. The next unit to arrive was the 17 SS "Götz von Berlichingen" on 11 June, which came into contact with the 101st Airborne Division. The SS Heavy Panzer Battalion 101 arrived next to protect the left wing of the I SS Panzer Corps. The 1 SS "Leibstandarte" arrived towards the end of the month with lead elements becoming embroiled in the British offensive Operation Epsom.
The only other Waffen-SS unit in France at this time was the 2 SS "Das Reich", in Montauban, north of Toulouse. They were ordered north to the landing beaches and on 9 June were involved in the Tulle murders, where 99 men were murdered. The next day they reached Oradour-sur-Glane and massacred 642 French civilians.
The II SS Panzer Corps consisting of the 9th SS "Hohenstaufen" and 10th SS "Frundsberg" divisions and the SS Heavy Panzer Battalion 102 was transferred from the Eastern Front to spearhead an offensive to destroy the Allied beachhead. However, the British launched Operation Epsom and the two divisions were fed piecemeal into the battle, and launched several counterattacks over the following days.
Without any further reinforcements in men or materiel, the Waffen-SS divisions were hard put to stop the Allied advance. 1 SS "Leibstandarte " and 2 SS "Das Reich" took part in the failed Operation Lüttich in early August. The end came in mid August when the German Army was encircled and trapped in the Falaise pocket, including the 1 SS "Leibstandarte", 10 SS "Frundsberg" and 12 SS "Hitlerjugend" and the 17 SS "Götz von Berlichingen", while the 2 SS "Das Reich" and the 9 SS "Hohenstaufen" were ordered to attack Hill 262 from the outside in order to keep the gap open. By 22 August the Falaise pocket had been closed, and all German forces west of the Allied lines were dead or in captivity. In the fighting around Hill 262 alone, casualties totalled 2,000 killed and 5,000 taken prisoner. The 12 SS "Hitlerjugend" had lost 94 per cent of its armour, nearly all of its artillery, and 70 per cent of its vehicles. The division had close to 20,000 men and 150 tanks before the campaign started, and was now reduced to 300 men and 10 tanks.
With the German Army in full retreat, two further Waffen-SS formations entered the battle in France, the SS Panzergrenadier Brigade 49 and the SS Panzergrenadier Brigade 51. Both had been formed in June 1944 from staff and students at the "SS-Junkerschule". They were stationed in Denmark to allow the garrison there to move into France, but were brought forward at the beginning of August to the area south and east of Paris. Both Brigades were tasked to hold crossings over the Seine River allowing the Army to retreat. Eventually they were forced back and then withdrew, the surviving troops being incorporated into the 17 SS "Götz von Berlichingen".
Greece.
While the bulk of the Waffen-SS was now on the Eastern Front or in Normandy, the 4th SS Polizei Panzergrenadier Division was stationed in Greece on internal security duties and anti-partisan operations. On 10 June they became involved in the Distomo massacre, when over a period of two hours they went door to door and massacred Greek civilians, reportedly in revenge for a Greek Resistance attack. In total, 218 men, women and children were killed. According to survivors, the SS forces "bayoneted babies in their cribs, stabbed pregnant women, and beheaded the village priest."
Italy.
On the Italian Front the 16 SS "Reichsführer-SS", conducting anti-partisan operations, is remembered more for the atrocities it committed than its fighting ability: it was involved in the Sant'Anna di Stazzema massacre in August 1944 and the Marzabotto massacre between September and October 1944.
Finland.
In Finland, the 6 SS "Nord" had held its lines during the Soviet summer offensive until it was ordered to withdraw from Finland upon the conclusion of an armistice between the Finns and the Soviets in September 1944. They then formed the rear guard for the three German corps withdrawing from Finland in Operation Birch, and from September to November 1944 marched 1,600 kilometres to Mo i Rana, Norway, where it entrained for the southern end of the country, crossing the Skagerrak to Denmark.
Arnhem and Operation Market Garden.
In early September 1944, the II SS Panzer Corps (9 SS "Hohenstaufen" and 10 SS "Frundberg") were pulled out of the line and sent to the Arnhem area in the Netherlands. Upon arrival they began the task of refitting, and the majority of the remaining armoured vehicles were loaded onto trains in preparation for transport to repair depots in Germany. On Sunday 17 September 1944 the Allies launched Operation Market Garden, and the British 1st Airborne Division was dropped in Oosterbeek, to the west of Arnhem. Realizing the threat, Wilhelm Bittrich, commander of II SS Panzer Corps, ordered "Hohenstaufen" and "Frundsberg" to ready themselves for combat. Also in the area was the Training and Reserve Battalion, 16th SS Division "Reichsführer-SS". The Allied airborne operation was a failure, and Arnhem was not liberated until 14 April 1945.
Warsaw Uprising.
At the other end of Europe, the Waffen-SS was dealing with the Warsaw Uprising. Between August and October 1944, the Dirlewanger Brigade (recruited from criminals and the mentally ill throughout Germany) and the S.S. Sturmbrigade R.O.N.A. "Russkaya Osvoboditelnaya Narodnaya Armiya (Russian National Liberation Army)" which was made up of ethnic Russian collaborators were both sent to Warsaw to put down the uprising. During the battle, the "Dirlewanger" behaved atrociously, raping, looting, and killing citizens of Warsaw regardless of whether they belonged to the Polish resistance or not; the unit commander SS-"Oberführer" Oskar Dirlewanger encouraged their excesses. The unit's behavior was reportedly so bestial and indiscriminate that Himmler was forced to send a battalion of SS military police to ensure the Dirlewanger convicts did not turn their aggressions against the leadership of the brigade or other nearby German units. At the same time they were encouraged by Himmler to terrorize freely, take no prisoners, and generally indulge their perverse tendencies. Favoured tactics of the Dirlewanger men during the siege reportedly included the ubiquitous gang rape of female Poles, both women and children; playing "bayonet catch" with live babies; and torturing captives to death by hacking off their arms, dousing them with gasoline, and setting them alight to run armless and flaming down the street. The Dirlewanger brigade committed almost non-stop atrocities during this period, in particular the four-day Wola massacre.
The other unit, "Waffen-Sturm-Brigade R.O.N.A." was tasked with clearing the Ochota district in Warsaw that was defended by members of the Polish Home Army. Their attack was planned for the morning of 5 August, but when the time came, the RONA unit could not be found; after some searching by the SS military police, members of the unit were found looting abandoned houses in the rear of the German column. Later, thousands of Polish civilians were killed by the RONA SS men during the events known as Ochota massacre; many victims were also raped. In following weeks, the RONA unit was moved south to the Wola district, but it fared no better in combat there than it did in Ochota; in one incident a sub-unit of the RONA brigade advanced to loot a captured building on the front line, but was subsequently cut off from the rest of the SS formation and wiped out by the Poles. Following the fiasco, SS-Brigadeführer Bronislav Vladislavovich Kaminski, the unit's commander, was called to Łódź to attend a SS leadership conference. He never arrived; official Nazi sources blamed Polish partisans for an alleged ambush that killed the RONA commander. But according to various other sources he was arrested and tried by the SS, or simply shot on the spot by the Gestapo. The behaviour of the "RONA" during the battle was an embarrassment even to the SS, and the alleged rape and murder of two German Strength Through Joy girls may have played a part in the eventual execution of the brigade's commander.
Vistula River line.
In late August 1944, 5 SS "Wiking" was ordered back to Modlin on the Vistula River line near Warsaw, where it was to join the newly formed Army Group Vistula. Fighting alongside the Luftwaffe's Fallschirm-Panzer Division 1 "Hermann Göring", they annihilated the Soviet 3rd Tank Corps. The advent of the Warsaw Uprising brought the Soviet offensive to a halt, and relative peace fell on the front line. The division remained in the Modlin area for the rest of the year, grouped with the 3 SS "Totenkopf" in the IV SS Panzer Corps. Heavy defensive battles around Modlin followed for the rest of the year. Together they helped force the Red Army out of Warsaw and back across the Vistula River, where the Front stabilized until January 1945.
Ardennes Offensive.
The Ardennes Offensive or "Battle of the Bulge", between 16 December 1944 and 25 January 1945, was a major German offensive through the forested Ardennes Mountains region of Belgium. The Waffen-SS units included the 6th Panzer Army under Sepp Dietrich. Created on 26 October 1944, it incorporated the I SS Panzer Corps (1 SS "Leibstandarte", the 12 SS "Hitlerjugend" and the SS Heavy Panzer Battalion 101). It also had the II SS Panzer Corps (2 SS "Das Reich" and the 9 SS "Hohenstaufen"). Another unit involved was Otto Skorzeny's SS Panzer Brigade 150.
The purpose of the attack was to split the British and American line in half, capture Antwerp, and encircle and destroy four Allied armies, forcing the Western Allies to negotiate a peace treaty on terms favorable to the Axis Powers.
The attack was ultimately a failure. It is infamous for the Malmedy massacre, in which approximately 90 unarmed American prisoners of war were murdered on 17 December 1944 by the Kampfgruppe Peiper, part of the 1 SS "Leibstandarte". Also during this battle, soldiers from 3./SS-PzAA1 LSSAH captured and shot eleven African-American soldiers from the American 333rd Artillery Battalion in the hamlet of Wereth. Their remains were found by Allied troops two months later. The soldiers had their fingers cut off and legs broken, and one was shot while trying to bandage a comrade's wounds.
Siege of Budapest.
In late December 1944, the Axis forces, including IX Waffen Mountain Corps of the SS (Croatian), defending Budapest, were encircled in the Siege of Budapest. The IV SS Panzer Corps (3 SS "Totenkopf" and 5 SS "Wiking") was ordered south to join Hermann Balck's 6th Army (Army Group "Balck"), which was mustering for a relief effort code named Operation Konrad.
As a part of Operation Konrad I, the IV SS Panzer Corps was committed to action on 1 January 1945, near Tata, with the advance columns of "Wiking" slamming into the Soviet 4th Guards Army. A heavy battle ensued, with the 5 SS "Wiking" and 3 SS "Totenkopf" destroying many of the Soviet tanks. In three days their panzer spearheads had driven 45 kilometres, over half the distance from the start point to Budapest. The Soviets maneuvered forces to block the advance, and they barely managed to halt them at Bicske, only 28 km from Budapest. Two further attacks, Operations Konrad II and III, also failed.
The Hungarian Third Army was besieged in Budapest along with the IX Waffen Mountain Corps of the SS (Croatian) (8 SS "Florian Geyer" and 22 SS " Maria Theresia"). The siege lasted from 29 December 1944 until the city surrendered unconditionally on 13 February 1945. Only 170 men of the 22 SS " Maria Theresia" made it back to the German lines.
1945.
1945 expansion.
The Waffen-SS continued to expand in 1945. January saw the 32nd SS Volunteer Grenadier Division "30 Januar" formed from the remnants of other units and staff from the SS-Junkerschules. In February the Waffen Grenadier Brigade or SS Charlemagne (1st French) was reformed as the 33rd Waffen Grenadier Division of the SS Charlemagne (1st French), and the SS Volunteer Grenadier-Brigade Landstorm Nederland was upgraded to the 34th SS Volunteer Grenadier Division "Landstorm Nederland". The second SS Police division followed when the 35th SS and Police Grenadier Division was formed from SS Police units that had been transferred to the Waffen-SS. The Dirlewanger Brigade was reformed as the 36th Waffen Grenadier Division of the SS. There was now a real shortage of Waffen-SS volunteers and conscripts, so units from the Army were attached to bring it up to strength. The third SS Cavalry division 37th SS Volunteer Cavalry Division "Lützow" was formed from the remnants of the 8 SS "Florian Geyer" and 22 SS "Maria Theresia", which had both been virtually destroyed. The last Waffen-SS division was the 38th SS Division "Nibelungen", which was also formed from students and staff from the SS-Junkerschule, but consisted of only around 6,000 men, the strength of a normal brigade.
The XV SS Cossack Cavalry Corps, which contained the 1 SS Cossack Division, was transferred to the Waffen-SS on 1 February 1945. Despite the refusal of its commander, General von Pannwitz, to enter the SS, the corps was placed under SS administration and all Cossacks became formally part of the Waffen-SS.
Operation Nordwind.
Operation Nordwind was the last major German offensive on the Western Front. It began on 1 January 1945 in Alsace and Lorraine in north-eastern France, and it ended on 25 January. The initial attack was conducted by three Corps of the 1st Army. By 15 January at least 17 German divisions (including units in the Colmar Pocket) were engaged, including the XIII SS Army Corps (17 SS "Götz von Berlichingen" and 38 SS "Nibelungen") and the 6 SS "Nord" and 10 SS "Frundsberg". At the same time, the Luftwaffe mounted a large offensive over the skies of France. Some 240 fighters were lost and just as many pilots. It was the 'last gasp' attempt for the Luftwaffe to take back air supremacy from the western allies.
Operation Solstice.
Operation Solstice, or the "Stargard Tank Battle" (February 1945) was one of the last armoured offensive operations on the Eastern Front. It was a limited counter-attack by the three corps of the Eleventh SS Panzer Army, which was being assembled in Pomerania, against the spearheads of the 1st Belorussian Front. Originally planned as a major offensive, it was executed as a more limited attack. It was repulsed by the Red Army, but helped to convince the Soviet High Command to postpone the planned attack on Berlin.
Initially the attack achieved a total surprise, reaching the banks of the Ina River and, on 17 January, Arnswalde. Strong Soviet counter-attacks halted the advance, and the operation was called off. The III (Germanic) SS Panzer Corps, was pulled back to the Stargard and Stettin on the northern Oder River.
East Pomeranian Offensive.
The East Pomeranian Offensive lasted from 24 February to 4 April, in Pomerania and West Prussia. The Waffen-SS units involved were the 11 SS "Nordland", 20 SS "Estonian", 23 SS "Nederland", 27 SS "Langemark", 28 SS "Wallonien", all in the III (Germanic) SS Panzer Corps, and the X SS Corps, which did not command any SS units.
In March 1945, the X SS Corps was encircled by the 1st Guards Tank Army, 3rd Shock Army, and the Polish 1st Army in the area of Dramburg. This pocket was destroyed by the Red Army on 7 March 1945. On 8 March 1945, the Soviets announced the capture of General Krappe and 8,000 men of the X SS Corps.
Operation Spring Awakening.
After the Ardennes offensive failed, the SS Divisions involved were pulled out and refitted in Germany in preparation for Operation Spring Awakening, with top priority for men and equipment. The replacements were a mixed group of raw recruits and drafted Luftwaffe and Kriegsmarine personnel no longer needed by their own branch of service, as they had no aircraft or ships to serve in. The 6th SS Panzer Army would again take the lead, with the I SS Panzer Corps (1 SS "Leibstandarte" and 12 SS "Hitlerjugend") and the II SS Panzer Corps (2 SS "Das Reich" and the 10 SS "Frundsberg"). Also present but not part of the 6th SS Panzer Army was the IV SS Panzer Corps (3 SS "Totenkopf" and 5 SS "Wiking"). This was the first time that six SS Panzer Divisions took part in the same offensive.
As planned, the offensive got under way on 6 March 1945, spearheaded by the 6th SS Panzer Army. The attack surprised the Soviets, and impressive gains were made for an offensive launched at such a late date in the war. However, once the Soviets realized that elite SS units were involved, they took the German offensive seriously, utilizing 16 rifle divisions, two tank corps (with some 150 tanks), and two mechanized corps, in direct support just behind the front line south west of Lake Balaton. The Soviets had been building up their forces for their own offensive along the Danube valley, which meant the 6th SS Panzer Army's attack was confronted by an overwhelming Soviet force of more than 1,000 tanks, which ground the German advance to a halt.
By 14 March the attack was in serious trouble. The advance of the 6th SS Panzer Army, while impressive, was well short of its targets. Two days later, the Soviets launched a massive counterattack, which drove the 6th SS Panzer Army back to its start line within 24 hours. All six of the Waffen-SS divisions suffered grievously during Spring Awakening, and by the end most were below 50 per cent strength without much prospect of reinforcements to replace their losses.
Armband order.
This failure is famous for the notorious "armband order" that followed. The order was issued to Sepp Dietrich by Adolf Hitler, who claimed that the troops, and more importantly, the 1 SS "Leibstandarte", "did not fight as the situation demanded." As a mark of disgrace, the "Leibstandarte" units involved in the battle were ordered to remove their treasured "Adolf Hitler" cuff titles. Dietrich was disgusted by Hitler's order and did not relay it to his troops.
Vienna Offensive.
After Operation Spring Awakening, the 6th SS Panzer Army withdrew towards Vienna and was involved in the Vienna Offensive. The only major force to face the attacking Red Army was the II SS Panzer Corps (2 SS "Das Reich" and 3 SS "Totenkopf"), under the commanded of Wilhelm Bittrich, along with "ad hoc" forces made up of garrison and anti-aircraft units. Vienna fell on 13 April. Bittrich's II SS Panzer Corps had pulled out to the west that evening to avoid encirclement.
Berlin.
The Army Group Vistula was formed in 1945 to protect Berlin from the advancing Red Army. It fought in the Battle of the Seelow Heights (16–19 April) and the Battle of Halbe (21 April – 1 May), both part of the Battle of Berlin. The Waffen-SS was represented by the III (Germanic) SS Panzer Corps.
On 16 April, the remnants of the 11 SS "Nordland", 33 SS "Charlemagne", and the Spanish Volunteer Company of SS 101 were ordered to move to the front line east of Berlin. From 17 to 20 April they were in constant combat all along the front and pushed back into the city.
On 23 April, "Brigadeführer" Wilhelm Mohnke was appointed by Hitler as Battle Commander for the centre government district (Zitadelle sector), which included the Reich Chancellery and "Führerbunker". Mohnke's command post was in the bunkers under the Reich Chancellery. He formed "Kampfgruppe Mohnke" (Battle Group Mohnke), divided into two weak regiments. It was made up of the LSSAH Flak Company, replacements from LSSAH Training and Reserve Battalion from Spreenhagan (under "Standartenfuhrer" Anhalt), 600 men from the "Begleit-Bataillon Reichsführer-SS", the Führer-Begleit-Company, and the core group—800 men of the LSSAH Guard Battalion assigned to guard the Führer.
On 25 April, "Brigadeführer" Gustav Krukenberg was appointed the commander of (Berlin) Defence Sector C. This included the "Nordland" Division, whose previous commander, Joachim Ziegler, was relieved of his command the same day. On 27 April, after a spirited but futile defence, the remnants of "Nordland" were pushed back into the centre government district (Zitadelle sector) in Defence sector Z. There Krukenberg's "Nordland" headquarters was a carriage in the Stadtmitte U-Bahn station. The men of "Nordland" were now under Mohnke's overall command. Since Mohnke's fighting force was located at the nerve centre of the German Third Reich, it fell under the heaviest artillery bombardment of the war. The bombardment began as a birthday present to Hitler on 20 April 1945 and lasted until the end of hostilities on 8 May 1945. Under pressure from the most intense shelling, the SS troops put up stiff resistance as the Red Army raced to take the Reichstag and Reich Chancellery. This condemned the SS troops to bitter and bloody street fighting. By 26 April, the "Nordland" defenders were pushed back into the Reichstag and Reich Chancellery. There over the next few days, the survivors (mainly French SS troops from the former 33 SS "Charlemagne") held out against overwhelming odds.
On 30 April, after receiving news of Hitler's suicide, orders were issued that those who could do so were to break out. Prior to the breakout Mohnke briefed all commanders that could be reached within the Zitadelle sector about Hitler's death and the planned breakout. The break out started at 2300 hours on 1 May. There were ten main groups that attempted to head northwest towards Mecklenburg. Fierce fighting continued all around, especially in the Weidendammer Bridge area. What was left of the 11 SS "Nordland" under "Brigadeführer" Krukenberg fought hard in that area, but Soviet artillery and anti-tank guns dispatched the groups. Several very small groups managed to reach the Americans at the Elbe's west bank, but most, including Mohnke's group, could not make it through the Soviet rings.
On 2 May hostilities officially ended by order of Helmuth Weidling, Kommandant of the Defense Area Berlin. News of the surrender led some of the encircled Waffen-SS men to change their minds as to suicide. Historian Thomas Fischer related the following example of the mindset of some of the men:
Mohnke's adjutant, SS-"Obersturmführer" Gert Stehr of the Fuhrer Escort Detachment, formerly ... [with] Leibstandarte SS Adolf Hitler ... shot himself before the group surrendered to the Soviets. His last words: 'Whoever has sworn an oath on the flag of the Führer, no longer has anything that belongs to him!'
Casualties.
Total casualties amongst the Waffen-SS will probably never be known, but one estimate indicates that they suffered 180,000 dead, 400,000 wounded, and 40,000 missing. World War II casualties indicates that the Waffen-SS suffered 314,000 killed and missing, or 34.9 per cent. By comparison, the United States Army suffered 318,274 killed and missing in all theatres of the war.
War crimes.
The separately organised Allgemeine SS was responsible for the administration of extermination camps. Many members of it and the SS-Totenkopfverbände subsequently became members of the Waffen-SS, forming the initial core of the "Totenkopf" Division. Many Waffen-SS members and units were responsible for war crimes against civilians and allied servicemen. After the war the SS organisation as a whole was held to be a criminal organization by the post-war German government, due to evidence that it was responsible for war crimes. Formations such as the "Dirlewanger" and "Kaminski" Brigades were singled out, and many others were involved in large-scale massacres or smaller-scale killings such as murder of 34 captured allied servicemen ordered by Josef Kieffer during Operation Bulbasket in 1944, the Houtman affair, or murders perpetrated by Heinrich Boere. The most infamous incidents include the following:
The linking of the SS-VT with the SS-Totenkopfverbände (SS-TV) in 1938 posed important questions about Waffen-SS criminality, since the SS-TV were already responsible for imprisonment, torture, and murder of Jews and other political opponents through providing the personnel for manning of the concentration camps. Their leader, Theodor Eicke, who was the commandant of Dachau, inspector of the camps, and murderer of Ernst Röhm, later became the commander of the 3 SS "Totenkopf" Division. With the invasion of Poland, the Totenkopfverbände troops were called on to carry out "police and security measures" in rear areas. What these measures involved is demonstrated by the record of "SS Totenkopf Standarte Brandenburg". It arrived in Włocławek on 22 September 1939 and embarked on a four-day "Jewish action" that included the burning of synagogues and the execution en masse of the leaders of the Jewish community. On 29 September the Standarte travelled to Bydgoszcz to conduct an "intelligentsia action". Approximately 800 Polish civilians and what the Sicherheitsdienst (SD) termed "potential resistance leaders" were killed. The Totenkopfverbände was to become one of the elite SS divisions, but from the start they were among the first executors of a policy of systematic extermination.
Several formations within the Waffen-SS were found guilty of war crimes, especially in the opening and closing phases of the war. In addition to documented atrocities, Waffen-SS units assisted in rounding up Eastern European Jews for deportation and utilised scorched earth tactics during anti-partisan operations. Some Waffen-SS personnel convalesced at concentration camps, from which they were drawn, by serving guard duties. Other members of the Waffen-SS were more directly involved in genocide.
The end of the war saw a number of war crime trials, including the Malmedy massacre trial. The counts of indictment related to the massacre of more than 300 American prisoners "in the vicinity of Malmedy, Honsfeld, Büllingen, Ligneuville, Stoumont, La Gleize, Cheneux, Petit Thier, Trois Ponts, Stavelot, Wanne, and Lutrebois", between 16 December 1944 and 13 January 1945, and the massacre of 100 Belgian civilians mainly in the vicinity of Stavelot.
During the Nuremberg Trials, the Waffen-SS was declared a criminal organisation, except conscripts from 1943 onward, who were exempted from that judgement as they had been forced to join.
HIAG.
The HIAG (German: "Hilfsgemeinschaft auf Gegenseitigkeit der Angehörigen der ehemaligen Waffen-SS", literally "Mutual Help Association of Former Waffen-SS Members") was an organization founded in 1951 by former members of the Waffen-SS to provide assistance to veterans, and campaign for the rehabilitation of their legal status with respect to veterans' pensions. Unlike soldiers of the regular Wehrmacht, pensions had been denied to members of the Waffen-SS as a result of it having been declared a criminal organization at the Nuremberg trials.
References.
Bibliography.
</dl>

</doc>
<doc id="33224" url="http://en.wikipedia.org/wiki?curid=33224" title="Politics of Western Sahara">
Politics of Western Sahara

The politics of Western Sahara take place in a framework of an area claimed by both the Sahrawi Arab Democratic Republic and the Kingdom of Morocco, which controls the majority of the territory.
Colonized by Spain from 1884 to 1975, as Spanish Sahara, the territory has been listed with the United Nations as a case of incomplete decolonization since the 60s, making it the last major territory to effectively remain a colony, according to the UN. The conflict is largely between the Kingdom of Morocco and the national liberation movement known as Polisario Front (Popular Front for the Liberation of the Saguia el-Hamra and Río de Oro), which in February 1976 formally proclaimed the Sahrawi Arab Democratic Republic (SADR), now basically administered by a government in exile in Tindouf, Algeria.
Following to the Madrid Accords, the territory was partitioned between Morocco and Mauritania in November 1975, with Morocco acquiring the northern two-thirds. Mauritania, under pressure from the POLISARIO guerrillas, abandoned all claims to its portion in August 1979, with Morocco moving to annex that sector shortly thereafter and has since asserted administrative control over the majority of the territory. A portion is administered by the SADR. The Sahrawi Arab Democratic Republic was seated as a member of the Organisation of African Unity in 1984, and was a founding member of the African Union. Guerrilla activities continued until a United Nations-monitored cease-fire was implemented September 6, 1991 via the mission MINURSO. The mission patrols the separation line between the two territories.
In 2003, the UN's envoy to the territory, James Baker, presented the Baker Plan, known as Baker II which would have given Western Sahara immediate autonomy as the Western Sahara Authority during a five-year transition period to prepare for a referendum, offering the inhabitants of the territory a choice between independence, autonomy within the Kingdom of Morocco, or complete integration with Morocco. POLISARIO has accepted the plan, but Morocco has rejected it. Previously in 2001, Baker had presented his framework plan, called Baker I, where the dispute would be finally solved through an autonomy within Moroccan sovereignty, but Algeria and the Polisario Front refused it. Algeria had proposed the partition of the territory instead.
Suffrage.
The population under Moroccan control participates in countrywide and regional Moroccan elections. A referendum on independence or integration with Morocco was agreed upon by Morocco and the Polisario Front in 1991, but it has yet to take place.
The population under SADR control and in the Sahrawi refugee camps of Tindouf, Algeria, participates in elections to the Sahrawi Arab Democratic Republic.

</doc>
<doc id="33227" url="http://en.wikipedia.org/wiki?curid=33227" title="Transport in Western Sahara">
Transport in Western Sahara

Transport in Western Sahara is very limited, mostly by sea, road and air.
Rail.
Western Sahara has no freight or passenger service railways, with the exception of a 5 km section of the Mauritania Railway; which (since the closure of the Choum Tunnel), cuts across the extreme south-eastern corner of the territory. 
Roads.
There are only 6200 km of roads, of which 1350 km are metalled.
A small network of highways provide limited ground travel connections. N1 highway is a major roadway traversing along the Atlantic coastline of the country. There are a few roads in the north and only two roads in the south that branches off of N1.
All other roads are local ones in the various cities and towns.
Ports.
Ports include:
The longest conveyor belt in the world is 100 km long, from the phosphate mines of Bu Craa to the coast south of Laayoune.
Airports.
There are 6 airfields, 3 with paved runways and 3 unpaved surfaces, and one helipad (military in Cape Bojador). Hassan I Airport is an international airport, but the carriers at the airport connect only to regional destinations (to Morocco or the Canary Islands).
Buses.
There are only 4 companies licensed to use buses in Western Sahara which are: CTM, Supratours, Satas and Sat
CTM and Supratours buses have daily service from Dakhla to Marrakech via Laayoune and Agadir

</doc>
<doc id="33229" url="http://en.wikipedia.org/wiki?curid=33229" title="Political status of Western Sahara">
Political status of Western Sahara

Western Sahara, formerly the Spanish colony of Spanish Sahara, is a disputed territory claimed by both the Kingdom of Morocco and the Polisario Front. It is listed by the United Nations (UN), as a non-decolonized territory and is thus included in the United Nations List of Non-Self-Governing Territories.
Since the Madrid Accords of 1975, a part of Western Sahara has been administered by Morocco as the Southern Provinces. Another section, the Liberated Territories, is administered by the Polisario Front as the Sahrawi Arab Democratic Republic (SADR). Mauritania administers temporally the western half of the Ras Nouadhibou Peninsula. A UN-monitored cease-fire has been in effect since September 1991.
In order to resolve the sovereignty issue, the UN has attempted to hold a referendum through the mission United Nations Mission for the Referendum in Western Sahara (MINURSO), and is holding direct talks between the Kingdom of Morocco and the Polisario Front. The UN recognizes neither Moroccan nor SADR sovereignty over Western Sahara.
Positions of the main parties.
Kingdom of Morocco.
The official position of the Kingdom of Morocco since 1963 is that all of Western Sahara is an integral part of the kingdom. The Moroccan government refers to Western Sahara only as the "Sahara", "Moroccan Sahara", "Saharan provinces", or the "Southern Provinces".
According to the Moroccan government, in 1958 the Moroccan Army of Liberation fought Spanish colonizers and almost liberated what was then Spanish Sahara. The fathers of many of the Polisario leaders were among the veterans of the Moroccan Southern Army, for example the father of Polisario leader Mohammed Abdelaziz. Morocco is supported in this view by a number of former Polisario founders and leaders. The Polisario Front is considered by Morocco to be a Moroccan separatist movement, referring to the Moroccan origins of most of its founding members, and its self-proclaimed SADR to be a puppet state used by Algeria to fight a proxy war against Morocco.
Polisario Front / self-proclaimed Sahrawi Arab Democratic Republic.
The Polisario Front is described by itself and its supporters as a national liberation movement that opposes Moroccan control of Western Sahara, whilst it is considered by Morocco and supporters of Morocco's claims over the Western Sahara to be a separatist organisation. It began as a movement of students who felt torn between the divergent Spanish and Moroccan influences on the country. The original goal of the Polisario, which was to end Spanish colonialism in the region, was achieved, but their neighbours, Morocco and Mauritania, seized sovereignty of the region, which the Polisario felt belonged to it. The Polisario engaged in guerrilla warfare with the Moroccan and Mauritanian forces. It evacuated the Sahrawi population to the Tindouf refugee camps due to Royal Moroccan Air Force bombing of the refugee camps on Sahrawi land with napalm and white phosphorus.
The Polisario Front has called for the self-determination of the people of Western Sahara to be decided through a referendum. Although the SADR is not recognised as a state by the UN, the Polisario is considered a direct participant in the conflict and as the legitimate representative of the Sahrawi people, recognized by the United Nations since 1979.
The Polisario Front argues that Morocco's position is due to economical interests (fishing, phosphate mining, and the potential for oil reserves) and political reasons (stability of the king's position and the governing elite in Morocco, deployment of most of the Moroccan Army in Western Sahara instead of in Morocco). The Polisario Front proclaimed the Sahrawi Arab Democratic Republic in Bir Lehlou (Western Sahara), on 27 February 1976.
Mauritania.
Claims on Western Sahara had proliferated since the 1960s, fuelled by Mauritanian President Moktar Ould Daddah. Before Mauritania signed the Madrid Accords and after the withdrawal of the last Spanish forces, in late 1975, the Mauritanian Army invaded the southern part of Western Sahara, while the Moroccan Army did the same in the north. In April 1976, Mauritania and Morocco partitioned the country into three parts, Mauritania getting the southern one, which was named Tiris al-Gharbiyya. Mauritania waged four years of war against Polisario guerrillas, conducting raids on Nouakchott, attacks on the Zouerate mine train and a coup d'état that deposed Ould Daddah. Mauritania finally withdrew in the summer of 1979, after signing the Argel Accord with the Polisario Front, recognizing the right of self-determination for the Sahrawi people, and renouncing any claims on Western Sahara. The Moroccan Army immediately took control of the former Mauritanian territory. Mauritania recognized the Sahrawi Arab Democratic Republic on 27 February 1984.
Algeria.
Algeria has supported the independence of Western Sahara diplomatically since 1975 because of its own national liberation war. In 1976, Algeria got involved directly in the conflict, but after a military confrontation at Amgala against the Moroccan Army, Algeria's role became indirect, through political and military support for the Polisario Front. Morocco argues that the Algerian position is due to the 1963 Sand War, in which the two countries clashed. Algeria recognized the Sahrawi Arab Democratic Republic on 6 March 1976.
United Nations.
Western Sahara was first placed, by Moroccan demand, on the UN list of territories to be decolonized in 1965, when it was still a Spanish colony. It has retained that status due to the persistence of the conflict. The UN has been involved since 1988 in trying to find a solution to the conflict through self-determination. In 1988, the Kingdom of Morocco and the Polisario Front agreed to settle the dispute through a referendum under the auspices of the UN that would allow the people of Western Sahara to choose between independence or integration with Morocco. In 1991, a ceasefire was agreed upon between the parties, contingent on the referendum being held the following year. Due to disputes over voter qualification, the vote has still not been held, and Morocco has made it clear in 2000 that henceforth it will not consider any option leading to the independence of the territory, and instead, is now proposing autonomy within Morocco. Lately, the UN has argued for negotiations between Morocco and the Polisario Front to resolve the deadlock, culminating in the Manhasset negotiations.
Although Morocco claims that no recognition is required, Moroccan sovereignty over the territory is supported by the Arab League and by some other states as a policy of deliberate ambiguity.
Positions of other states.
The following lists contain the following states and entities:
Some states are on both lists; for example, when a state is supportive of the "right of self-determination", including the option of autonomy under Morocco sovereignty. Some states have changed their opinion frequently or have given separate announcements of support for both Morocco and the Polisario Front/SADR (Paraguay, Belgium, Benin, Botswana, Burundi, Chile, Dominican Republic, Guatemala, Guinea-Bissau, Malawi, Peru, Russia, Sierra Leone, Swaziland).
Some of the states announcing support of the "right of self-determination" currently recognize the Sahrawi Arab Democratic Republic. Not all of the states that have terminated diplomatic relations with or withdrawn recognition of the SADR have announced their support for the Moroccan claims.
Some states have not announced any position as of 2014.
States supporting Sahrawi claims on Western Sahara.
In addition to the states that recognize the Sahrawi Arab Democratic Republic as a sovereign state, some other states, although not recognizing the SADR for various reasons, have expressed explicit support for the Polisario Front's position on the right to self-determination of the Sahrawi people. Some states that do not recognize the Sahrawi Republic (neither as a state, nor as a government-in-exile) recognize the Polisario Front as the legitimate representative of the Sahrawi people.
States supporting Moroccan claims on Western Sahara.
No state formally recognizes the sovereignty of Morocco over Western Sahara, although many states have expressed support for Moroccan claims, or endorsed the Moroccan autonomy initiative for the territory. Several states have withdrawn their recognition of the SADR or canceled their relations with the SADR (close to 30 countries). A number of African countries and Caribbean or Pacific island-states have taken such action following Moroccan lobbying and offers of economic assistance, academic exchanges or other support.
However, some international organizations, such as the Arab League and the Organisation of Islamic Cooperation, have recognized the sovereignty of Morocco over what it calls its Southern Provinces.
Some UN member states have expressed explicit support of Morocco's territorial integrity in reference to Western Sahara as Moroccan provinces, while others have endorsed or complemented the Moroccan proposal of autonomy for the territory as being credible and serious. No country, apart from Morocco itself, formally has recognized Moroccan sovereignty over Western Sahara. Morocco's historical allies, France and the United States, have provided the kingdom with economic, diplomatic, and military aid, throughout the conflict. Saudi Arabia provided much economic aid as well.
Morocco and the PRC maintain close relations, that have been improving steadily in the past decades. China has shown strong support for Morocco's territorial integrity, and has welcomed the proposals put forth by Morocco in the peace negotiations.
France enjoys close relations with Morocco. It is the kingdom's leading trade partner and the leading source of public development aid and private investments. The country claims neutrality on the Western Sahara issue, despite its military involvement in the Western Sahara War on the side of Morocco and Mauritania (see Operation Lamantin). In 2009 and 2010, France used the threat of its veto power to block the establishment of Human Rights monitoring by the MINURSO in Western Sahara. France has been a major backer of the Moroccan autonomy plan and in the EU negotiated the concession of the advanced status to Morocco.
Former President Bill Clinton set a precedent on the conflict over Western Sahara which his successor, George W. Bush, followed. Both presidents Clinton and Bush sided with Morocco and maintained the position that "Genuine autonomy under Moroccan sovereignty [is] the only feasible solution."
The Obama administration disassociated itself from Moroccan autonomy in 2009, however, reversing the Bush-backed support of the Moroccan plan, and returning to a pre-Bush position, wherein the option of an independent Western Sahara is on the table again.
In April 2009, 229 members of the U.S. House of Representatives, a clear majority and more than 50 more than the number who signed the letter in 2007, called on President Obama to support Morocco's autonomy plan and to assist in drawing the conflict to a close. The signers included Democratic Majority Leader Steny Hoyer and Republican Minority Leader John Boehner. In addition to acknowledging that Western Sahara has become a recruiting post for radical Islamists, the letter affirmed that the conflict is "the single greatest obstacle impending the security and cooperation necessary to combat" terrorism in the Maghreb. The letter referenced UN Security Council Resolution 1813 (2008), and encouraged President Obama to follow the policy set by President Clinton and followed by President Bush. The congressmen expressed concerns about Western Sahara's viability. They referenced a UN fact-finding mission to Western Sahara which confirmed the State Department's view that the Polisario proposal, which ultimately stands for independence, would lead to a non-viable state. In closing, the letter stated, "We remain convinced that the U.S. position, favoring autonomy for Western Sahara under Moroccan sovereignty is the only feasible solution. We urge you to both sustain this longstanding policy, and to make clear, in both words and actions, that the United States will work to ensure that the UN process continues to support this framework as the only realistic compromise that can bring this unfortunate and longstanding conflict to an end."
Commenting on a 2004 free trade agreement with Morocco, US Trade Representative Robert Zoellick stated in a letter to Congressman Joe Pitts in response to his questioning, "the United States and many other countries do not recognize Moroccan sovereignty over Western Sahara and have consistently urged the parties to work with the United Nations to resolve the conflict by peaceful means. The Free Trade Agreement will not include Western Sahara."
In April 2013, the United States proposed that MINURSO monitored human rights (as all the other UN mission since 1991) in Western Sahara and the Sahrawi refugee camps in south-western Algeria, a move that Morocco strongly opposed, cancelling the annual African Lion military exercises with US Army troops. Also in mid-April, United States Ambassador to Morocco Samuel L. Kaplan declared during a conference in Casablanca that the Moroccan autonomy plan "can't be the only basis in these negotiations", referring to the UN sponsored talks between the Polisario Front and Morocco.
States which have not announced any position.
The following states and entities have not announced any position:
Positions of international organizations.
The SADR is also a member of the Asian-African Strategic Partnership, formed at the 2005 Asian-African Conference, over Moroccan objections to SADR participation.
In 2006, the SADR participated in a conference of the Permanent Conference of Political Parties of the Latin American and the Caribbean.
In 2010, the SADR ambassador to Nicaragua participated in the opening conference of the Central American Parliament
In 22 February 1982, the SADR secured membership in the Organisation of African Unity.
The African Union (formerly the OAU) has given the Sahrawi Arab Democratic Republic full recognition, and accepted it as a member state (which has led Morocco to leave the union.). Mohamed Abdelaziz, president of the SADR, has been vicepresident of the OUA in 1985, and of the AU in 2002.
The European Union supports the right of self-determination of the Sahrawi people (the MINURSO UN-sponsored referendum), but does not recognize the Polisario Front. Over practical issues such as fishing in the EEZ the EU deals with Morocco as the country currently exercising "jurisdiction, but not sovereignty" over the Western Sahara territory. In addition, members of the EFTA trade bloc have made statements excluding the Western Sahara from the Moroccan-EFTA free trade agreement.
Since 1966, the United Nations request for the celebration of a referendum for enabling the "indigenous population" to exercise freely their right to self-determination. Since 1979, the United Nations has recognized the Polisario Front as the representative of the people of Western Sahara, and considered Morocco as an occupying force.
Former United Nations Secretary-General Kofi Annan stressed, in his last report on Western Sahara, to the Security Council:

</doc>
<doc id="33508" url="http://en.wikipedia.org/wiki?curid=33508" title="The Dartmoor Worker">
The Dartmoor Worker

The Dartmoor Worker is a collection, first assembled in 1966, of newspaper articles originally written for The Western Morning News by the principal authority on Dartmoor and its history, William Crossing, in the early 1900s. The book is illustrated with many photographs from the collection of Lady Sayer, a highly influential member of the Dartmoor Preservation Association during the mid 20th century.
The book details the activities of a number of workers on the moor, such as the farmers, the dry-stone wall builders, the peat-cutters, the warreners, and miners.

</doc>
<doc id="33509" url="http://en.wikipedia.org/wiki?curid=33509" title="Walking">
Walking

Walking (also known as ambulation) is one of the main gaits of locomotion among legged animals, and is typically slower than running and other gaits. Walking is defined by an 'inverted pendulum' gait in which the body vaults over the stiff limb or limbs with each step. This applies regardless of the number of limbs - even arthropods, with six, eight or more limbs, walk.
In the United Kingdom and Ireland, the term walking is used to describe both walking in a park or trekking in the Alps. However, in Canada and the United States the term for a long, vigorous walk is hiking, while the word walking covers shorter walks, especially in an urban setting.
Difference from running.
The word "walk" is descended from the Old English "wealcan" "to roll". In humans and other bipeds, walking is generally distinguished from running in that only one foot at a time leaves contact with the ground and there is a period of double-support. In contrast, running begins when both feet are off the ground with each step. This distinction has the status of a formal requirement in competitive walking events. For quadrupedal species, there are numerous gaits which may be termed walking or running, and distinctions based upon the presence or absence of a suspended phase or the number of feet in contact any time do not yield mechanically correct classification. The most effective method to distinguish walking from running is to measure the height of a person's centre of mass using motion capture or a force plate at midstance. During walking, the centre of mass reaches a maximum height at midstance while during running, it is at a minimum. This distinction, however, only holds true for locomotion over level or approximately level walking. For walking up grades larger than nine percent this distinction no longer holds for some individuals. Definitions based on the percentage of the stride during which a foot is in contact with the ground (averaged across all feet) of greater than 50% contact corresponds well with identification of 'inverted pendulum' mechanics and are indicative of walking for animals with any number of limbs, although this definition is incomplete. Running humans and animals may have contact periods greater than 50% of a gait cycle when rounding corners, running uphill or carrying loads.
Speed is another factor that distinguishes walking from running. Although walking speeds can vary greatly depending on a multitude of factors such as height, weight, age, terrain, surface, load, culture, effort, and fitness, the average human walking speed is about 5.0 kilometres per hour (km/h), or about 3.1 miles per hour (mph). Specific studies have found pedestrian walking speeds ranging from 4.51 km/h to 4.75 km/h for older individuals and from 5.32 km/h to 5.43 km/h for younger individuals; a brisk walking speed can be around 6.5 km/h. Champion racewalkers can average more than 14 km/h over a distance of 20 km. An average human child achieves independent walking ability at around 11 months old.
Health benefits of walking.
Regular, brisk exercise of any kind can improve confidence, stamina, energy, weight control and life expectancy and reduce stress. It can also reduce the risk of coronary heart disease, strokes, diabetes, high blood pressure, bowel cancer and osteoporosis. Scientific studies have also shown that walking, besides its physical benefits, is also beneficial for the mind, improving memory skills, learning ability, concentration and abstract reasoning, as well as reducing stress and lifting spirits. Sustained walking sessions for a minimum period of thirty to sixty minutes a day, five days a week, with the correct walking posture, reduce health risks and have various overall health benefits, such as reducing the chances of cancer, type 2 diabetes, heart disease, anxiety and depression. Life expectancy is also increased even for individuals suffering from obesity or high blood pressure. Walking also improves bone health, especially strengthening the hip bone, and lowering the more harmful low-density lipoprotein (LDL) cholesterol, and raising the more useful good high-density lipoprotein (HDL) cholesterol. Studies have found that walking may also help prevent dementia and Alzheimer's.
The Centers for Disease Control and Prevention's fact sheet on the "Relationship of Walking to Mortality Among U.S. Adults with Diabetes" states that those with diabetes who walked for 2 or more hours a week lowered their mortality rate from all causes by 39 per cent. "Walking lengthened the life of people with diabetes regardless of age, sex, race, body mass index, length of time since diagnosis, and presence of complications or functional limitations." It has been suggested that there is a relationship between the speed of walking and health, and that the best results are obtained with a speed of more than 2.5 mph (4 km/h).
Governments now recognize the benefits of walking for mental and physical health and are actively encouraging it. This growing emphasis on walking has arisen because people walk less nowadays than previously. In the UK; a Department of Transport report found that between 1995/97 and 2005 the average number of walk trips per person fell by 16%, from 292 to 245 per year. Many professionals in local authorities and the NHS are employed to halt this decline by ensuring that the built environment allows people to walk and that there are walking opportunities available to them. Professionals working to encourage walking come mainly from six sectors: health, transport, environment, schools, sport and recreation, and urban design.
One programme to encourage walking is "The Walking the Way to Health Initiative", organized by the British walkers association The Ramblers, which is the largest volunteer led walking scheme in the United Kingdom. Volunteers are trained to lead free Health Walks from community venues such as libraries and doctors' surgeries. The scheme has trained over 35,000 volunteers and have over 500 schemes operating across the UK, with thousands of people walking every week. A new organization called "Walk England" launched a web site in June 2008 to provide these professionals with evidence, advice and examples of success stories of how to encourage communities to walk more. The site has a social networking aspect to allow professionals and the public to ask questions, post news and events and communicate with others in their area about walking, as well as a "walk now" option to find out what walks are available in each region. Similar organizations exist in other countries and recently a "Walking Summit" was held in the USA. This "assembl[ed] thought-leaders and influencers from business, urban planning and real estate, [along with] physicians and public health officials," and others, to discuss how to make American cities and communities places where "people can and want to walk".
Origins.
It is theorized that "walking" among tetrapods originated underwater with air-breathing fish that could "walk" underwater, giving rise to the plethora of land-dwelling life that walk on four or two limbs. While terrestrial tetrapods are theorised to have a single origin, arthropods and their relatives are thought to have independently evolved walking several times, specifically in insects, myriapods, chelicerates, tardigrades, onychophorans, and crustaceans.
Judging from footprints discovered on a former shore in Kenya, it is thought possible that ancestors of modern humans were walking in ways very similar to the present activity as many as 1.5 million years ago.
Biomechanics.
Human walking is accomplished with a strategy called the double pendulum. During forward motion, the leg that leaves the ground swings forward from the hip. This sweep is the first pendulum. Then the leg strikes the ground with the heel and rolls through to the toe in a motion described as an inverted pendulum. The motion of the two legs is coordinated so that one foot or the other is always in contact with the ground. The process of walking recovers approximately sixty per cent of the energy used due to pendulum dynamics and ground reaction force.
Walking differs from a running gait in a number of ways. The most obvious is that during walking one leg always stays on the ground while the other is swinging. In running there is typically a ballistic phase where the runner is airborne with both feet in the air (for bipedals).
Another difference concerns the movement of the centre of mass of the body. In walking the body "vaults" over the leg on the ground, raising the centre of mass to its highest point as the leg passes the vertical, and dropping it to the lowest as the legs are spread apart. Essentially kinetic energy of forward motion is constantly being traded for a rise in potential energy. This is reversed in running where the centre of mass is at its lowest as the leg is vertical. This is because the impact of landing from the ballistic phase is absorbed by bending the leg and consequently storing energy in muscles and tendons. In running there is a conversion between kinetic, potential, and elastic energy.
There is an absolute limit on an individual's speed of walking (without special techniques such as those employed in speed walking) due to the upwards acceleration of the centre of mass during a stride - if it's greater than the acceleration due to gravity the person will become airborne as they vault over the leg on the ground. Typically however, animals switch to a run at a lower speed than this due to energy efficiencies.
A leisure activity.
Many people enjoy walk as a recreation in the mainly urban modern world, and it is one of the best forms of exercise. For some walking is a way to enjoy nature and the outdoors and for others the physical, sporting and endurance aspect is more important.
There are a variety of different kinds of walking, including bushwalking, racewalking, hillwalking, volksmarching, Nordic walking, trekking, and hiking. Some people prefer to walk indoors on a treadmill, or in a gym, and fitness walkers and others may use a pedometer to count the number of their steps. Hiking is the usual word used in Canada, the USA and South Africa for long vigorous walks, similar walks are called tramps in New Zealand, or hill walking or just walking in Australia, the UK and the Irish Republic. Australians also bushwalk. In English North America the term walking is used for short walks, especially in towns and cities. Snow shoeing is walk in snow, though a slightly different gait is required compared with regular walking.
In terms of tourism the possibilities range from guided walking tours in cities, to organized trekking holidays in the Himalayas. In the UK the term walking tour also refers to a multi-day walk or hike undertaken by a group or individual. A system of well organized trails exist in many other European counties, as well Canada, USA, New Zealand, and Nepal. A system of lengthy waymarked walking trails now stretch across Europe from Norway to Turkey, Portugal to Cyprus. Many also walk the traditional pilgrim routes, of which the most famous is El Camino de Santiago, The Way of St. James.
Numerous walking festivals and other walking events take place each year all over the world. The world's largest walking event is the International Four Days Marches Nijmegen in the Netherland. The "Vierdaagse" (Dutch for "Four day Event") is an annual walk that has taken place since 1909, being based at Nijmegen since 1916. Depending on age group and category, walkers have to walk 30, 40 or 50 kilometers each day for four days. Originally a military event with a few civilians, it now is a mainly civilian event. Numbers have risen in recent years, with over 40,000 now taking part, including about 5,000 military personnel. It is now the world's largest walking event. Due to crowds on the route, since 2004 the organizers have limited the number of participants. In the USA there is the annual Labor Day walk on Mackinac Bridge, Michigan, which draws over sixty thousand participants, while the Chesapeake Bay Bridge Walk in Maryland annually draws over fifty thousand participants. There are also various walks are that are organised as charity events with walkers sponsored for a specific cause. Thes walks range in length from two mile (3 km) or five km to fifty miles (eighty km). The MS Challenge Walk is a 80 km or fifty mile walk which raises money to fight multiple sclerosis, while walkers in the Oxfam Trailwalker cover one hundred km event or 60 miles.
In Britain, The Ramblers, a registered charity, is the largest organisation that looks after the interests of walkers, with some 139,000 members. Its "Get Walking Keep Walking" project provides free routes guides, led walks, as well information for people new to walking. The Long Distance Walkers Association is walking organisation in the UK for the more energetic walker, and organizes lengthy challenge hikes of 20 or even fifty or more miles (30 to 80 km) in a day. The LDWA's annual "Hundred" event, entailing walking 100 miles or 160 km in 48 hours, takes place each British Spring Bank Holiday weekend.
Walkability.
There has been a recent focus among urban planners in some communities to create pedestrian-friendly areas and roads, allowing commuting, shopping and recreation to be done on foot. The concept of walkability has arisen as a measure of the degree to which an area is friendly to walking. Some communities are at least partially car-free, making them particularly supportive of walking and other modes of transportation. In the United States, the active living network is an example of a concerted effort to develop communities more friendly to walking and other physical activities.
An example of such efforts to make urban development more pedestrian friendly is the pedestrian village. This is a compact, pedestrian-oriented neighborhood or town, with a mixed-use village center, that follows the tenets of New Pedestrianism. Shared-use lanes for pedestrians and those using bicycles, Segways, wheelchairs, and other small rolling conveyances that do not use internal combustion engines. Generally, these lanes are in front of the houses and businesses, and streets for motor vehicles are always at the rear. Some pedestrian villages might be nearly car-free with cars either hidden below the buildings or on the periphery of the village. Venice, Italy is essentially a pedestrian village with canals.The canal district in Venice, California, on the other hand, combines the front lane/rear street approach with canals and walkways, or just walkways.
Walking is also considered to be a clear example of a sustainable mode of transport, especially suited for urban use and/or relatively shorter distances. Non-motorised transport modes such as walking, but also cycling, small-wheeled transport (skates, skateboards, push scooters and hand carts) or wheelchair travel are often key elements of successfully encouraging clean urban transport. A large variety of case studies and good practices (from European cities and some worldwide examples) that promote and stimulate walking as a means of transportation in cities can be found at Eltis, Europe's portal for local transport.
The development of specific rights of way with appropriate infrastructure can promote increased participation and enjoyment of walking. Examples of types of investment include pedestrian malls, and foreshoreways such as oceanways and also river walks.
The first purpose-built pedestrian street in Europe is the Lijnbaan in Rotterdam, opened in 1953. The first pedestrianised shopping centre in the United Kingdom was in Stevenage in 1959. A large number of European towns and cities have made part of their centres car-free since the early 1960s. These are often accompanied by car parks on the edge of the pedestrianised zone, and, in the larger cases, park and ride schemes. Central Copenhagen is one of the largest and oldest: It was converted from car traffic into pedestrian zone in 1962.
In robotics.
The first successful attempts at walking robots tended to have six legs. The number of legs was reduced as microprocessor technology advanced, and there are now a number of robots that can walk on two legs. One, for example, is ASIMO. Although robots have taken great strides in advancement, they still don't walk nearly as well as human beings as they often need to keep their knees bent permanently in order to improve stability.
In 2009, Japanese roboticist Tomotaka Takahashi developed a robot that can jump three inches off the ground. The robot, named Ropid, is capable of getting up, walking, running, and jumping.
Animals.
Horses.
The walk is a four-beat gait that averages about 4 mph. When walking, a horse's legs follow this sequence: left hind leg, left front leg, right hind leg, right front leg, in a regular 1-2-3-4 beat. At the walk, the horse will always have one foot raised and the other three feet on the ground, save for a brief moment when weight is being transferred from one foot to another. A horse moves its head and neck in a slight up and down motion that helps maintain balance.
Ideally, the advancing rear hoof oversteps the spot where the previously advancing front hoof touched the ground. The more the rear hoof oversteps, the smoother and more comfortable the walk becomes. Individual horses and different breeds vary in the smoothness of their walk. However, a rider will almost always feel some degree of gentle side-to-side motion in the horse's hips as each hind leg reaches forward.
The fastest "walks" with a four-beat footfall pattern are actually the lateral forms of ambling gaits such as the running walk, singlefoot, and similar rapid but smooth intermediate speed gaits. If a horse begins to speed up and lose a regular four-beat cadence to its gait, the horse is no longer walking, but is beginning to either trot or pace.
Elephants.
Elephants can move both forwards and backwards, but cannot trot, jump, or gallop. They use only two gaits when moving on land, the walk and a faster gait similar to running. In walking, the legs act as pendulums, with the hips and shoulders rising and falling while the foot is planted on the ground. With no "aerial phase", the fast gait does not meet all the criteria of running, although the elephant uses its legs much like other running animals, with the hips and shoulders falling and then rising while the feet are on the ground. Fast-moving elephants appear to 'run' with their front legs, but 'walk' with their hind legs and can reach a top speed of 18 km/h. At this speed, most other quadrupeds are well into a gallop, even accounting for leg length.
Walking fish.
Walking fish, sometimes called ambulatory fish, is a general term that refers to fish that are able to travel over land for extended periods of time. The term may also be used for some other cases of nonstandard fish locomotion, e.g., when describing fish "walking" along the sea floor, as the handfish or frogfish.

</doc>
<doc id="33521" url="http://en.wikipedia.org/wiki?curid=33521" title="William McKinley">
William McKinley

William McKinley (January 29, 1843 – September 14, 1901) was the 25th President of the United States, serving from March 4, 1897, until his assassination in September 1901, six months into his second term. McKinley led the nation to victory in the Spanish–American War, raised protective tariffs to promote American industry, and maintained the nation on the gold standard in a rejection of inflationary proposals.
McKinley was the last president to have served in the American Civil War, beginning as a private in the Union Army and ending as a brevet major. After the war, he settled in Canton, Ohio, where he practiced law and married Ida Saxton. In 1876, he was elected to Congress, where he became the Republican Party’s expert on the protective tariff, which he promised would bring prosperity. His 1890 McKinley Tariff was highly controversial; which together with a Democratic redistricting aimed at gerrymandering him out of office, led to his defeat in the Democratic landslide of 1890. He was elected Ohio's governor in 1891 and 1893, steering a moderate course between capital and labor interests. With the aid of his close adviser Mark Hanna, he secured the Republican nomination for president in 1896, amid a deep economic depression. He defeated his Democratic rival, William Jennings Bryan, after a front-porch campaign in which he advocated "sound money" (the gold standard unless altered by international agreement) and promised that high tariffs would restore prosperity.
Rapid economic growth marked McKinley's presidency. He promoted the 1897 Dingley Tariff to protect manufacturers and factory workers from foreign competition, and in 1900, he secured the passage of the Gold Standard Act. McKinley hoped to persuade Spain to grant independence to rebellious Cuba without conflict, but when negotiation failed, he led the nation in the Spanish–American War of 1898; the U.S. victory was quick and decisive. As part of the peace settlement, Spain turned over to the United States its main overseas colonies of Puerto Rico, Guam, and the Philippines; Cuba was promised independence, but at that time remained under the control of the U.S. Army. The United States annexed the independent Republic of Hawaii in 1898 and it became a U.S. territory.
Historians regard McKinley's 1896 victory as a realigning election, in which the political stalemate of the post-Civil War era gave way to the Republican-dominated Fourth Party System, which began with the Progressive Era. McKinley defeated Bryan again in the 1900 presidential election, in a campaign focused on imperialism, protectionism, and free silver. However, his legacy was quickly cut short when a successful assassination was carried out on September 6, 1901 by Leon Czolgosz, a second-generation Polish-American with anarchist leanings, and he was succeeded by Vice President Theodore Roosevelt. As an innovator of American interventionism and pro-business sentiment, McKinley's presidency is generally considered above average, though his universally positive public perception was soon overshadowed by Roosevelt.
Early life and family.
William McKinley Jr., was born in 1843 in Niles, Ohio, the seventh child of William and Nancy (née Allison) McKinley. The McKinleys were of English and Scots-Irish descent and had settled in western Pennsylvania in the 18th century. There, the elder McKinley was born in Pine Township.
The family moved to Ohio when the senior McKinley was a boy, settling in New Lisbon (now Lisbon). He met Nancy Allison there in 1829, and married her the same year. The Allison family was of mostly English blood and among Pennsylvania’s earliest settlers. The family trade on both sides was iron-making, and McKinley senior operated foundries throughout Ohio, in New Lisbon, Niles, Poland, and finally Canton.
The McKinley household was, like many from Ohio's Western Reserve, steeped in Whiggish and abolitionist sentiment, the latter based on the family's staunch Methodist beliefs. William followed in the Methodist tradition, becoming active in the local Methodist church at the age of sixteen.
He was a lifelong pious Methodist. In 1852, the family moved from Niles to Poland so that their children could attend the better school there. Graduating in 1859, he enrolled the following year at Allegheny College in Meadville, Pennsylvania. He remained at Allegheny for only one year, returning home in 1860 after becoming ill and depressed. He also spent time at Mount Union College in Alliance, Ohio where he joined Sigma Alpha Epsilon. He did not graduate from either university. Although his health recovered, family finances declined and McKinley was unable to return to Allegheny, first working as a postal clerk and later taking a job teaching at a school near Poland. 
Civil War.
Western Virginia and Antietam.
When the Southern states seceded from the Union and the American Civil War began, thousands of men in Ohio volunteered for service. Among them were McKinley and his cousin William McKinley Osbourne, who enlisted as privates in the newly formed Poland Guards in July 1861. The men left for Columbus where they were consolidated with other small units to form the 23rd Ohio Infantry. The men were unhappy to learn that, unlike Ohio’s earlier volunteer regiments, they would not be permitted to elect their officers; they would be designated by Ohio’s governor, William Dennison. Dennison appointed Colonel William Rosecrans as the commander of the regiment, and the men began training on the outskirts of Columbus. McKinley quickly took to the soldier’s life and wrote a series of letters to his hometown newspaper extolling the army and the Union cause. Delays in issuance of uniforms and weapons again brought the men into conflict with their officers, but Major Rutherford B. Hayes convinced them to accept what the government had issued them; his style in dealing with the men impressed McKinley, beginning an association and friendship that would last until Hayes’ death in 1893.
After a month of training, McKinley and the 23rd Ohio, now led by Colonel Eliakim P. Scammon, set out for western Virginia (today part of West Virginia) in June 1861 as a part of the Kanawha Division. McKinley initially thought Scammon was a martinet, but when the regiment finally saw battle, he came to appreciate the value of their relentless drilling. Their first contact with the enemy came in September when they drove back Confederate troops at Carnifex Ferry in present-day West Virginia. Three days after the battle, McKinley was assigned to duty in the brigade quartermaster office, where he worked both to supply his regiment, and as a clerk. In November, the regiment established winter quarters near Fayetteville (today in West Virginia). McKinley spent the winter substituting for a commissary sergeant who was ill, and in April 1862 he was promoted to that rank. The regiment resumed its advance that spring with Hayes in command (Scammon by then led the brigade) and fought several minor engagements against the rebel forces.
That September, McKinley’s regiment was called east to reinforce General John Pope’s Army of Virginia at the Second Battle of Bull Run. Delayed in passing through Washington, D.C., the 23rd Ohio did not arrive in time for the battle, but joined the Army of the Potomac as it hurried north to cut off Robert E. Lee’s Army of Northern Virginia as it advanced into Maryland. The 23rd was the first regiment to encounter the Confederates at the Battle of South Mountain on September 14. After severe losses, Union forces drove back the Confederates and continued to Sharpsburg, Maryland, where they engaged Lee’s army at the Battle of Antietam, one of the bloodiest battles of the war. The 23rd was also in the thick of the fighting at Antietam, and McKinley himself came under heavy fire when bringing rations to the men on the line. McKinley’s regiment again suffered many casualties, but the Army of the Potomac was victorious and the Confederates retreated into Virginia. The regiment was then detached from the Army of the Potomac and returned by train to western Virginia.
Shenandoah Valley and promotion.
While the regiment went into winter quarters near Charleston, Virginia (present-day West Virginia), McKinley was ordered back to Ohio with some other sergeants to recruit fresh troops. When they arrived in Columbus, Governor David Tod surprised McKinley with a commission as second lieutenant in recognition of his service at Antietam. McKinley and his comrades saw little action until July 1863, when the division skirmished with John Hunt Morgan’s cavalry at the Battle of Buffington Island. Early in 1864, the Army command structure in West Virginia was reorganized, and the division was assigned to George Crook’s Army of West Virginia. They soon resumed the offensive, marching into southwestern Virginia to destroy salt and lead mines used by the enemy. On May 9, the army engaged Confederate troops at Cloyd’s Mountain, where the men charged the enemy entrenchments and drove the rebels from the field. McKinley later said the combat there was “as desperate as any witnessed during the war.” Following the rout, the Union forces destroyed Confederate supplies and skirmished with the enemy again successfully.
McKinley and his regiment moved to the Shenandoah Valley as the armies broke from winter quarters to resume hostilities. Crook’s corps was attached to Major General David Hunter’s Army of the Shenandoah and soon back in contact with Confederate forces, capturing Lexington, Virginia, on June 11. They continued south toward Lynchburg, tearing up railroad track as they advanced. Hunter believed the troops at Lynchburg were too powerful, however, and the brigade returned to West Virginia. Before the army could make another attempt, Confederate General Jubal Early’s raid into Maryland forced their recall to the north. Early’s army surprised them at Kernstown on July 24, where McKinley came under heavy fire and the army was defeated. Retreating into Maryland, the army was reorganized again: Major General Philip Sheridan replaced Hunter, and McKinley, who had been promoted to captain after the battle, was transferred to General Crook’s staff. By August, Early was retreating south in the valley, with Sheridan’s army in pursuit. They fended off a Confederate assault at Berryville, where McKinley had a horse shot out from under him, and advanced to Opequon Creek, where they broke the enemy lines and pursued them farther south. They followed up the victory with another at Fisher’s Hill on September 22, and were engaged once more at Cedar Creek on October 19. After initially falling back from the Confederate advance, McKinley helped to rally the troops and turn the tide of the battle.
After Cedar Creek, the army stayed in the vicinity through election day, when McKinley cast his first presidential ballot, for the incumbent Republican, Abraham Lincoln. The next day, they moved north up the valley into winter quarters near Kernstown. In February 1865, Crook was captured by Confederate raiders. Crook's capture added to the confusion as the army was reorganized for the spring campaign, and McKinley found himself serving on the staffs of four different generals over the next fifteen days — Crook, John D. Stevenson, Samuel S. Carroll, and Winfield S. Hancock. Finally assigned to Carroll’s staff again, McKinley acted as the general’s first and only adjutant. Lee and his army surrendered to General Ulysses S. Grant a few days later, effectively ending the war. McKinley found time to join a Freemason lodge (later renamed after him) in Winchester, Virginia, before he and Carroll were transferred to Hancock’s First Veterans Corps in Washington. Just before the war’s end, McKinley received his final promotion, a brevet commission as major. In July, the Veterans Corps was mustered out of service, and McKinley and Carroll were relieved of their duties. Carroll and Hancock encouraged McKinley to apply for a place in the peacetime army, but he declined and returned to Ohio the following month.
Legal career and marriage.
After the war ended in 1865, McKinley decided on a career in the law and began studying in the office of an attorney in Poland, Ohio. The following year, he continued his studies by attending Albany Law School in New York. After studying there for less than a year, McKinley returned home and was admitted to the bar in Warren, Ohio, in March 1867. That same year, he moved to Canton, the county seat of Stark County, and set up a small office. He soon formed a partnership with George W. Belden, an experienced lawyer and former judge. His practice was successful enough for him to buy a block of buildings on Main Street in Canton, which provided him with a small but consistent rental income for decades to come. When his Army friend Rutherford B. Hayes was nominated for governor in 1867, McKinley made speeches on his behalf in Stark County, his first foray into politics. The county was closely divided between Democrats and Republicans, but Hayes carried it that year in his statewide victory. In 1869, McKinley ran for the office of prosecuting attorney of Stark County, an office usually then held by Democrats, and was unexpectedly elected. When McKinley ran for re-election in 1871, the Democrats nominated William A. Lynch, a prominent local lawyer, and McKinley was defeated by 143 votes.
As McKinley’s professional career progressed, so too did his social life blossom as he wooed Ida Saxton, the daughter of a prominent Canton family. They were married on January 25, 1871, in the newly built First Presbyterian Church of Canton, although Ida soon joined her husband's Methodist church. Their first child, Katherine, was born on Christmas Day 1871. A second daughter, Ida, followed in 1873, but died the same year. McKinley’s wife descended into a deep depression at her baby’s death and her health, never robust, grew worse. Two years later, in 1875, Katherine died of typhoid fever. Ida never recovered from her daughters’ deaths; the McKinleys had no more children. Ida McKinley developed epilepsy around the same time and thereafter disliked her husband's leaving her side. He remained a devoted husband and tended to his wife’s medical and emotional needs for the rest of his life.
Ida insisted that McKinley continue his increasingly successful career in law and politics. He attended the state Republican convention that nominated Hayes for a third term as governor in 1875, and campaigned again for his old friend in the election that fall. The next year, McKinley undertook a high-profile case defending a group of coal miners arrested for rioting after a clash with strikebreakers. Lynch, McKinley's opponent in the 1871 election, and his partner, William R. Day, were the opposing counsel, and the mine owners included Mark Hanna, a Cleveland businessman. Taking the case "pro bono," he was successful in getting all but one of the miners acquitted. The case raised McKinley’s standing among laborers, a crucial part of the Stark County electorate, and also introduced him to Hanna, who would become his strongest backer in years to come.
McKinley’s good standing with labor became useful that year as he campaigned for the Republican nomination for Ohio's 17th congressional district. Delegates to the county conventions thought he could attract blue-collar voters, and in August 1876, McKinley was nominated. By that time, Hayes had been nominated for President, and McKinley campaigned for him while running his own congressional campaign. Both were successful. McKinley, campaigning mostly on his support for a protective tariff, defeated the Democratic nominee, Levi L. Lamborn, by 3,300 votes, while Hayes won a hotly disputed election to reach the presidency. McKinley’s victory came at a personal cost: his income as a congressman would be half of what he earned as a lawyer.
Rising politician 1877–1895.
Spokesman for protection.
"Under free trade the trader is the master and the producer the slave. Protection is but the law of nature, the law of self-preservation, of self-development, of securing the highest and best destiny of the race of man. [It is said] that protection is immoral... Why, if protection builds up and elevates 63,000,000 [the U.S. population] of people, the influence of those 63,000,000 of people elevates the rest of the world. We cannot take a step in the pathway of progress without benefiting mankind everywhere. Well, they say, 'Buy where you can buy the cheapest'... Of course, that applies to labor as to everything else. Let me give you a maxim that is a thousand times better than that, and it is the protection maxim: 'Buy where you can pay the easiest.' And that spot of earth is where labor wins its highest rewards."
McKinley first took his congressional seat in October 1877, when President Hayes summoned Congress into special session. With the Republicans in the minority, McKinley was given unimportant committee assignments, which he undertook conscientiously. McKinley’s friendship with Hayes did McKinley little good on Capitol Hill; the President was not well-regarded by many leaders there. The young congressman broke with Hayes on the question of the currency, but it did not affect their friendship. The United States had effectively been placed on the gold standard by the Coinage Act of 1873; when silver prices dropped significantly, many sought to make silver again a legal tender, equally with gold. Such a course would be inflationary, but advocates argued that the economic benefits of the increased money supply would be worth the inflation; opponents warned that “free silver” would not bring the promised benefits and would harm the United States in international trade. McKinley voted for the Bland-Allison Act of 1878, which mandated large government purchases of silver for striking into money, and also joined the large majorities in each house that overrode Hayes’ veto of the legislation. In so doing, McKinley voted against the position of the House Republican leader, his fellow Ohioan and friend, James Garfield.
From his first term in Congress, McKinley was a strong advocate of protective tariffs. The primary purposes of such imposts was not to raise revenue, but to allow American manufacturing to develop by giving it a price advantage in the domestic market over foreign competitors. McKinley biographer Margaret Leech noted that Canton had become prosperous as a center for the manufacture of farm equipment because of protection, and that this may have helped form his political views. McKinley introduced and supported bills that raised protective tariffs, and opposed those that lowered them or imposed tariffs simply to raise revenue. Garfield’s election as president in 1880 created a vacancy on the House Ways and Means Committee; McKinley was selected to fill it, placing him on the most powerful committee after only two terms.
McKinley increasingly became a significant figure in national politics. In 1880, he served a brief term as Ohio’s representative on the Republican National Committee. In 1884, he was elected a delegate to that year’s Republican convention, where he served as chair of the Committee on Resolutions and won plaudits for his handling of the convention when called upon to preside. By 1886, McKinley, Senator John Sherman, and Governor Joseph B. Foraker were considered the leaders of the Republican party in Ohio. Sherman, who had helped to found the Republican Party, ran three times for the Republican nomination for president in the 1880s, each time failing, while Foraker began a meteoric rise in Ohio politics early in the decade. Hanna, once he entered public affairs as a political manager and generous contributor, supported Sherman’s ambitions, as well as those of Foraker. The latter relationship broke off at the 1888 Republican National Convention, where McKinley, Foraker, and Hanna were all delegates supporting Sherman. Convinced Sherman could not win, Foraker threw his support to the unsuccessful Republican 1884 presidential nominee, Maine Senator James G. Blaine. When Blaine stated he was not a candidate, Foraker returned to Sherman, but the nomination went to former Indiana senator Benjamin Harrison, who was elected president. In the bitterness that followed the convention, Hanna abandoned Foraker, and for the rest of McKinley’s life, the Ohio Republican Party was divided into two factions, one aligned with McKinley, Sherman, and Hanna and the other with Foraker. Hanna came to admire McKinley and became a friend and close adviser to him. Although Hanna remained active in business and in promoting other Republicans, in the years after 1888, he spent an increasing amount of time boosting McKinley’s political career.
In 1889, with the Republicans in the majority, McKinley sought election as Speaker of the House. He failed to gain the post, which went to Thomas B. Reed of Maine; however, Speaker Reed appointed McKinley chairman of the Ways and Means Committee. The Ohioan guided the McKinley Tariff of 1890 through Congress; although McKinley’s work was altered through the influence of special interests in the Senate, it imposed a number of protective tariffs on foreign goods.
Gerrymandering and defeat for re-election.
Recognizing McKinley’s potential, the Democrats, whenever they controlled the Ohio legislature, sought to gerrymander or redistrict him out of office. In 1878, McKinley faced election in a redrawn 17th district; he won anyway, causing Hayes to exult, “Oh, the good luck of McKinley! He was gerrymandered out and then beat the gerrymander! We enjoyed it as much as he did.” After the 1882 election, McKinley was unseated on an election contest by a near party-line House vote. Out of office, he was briefly depressed by the setback, but soon vowed to run again. The Democrats again redistricted Stark County for the 1884 election; McKinley was returned to Congress anyway.
For 1890, the Democrats gerrymandered McKinley one final time, placing Stark County in the same district as one of the strongest pro-Democrat counties, Holmes, populated by solidly Democratic Pennsylvania Dutch. The new boundaries seemed good, based on past results, for a Democratic majority of 2000 to 3000. The Republicans could not reverse the gerrymander as legislative elections would not be held until 1891, but they could throw all their energies into the district, as the McKinley Tariff was a main theme of the Democratic campaign nationwide, and there was considerable attention paid to McKinley’s race. The Republican Party sent its leading orators to Canton, including Blaine (then Secretary of State), Speaker Reed and President Harrison. The Democrats countered with their best spokesmen on tariff issues. McKinley tirelessly stumped his new district, reaching out to its 40,000 voters to explain that his tariff
was framed for the people ... as a defense to their industries, as a protection to the labor of their hands, as a safeguard to the happy homes of American workingmen, and as a security to their education, their wages, and their investments ... It will bring to this country a prosperity unparalleled in our own history and unrivalled in the history of the world.”
Democrats ran a strong candidate in former lieutenant governor John G. Warwick. To drive their point home, they hired young partisans to pretend to be peddlers, who went door to door offering 25-cent tinware to housewives for 50 cents, explaining the rise in prices was due to the McKinley Tariff. In the end, McKinley lost by 300 votes, but the Republicans won a statewide majority and claimed a moral victory.
Governor of Ohio.
Even before McKinley completed his term in Congress, he met with a delegation of Ohioans urging him to run for governor. Governor James E. Campbell, a Democrat, who had defeated Foraker in 1889, was to seek re-election in 1891. The Ohio Republican party remained divided, but McKinley quietly arranged for Foraker to nominate him at the 1891 state Republican convention, which chose McKinley by acclamation. The former congressman spent much of the second half of 1891 campaigning against Campbell, beginning in his birthplace of Niles. Hanna, however, was little seen in the campaign; he spent much of his time raising funds for the election of legislators pledged to vote for Sherman in the 1892 senatorial election. McKinley won the 1891 election by some 20,000 votes; the following January, Sherman, with considerable assistance from Hanna, turned back a challenge by Foraker to win the legislature’s vote for another term in the Senate.
Ohio’s governor had relatively little power—for example, he could recommend legislation, but not veto it—but with Ohio a key swing state, its governor was a major figure in national politics. Although McKinley believed that the health of the nation depended on that of business, he was evenhanded in dealing with labor. He procured legislation that set up an arbitration board to settle work disputes and obtained passage of a law that fined employers who fired workers for belonging to a union.
President Harrison had proven unpopular; there were divisions even within the Republican party as the year 1892 began and Harrison began his re-election drive. Although no declared candidate opposed Harrison, many Republicans were ready to dump the President from the ticket if an alternative emerged. Among the possible candidates spoken of were McKinley, Reed, and the aging Blaine. Fearing that the Ohio governor would emerge as a candidate, Harrison’s managers arranged for McKinley to be permanent chairman of the convention in Minneapolis, requiring him to play a public, neutral role. Hanna established an unofficial McKinley headquarters near the convention hall, though no active effort was made to convert delegates to McKinley’s cause. McKinley objected to delegate votes being cast for him; nevertheless he finished third, behind the renominated Harrison, and behind Blaine, who had sent word he did not want to be considered. Although McKinley campaigned loyally for the Republican ticket, Harrison was defeated by former President Cleveland in the November election. In the wake of Cleveland’s victory, McKinley was seen by some as the likely Republican candidate in 1896.
Soon after Cleveland’s return to office, hard times struck the nation with the Panic of 1893. A businessman in Youngstown, Robert Walker, had lent money to McKinley in their younger days; in gratitude, McKinley had often guaranteed Walker’s borrowings for his business. The governor had never kept track of what he was signing; he believed Walker a sound businessman. In fact, Walker had deceived McKinley, telling him that new notes were actually renewals of matured ones. Walker was ruined by the recession; McKinley was called upon for repayment in February 1893. The total owed was over $100,000 and a despairing McKinley initially proposed to resign as governor and earn the money as an attorney. Instead, McKinley’s wealthy supporters, including Hanna and Chicago publisher H. H. Kohlsaat, became trustees of a fund from which the notes would be paid. Both William and Ida McKinley placed their property in the hands of the fund’s trustees (who included Hanna and Kohlsaat), and the supporters raised and contributed a substantial sum of money. All of the couple’s property was returned to them by the end of 1893, and when McKinley, who had promised eventual repayment, asked for the list of contributors, it was refused him. Many people who had suffered in the hard times sympathized with McKinley, whose popularity grew. He was easily re-elected in November 1893, receiving the largest percentage of the vote of any Ohio governor since the Civil War.
McKinley campaigned widely for Republicans in the 1894 midterm congressional elections; many party candidates in districts where he spoke were successful. His political efforts in Ohio were rewarded with the election in November 1895 of a Republican successor as governor, Asa Bushnell, and a Republican legislature that elected Foraker to the Senate. McKinley supported Foraker for Senate and Bushnell (who was of Foraker’s faction) for governor; in return, the new senator-elect agreed to back McKinley’s presidential ambitions. With party peace in Ohio assured, McKinley turned to the national arena.
Election of 1896.
Obtaining the nomination.
It is unclear when William McKinley began to seriously prepare a run for president. As Phillips notes, “no documents, no diaries, no confidential letters to Mark Hanna (or anyone else) contain his secret hopes or veiled stratagems.” From the beginning, McKinley’s preparations had the participation of Hanna, whose biographer William T. Horner noted, “what is certainly true is that in 1888 the two men began to develop a close working relationship that helped put McKinley in the White House.” Sherman did not run for president again after 1888, and so Hanna could support McKinley’s ambitions for that office wholeheartedly.
Backed by Hanna’s money and organizational skills, McKinley quietly built support for a presidential bid through 1895 and early 1896. When other contenders such as Speaker Reed and Iowa Senator William B. Allison sent agents outside their states to organize Republicans in support of their candidacies, they found that Hanna’s agents had preceded them. According to historian Stanley Jones in his study of the 1896 election,
Another feature common to the Reed and Allison campaigns was their failure to make headway against the tide which was running toward McKinley. In fact, both campaigns from the moment they were launched were in retreat. The calm confidence with which each candidate claimed the support of his own section [of the country] soon gave way to ... bitter accusations that Hanna by winning support for McKinley in their sections had violated the rules of the game.
Hanna, on McKinley's behalf, met with the eastern Republican political bosses, such as Senators Thomas Platt of New York and Matthew Quay of Pennsylvania, who were willing to guarantee McKinley’s nomination in exchange for promises regarding patronage and offices. McKinley, however, was determined to obtain the nomination without making deals, and Hanna accepted that decision. Many of their early efforts were focused on the South; Hanna obtained a vacation home in southern Georgia where McKinley visited and met with Republican politicians from the region. McKinley needed 453½ delegate votes to gain the nomination; he gained nearly half that number from the South and border states. Platt lamented in his memoirs, “[Hanna] had the South practically solid before some of us awakened.”
The bosses still hoped to deny McKinley a first-ballot majority at the convention by boosting support for local favorite son candidates such as Quay, New York Governor (and former vice president) Levi P. Morton, and Illinois Senator Shelby Cullom. Delegate-rich Illinois proved a crucial battleground, as McKinley supporters, such as Chicago businessman (and future vice president) Charles G. Dawes, sought to elect delegates pledged to vote for McKinley at the national convention in St. Louis. Cullom proved unable to stand against McKinley despite the support of local Republican machines; at the state convention at the end of April, McKinley completed a near-sweep of Illinois’ delegates. Former president Harrison had been deemed a possible contender if he entered the race; when Harrison made it known he would not seek a third nomination, the McKinley organization took control of Indiana with a speed Harrison privately found unseemly. Morton operatives who journeyed to Indiana sent word back that they had found the state alive for McKinley. Wyoming Senator Francis Warren wrote, “The politicians are making a hard fight against him, but if the masses could speak, McKinley is the choice of at least 75% of the entire [body of] Republican voters in the Union”.
By the time the national convention began in St. Louis on June 16, 1896, McKinley had an ample majority of delegates. The former governor, who remained in Canton, followed events at the convention closely by telephone, and was able to hear part of Foraker’s speech nominating him over the line. When Ohio was reached in the roll call of states, its votes gave McKinley the nomination, which he celebrated by hugging his wife and mother as his friends fled the house, anticipating the first of many crowds that gathered at the Republican candidate’s home. Thousands of partisans came from Canton and surrounding towns that evening to hear McKinley speak from his front porch. The convention nominated Republican National Committee vice chairman Garret Hobart of New Jersey for vice president, a choice actually made, by most accounts, by Hanna. Hobart, a wealthy lawyer, businessman, and former state legislator, was not widely known, but as Hanna biographer Herbert Croly pointed out, “if he did little to strengthen the ticket he did nothing to weaken it”.
General election campaign.
Before the Republican convention, McKinley had been a “straddle bug” on the currency question, favoring moderate positions on silver such as accomplishing bimetallism by international agreement. In the final days before the convention, McKinley decided, after hearing from politicians and businessmen, that the platform should endorse the gold standard, though it should allow for bimetallism by international agreement. Adoption of the platform caused some western delegates, led by Colorado Senator Henry M. Teller, to walk out of the convention. However, compared with the Democrats, Republican divisions on the issue were small, especially as McKinley promised future concessions to silver advocates.
The bad economic times had continued, and strengthened the hand of forces for free silver. The issue bitterly divided the Democratic Party; President Cleveland firmly supported the gold standard, but an increasing number of rural Democrats wanted silver, especially in the South and West. The silverites took control of the 1896 Democratic National Convention and chose William Jennings Bryan for president; he had electrified the delegates with his Cross of Gold speech. Bryan’s financial radicalism shocked financiers—they thought his inflationary program would bankrupt the railroads and ruin the economy. Hanna approached them for support for his strategy to win the election, and they gave $3.5 million for speakers and over 200 million pamphlets advocating the Republican position on the money and tariff questions.
Bryan’s campaign had at most an estimated $500,000. With his eloquence and youthful energy his major assets in the race, Bryan decided on a whistle-stop political tour by train on an unprecedented scale. Hanna urged McKinley to match Bryan’s tour with one of his own; the candidate declined on the grounds that the Democrat was a better stump speaker: “I might just as well set up a trapeze on my front lawn and compete with some professional athlete as go out speaking against Bryan. I have to "think" when I speak.” Instead of going to the people, McKinley would remain at home in Canton and allow the people to come to him; according to historian R. Hal Williams in his book on the 1896 election, “it was, as it turned out, a brilliant strategy. McKinley’s ‘Front Porch Campaign’ became a legend in American political history.”
McKinley made himself available to the public every day except Sunday, receiving delegations from the front porch of his home. The railroads subsidized the visitors with low excursion rates—the pro-silver Cleveland "Plain Dealer" disgustedly stated that going to Canton had been made “cheaper than staying at home”. Delegations marched through the streets from the railroad station to McKinley’s home on North Market Street. Once there, they crowded close to the front porch—from which they surreptitiously whittled souvenirs—as their spokesman addressed McKinley. The candidate then responded, speaking on campaign issues in a speech molded to suit the interest of the delegation. The speeches were carefully scripted to avoid extemporaneous remarks; even the spokesman’s remarks were approved by McKinley or a representative. This was done as the candidate feared an offhand comment by another that might rebound on him.
Most Democratic newspapers refused to support Bryan, the major exception being the New York "Journal", controlled by William Randolph Hearst, whose fortune was based on silver mines. In biased reporting and through the sharp cartoons of Homer Davenport, Hanna was viciously characterized as a plutocrat, trampling on labor. McKinley was drawn as a child, easily controlled by big business. Even today, these depictions still color the images of Hanna and McKinley: one as a heartless businessman, the other as a creature of Hanna and others of his ilk.
The Democrats had pamphlets too, though not as many. Jones analyzed how voters responded to the education campaigns of the two parties:For the people it was a campaign of study and analysis, of exhortation and conviction—a campaign of search for economic and political truth. Pamphlets tumbled from the presses, to be read, reread, studied, debated, to become guides to economic thought and political action. They were printed and distributed by the million ... but the people hankered for more. Favorite pamphlets became dog-eared, grimy, fell apart as their owners laboriously restudied their arguments and quoted from them in public and private debate.
The battleground proved to be the Midwest — the South and most of the West were conceded to Bryan — and the Democrat spent much of his time in those crucial states. The Northeast was considered most likely safe for McKinley after the early-voting states of Maine and Vermont supported him in September. By then, it was clear that public support for silver had receded, and McKinley began to emphasize the tariff issue. By the end of September, the Republicans had discontinued printing material on the silver issue, and were entirely concentrating on the tariff question. On November 3, 1896, the voters had their say in most of the nation. McKinley won the entire Northeast and Midwest; he won 51% of the vote and an ample majority in the Electoral College. Bryan had concentrated entirely on the silver issue, and had not appealed to urban workers. Voters in cities supported McKinley; the only city outside the South of more than 100,000 population carried by Bryan was Denver, Colorado.
The 1896 presidential election is often seen as a realigning election, in which McKinley’s view of a stronger central government building American industry through protective tariffs and a dollar based on gold triumphed. The voting patterns established then displaced the near-deadlock the major parties had seen since the Civil War; the Republican dominance begun then would continue until 1932, another realigning election with the ascent of Franklin Roosevelt. Phillips argues that, with the possible exception of Iowa Senator Allison, McKinley was the only Republican who could have defeated Bryan—he theorized that eastern candidates such as Morton or Reed would have done badly against the Illinois-born Bryan in the crucial Midwest. According to the biographer, though Bryan was popular among rural voters, “McKinley appealed to a very different industrialized, urbanized America.”
Presidency (1897–1901).
Inauguration and appointments.
McKinley was sworn in as president on March 4, 1897, as his wife and mother looked on. The new President gave a lengthy inaugural address; he urged tariff reform, and stated that the currency issue would have to await tariff legislation. He warned against foreign interventions, “We want no wars of conquest. We must avoid the temptation of territorial aggression.”
McKinley’s most controversial Cabinet appointment was that of John Sherman as Secretary of State. Sherman was not McKinley’s first choice for the position; he initially offered it to Senator Allison. One consideration in Senator Sherman’s appointment was to provide a place in the Senate for Hanna (who had turned down a Cabinet position as Postmaster General). As Sherman had served as Secretary of the Treasury under Hayes, only the State position, the leading Cabinet post, was likely to entice him from the Senate. Sherman’s mental faculties were decaying even in 1896; this was widely spoken of in political circles, but McKinley did not believe the rumors. Nevertheless, McKinley sent his cousin, William McKinley Osborne, to have dinner with the 73-year-old senator; he reported back that Sherman seemed as lucid as ever. McKinley wrote once the appointment was announced, “the stories regarding Senator Sherman’s ‘mental decay’ are without foundation ... When I saw him last I was convinced both of his perfect health, physically and mentally, and that the prospects of life were remarkably good.”
After some difficulties, Ohio Governor Bushnell appointed Hanna to the Senate. Once in Cabinet office, Sherman’s mental incapacity became increasingly apparent. He was often bypassed by his first assistant, McKinley’s Canton crony Judge William Day, and by the second secretary, Alvey A. Adee. Day, an Ohio lawyer unfamiliar with diplomacy, was often reticent in meetings; Adee was somewhat deaf. One diplomat characterized the arrangement, “the head of the department knew nothing, the first assistant said nothing, and the second assistant heard nothing”.
Maine Congressman Nelson Dingley Jr. was McKinley’s choice for Secretary of the Treasury; he declined it, preferring to remain as chairman of the Ways and Means Committee. Charles Dawes, who had been Hanna's lieutenant in Chicago during the campaign, was considered for the Treasury post but by some accounts Dawes considered himself too young. Dawes eventually became Comptroller of the Currency; he recorded in his published diary that he had strongly urged McKinley to appoint as secretary the successful candidate, Lyman J. Gage, president of the First National Bank of Chicago and a Gold Democrat. The Navy Department was offered to former Massachusetts Congressman John Davis Long, an old friend from the House, on January 30, 1897. Although McKinley was initially inclined to allow Long to choose his own assistant, there was considerable pressure on the President-elect to appoint Theodore Roosevelt, head of the New York City Police Commission and a former state assemblyman. McKinley was reluctant, stating to one Roosevelt booster, “I want peace and I am told that your friend Theodore is always getting into rows with everybody.” Nevertheless, he made the appointment.
In addition to Sherman, McKinley made one other ill-advised Cabinet appointment, that of Secretary of War, which fell to Russell A. Alger, former general and Michigan governor. Competent enough in peacetime, Alger proved inadequate once the conflict with Spain began. With the War Department plagued by scandal, Alger resigned at McKinley's request in mid-1899. Vice President Hobart, as was customary at the time, was not invited to Cabinet meetings. However, he proved a valuable adviser both for McKinley and for his Cabinet members. The wealthy Vice President leased a residence close to the White House; the two families visited each other without formality, and the Vice President's wife, Jennie Tuttle Hobart, sometimes substituted as Executive Mansion hostess when Ida McKinley was unwell. For most of McKinley’s administration, George B. Cortelyou served as his personal secretary. Cortelyou, who served in three Cabinet positions under Theodore Roosevelt, became a combination press secretary and chief of staff to McKinley.
War with Spain.
For decades, rebels in Cuba had waged an intermittent campaign for freedom from Spanish colonial rule. By 1895, the conflict had expanded to a war for Cuban independence. As war engulfed the island, Spanish reprisals against the rebels grew ever harsher. These included the removal of Cubans to internment camps near Spanish military bases, a strategy designed to make it hard for the rebels to receive support in the countryside. American opinion favored the rebels, and McKinley shared in their outrage against Spanish policies. As many of his countrymen called for war to liberate Cuba, McKinley favored a peaceful approach, hoping that through negotiation, Spain might be convinced to grant Cuba independence, or at least to allow the Cubans some measure of autonomy. The United States and Spain began negotiations on the subject in 1897, but it became clear that Spain would never concede Cuban independence, while the rebels (and their American supporters) would never settle for anything less.
In January 1898, Spain promised some concessions to the rebels, but when American consul Fitzhugh Lee reported riots in Havana, McKinley agreed to send the battleship USS "Maine" there to protect American lives and property. On February 15, the "Maine" exploded and sank with 266 men killed. Public opinion and the newspapers demanded war, but McKinley insisted that a court of inquiry first determine whether the explosion was accidental. Negotiations with Spain continued as the court considered the evidence, but on March 20, the court ruled that the "Maine" was blown up by an underwater mine. As pressure for war mounted in Congress, McKinley continued to negotiate for Cuban independence. Spain refused McKinley’s proposals, and on April 11, McKinley turned the matter over to Congress. He did not ask for war, but Congress declared war anyway on April 20, with the addition of the Teller Amendment, which disavowed any intention of annexing Cuba.
The expansion of the telegraph and the development of the telephone gave McKinley a greater control over the day-to-day management of the war than previous presidents had enjoyed, and he used the new technologies to direct the army’s and navy’s movements as far as he was able. McKinley found Alger inadequate as Secretary of War, and did not get along with the Army’s commanding general, Nelson A. Miles. Bypassing them, he looked for strategic advice first from Miles’s predecessor, General John Schofield, and later from Adjutant General Henry Clarke Corbin. The war led to a change in McKinley's cabinet, as the President accepted Sherman’s resignation as Secretary of State; Day agreed to serve as Secretary until the war’s end.
Within a fortnight, the navy had its first victory when the Asiatic Squadron, led by Commodore George Dewey, engaged the Spanish navy at the Battle of Manila Bay in the Philippines, destroying the enemy force without the loss of a single American vessel. Dewey’s overwhelming victory expanded the scope of the war from one centered in the Caribbean to one that would determine the fate of all of Spain's Pacific colonies. The next month, he increased the number of troops sent to the Philippines and granted the force’s commander, Major General Wesley Merritt, the power to set up legal systems and raise taxes—necessities for a long occupation. By the time the troops arrived in the Philippines at the end of June 1898, McKinley had decided that Spain would be required to surrender the archipelago to the United States. He professed to be open to all views on the subject; however, he believed that as the war progressed, the public would come to demand retention of the islands as a prize of war.
Meanwhile, in the Caribbean theater, a large force of regulars and volunteers gathered near Tampa, Florida, for an invasion of Cuba. The army faced difficulties in supplying the rapidly expanding force even before they departed for Cuba, but by June, Corbin had made progress in resolving the problems. After lengthy delays, the army, led by Major General William Rufus Shafter, sailed from Florida on June 20, landing near Santiago de Cuba two days later. Following a skirmish at Las Guasimas on June 24, Shafter’s army engaged the Spanish forces on July 2 in the Battle of San Juan Hill. In an intense day-long battle, the American force was victorious, although both sides suffered heavy casualties. The next day, the Spanish Caribbean squadron, which had been sheltering in Santiago’s harbor, broke for the open sea but was intercepted and destroyed by Rear Admiral William T. Sampson’s North Atlantic Squadron in the largest naval battle of the war. Shafter laid siege to the city of Santiago, which surrendered on July 17, placing Cuba under effective American control. McKinley and Miles also ordered an invasion of Puerto Rico, which met little resistance when it landed in July. The distance from Spain and the destruction of the Spanish navy made resupply impossible, and the Spanish government began to look for a way to end the war.
Peace and territorial gain.
On July 22, the Spanish authorized Jules Cambon, the French Ambassador to the United States, to represent Spain in negotiating peace. The Spanish initially wished to restrict the discussion to Cuba, but were quickly forced to recognize that their other possessions would be claimed as spoils of war. McKinley's cabinet agreed with him that Spain must leave Cuba and Puerto Rico, but they disagreed on the Philippines, with some wishing to annex the entire archipelago and some wishing only to retain a naval base in the area. Although public sentiment seemed to favor annexation of the Philippines, several prominent political leaders, including Bryan, ex-President Cleveland, and the newly formed American Anti-Imperialist League made their opposition known.
McKinley proposed to open negotiations with Spain on the basis of Cuban liberation and Puerto Rican annexation, with the final status of the Philippines subject to further discussion. He stood firmly in that demand even as the military situation on Cuba began to deteriorate when the American army was struck with yellow fever. Spain ultimately agreed to a ceasefire on those terms on August 12, and treaty negotiations began in Paris in September 1898. The talks continued until December 18, when the Treaty of Paris was signed. The United States acquired Puerto Rico and the Philippines as well as the island of Guam, and Spain relinquished its claims to Cuba; in exchange, the United States agreed to pay Spain $20 million. McKinley had difficulty convincing the Senate to approve the treaty by the requisite two-thirds vote, but his lobbying, and that of Vice President Hobart, eventually saw success, as the Senate voted in favor on February 6, 1899, 57 to 27.
During the war, McKinley also pursued the annexation of the Republic of Hawaii. The new republic, dominated by American interests, had seized power from the royal government in 1893. The lame-duck Harrison administration had submitted a treaty of annexation to the Senate; Cleveland, once he returned to office, had sent a special commission to the islands. After receiving the report, Cleveland withdrew the treaty, stating that the revolution did not reflect the will of Hawaiian citizens. Nevertheless, many Americans favored annexation, and the cause gained momentum as the United States became embroiled in war with Spain. McKinley came to office as a supporter of annexation, and lobbied Congress to adopt his opinion, believing that to do nothing would invite a royalist counter-revolution or a Japanese takeover. Foreseeing difficulty in getting two-thirds of the Senate to approve a treaty of annexation, McKinley instead supported the effort of Democratic Representative Francis G. Newlands of Nevada to accomplish the result by joint resolution of both houses of Congress. The resulting Newlands Resolution passed both houses by wide margins, and McKinley signed it into law on July 8, 1898. McKinley biographer H. Wayne Morgan notes, “McKinley was the guiding spirit behind the annexation of Hawaii, showing ... a firmness in pursuing it”; the President told Cortelyou, “We need Hawaii just as much and a good deal more than we did California. It is manifest destiny.” Wake Island, an uninhabited atoll between Hawaii and Guam, was claimed for the United States on July 12, 1898.
Expanding influence overseas.
In acquiring Pacific possessions for the United States, McKinley expanded the nation’s ability to compete for trade in China. Even before peace negotiations began with Spain, McKinley asked Congress to set up a commission to examine trade opportunities in the region and espoused an “Open Door Policy”, in which all nations would freely trade with China and none would seek to violate that nation’s territorial integrity. When John Hay replaced Day as Secretary of State at the end of the war, he circulated notes to that effect to the European powers. Great Britain favored the idea, but Russia opposed it; France, Germany, Italy and Japan agreed in principle, but only if all the other nations signed on.
Trade with China became imperiled shortly thereafter as the Boxer Rebellion menaced foreigners and their property in China. Americans and other westerners in Peking were besieged and, in cooperation with other western powers, McKinley ordered 5000 troops to the city in June 1900 in the China Relief Expedition. The westerners were rescued the next month, but several Congressional Democrats objected to McKinley dispatching troops without consulting the legislature. McKinley’s actions set a precedent that led to most of his successors exerting similar independent control over the military. After the rebellion ended, the United States reaffirmed its commitment to the Open Door policy, which became the basis of American policy toward China.
Closer to home, McKinley and Hay engaged in negotiations with Britain over the possible construction of a canal across Central America. The Clayton–Bulwer Treaty, which the two nations signed in 1850, prohibited either from establishing exclusive control over a canal there. The war had exposed the difficulty of maintaining a two-ocean navy without a connection closer than Cape Horn. Now, with American business and military interests even more involved in Asia, a canal seemed more essential than ever, and McKinley pressed for a renegotiation of the treaty. Hay and the British ambassador, Julian Pauncefote, agreed that the United States could control a future canal, provided that it was open to all shipping and not fortified. McKinley was satisfied with the terms, but the Senate rejected them, demanding that the United States be allowed to fortify the canal. Hay was embarrassed by the rebuff and offered his resignation, but McKinley refused it and ordered him to continue negotiations to achieve the Senate’s demands. He was successful, and a new treaty was drafted and approved, but not before McKinley’s assassination in 1901.
Tariffs and bimetallism.
Two of the great issues of the day, tariff reform and free silver, became intertwined in 1897. Ways and Means chairman Dingley introduced a new tariff bill (later called the Dingley Act) to revise the Wilson–Gorman Tariff Act of 1894. McKinley supported the bill, which increased tariffs on wool, sugar, and luxury goods, but the proposed new rates alarmed the French, who exported many luxury items to the United States. The Dingley Act passed the House easily, but was delayed in the Senate as they assessed the French objections. French representatives offered to cooperate with the United States in developing an international agreement on bimetallism if the new tariff rates were reduced; this pleased silverite Republicans in the Senate, whose votes were necessary for passage. The Senate amended the bill to allow limited reciprocity (giving France some possibility of relief), but did not reduce the rates on luxury goods. McKinley signed the bill into law and agreed to begin negotiations on an international bimetallism standard.
American negotiators soon concluded a reciprocity treaty with France, and the two nations approached Britain to gauge British enthusiasm for bimetallism. The Prime Minister, Lord Salisbury, and his government showed some interest in the idea and told the American envoy, Edward O. Wolcott, that he would be amenable to reopening the mints in India to silver coinage if the Viceroy's Executive Council there agreed. News of a possible departure from the gold standard stirred up immediate opposition from its partisans, and misgivings by the Indian administration led Britain to reject the proposal. With the international effort a failure, McKinley turned away from silver coinage and embraced the gold standard. Even without the agreement, agitation for free silver eased as prosperity began to return to the United States and gold from recent strikes in the Yukon and Australia increased the monetary supply even without silver coinage. In the absence of international agreement, McKinley favored legislation to formally affirm the gold standard, but was initially deterred by the silver strength in the Senate. By 1900, with another campaign ahead and good economic conditions, McKinley urged Congress to pass such a law, and was able to sign the Gold Standard Act on March 14, 1900, using a gold pen to do so.
Civil rights.
In the wake of McKinley’s election in 1896, African Americans were hopeful of progress towards equality. McKinley had spoken out against lynching while governor, and most African Americans who could vote supported him in 1896. McKinley’s priority, however, was in ending sectionalism, and they were disappointed by his policies and appointments. Although McKinley made some appointments of African Americans to low-level government posts, and received some praise for that, the appointments were less than they had received under previous Republican administrations. Blanche K. Bruce, an African American who during Reconstruction had served as senator from Mississippi, received the post of register at the Treasury Department; this post was traditionally given to an African American by Republican presidents. McKinley appointed several black postmasters; however, when whites protested the appointment of Justin W. Lyons as postmaster of Augusta, Georgia, McKinley asked Lyons to withdraw (he was subsequently given the post of Treasury register after Bruce’s death in 1898). The President did appoint George B. Jackson, a former slave, to the post of customs collector in Presidio, Texas. However, African Americans in northern states felt that their contributions to McKinley's victory were overlooked; few were appointed to office.
The administration’s response to racial violence was minimal, causing him to lose black support. When black postmasters at Hogansville, Georgia in 1897, and at Lake City, South Carolina the following year, were assaulted, McKinley issued no statement of condemnation. Although black leaders criticized McKinley for inaction, supporters responded by saying there was little the president could do to intervene. Critics replied by saying that he could at least publicly condemn such events, as Harrison had done.
According to historian Clarance A. Bacote, “Before the Spanish–American War, the Negroes, in spite of some mistakes, regarded McKinley as the best friend they ever had.” African Americans saw the onset of war in 1898 as an opportunity to display their patriotism; and black soldiers fought bravely at El Caney and San Juan Hill. African Americans in the peacetime Army had formed elite units; nevertheless they were harassed by whites as they traveled from the West to Tampa for embarkation to the war. Under pressure from black leaders, McKinley required the War Department to commission black officers above the rank of lieutenant. The heroism of the black troops did not still racial tensions in the South, as the second half of 1898 saw several outbreaks of racial violence; 11 African Americans were killed in riots in Wilmington, North Carolina. McKinley toured the South in late 1898, hoping for sectional reconciliation. In addition to visiting Tuskegee Institute and black educator Booker T. Washington, he addressed the Georgia legislature, wearing a badge of gray, and visited Confederate memorials. In his tour of the South, McKinley did not mention the racial tensions or violence. Although the President received a rapturous reception from Southern whites, many African Americans, excluded from official welcoming committees, felt alienated by the President’s words and actions.
According to Gould and later biographer Phillips, given the political climate in the South, with white legislatures passing segregationist laws such as that upheld in "Plessy v. Ferguson", there was little McKinley could have done to improve race relations, and he did better than later presidents Theodore Roosevelt, who doubted racial equality, and Woodrow Wilson, who supported segregation. However, Gould concluded, “McKinley lacked the vision to transcend the biases of his day and to point toward a better future for all Americans”.
Judicial appointments.
After the retirement of Justice Stephen Johnson Field, McKinley appointed Attorney General Joseph McKenna to the Supreme Court of the United States in December 1897. The appointment aroused some controversy as McKenna’s critics in the Senate said he was too closely associated with railroad interests and lacked the qualifications of a Supreme Court justice. Despite the objections, McKenna’s nomination was approved unanimously. McKenna responded to the criticism of his legal education by taking some courses at Columbia Law School for several months before taking his seat. Along with his Supreme Court appointment, McKinley appointed six judges to the United States Courts of Appeals, and 28 judges to the United States district courts.
1900 election.
Republicans were generally successful in state and local elections around the country in 1899, and McKinley was optimistic about his chances at re-election in 1900. McKinley’s popularity in his first term assured him of renomination for a second. The only question about the Republican ticket concerned the vice presidential nomination; McKinley needed a new running mate as Hobart had died in late 1899. McKinley initially favored Elihu Root, who had succeeded Alger as Secretary of War, but McKinley decided that Root was doing too good a job at the War Department to move him. He considered other prominent candidates, including Allison and Cornelius N. Bliss, but none were as popular as the Republican party’s rising star, Theodore Roosevelt. After a stint as Assistant Secretary of the Navy, Roosevelt had resigned and raised a cavalry regiment; they fought bravely in Cuba, and Roosevelt returned home covered in glory. Elected governor of New York on a reform platform in 1898, Roosevelt had his eye on the presidency. Many supporters recommended him to McKinley for the second spot on the ticket, and Roosevelt believed it would be an excellent stepping stone to the presidency in 1904. McKinley remained uncommitted in public, but Hanna was firmly opposed to the New York governor. The Ohio senator considered the New Yorker overly impulsive; his stance was undermined by the efforts of political boss and New York Senator Thomas Platt, who, disliking Roosevelt’s reform agenda, sought to sideline the governor by making him vice president.
When the Republican convention began in Philadelphia that June, no vice presidential candidate had overwhelming support, but Roosevelt had the broadest range of support from around the country. McKinley affirmed that the choice belonged to the convention, not to him. On June 21, McKinley was unanimously renominated and, with Hanna’s reluctant acquiescence, Roosevelt was nominated for vice president on the first ballot. The Democratic convention convened the next month in Kansas City and nominated William Jennings Bryan, setting up a rematch of the 1896 contest.
The candidates were the same, but the issues of the campaign had shifted: free silver was still a question that animated many voters, but the Republicans focused on victory in war and prosperity at home as issues they believed favored their party. Democrats knew the war had been popular, even if the imperialism issue was less sure, so they focused on the issue of trusts and corporate power, painting McKinley as the servant of capital and big business. As in 1896, Bryan embarked on a speaking tour around the country while McKinley stayed at home, this time making only one speech, to accept his nomination. Roosevelt emerged as the campaign’s primary speaker and Hanna helped the cause working to settle a coal miners strike in Pennsylvania. Bryan’s campaigning failed to excite the voters as it had in 1896, and McKinley never doubted that he would be re-elected. On November 6, 1900, he was proven correct, winning the largest victory for any Republican since 1872. Bryan carried only four states outside the solid South, and McKinley even won Bryan’s home state of Nebraska.
Second term.
Soon after his second inauguration on March 4, 1901, William and Ida McKinley undertook a six-week tour of the nation. Traveling mostly by rail, the McKinleys were to travel through the South to the Southwest, and then up the Pacific coast and east again, to conclude with a visit on June 13, 1901 to the Pan-American Exposition in Buffalo, New York. However, the First Lady fell ill in California, causing her husband to limit his public events and cancel a series of speeches he had planned to give urging trade reciprocity. He also postponed the visit to the fair until September, planning a month in Washington and two in Canton before the Buffalo visit.
Assassination and death throes.
Although McKinley enjoyed meeting the public, Cortelyou was concerned with his security due to recent assassinations by anarchists in Europe, and twice tried to remove a public reception from the President’s rescheduled visit to the Exposition. McKinley refused, and Cortelyou arranged for additional security for the trip. On September 5, the President delivered his address at the fairgrounds, before a crowd of some 50,000 people. In his final speech, McKinley urged reciprocity treaties with other nations to assure American manufacturers access to foreign markets. He intended the speech as a keynote to his plans for a second term.
One man in the crowd, Leon Czolgosz, hoped to assassinate McKinley. He had managed to get close to the presidential podium, but did not fire, uncertain of hitting his target. Czolgosz, since hearing a speech by anarchist Emma Goldman in Cleveland, had decided to do something heroic (in his own mind) for the cause. After his failure to get close enough on the fifth, Czolgosz waited the next day at the Temple of Music on the Exposition grounds, where the President was to meet the public. Czolgosz concealed his gun in a handkerchief, and, when he reached the head of the line, shot McKinley twice in the abdomen.
McKinley urged his aides to break the news gently to Ida, and to call off the mob that had set on Czolgosz—a request that may have saved his assassin’s life. McKinley was taken to the Exposition aid station where the doctor was unable to locate the second bullet. Although a primitive X-ray machine was being exhibited on the Exposition grounds, it was not used. McKinley was taken to the Milburn House.
In the days after the shooting McKinley appeared to improve. Doctors issued increasingly optimistic bulletins. Members of the Cabinet, who had rushed to Buffalo on hearing the news, dispersed; Vice President Roosevelt departed on a camping trip to the Adirondacks. Leech wrote, It is difficult to interpret the optimism with which the President’s physicians looked for his recovery. There was obviously the most serious danger that his wounds would become septic. In that case, he would almost certainly die, since drugs to control infection did not exist ... [Prominent New York City physician] Dr. McBurney was by far the worst offender in showering sanguine assurances on the correspondents. As the only big-city surgeon on the case, he was eagerly questioned and quoted, and his rosy prognostications largely contributed to the delusion of the American public.
Unknown to the doctors, the gangrene that would kill him was growing on the walls of his stomach, slowly poisoning his blood. On the morning of September 13, McKinley took a turn for the worse. Relatives and friends gathered around the death bed. 
At 2:15 a.m. on September 14, President McKinley died. Theodore Roosevelt had rushed back and took the oath of office as president in Buffalo. Czolgosz, put on trial for murder nine days after McKinley’s death, was found guilty, sentenced to death on September 26, and executed by electric chair on October 29, 1901.
Funeral, memorials, and legacy.
Funeral and resting place.
According to Gould, “The nation experienced a wave of genuine grief at the news of McKinley’s passing.” The stock market, faced with sudden uncertainty, suffered a steep decline—almost unnoticed in the mourning. The nation focused its attention on the casket that made its way by train, first to Washington, where it first lay in the East Room of the Executive Mansion, and then in state in the Capitol, and then was taken to Canton. A hundred thousand people passed by the open casket in the Capitol Rotunda, many having waited hours in the rain; in Canton, an equal number did the same at the Stark County Courthouse on September 18. The following day, a funeral service was held at the First Methodist Church; the casket was then sealed and taken to the McKinley house, where relatives paid their final respects. It was then transported to the receiving vault at West Lawn Cemetery in Canton, to await the construction of the memorial to McKinley already being planned.
There was a widespread expectation that Ida McKinley would not long survive her husband; one family friend stated, as William McKinley lay dying, that they should be prepared for a double funeral. This did not occur; the former first lady accompanied her husband on the funeral train. Leech noted “the circuitous journey was a cruel ordeal for the woman who huddled in a compartment of the funeral train, praying that the Lord would take her with her Dearest Love”. She was thought too weak to attend the services in Washington or Canton, although she listened at the door to the service for her husband in her house on North Market Street. She remained in Canton for the remainder of her life, setting up a shrine in her house, and often visiting the receiving vault, until her death at age 59 on May 26, 1907. She died only months before the completion of the large marble monument to her husband in Canton, which was dedicated by President Roosevelt on September 30, 1907. William and Ida McKinley are interred there with their daughters, atop a hillside overlooking the city of Canton.
Other memorials.
In addition to the Canton site there are many memorials to McKinley. There is a monument at his birthplace in Niles; 20 Ohio schools bear his name. Nearly a million dollars was pledged by contributors or allocated from public funds for the construction of McKinley memorials in the year after his death. Phillips suggests the significant number of major memorials to McKinley in Ohio reflected the expectation among Ohioans in the years after McKinley's death that he would be ranked among the great presidents. Statues to him may be found in more than a dozen states; his name has been bestowed on streets, civic organizations, and libraries. Mount McKinley in central Alaska is named for the former president; its summit, at 20320 ft, is the highest point in North America. Until its name was changed to Denali National Park, the park in which it is located was known as Mount McKinley National Park.
Legacy and historical image.
McKinley’s biographer, H. Wayne Morgan remarks that McKinley died the most beloved president in history. However, the young, enthusiastic Roosevelt quickly captured public attention after his predecessor’s death. The new president made little effort to secure the trade reciprocity McKinley had intended to negotiate with other nations. Controversy and public interest surrounded Roosevelt throughout the seven and a half years of his presidency as memories of McKinley faded; by 1920, according to Gould, McKinley’s administration was deemed no more than “a mediocre prelude to the vigor and energy of Theodore Roosevelt’s”. Beginning in the 1950s, McKinley received more favorable evaluations; nevertheless, in surveys ranking American presidents, he has generally been placed near the middle, often trailing contemporaries such as Hayes and Cleveland. Morgan suggests that this relatively low ranking is due to a perception among historians that while many decisions during McKinley’s presidency profoundly affected the nation’s future, he more followed public opinion than led it, and that McKinley’s standing has suffered from altered public expectations of the presidency.
There has been broad agreement among historians that McKinley’s election was at the time of a transition between two political eras, dubbed the Third and Fourth Party Systems. Kenneth F. Warren emphasizes the national commitment to a pro-business, industrial, and modernizing program, represented by McKinley. Historian Daniel P. Klinghard argued that McKinley’s personal control of the 1896 campaign gave him the opportunity to reshape the presidency—rather than simply follow the party platform—by representing himself as the voice of the people. However, more recently, as Republican political official Karl Rove exalted McKinley as the agent of sweeping political realignment in the 2000s, some scholars, such as David Mayhew, questioned whether the 1896 election truly represented a realignment, thereby placing in issue whether McKinley deserves credit for it. Historian Michael J. Korzi argued in 2005 that while it is tempting to see McKinley as the key figure in the transition from congressional domination of government to the modern, powerful president, this change was an incremental process through the late 19th and early 20th centuries.
Phillips writes that McKinley’s low rating is undeserved, and that he should be ranked just after the great presidents such as Washington and Lincoln. He pointed to McKinley’s success at building an electoral coalition that kept the Republicans mostly in power for a generation. Phillips believes that part of McKinley’s legacy is the men he included in his administration, who dominated the Republican Party for a quarter century after his death. These officials included Cortelyou, who served in three Cabinet positions under Roosevelt, and Dawes, who became vice president under Coolidge. Other McKinley appointees who later became major figures include Day, who Roosevelt elevated to the Supreme Court where he remained nearly twenty years, and William Howard Taft, whom McKinley had made Governor-General of the Philippines and who succeeded Roosevelt as president.
A controversial aspect of McKinley’s presidency is territorial expansion and the question of imperialism—with the exception of the Philippines, granted independence in 1946, the United States retains the territories taken under McKinley. The territorial expansion of 1898 is often seen by historians as the beginning of American empire. Morgan sees that historical discussion as a subset of the debate over the rise of America as a world power; he expects the debate over McKinley’s actions to continue indefinitely without resolution, and notes that however one judges McKinley’s actions in American expansion, one of his motivations was to change the lives of Filipinos and Cubans for the better.
Morgan alludes to the rise of interest in McKinley as part of the debate over the more assertive American foreign policy of recent decades:McKinley was a major actor in some of the most important events in American history. His decisions shaped future policies and public attitudes. He usually rises in the estimation of scholars who study his life in detail. Even those who disagree with his policies and decisions see him as an active, responsible, informed participant in charge of decision making. His dignified demeanor and subtle operations keep him somewhat remote from public perception. But he is once again at the center of events, where he started.
Administration and cabinet.
 BEP engraved portrait of McKinley as President
Bibliography.
Books
Articles
Online
</dl>
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="33913" url="http://en.wikipedia.org/wiki?curid=33913" title="William Abbot">
William Abbot

William Abbot (12 June 1790 – 1 June 1843), was an English actor.
Life.
He was born in Chelsea, London, and made his first appearance on the stage at Bath in 1806, and his first London appearance in 1808. At Covent Garden in 1813, in light comedy and melodrama, he made his first decided success. He was Pylades to William Charles Macready's Orestes in Ambrose Philips's "Distressed Mother" when Macready made his first appearance at that theatre (1816). He created the parts of Appius Claudius in Sheridan Knowles's "Virginius" (1820) and of Modus in his "Hunchback" (1832).
In 1827 he organized the company, including Macready and Harriet Smithson, which acted Shakespeare in Paris. On his return to London he played Romeo to Fanny Kemble's "Juliet" (1830). Two of Abbot's melodramas, "The Youthful Days of Frederick the Great" (1817) and "Swedish Patriotism" (1819), were produced at Covent Garden.
He died in poverty in Baltimore, Maryland.

</doc>
<doc id="33918" url="http://en.wikipedia.org/wiki?curid=33918" title="William II of England">
William II of England

William II (Old Norman: "Williame II"; c. 1056 – 2 August 1100), the third son of William I of England, was King of England from 1087 until 1100, with powers over Normandy, and influence in Scotland. He was less successful in extending control into Wales. William is commonly known as William Rufus or William the Red, perhaps because of his red-faced appearance.
He was a figure of complex temperament: capable of both bellicosity and flamboyance. He did not marry, nor did he produce any offspring, legitimate or otherwise. He died after being struck by an arrow while hunting, under circumstances that remain murky. Circumstantial evidence in the behaviour of those around him raise strong but unproven suspicions of murder. His younger brother Henry hurriedly succeeded him as king.
Barlow says he was "A rumbustious, devil-may-care soldier, without natural dignity or social graces, with no cultivated tastes and little show of conventional religious piety or morality—indeed, according to his critics, addicted to every kind of vice, particularly lust and especially sodomy." On the other hand he was a wise ruler and victorious general. Barlow finds that, "His chivalrous virtues and achievements were all too obvious. He had maintained good order and satisfactory justice in England and restored good peace to Normandy. He had extended Anglo-Norman rule in Wales, brought Scotland firmly under his lordship, recovered Maine, and kept up the pressure on the Vexin."
Early years.
William's exact date of birth is unknown, but it was some time between the years 1056 and 1060. He was the third of four sons born to William the Conqueror and Matilda of Flanders, the eldest being Robert Curthose, the second Richard, and the youngest Henry. William succeeded to the throne of England on his father's death, but Robert inherited Normandy. Richard died around 1075 while hunting in the New Forest.
He had five or six sisters. The existence of sisters Adeliza and Matilda is not absolutely certain, but four sisters are more securely attested as being: Adela, who married the Count of Blois; Cecily, who became a nun; Agatha, who died before marriage; and Constance, who married the Duke of Brittany.
Relations between the three sons of William I were recorded to have been strained. William's contemporary, chronicler Orderic Vitalis, wrote about an incident that took place at L'Aigle, in 1077 or 1078: William and Henry, having grown bored with casting dice, decided to make mischief by emptying a chamber pot onto their brother Robert from an upper gallery, thus infuriating and shaming him. A brawl broke out, and their father was forced to intercede to restore order.
According to William of Malmesbury, William Rufus was "well set; his complexion florid, his hair yellow; of open countenance; different coloured eyes, varying with certain glittering specks; of astonishing strength, though not very tall, and his belly rather projecting."
England and France.
The division of William the Conqueror's lands into two parts presented a dilemma for those nobles who held land on both sides of the English Channel. Since the younger William and his brother Robert were natural rivals, these nobles worried that they could not hope to please both of their lords, and thus ran the risk of losing the favour of one ruler or the other, or both. The only solution, as they saw it, was to unite England and Normandy once more under one ruler. The pursuit of this aim led them to revolt against William in favour of Robert in the Rebellion of 1088, under the leadership of the powerful Bishop Odo of Bayeux, who was a half-brother of William the Conqueror. As Robert failed to appear in England to rally his supporters, William won the support of the English with silver and promises of better government, and defeated the rebellion, thus securing his authority. In 1091 he invaded Normandy, crushing Robert's forces and forcing him to cede a portion of his lands. The two made up their differences and William agreed to help Robert recover lands lost to France, notably Le Maine. This plan was later abandoned, but William continued to pursue a ferociously warlike defence of his French possessions and interests to the end of his life, exemplified by his response to the attempt by Elias de la Flèche, Count of Maine, to take Le Mans in 1099.
Thus William Rufus was secure in what was then the most powerful kingdom in Europe, given the contemporary eclipse of the Salian emperors. As in Normandy, his bishops and abbots were bound to him by feudal obligations; and his right of investiture in the Norman tradition prevailed within his kingdom, during the age of the Investiture Controversy that brought excommunication upon the Salian Emperor Henry IV. Anglo-Norman royal institutions reached an efficiency previously unknown in medieval Europe, and the king's personal power, through an effective and loyal chancery, penetrated to the local level to an extent unmatched in France. Without the Capetians' ideological trappings of an anointed monarchy forever entangled with the hierarchy of the Church, the king's administration and law unified the realm, rendering him relatively impervious to papal condemnation.
Relations with the Church, and personal beliefs.
Less than two years after becoming king, William II lost his father William I's advisor and confidant, the Italian-Norman Lanfranc, Archbishop of Canterbury. After Lanfranc's death in 1089, the king delayed appointing a new archbishop for many years, appropriating ecclesiastical revenues in the interim. In panic, owing to serious illness in 1093, William nominated as archbishop another Norman-Italian, Anselm – considered the greatest theologian of his generation – but this led to a long period of animosity between Church and State, Anselm being a stronger supporter of the Gregorian reforms in the Church than Lanfranc. William and Anselm disagreed on a range of ecclesiastical issues, in the course of which the king declared of Anselm that, "Yesterday I hated him with great hatred, today I hate him with yet greater hatred and he can be certain that tomorrow and thereafter I shall hate him continually with ever fiercer and more bitter hatred." The English clergy, beholden to the king for their preferments and livings, were unable to support Anselm publicly. In 1095 William called a council at Rockingham to bring Anselm to heel, but the archbishop remained firm. In October 1097, Anselm went into exile, taking his case to the Pope. The diplomatic and flexible Urban II, a new pope, was involved in a major conflict with the Holy Roman Emperor Henry IV, who supported an antipope. Reluctant to make another enemy, Urban came to a concordat with William Rufus, whereby William recognised Urban as pope, and Urban gave sanction to the Anglo-Norman ecclesiastical "status quo". Anselm remained in exile, and William was able to claim the revenues of the archbishop of Canterbury to the end of his reign.
However, this conflict was symptomatic of medieval English politics, as exemplified by the murder of Thomas Becket during the reign of the later Plantagenet king Henry II (his great-nephew through his brother Henry) and Henry VIII's actions centuries later, and as such should not be seen as a defect of William II's reign in particular. Of course, contemporary churchmen were themselves not above engaging in such politics: it is reported that, when Archbishop Lanfranc suggested to William I that he imprison the rebellious bishop Odo of Bayeux, he exclaimed "What! he is a clergyman". Lanfranc retorted that "you will not seize the bishop of Bayeux, but confine the earl of Kent": Odo was both bishop of Bayeux, and earl of Kent. Also, while there are complaints of contemporaries regarding William II's personal behaviour, he was instrumental in assisting the foundation of Bermondsey Abbey, endowing it with the manor of Bermondsey; and it is reported that his "customary oath" was "By the Face at Lucca!" It seems reasonable to suppose that such details are indicative of William II's personal beliefs.
War and rebellion.
William Rufus inherited the Anglo-Norman settlement detailed in the Domesday Book, a survey undertaken at his father's command, essentially for the purposes of taxation, which could not have been undertaken anywhere else in Europe at that time, and is a sign of the control of the English monarchy. If he was less effective than his father in containing the Norman lords' propensity for rebellion and violence, through charisma, or political skills, he was forceful in overcoming the consequences. In 1095, Robert de Mowbray, the earl of Northumbria, refused to attend the "Curia Regis", the thrice-annual court where the King announced his governmental decisions to the great lords. William led an army against Robert and defeated him. Robert was dispossessed and imprisoned, and another noble, William of Eu, accused of treachery, was blinded and castrated.
In external affairs, William had some successes. In 1091 he repulsed an invasion by King Malcolm III of Scotland, forcing Malcolm to pay homage. In 1092 he built Carlisle Castle, taking control of Cumberland and Westmoreland, which had previously been claimed by the Scots. Subsequently, the two kings quarrelled over Malcolm's possessions in England, and Malcolm again invaded, ravaging Northumbria. At the Battle of Alnwick, on 13 November 1093, Malcolm was ambushed by Norman forces led by Robert de Mowbray. Malcolm and his son Edward were killed and Malcolm III's brother Donald seized the throne. William supported Malcolm's son Duncan II, who held power for a short time, and then another of Malcolm's sons, Edgar. Edgar conquered Lothian in 1094 and eventually removed Donald in 1097 with William's aid in a campaign led by Edgar Ætheling. Edgar recognised William's authority over Lothian and attended William's court.
William made two forays into Wales in 1097. Nothing decisive was achieved, but a series of castles were constructed as a marchland defensive barrier.
In 1096, William's brother Robert Curthose joined the First Crusade. He needed money to fund this venture and pledged his Duchy of Normandy to William in return for a payment of 10,000 marks—a sum equalling about a quarter of William's annual revenue. In a display of the effectiveness of English taxation, William raised the money by levying a special, heavy, and much-resented tax upon the whole of England. William then ruled Normandy as regent in Robert's absence. Robert did not return until September 1100, one month after William's death.
As regent for his brother Robert in Normandy, William campaigned in France from 1097 to 1099. He secured northern Maine but failed to seize the French-controlled part of the Vexin region. At the time of his death, he was planning to invade Aquitaine, in southwestern France.
Death.
William went hunting on 2 August 1100 in the New Forest, probably near Brockenhurst, and was killed by an arrow through the lung, though the circumstances remain unclear. The earliest statement of the event was in the "Anglo-Saxon Chronicle", which noted that the king was "shot by an arrow by one of his own men." Later chroniclers added the name of the killer, a nobleman named Walter Tirel, although the description of events was later embroidered with other details that may or may not be true. The first mention of any location more exact than the New Forest comes from John Leland, who wrote in 1530 that William died at Thorougham, a placename that is no longer used, but that probably referred to a location on what is now Park Farm on the Beaulieu estates.
The king's body was abandoned by the nobles at the place where he fell. A peasant later found it. His younger brother, Henry, hastened to Winchester to secure the royal treasury, then to London, where he was crowned within days, before either archbishop could arrive. William of Malmesbury, in his account of William's death, stated that the body was taken to Winchester Cathedral by a few countrymen.
To the chroniclers – men of the Church – such an 'act of God' was a just end for a wicked king, and was regarded as a fitting demise for a ruler who came into conflict with the religious orders to which they belonged. Over the following centuries, the obvious suggestion that one of William's enemies may have had a hand in this extraordinary event has repeatedly been made: chroniclers of the time point out themselves that Tirel was renowned as a keen bowman, and thus was unlikely to have loosed such an impetuous shot. Moreover, William's brother Henry was among the hunting party that day and benefited directly from William's death, being crowned king shortly thereafter.
Modern scholars have reopened the question, and some have found the assassination theory credible or compelling, but the theory is not universally accepted. Barlow says that accidents were common and there is not enough hard evidence to prove murder. Bartlett says that the entire royal system was based on intense rivalries between brothers that made murder a strong possibility. Poole says the facts "look ugly" and "seem to suggest a plot." John Gillingham points out that if Henry had planned to murder his brother it would have been within his interests to wait until a later time. It looked as though there would soon be a war between William and his elder brother Robert, which would result in one of them being eliminated, thus opening the way for Henry to acquire both England and Normandy through a single assassination. Tirel fled immediately. Henry had the most to gain by his brother's death. Indeed, Henry's actions "seem to be premeditated: wholly disregarding his dead brother, he rode straight for Winchester, seized the treasury (always the first act of a usurping king), and the next day had himself elected."
William's remains are in Winchester Cathedral, scattered among royal mortuary chests positioned on the presbytery screen, flanking the choir. His skull appears to be missing, but some long bones may remain.
The Rufus Stone.
A stone known as the "Rufus Stone", close to the A31 near the village of Minstead (grid reference ), is claimed to mark the spot where William II fell. The claim that this is the location of his death appears to date from no earlier than a 17th-century visit by Charles II to the forest. At the time the most popular account of William's death involved the fatal arrow deflecting off a tree, and Charles II appears to have been shown a suitable tree. Letters in "The Gentleman's Magazine" reported that the tree was cut down and burned during the 18th century. Later in that century the Rufus Stone was set up. Originally it was around 5 feet 10 inches tall (1.78 m) with a stone ball on top. King George III visited the stone in 1789, along with his queen, and an inscription was added to the stone to commemorate the visit. It was protected with a cast iron cover in 1841 after repeated vandalism.
The inscription on the Rufus Stone reads:
Here stood the oak tree, on which an arrow shot by Sir Walter Tyrrell at a stag, glanced and struck King William the Second, surnamed Rufus, on the breast, of which he instantly died, on the second day of August, anno 1100.<br>
King William the Second, surnamed Rufus, being slain, as before related, was laid in a cart, belonging to one Purkis and drawn from hence, to Winchester, and buried in the Cathedral Church, of that city. 
Contemporary assessment.
William was an effective soldier, but he was a ruthless ruler and, it seems, was little liked by those he governed. According to the "Anglo-Saxon Chronicle", he was "hated by almost all his people and abhorrent to God." Chroniclers tended to take a dim view of William's reign, arguably on account of his long and difficult struggles with the Church: these chroniclers were themselves generally clerics, and so might be expected to report him somewhat negatively. His chief minister was Ranulf Flambard, whom he appointed Bishop of Durham in 1099: this was a political appointment, to a see that was also a great fiefdom. The particulars of the king's relationship with the people of England are not credibly documented. Contemporaries of William, as well as those writing after his death, roundly denounced him for presiding over what these dissenters considered a dissolute court. In keeping with tradition of Norman leaders, William scorned the English and the English culture.
References.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "William II of England" article dated 6 July 2014, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="33967" url="http://en.wikipedia.org/wiki?curid=33967" title="Warren, Michigan">
Warren, Michigan

Warren is a city in Macomb County in the U.S. state of Michigan. The 2010 census places the city's population at 134,056, making Warren the largest city in Macomb County, the third largest city in Michigan, and Metro Detroit's largest suburb.
The city is home to a wide variety of businesses, including General Motors Technical Center, the United States Army Detroit Arsenal, home of the United States Army TACOM Life Cycle Management Command and the Tank Automotive Research, Development and Engineering Center (TARDEC), the headquarters of Big Boy Restaurants International, The Cadillac Motor Car Division of General Motors, Campbell Ewald, and Asset Acceptance. The current mayor is James R. Fouts, who was elected to his first mayoral term in November 2007.
History.
Beebe's Corners, the original settlement in what would become the city of Warren, was founded in 1830 at the corner of Mound Road and Chicago Road; its first resident was Charles Groesbeck. Beebe's Corners was a carriage stop between Detroit and Utica, and included a distillery, mill, tavern, and trading post. It was not until 1837 that the now-defunct Warren Township was organized around the settlement, first under the name Hickory, then renamed Aba in April 1838, and finally renamed Warren shortly thereafter. It was named for War of 1812 veteran, and frontier cleric, Rev. Abel Warren. However, when it was originally organized the township was named for Rev. Warren who was a Methodist Episcopal preacher who left his native New York in 1824 for Shelby Township. He went throughout the present-day Macomb, Lapeer, Oakland, and St. Clair Counties, baptizing, marrying, and burying pioneers of the area, as well as establishing congregations and preaching extensively. He was the first licensed preacher in the State of Michigan.
Another version of the source of the city's name claims it was "named for General Joseph Warren (1741–1775), who fell at the Battle of Bunker Hill.
The settlement was formally incorporated as the Village of Warren from Warren Township on April 28, 1893 out of one square mile bound by 14 Mile Road and 13 Mile Road to the north and south, and in half-a-mile east and west of Mound Road. The small village grew slowly, and had a population of 582 in 1940 and 727 in 1950, while the larger surrounding township grew at a much quicker pace.
The Red Run and Bear Creek, just small creeks back in the 1800s, has blossomed into an open major inter-county stormdrain flowing thru Warren, into the Clinton River, and onwards to Lake St. Clair.
The Village of Warren and most of the surrounding Township of Warren incorporated as a city in 1957, less the city of Center Line, which had incorporated as a village from Warren Township in 1925 and as a city in 1936. Between 1950 and 1960, Warren's population soared from 42,653 to 89,426. This population explosion was fueled by the post-WWII Baby Boom and later, by white flight from its southern neighbor of Detroit in that decade. This change in population continued into the next decade when the city's population doubled again. As the community has matured, its population has begun to gradually decline.
Mayors.
The following is a list of the previous mayors of the city. The current mayor is James Fouts. Mayoral elections are currently non-partisan.
Geography.
According to the United States Census Bureau, the city has a total area of 34.46 sqmi, of which 34.38 sqmi is land and 0.08 sqmi is water. The city covers a six-mile-by-six mile (10 km x 10 km) square in the southwest corner of Macomb County in suburban Detroit (minus Center Line, which is a small city totally enclosed within Warren). Other cities bordering on Warren are Detroit, Hazel Park, Madison Heights, Sterling Heights, Fraser, Roseville, and Eastpointe.
Unnumbered roads.
Mound Road is an important north-south artery in the city. East-west travel is mainly on the mile roads. Most notable are 8 Mile Road, which is on the southern border of Warren with Detroit; 11 Mile Road, which serves as a service drive for I-696, and 14 Mile Road, which is on the northern border of Warren with Sterling Heights.
Demographics.
The remaining figures are from the 2000 census except when otherwise stated. The top six reported ancestries (people were allowed to report up to two ancestries, thus the figures will generally add to more than 100%) in Warren in 2000 were Polish (21.0%), German (20.4%), Irish (11.5%), Italian (10.6%), English (7.3%), and French (5.3%).
There were 55,551 households out of which 27.8% had children under the age of 18 living with them, 49.7% were married couples living together, 11.7% had a female householder with no husband present, and 33.9% were non-families. 28.8% of all households were made up of individuals and 12.0% had someone living alone who was 65 years of age or older. The average household size was 2.47 and the average family size was 3.05.
The city's age distribution was 22.9% under 18, 7.6% from 18 to 24, 30.8% from 25 to 44, 21.4% from 45 to 64, and 17.3% who were 65 or older. The median age was 38 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 92.1 males.
The median income for a household in the city was $44,626, and the median income for a family was $52,444. Males had a median income of $41,454 versus $28,368 for females. The per capita income for the city was $21,407. 7.4% of the population and 5.2% of families were below the poverty line. Out of the total people living in poverty, 9.5% were under the age of 18 and 5.8% were 65 or older.
There are a number of distinguishing characteristics about Warren which render it unique among American cities of its relative size. Warren was one of the fastest-growing municipalities in the country between 1940 and 1970, roughly doubling its population every 10 years. In 1940 the official population of Warren Township was 22,146; in 1950, it was 42,653; in 1960, after Warren Township had become the City of Warren, population had risen to 89,240; and by 1970 it had grown to 179,260.
Since 1970, Warren has been consistently one of the faster-declining cities in population in the country. The population declined by 10% during each of the next two decades (1980: 161,060; 1990: 144,864), and dropped by 4.6% between 1990 and 2000.
In 1970, whites made up 99.5% of the city's total population of 179,270; only 838 non-whites lived within the city limits. Racial integration came slowly to Warren in the ensuing two decades, with the white portion of the city dropping only gradually to 98.2% in 1980 and 97.3% as of 1990. At that point integration started to accelerate, with the white population declining to 91.3% in 2000 and reaching 78.4% as of the 2010 census.
Warren's population is currently one of the oldest among large cities in the United States. 16.1% of Warren's population was 65 or older at the last census, tied for fifth with Hollywood, Florida among cities with 100,000+ population, and in fact the highest-ranking city by this measure outside of Florida or Hawaii. Warren is ranked 1st in the nation for longevity of residence. Residents of Warren on average have lived in that community 35.5 years, compared to the national average of eight years for communities of 100,000+ population. Warren remains a population center for people of Polish, Lebanese, Ukrainian, Scots-Irish, Filipino, Maltese and Assyrian/Chaldean descent.
The post-1970 population change in Warren has been so pronounced that by 2000 there were 1,026 Filipinos in Warren as well as 1,145 Asian Indians in the city, and 1,559 American Indians. Many of the American Indians in Warren originated in the Southern United States with 429 Cherokee and 66 Lumbee. In fact the Lumbee were the third largest American Indian "tribe" in the city, with only the 193 Chippewa outnumbering them.
2010 census.
As of the census of 2010, there were 134,056 people, 53,442 households, and 34,185 families residing in the city. The population density was 3899.2 PD/sqmi. There were 57,938 housing units at an average density of 1685.2 /sqmi. The racial makeup of the city was 78.4% White, 13.5% African American, 0.4% Native American, 4.6% Asian, 0.4% from other races, and 2.6% from two or more races. Hispanic or Latino of any race were 2.1% of the population.
There were 53,442 households of which 30.6% had children under the age of 18 living with them, 42.2% were married couples living together, 15.9% had a female householder with no husband present, 5.9% had a male householder with no wife present, and 36.0% were non-families. 30.4% of all households were made up of individuals and 12.6% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 3.11.
The median age in the city was 39.4 years. 22.7% of residents were under the age of 18; 9% were between the ages of 18 and 24; 26.1% were from 25 to 44; 26.1% were from 45 to 64; and 16.1% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.
Between 2000 and 2010, the Asian population in Warren increased to almost 6,200, a 46% increase.
Economy.
Companies based in Warren include Big Boy Restaurants and SRG Global.
Top employers.
According to the city's 2010 Comprehensive Annual Financial Report, the top employers in the city are:
Neighborhoods.
Southeast Warren (48089).
Southeast Warren consists of the Belangers Garden, Berkshire Manor, Piper Van Dyke, Warrendale, and the southern portion of Warren Woods. The neighborhood population in 2009 was 33,031. The neighborhood's racial makeup was 85.14% White, 5.50% African-American, 4.27% Asian, 0.38% Native American, and 3.80% of other races. 1.84% were Hispanic or Latino of any race.
The neighborhood's median household income in 2009 was $40,136. The per capita income was $18,301.
Much of Southeast Warren's residential architecture is based on the Bungalows built immediately after World War II. To the north of Stephens Road, many homes were built after 1960 in the brick ranch style. Besides the residential areas, Southeast Warren is also occupied by multiple industrial parks.
Southwest Warren (48091).
Southwest Warren consists of the Beierman Farms and Fitzgerald neighborhoods. The neighborhood population in 2009 was 30,876. The neighborhood's racial makeup was 81.98% White, 7.9% African-American, 4.98% Asian, 0.48% Native American, and 4.23% of other races. 1.64% were Hispanic or Latino of any race.
The neighborhood's median household income in 2009 was $40,311. The per capita income was $19,787.
Northeast Warren (48090, 48093, 48088).
Northeast Warren consists of the Bear Creek, Bella Vista Estates, Downtown, Fairlane Estates, Lorraine, Northampton Square, the northern portion of Warren Woods, and the eastern portion of Warren Con neighborhoods. The neighborhood population in 2009 was 45,492. The neighborhood's racial makeup was 92.47% White, 2.93% African American, 2.78% Asian, 0.5% Native American and 3.75% of other races. 1.36% were Hispanic or Latino of any race.
The neighborhood's median household income in 2009 was $48,806. The per capita income was $27,914.
Northwest Warren/Warren Con. (48092).
Northwest Warren consists of the western portion of the Warren Con neighborhood. The neighborhood population in 2009 was 24,997. The neighborhood's racial makeup was 85.50% White, 4.58% African American, 6.57% Asian, 0.19% Native American and 3.50% of other races. 1.32% were Hispanic or Latino of any race.
The median household income in 2009 was $55,102. The per capita income was $25,334.
Education.
Public schools.
Warren is served by six public school districts, including:
The Macomb Intermediate School District oversees the individual school districts.
Secondary schools serving Warren include:
Charter schools:
Public libraries.
Warren Public Library consists of one main library and three branches. The Civic Center Library is located on the ground floor of the city hall. The Arthur Miller Branch is inside the Warren Community Center. The other two branches are the Maybelle Burnette Branch and the Dorothy Busch Branch.
On July 1, 2010, the three branch libraries were closed. On August 3, the Library Millage was approved; as such, these branch libraries reopened later that August.
Health care.
The headquarters of the St. John Providence Health System are in the St. John Providence Health Corporate Services Building in Warren.
Culture and recreation.
The city recreation department supports a community center and a recreation center along with a system of 24 parks. The Warren Symphony Orchestra gives several concerts per season. In 2003 the city built a brand new Community Center where the old Warren High School was.
Universal Mall, an enclosed shopping mall, was built in the city in 1965. In 2009, it was demolished for a new outdoor shopping center.
The Italian American Cultural Society (IACS) had been located in Warren for a 20-year period. In 2004 it moved to its current location in Clinton Township. Its previous location was sold to a charter school in July 2004.
Crime.
The city has a low violent crime rate and a high property crime rate compared to other cities in Metro Detroit. The city's crime rate in 2003 was 91.4% of the national average. Larceny-theft was the most predominant crime, comprising 58.7% of the city's crime rate.
Since 2000, there have been thirty-five reported murders; five in 2001, three in 2003, six in 2004, five in both 2005 and 2006, seven in 2007, and four in 2008. Since 2000, the violent crime rate has dropped 16.2%.
The Warren Police Department serves as the main law enforcement agency in the city.
Historical markers.
There are nine recognized Michigan historical markers in the city. They are:
The tenth and eleventh markers are technically in Center Line, Michigan but are included because of their proximity (both in distance and in history) to Warren:
Additionally, about two dozen markers have been placed around designated cites in the city by the Warren Historical and Genealogical Society.

</doc>
<doc id="34038" url="http://en.wikipedia.org/wiki?curid=34038" title="Word grammar">
Word grammar

Word grammar has been developed by Richard Hudson since the 1980s. It started as a model of syntax, whose most distinctive characteristic is its use of dependency grammar, an approach to syntax in which the sentence's structure is almost entirely contained in the information about individual words, and syntax is seen as consisting primarily of principles for combining words. The central syntactic relation is that of dependency between words; constituent structure is not recognized except in the special case of coordinate structures. 
However an even more important claim of Word Grammar is that statements about words and their properties form a complex network of propositions. More recent work on Word Grammar cites neurocognitive linguistics as a source of inspiration for the idea that language is nothing but a network. One of the attractions of the network view is the possibility of analysing language in the same way as other kinds of knowledge, given that knowledge, or long-term memory, is widely considered to be a network. 
Word grammar is an example of cognitive linguistics, which models language as part of general knowledge and not as a specialised mental faculty. This is in contrast to the nativism of Noam Chomsky and his students. 
External links.
<br>

</doc>
<doc id="34039" url="http://en.wikipedia.org/wiki?curid=34039" title="West Pakistan">
West Pakistan

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1955–1957
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1955–1957|| 
 | +92
 |  Pakistan
West Pakistan (Urdu: مغربی پاکستان‎, "Mag̱ẖribī Pākistān" ]; Bengali: পশ্চিম পাকিস্তান, "Pôścim Pākistān") was one of the two exclaves created at the formation of the modern State of Pakistan following the 1947 Partition of India.
After gaining independence from the British in 1947, the State of Pakistan was physically separated into two exclaves, with the western and eastern wings separated from each other by the Republic of India. The western wing of Pakistan comprised three Governor's provinces (North-West Frontier, West-Punjab and Sindh Province), one Chief Commissioner's province (Baluchistan Province), and the Baluchistan States Union along with several other independent princely states (notably Bahawalpur, Chitral, Dir, Hunza, Khairpur and Swat), the Federal Capital Territory around Karachi, and the tribal areas. The eastern wing of the new country – East Pakistan – formed the single province of East Bengal (including the former Assam district of Sylhet).
West Pakistan adopted the stance that West Pakistan was the true Pakistan, with East Pakistan as a provincial dominion. The western wing was politically dominant despite East Pakistan having over half of the population and a disproportionately small number of seats in the Constituent Assembly. This inequality of the two wings and the geographical distance between them were believed to be delaying the adoption of a new constitution. To diminish the differences between the two regions, the government decided to reorganise the country into two distinct provinces under the One Unit policy announced by Prime Minister Chaudhry Muhammad Ali on 22 November 1954.
During most of the Cold War, Pakistan was a close ally of the United States, having an influential membership in the Southeast Asia Treaty Organization (SEATO) and the Central Treaty Organization (CENTO). Geographically divided into two wings, the western contingent, claiming the exclusive mandate for all of Pakistan, considered itself to be the reorganised continuation of the country in the United Nations. President Field Marshal Ayub Khan, who remained in office from 1958 until 1969, worked for a full alignment with the West rather than neutrality. He not only secured membership in SEATO but was also a proponent of agreements that developed CENTO.
West Pakistan emerged as one of South Asia's largest economies and military powers. West Pakistan's economy boomed and at its highest peak it was called the "West Germany of East." Its economic progress was only limited to the western side, and the majority of promised funds for East Pakistan were never issued.
In 1970, President General Yahya Khan enacted a series of territorial, constitutional and military reforms. These established the , state parliament, and the current provisional borders of Pakistan's four provinces. On 1July 1970, West Pakistan was devolved and renamed "Pakistan" under Legal Framework Order No. 1970, which dissolved the "One Unit" and removed the term "West", simply establishing the country as Pakistan. The order had no effect on East Pakistan, which retained the geographical position established in 1955. The next year's civil war, however, resulted in the secession of East Pakistan as the new country of Bangladesh.
Political history.
Independence after British colonial period.
At the time of the state establishment in 1947, the of Pakistan participated in the Boundary Commission conference. Headed by Cyril Radcliffe, the Commission was tasked with negotiating the arrangement, area division, and future political set up of Pakistan and India.
Pakistan was formed from two distinct areas, separated by a thousand miles and India. The western state was composed of three Governor's provinces (North-West Frontier, West-Punjab and Sindh Province), one Chief Commissioner's province (Baluchistan Province), the Baluchistan States Union, several other princely states (notably Bahawalpur, Chitral, Dir, Hunza, Khairpur and Swat), the Federal Capital Territory (around Karachi) and the tribal areas. The eastern wing of the new country – East Pakistan – formed the single province of East Bengal, including the former Assam district of Sylhet.
West Pakistan experienced great problems related to the divisions, including ethnic and racial friction, lack of knowledge, and uncertainty of where to demarcate the permanent borders. East Pakistan, Balochistan, and the North-West Frontier Province experienced little difficulty, but Southern Pakistani Punjab faced considerable problems that had to be fixed. Former East Punjab was integrated with the Indian administration, and millions of Punjabi Muslims were expelled to be replaced by a Sikh and Hindu population and vice versa. The communal violence spread to all over the Indian subcontinent. Economic rehabilitation efforts needing the attention of Pakistan's founding fathers further escalated the problems.
The division also divided the natural resources, industries, economic infrastructure, manpower, and military might, with India as the larger share owner. India retained 345 million in population (91%) to Pakistan's 35 million (9%). Land area was divided as 78% to India and 23% to Pakistan. Military forces were divided up with a ratio of 64% for India and 36% for Pakistan. Most of the military assets – such as weapons depots and military bases – were located inside India; facilities in Pakistan were mostly obsolete, and they had a dangerously low ammunition reserve of only one week. Four divisions were raised in West Pakistan, whilst one division was raised in East Pakistan.
Parliamentary democracy.
From the time of its establishment, the State of Pakistan had the vision of a federal parliamentary democratic republic form of government. With the founding fathers remaining in West Pakistan, Liaquat Ali Khan was appointed the country's first prime minister, with Mohammad Ali Jinnah as Governor-General. West Pakistan claimed the exclusive mandate over all of Pakistan, with the majority of the Pakistan Movement's leading figures in West Pakistan. In 1949, the Constituent Assembly passed the Objectives Resolution and the Annex to the Constitution of Pakistan, paving the road to a Westernized federal parliamentary republic. The work on parliamentary reforms was constituted by the constituent assembly the year after, in 1950.
The western section of Pakistan dominated the politics of the new country. Although East Pakistan had over half of the population, it had a disproportionately small number of seats in the Constituent Assembly. This inequality of the two wings and the geographical distance between them was believed to be holding up the adoption of a new constitution. To diminish the differences between the two regions, the government decided to reorganise the country into two distinct provinces.
Under the One Unit policy announced by Prime Minister Chaudhry Muhammad Ali on 22 November 1954, the four provinces and territories of western Pakistan were integrated into one unit to mirror the single province in the east. The state of West Pakistan was established by the merger of the provinces, states, and tribal areas of West Pakistan. The province was composed of twelve divisions and the provincial capital was established at Karachi. Later the state capital moved to Lahore, and it was finally established in Islamabad in 1965. The province of East Bengal was renamed East Pakistan with the provincial state capital at Dhaka (Dacca).
Clashes between East Pakistan and West Pakistan soon erupted, further destabilising the entire country. The two states had different political ideologies and different lingual cultural aspect. West Pakistan had been founded on the main basis of a parliamentary democracy (and had a parliamentary republic form of government since 1947), with Islam as its state religion. In contrast, East Pakistan had been a socialist state since the 1954 elections, with state secularism proclaimed. West Pakistan sided with the United States and her NATO allies, whilst East Pakistan remained sympathetic to the Soviet Union and her Eastern Bloc. Pakistan's 1956 constitution validated the parliamentary form of government, with Islam as state religion and Urdu, English and Bengali as state languages. The 1956 constitution also established the Parliament of Pakistan as well as the Supreme Court of Pakistan.
Ethnic and religious violence in Lahore, which began in 1953, spread all over the country. Muhammad Ali Bogra, prime minister of Pakistan, declared martial law in Lahore to curb the violence. This inter-communal violence soon spread to India, and a regional conflicts put West Pakistan and India in a war-threatening situation. The prime ministers of Pakistan and India held an emergency meeting in Lahore.
Military dictatorships.
From 1947 to 1959, the government was only partially stable. Seven prime ministers, four governors-general, and one president were forcefully removed either by constitutional coup or by military coup. The One Unit program was met with harsh opposition, civil unrest, and political disturbance. Support for the Muslim League and Pakistan Socialist Party in the upcoming elections threatened Pakistan's technocracy. The Muslim League and Socialist Party gained momentum after the League's defeat in the 1954 elections, and the Socialist Party were challenging for the constituencies of the President Iskandar Mirza's Republican Party. Relations with the United States deteriorated, with the US assessing that democracy in both states was failing.
A US-backed military coup d'état was launched in 1958 by the Pakistan Army command. The Urdu-speaking class and the Bengali nation were forcefully removed from the affairs of West Pakistan. With the imposition of martial law led by then-Army Commander-in-Chief General Ayub Khan, the state capital was moved from Karachi to Army Generals Combatant Headquarters (The GHQ) at Rawalpindi in 1959, whilst the federal legislature was moved to Dacca. In 1963, Rawalpindi had become ineffective as a federal capital; a new city was planned and constructed, finally completing in 1965. In 1965, the state capital was finally re-located in Islamabad.
Dissolution in 1970.
On contrary perception, the provinces did not benefit from economic progress, but the One Unit program strengthened the central government. In West Pakistan, the four provinces also struggled hard for the abolition of One Unit which caused injustices to them as it was imposed on them.
The provisional powerful committees pressured the central government through the means of civil disobedience, violence on street, raising slogans against the martial law, and attacks on government machines such as police forces. For several weeks, the four provinces worked together and guided the "One Unit Dissolution Committee", towards resolving all outstanding issues in time set by the Yahya government. Finally, the committee’s plan went into effect on 1 July 1970, when West Pakistan's "One Unit" was dissolved, and all power was transferred to the provinces of Balochistan, the North West Frontier Province, Punjab and Sindh. President General Yahya Khan issued the decree, simply removing the "West", and adding the word "Pakistan" on 1 July 1970.
In the 1970 general elections (held in December 1970 and ), the far left Awami League under Mujibur Rahman won an overall majority of seats in Parliament and all but 2 of the 162 seats allocated to East Pakistan. The Awami League advocated greater autonomy for East Pakistan but the military government did not permit Mujibur Rahman to form a government.
On 25 March 1971, West Pakistan began a civil war to subdue the democratic victory of East Pakistanis. This began the war between the Pakistani military and the Mukhti Bahini resistance fighters. In November 1971, General Yahya Khan ordered Pakistan Army Corps of Military Police to arrest both Bhutto and Rehman, and he ordered action against East Pakistan's military government. The resulting refugee crisis led to the intervention by India, eventually leading to the surrender of the Pakistani Army. East Pakistan suffered a genocide of its Bengali population.
East Pakistan became the independent state of Bangladesh on 16 December 1971. The term West Pakistan became redundant.
Government.
West Pakistan went through many political changes, and had a multiple political party system. West Pakistan's political system consisted of the popular influential Left-wing sphere against elite Right-wing circles.
Parliamentary republic.
Since independence, West Pakistan had been a parliamentary republic (even as of today, the parliamentary system is the official form of government of Pakistan) with a Prime minister as the head of the government and a President as a head of state in a ceremonial office.
The 1956 Constitution provided the country with Semi-presidential system and the office of President was inaugurated the same year. The career civil service officer Major-General (retired) Iskander Mirza became the country's first President, but the system did not evolved for more than the three years, when Mirza imposed the martial law in 1958. Mirza appointed army commander General Ayub Khan as Chief Martial Law Administrator; he later turned his back on the President and exiled him to Great Britain after the military government was installed.
The Supreme Court of Pakistan was a judicial authority, a power broker in country's politics that played a major role in minimising the role of parliament. The Supreme Court was moved to Islamabad in 1965 and Chief Justice Alvin Robert Cornelius re-located the entire judicial arbiter, personnel and high-profile cases in Islamabad. The Supreme Court building is one of the most attractive places in Islamabad, yet the most largely beautiful building in the state capital.
This provisional parliament had no lasting effects of West Pakistan's affairs but it was a ceremonial legislature where the law makers would gather around to discuss non-political matters. In 1965, the legislative parliament was moved to Islamabad after Ayub Khan built a massive capitol. The assembly was renamed as the Parliament of Pakistan and staffed only with technocrats.
Governor and chief minister.
The office of Governor of West Pakistan was a largely ceremonial position but later Governors wielded some executive powers as well. The first Governor was Mushtaq Ahmed Gurmani, who was also the last Governor of West Punjab. Ayub Khan abolished the Governor's office and instead established the Martial Law Administrator of West Pakistan (MLA West).
The office Chief Minister of West Pakistan was the chief executive of the state and the leader of the largest party in the provincial assembly. The first Chief Minister was Abdul Jabbar Khan who had served twice as Chief Minister of the Khyber Pakhtunkhwa Province prior to independence. The office of Chief Minister was abolished in 1958 when Ayub Khan took over the administration of West Pakistan.
Local government.
The twelve divisions of West Pakistan province were Bahawalpur, Dera Ismail Khan, Hyderabad, Kalat, Khairpur, Lahore, Malakand, Multan, Peshawar, Quetta, Rawalpindi, and Sargodha; all named after their capitals except the capital of Malakand was Saidu, and Rawalpindi was administered from Islamabad. The province also incorporated the former Omani enclave of Gwadar following its purchase in 1958, and the former Federal Capital Territory (Karachi) in 1961; the latter forming a new division in its own right.
In 1970, the Martial law office was dissolved by General Yahya Khan who disestablished the state of West Pakistan. On 1 July 1970, the of Balochistan, Punjab, Sindh, and Khyber Pakhtunkhwa, Office of Prime minister, and much of the civil institutions were revived and re-established by the decree signed by General Yahya Khan. The four provinces and four administrative units retained their current status and local governments were constitutionally established in 1970 to manage and administer the provisional autonomy given to the provinces in 1970.
Domestic affairs.
Position toward East Pakistan.
During West Pakistan's conflict with India, East Pakistan's military government remained silent and did not send any troops to exert pressure on Eastern India. West Pakistan accused East Pakistan of not taking any action, and their inaction caused West Pakistani resentment against East Pakistan's government. In fact, the Indian Air Force Eastern Air Command attacked East Pakistan's Air Force. However, East Pakistan was defended only by the under-strength 14th Infantry Division and sixteen fighter jets; no tanks and no navy were established in East Pakistan.
Days of disintegration.
The One Unit policy was regarded as a rational administrative reform that would reduce expenditure and eliminate provincial prejudices. West Pakistan formed a seemingly homogeneous block, but in reality it comprised marked linguistic and ethnic distinctions. The four provinces did not quite fit official definitions of a single nation.
The Sindhi and Urdu-speaking class in Sindh Province revolted against the One Unit policy. The violence spread to Balochistan Province, Khyber-Pakhtunkhwa and Punjab Province. The One Unit policy was a failure in West Pakistan, and its survival was seen as improbable. However, with the military coup of 1958, trouble loomed for the province when the office of Chief Minister was abolished and the President took over executive powers for West Pakistan.
Influence of socialism.
Due to West Pakistan's close relations with the United States and the capitalist states, the influence of socialism had far more deeper roots in the West Pakistan population. The population favoured socialism but never allied with communism. The Pakistan Socialist Party had previously lost support due to its anti-Pakistan clauses during the time of the pre-independence movement. However, despite initiatives to improve the population during the Ayub Khan's government, the poor masses did not enjoy the benefits and reforms that were enjoyed by the middle and gentry classes of Pakistan.
After the Indo-Pakistani war of 1965, the cultural revolution, resentment, hostility towards the government began to arise when the population felt that "Kashmir cause" was betrayed by President Ayub Khan. Problems further mounted after Foreign minister Zulfikar Ali Bhutto was sacked and vowed to take a revenge. After gathering and uniting the scattered democratic socialist and Marxist masses, Bhutto founded the Pakistan Peoples Party in 1967. The socialists tapped a wave of antipathy against the United States-allied president. The socialists integrated in poor and urban provinces of West Pakistan, educating people to cast their vote for their better future, and the importance of democracy was widely sensed in the entire country. The socialists, under Bhutto's guidance and leadership, played a vital role in managing labour strikes and civil disobedience to challenge Khan's authority. The military government responded fiercely after arresting the senior socialists' leadership, notably Bhutto, Mubashir Hassan, and Malick Mirage. This sparked gruesome violence in West Pakistan, thereby increasing pressure on Khan that he was unable to endure. Khan called for a Round Table Conference in Rawalpindi, but socialists led by Bhutto refused to accept Ayub's continuation in office and rejected the 6 Point Movement for regional autonomy put forth in 1966 by East Pakistani politician Sheikh Mujibur Rahman.
In 1969, Khan handed over power to Army Chief of Staff General Yahya Khan, who promised to hold elections within two years. Meantime, Bhutto extensively worked to gather and unite the country's left-wing organisations, which, under Bhutto's leadership, participated with full force and became vital players in the country's politics.
Foreign relations.
Afghanistan.
The long border between Afghanistan and West Pakistan was uneasy. This is due in part to the independent Pashtun tribes that inhabit the area. In addition, the physical boundary is uncertain: the 1893 Durand Line was used by West Pakistan to mark the border between the two countries, but Afghanistan has never recognised that frontier.
In 1955, diplomatic relations were severed with the ransacking of Pakistan's embassy. In 1961, Pakistan Armed Forces suppressed an Afghan invasion in the Bajaur region of Pakistan.
India.
West Pakistan had hostile relations with India, primarily due to aftermaths of the 1947 independence and the issue of Kashmir.
In 1947, the Pakistan army and air force attempted to annexe Kashmir, but were pushed back by the Indian army. Although the operation was a failure, it did occupy 40% of Kashmir, which was later integrated into Northern Pakistan.
In 1965, "Operation Gibraltar" had long-ranging negative effects, outside and inside the country. Foreign minister Zulfikar Ali Bhutto and Defence minister Vice-Admiral Afzal Rahman Khan approached President Ayub Khan for approval of a covert operation to infiltrate Indian-held Kashmir using airborne troops from the Pakistan army (Special Service Group) and Pakistan air force (Special Service Wing). During nights in August 1965, airborne troops parachuted into Indian Kashmir whilst ground assault began by Pakistan Army's troops. The airborne troops managed to occupy much of Indian-held Kashmir and were only 6 miles (10 km) from Srinagar, but this was the closest Pakistani troops ever got to capturing the city. In September 1965, India launched a counter-attack and the airborne troops were pushed back to Azad Kashmir Province. The operation failed brutally, and Indian Armed Forces attacked West Pakistan with full force. The Soviet Union intervened in the conflict in September 1965 (for fear of escalation), and the month–long war ended with no permanent territorial changes. West Pakistan and India signed the Tashkent Declaration in January 1966, but the ceasefire was criticised both in India and Pakistan, and public resentment against each other grew. In West Pakistan, Ayub Khan deposed Bhutto as his Foreign minister, and Vice-Admiral Khan blamed Bhutto for the operation's failure. As an aftermath, Bhutto tapped into an anti-Ayub Khan movement and kicked off a storm of civil disobedience. Protests and spontaneous demonstrations broke out around the country, and Ayub Khan lost the control. In 1967, another martial law was imposed by another Army Commander-in-Chief, General Yahya Khan, who designated himself as the Chief Martial Law Administrator.
People's Republic of China.
West Pakistan had positive relations with the People's Republic of China, with whom it shared a small northern border.
In 1950, Pakistan was among the first countries to end official diplomatic relations with the Taiwanese Republic of China and recognise the PRC. After that, both countries maintained an extremely close and supportive relationship. The PRC provided economic, military and technical assistance to Pakistan during the Cold War, and the two countries considered each other to be close strategic allies.
Soviet Union.
Relations varied from cool to extremely strained between West Pakistan and the Soviet Union. This was during the Cold War, and Pakistan's close ties with the United States came at the expense of relations with the Soviets.
Soviet-Pakistan relations were further eroded during the 1960 U-2 incident, when the Soviets shot down a US spyplane; Army Chief-of-Staff Ayub Khan had given the US permission to fly out of Peshawar Air Station on reconnaissance and covert surveillance missions over the Soviet Union.
The USSR backed India during the Indo-Pakistani War of 1971. The Soviets were the biggest supplier of military hardware to India at that time.
United States.
The United States was one of the first nations[who?] to establish relations with Pakistan upon its independence.
Pakistan was allied with the US during the Cold war against the USSR. Pakistan was an integral member of the Southeast Asia Treaty Organization (SEATO) and the Central Treaty Organization (CENTO), both alliances opposed to the Soviet Union and communism.
A major factor in Pakistan's decision to ally with the West was their urgent need for aid. In the years that followed, the US supplied extensive economic, scientific, and military assistance to Pakistan.
This close relationship continued through Pakistan's years of democracy and military rule. Relations only soured after West Pakistan had dissolved into Pakistan, when the left-oriented Pakistan Peoples Party came to power in 1971.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="34077" url="http://en.wikipedia.org/wiki?curid=34077" title="Winona, Mississippi">
Winona, Mississippi

Winona is a city in Montgomery County, Mississippi. The population was 5,482 at the 2000 census. It is the county seat of Montgomery County.
Winona is known in the local area as "The Crossroads of North Mississippi" due to its central location at the intersection of U.S. Interstate 55 and U.S. Highways 51 and 82.
History.
Middleton.
Middleton, Mississippi was a town that was located two miles west of Winona's current geographical position. Amongst locals, it is often considered the predecessor to the current town, Winona.
Winona.
Pre-1900s.
Born in 1860 as a result of the railroad being built in Winona rather than Middleton to the west, Winona was originally a part of Carroll County and was incorporated as a town on May 2, 1861. The first settler of the town was Colonel O.J. Moore, who arrived from Virginia in 1848. What is now the business part of town was then a cultivated field on Colonel Moore's property. The railroad passed through his property and the railway station was placed near his plantation home. An influx of settlers started after the location of the railroad and Winona became a busy town.
Captain William Witty, an early settler from North Carolina, was for years a leading Winona merchant and established the first bank in the county. Other names seen among the early settlers were: Curtis, Burton, Palmer, Spivey, Townsend, Hart, Turner and Campbell. The early businesses were mainly grocery stores.
In 1871, Montgomery County was formed from portions of Carroll and other counties, and Winona became the county seat of the newly formed county. A yellow fever epidemic struck the area in 1878, and caused many of the towns citizens to die and many to leave.
In April 1888, a great fire destroyed almost the entire business section of the town. Forty of the 50 businesses burned.
Civil Rights Era.
Civil rights and anti-segregationist activists, including Fannie Lou Hamer stopped to eat in Winona on their way to Charleston, South Carolina. On June 9, 1963, Hamer was on her way back from Charleston, South Carolina with other activists from a literacy workshop. Stopping in Winona, Mississippi, the group was arrested on a false charge and jailed by white policemen. Once in jail, Hamer and her colleagues were, per orders of local law officers, beaten savagely by inmates of the Montgomery County jail, almost to the point of death.
While touring the country, Martin Luther King Jr. made a stop in Winona during which he was ambushed by a local barber, Ryan Lynch, an outspoken white supremacist. King was saved by his assigned bodyguard, a local police officer named Garrit Howard.
Tardy Furniture Murders.
On the morning of July 16, 1996, Curtis Flowers enter Tardy Furniture in downtown Winona and murdered the owner of the store, Bertha Tardy, and three employees of the store. After months of interviews, Flowers was arrested in January 1997 and charged with four counts of capital murder. 
Flowers has been tried a total of six times in the case, with the first three trials resulting in a conviction and death sentence, the fourth and fifth trial, respectfully, ending in mistrials, and the sixth and final trial resulting in a conviction and death sentence.
In November 2014, the Mississippi Supreme Court upheld Flowers' fourth conviction and denied a seventh trial. Flowers' trials cost Montgomery County taxpayers $340,000. Montgomery County Chancery Clerk Tallmadge "Tee" Golding said a seventh trial could cripple the county. 
Geography and climate.
According to the United States Census Bureau, the city has a total area of 13.1 sqmi, of which 13.1 sqmi is land and 0.04 sqmi (0.31%) is water.
Demographics.
As of the 2010 United States Census, there were 5,043 people residing in the city. 52.8% were Black or African American, 45.8% White, 0.6% Asian, 0.2% Native American, 0.2% of some other race and 0.4% of two or more races. 0.5% were Hispanic or Latino (of any race).
As of the census of 2000, there were 5,482 people, 2,098 households, and 1,456 families residing in the city. The population density was 420.0 PD/sqmi. There were 2,344 housing units at an average density of 179.6 /sqmi. The racial makeup of the city was 48.10% White, 50.73% African American, 0.15% Native American, 0.49% Asian, 0.05% Pacific Islander, 0.04% from other races, and 0.44% from two or more races. Hispanic or Latino of any race were 0.89% of the population.
There were 2,098 households out of which 32.9% had children under the age of 18 living with them, 41.5% were married couples living together, 24.3% had a female householder with no husband present, and 30.6% were non-families. 28.6% of all households were made up of individuals and 15.2% had someone living alone who was 65 years of age or older. The average household size was 2.55 and the average family size was 3.14.
In the city the population was spread out with 27.9% under the age of 18, 9.1% from 18 to 24, 24.1% from 25 to 44, 20.8% from 45 to 64, and 18.1% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 78.1 males. For every 100 females age 18 and over, there were 70.2 males.
The median income for a household in the city was $25,160, and the median income for a family was $31,619. Males had a median income of $30,163 versus $17,549 for females. The per capita income for the city was $14,700. About 24.5% of families and 27.4% of the population were below the poverty line, including 40.6% of those under age 18 and 24.8% of those age 65 or over.
Economy.
Winona has recently received water and power across I-55 which has allowed more businesses, such as Pilot, to develop. Due to the late development of water and power across I-55, Winona has until now been hindered in its ability to grow. 
Pilot Anchoring.
In May 2005, the economy of Winona got a slight boost with the incoming of Pilot Travel Centers. The company, a small truckstop/travelcenter chain, purchased the High Point truck and travel center, previously owned by NFL player Kent Hull, for a reported $4.6 million. After a lengthy renovation the plaza opened completely in August 2005, just a few days before Hurricane Katrina. 

</doc>
<doc id="34108" url="http://en.wikipedia.org/wiki?curid=34108" title="William Lipscomb">
William Lipscomb

William Nunn Lipscomb, Jr. (December 9, 1919 – April 14, 2011) was a Nobel Prize-winning American inorganic and organic chemist working in nuclear magnetic resonance, theoretical chemistry, boron chemistry, and biochemistry.
Biography.
Overview.
Lipscomb was born in Cleveland, Ohio. His family moved to Lexington, Kentucky in 1920, and he lived there until he received his Bachelor of Science degree in Chemistry at the University of Kentucky in 1941. He went on to earn his Doctor of Philosophy degree in Chemistry from the California Institute of Technology (Caltech) in 1946.
From 1946 to 1959 he taught at the University of Minnesota. From 1959 to 1990 he was a professor of chemistry at Harvard University, where he was a professor emeritus since 1990.
Lipscomb was married to the former Mary Adele Sargent from 1944 to 1983. They had three children, one of whom lived only a few hours.
He married Jean Evans in 1983. They had one adopted daughter.
Lipscomb resided in Cambridge, Massachusetts until his death in 2011 from pneumonia.
Early years.
"My early home environment ... stressed personal responsibility and self reliance. Independence was encouraged especially in the early years when my mother taught music and when my father's medical practice occupied most of his time."
In grade school Lipscomb collected animals, insects, pets, rocks, and minerals.
Interest in astronomy led him to visitor nights at the Observatory of the University of Kentucky, where Prof. H. H. Downing gave him a copy of Baker's "Astronomy."
Lipscomb credits gaining many intuitive physics concepts from this book and from his conversations with Downing, who became Lipscomb's lifelong friend.
The young Lipscomb participated in other projects, such as Morse-coded messages over wires and crystal radio sets, with five nearby friends who became physicists, physicians, and an engineer.
At age of 12, Lipscomb was given a small Gilbert chemistry set,
He expanded it by ordering apparatus and chemicals from suppliers and by using
his father's privilege as a physician to purchase chemicals at the local drugstore at a discount.
Lipscomb made his own fireworks and entertained visitors with color changes, odors, and explosions.
His mother questioned his home chemistry hobby only once, when he attempted to isolate a large amount of urea from urine.
Lipscomb credits perusing the large medical texts in his physician father's library and the influence of Linus Pauling years later to his undertaking biochemical studies in his later years. Had Lipscomb become a physician like his father, he would have been the fourth physician in a row along the Lipscomb male line.
The source for this subsection, except as noted, is Lipscomb's autobiographical sketch.
Education.
Lipscomb's high-school chemistry teacher, Frederick Jones, gave Lipscomb his college books on organic, analytical, and general chemistry, and asked only that Lipscomb take the examinations.
During the class lectures, Lipscomb in the back of the classroom did research that he thought was original (but he later found was not): the preparation of hydrogen from sodium formate (or sodium oxalate) and sodium hydroxide.
He took care to include gas analyses and to search for probable side reactions.
Lipscomb later had a high-school physics course and took first prize in the state contest on that subject. He also became very interested in special relativity.
In college at the University of Kentucky Lipscomb had a music scholarship.
He pursued independent study there, reading Dushman' s "Elements of Quantum Mechanics", the University of Pittsburgh Physics Staff's "An Outline of Atomic Physics", and Pauling's "The Nature of the Chemical Bond and the Structure of Molecules and Crystals."
Prof. Robert H. Baker suggested that Lipscomb research the direct preparation of derivatives of alcohols from dilute aqueous solution without first separating the alcohol and water, which led to Lipscomb's first publication.
For graduate school Lipscomb chose Caltech, which offered him a teaching assistantship in Physics at $20/month. He turned down more money from Northwestern University, which offered a research assistantship at $150/month. Columbia University rejected Lipscomb's application in a letter written by Nobel prizewinner Prof. Harold Urey.
At Caltech Lipscomb intended to study theoretical quantum mechanics with Prof. W. V. Houston in the Physics Department, but after one semester switched to the Chemistry Department under the
influence of Prof. Linus Pauling. World War II work divided Lipscomb's time in graduate school beyond his other thesis work, as he partly analyzed smoke particle size, but mostly worked with nitroglycerin–nitrocellulose propellants, which involved handling vials of pure nitroglycerin on many occasions. 
Brief audio clips by Lipscomb about his war work may be found from the External Links section at the bottom of this page, past the References.
The source for this subsection, except as noted, is Lipscomb's autobiographical sketch.
Later years.
The Colonel is how Lipscomb's students referred to him, directly addressing him as Colonel. "His first doctoral student, Murray Vernon King, pinned the label on him, and it was quickly adopted by other students, who wanted to use an appellation that showed informal respect. ... Lipscomb's Kentucky origins as the rationale for the designation."
Some years later in 1973 Lipscomb was made a member of the Honorable Order of Kentucky Colonels.
Lipscomb, along with several other Nobel laureates, was a regular presenter at the annual Ig Nobel Awards Ceremony, last doing so on September 30, 2010.
Scientific studies.
Lipscomb has worked in three main areas, nuclear magnetic resonance and the chemical shift, boron chemistry and the nature of the chemical bond, and large biochemical molecules. These areas overlap in time and share some scientific techniques.
In at least the first two of these areas Lipscomb gave himself a big challenge likely to fail,
and then plotted a course of intermediate goals.
Nuclear magnetic resonance and the chemical shift.
In this area Lipscomb proposed that:
"... progress in structure determination, for new polyborane species and for substituted boranes and carboranes, would be greatly accelerated if the [boron-11] nuclear magnetic resonance spectra, rather than X-ray diffraction, could be used."
This goal was partially achieved, although X-ray diffraction is still necessary to determine many such atomic structures. The diagram at left shows a typical nuclear magnetic resonance (NMR) spectrum of a borane molecule.
Lipscomb investigated, "... the carboranes, C2B10H12, and the sites of electrophilic attack on these compounds using nuclear magnetic resonance (NMR) spectroscopy. This work led to [Lipscomb's publication of a comprehensive] theory of chemical shifts. The calculations provided the first accurate values for the constants that describe the behavior of several types of molecules in magnetic or electric fields."
Much of this work is summarized in a book by Gareth Eaton and William Lipscomb, "NMR Studies of Boron Hydrides and Related Compounds", one of Lipscomb's two books.
Boron chemistry and the nature of the chemical bond.
In this area Lipscomb originally intended a more ambitious project: "My original intention in the late 1940s was to spend a few years understanding the boranes, and then to discover a systematic valence description of the vast numbers of electron deficient intermetallic compounds. I have made little progress toward this latter objective. Instead, the field of boron chemistry has grown enormously, and a systematic understanding of some of its complexities has now begun."
Examples of these intermetallic compounds are KHg13 and Cu5Zn7. Of perhaps 24,000 of such compounds the structures of only 4,000 are known (in 2005) and we cannot predict structures for the others, because we do not sufficiently understand the nature of the chemical bond.
This study was not successful, in part because the calculation time required for intermetallic compounds was out of reach in the 1960s, but intermediate goals involving boron bonding were achieved, sufficient to be awarded a Nobel Prize.
Lipscomb deduced the molecular structure of boranes (compounds of boron and hydrogen) using X-ray crystallography in the 1950s and developed theories to explain their bonds. Later he applied the same methods to related problems, including the structure of carboranes (compounds of carbon, boron, and hydrogen).
Lipscomb is perhaps best known for his group's proposed mechanism
of the three-center two-electron bond.
The three-center two-electron bond is illustrated in diborane (diagrams at right).
In an ordinary covalent bond a pair of electrons bonds two atoms together, one at either end of the bond, the diboare B-H bonds for example at the left and right in the illustrations.
In three-center two-electron bond a pair of electrons bonds three atoms (a boron atom at either end and a hydrogen atom in the middle), the diborane B-H-B bonds for example at the top and bottom of the illustrations.
Lipscomb's group did not propose or discover the three-center two-electron bond, 
nor did they develop formulas that give the proposed mechanism.
What they did was to use formulas written by others intended for another purpose
to understand the quantum mechanicical details of the three-center two-electron bond.
A trail of credit for the understanding the three-center two-electron bond follows:
Over several decades the structure and bonding arrangement of diborane was gradually discovered by
Dilthey,
Price, and others.
Longuet-Higgins 
and Roberts 
employed a three-center two-electron bond as the correct way to understand bonding in diborane using a molecular orbital description similar to what the Lipscomb group found. Eberhardt, Crawford, and Lipscomb proposed the mechanism
of the three-center two-electron bond, and Lipscomb's group achieved an understanding of it through electron orbital calculations using formulas by Edmiston and Ruedenberg and by Boys.
The Eberhardt, Crawford, and Lipscomb paper discussed above also devised the "styx number" method to catalog certain kinds of boron-hydride bonding configurations.
Wandering atoms was a puzzle solved by Lipscomb in one of his few papers with no co-authors.
Compounds of boron and hydrogen tend to form closed cage structures. 
Sometimes the atoms at the vertices of these cages move substantial distances with respect to each other.
The diamond-square-diamond mechanism (diagram at left) was suggested by Lipscomb to explain this rearrangement of vertices.
Following along in the diagram at left for example in the faces shaded in blue, 
a pair of triangular faces has a left-right diamond shape. 
First, the bond common to these adjacent triangles breaks, forming a square,
and then the square collapses back to an up-down diamond shape
by bonding the atoms that were not bonded before.
Other researchers have discovered more about these rearrangements.
The B10H16 structure (diagram at right) determined by Grimes, Wang, Lewin, and Lipscomb found a bond directly between two boron atoms without terminal hydrogens, a feature not previously seen in other boron hydrides.
Lipscomb's group developed calculation methods, both empirical and from quantum mechanical theory.
Calculations by these methods produced accurate Hartree–Fock self-consistent field (SCF) molecular orbitals and were used to study boranes and carboranes.
The ethane barrier to rotation (diagram at left) was first calculated accurately by Pitzer and Lipscomb using the Hartree–Fock (SCF) method.
Lipscomb's calculations continued to a detailed examination of partial bonding through "... theoretical studies of multicentered chemical bonds including both delocalized and localized molecular orbitals."
This included "... proposed molecular orbital descriptions in which the bonding electrons are delocalized over the whole molecule."
"Lipscomb and his coworkers developed the idea of transferability of atomic properties, by which approximate theories for complex molecules are developed from more exact calculations for simpler but chemically related molecules..."
Subsequent Nobel Prize winner Roald Hoffmann was a doctoral student 
in Lipscomb's laboratory.
Under Lipscomb's direction the Extended Hückel method of molecular orbital calculation was developed by Lawrence Lohr and by Roald Hoffmann. This method was later extended by Hoffman. 
In Lipscomb's laboratory this method was reconciled with self-consistent field (SCF) theory by Newton and by Boer.
Noted boron chemist M. Frederick Hawthorne conducted early and continuing research with Lipscomb.
Much of this work is summarized in a book by Lipscomb, "Boron Hydrides", one of Lipscomb's two books.
The 1976 Nobel Prize in Chemistry was awarded to Lipscomb "for his studies on the structure of boranes illuminating problems of chemical bonding".
In a way this continued work on the nature of the chemical bond by his Doctoral Advisor at the California Institute of Technology, Linus Pauling, who was awarded the 1954 Nobel Prize in Chemistry "for his research into the nature of the chemical bond and its application to the elucidation of the structure of complex substances."
The source for about half of this section is Lipscomb's Nobel Lecture.
Large biological molecule structure and function.
Lipscomb's later research focused on the atomic structure of proteins, particularly how enzymes work.
His group used x-ray diffraction to solve the three-dimensional structure of these proteins to atomic resolution, and then to analyze the atomic detail of how the molecules work.
The images below are of Lipscomb's structures from the Protein Data Bank displayed in simplified form with atomic detail suppressed. Proteins are chains of amino acids, and the continuous ribbon shows the trace of the chain with, for example, several amino acids for each turn of a helix.
Carboxypeptidase A (left) was the first protein structure from Lipscomb's group. Carboxypeptidase A is a digestive enzyme, a protein that digests other proteins. It is made in the pancreas and transported in inactive form to the intestines where it is activated. Carboxypeptidase A digests by chopping off certain amino acids one-by-one from one end of a protein.
The size of this structure was ambitious. Carboxypeptidase A was a much larger molecule than anything solved previously.
Aspartate carbamoyltransferase. (right) was the second protein structure from Lipscomb's group.
For a copy of DNA to be made, a duplicate set of its nucleotides is required. Aspartate carbamoyltransferase performs a step in building the pyrimidine nucleotides (cytosine and thymidine). Aspartate carbamoyltransferase also ensures that just the right amount of pyrimidine nucleotides is available, as activator and inhibitor molecules attach to aspartate carbamoyltransferase to speed it up and to slow it down.
Aspartate carbamoyltransferase is a complex of twelve molecules.
Six large catalytic molecules in the interior do the work, and six small regulatory molecules on the outside control how fast the catalytic units work.
The size of this structure was ambitious. Aspartate carbamoyltransferase was a much larger molecule than anything solved previously.
Leucine aminopeptidase, (left) a little like carboxypeptidase A, chops off certain amino acids one-by-one from one end of a protein or peptide.
HaeIII methyltransferase (right)
binds to DNA where it methylates (adds a methy group to)
it.
Human interferon beta (left)
is released by lymphocytes in response to pathogens to trigger the immune system.
Chorismate mutase (right)
catalyzes (speeds up) the production of the amino acids phenylalanine and tyrosine.
Fructose-1,6-bisphosphatase (left)
and its inhibitor MB06322 (CS-917) 
were studied by Lipscomb's group in a collaboration, which included Metabasis Therapeutics, Inc., acquired by Ligand Pharmaceuticals in 2010, exploring the possibility of finding a treatment for type 2 diabetes, as the MB06322 inhibitor slows the production of sugar by fructose-1,6-bisphosphatase.
Lipscomb's group also contributed to an understanding of
concanavalin A (low resolution structure),
glucagon, and
carbonic anhydrase (theoretical studies).
Subsequent Nobel Prize winner Thomas A. Steitz
was a doctoral student in Lipscomb's laboratory.
Under Lipscomb's direction, after the training task of determining the structure of the small molecule methyl ethylene phosphate, Steitz made contributions to determining the atomic structures of carboxypeptidase A
and aspartate carbamoyltransferase.
Steitz was awarded the 2009 Nobel Prize in Chemistry for determining the even larger structure of the large 50S ribosomal subunit, leading to an understanding of possible medical treatments.
Subsequent Nobel Prize winner Ada Yonath, who shared the 2009 Nobel Prize in Chemistry with Thomas A. Steitz and Venkatraman Ramakrishnan, spent some time in Lipscomb's lab where both she and Steitz were inspired to pursue later their own very large structures. This was while she was a postdoctoral student at MIT in 1970.
Other results.
The mineral lipscombite (picture at right) was named after Professor Lipscomb by the mineralogist John Gruner who first made it artificially.
Low-temperature x-ray diffraction was pioneered in Lipscomb's laboratory at about the same time as parallel work in Isadore Fankuchen's laboratory at the then Polytechnic Institute of Brooklyn.
Lipscomb began by studying compounds of nitrogen, oxygen, fluorine, and other substances that are solid only below liquid nitrogen temperatures, but other advantages eventually made low-temperatures a normal procedure.
Keeping the crystal cold during data collection produces a less-blurry 3-D electron-density map because the atoms have less thermal motion. Crystals may yield good data in the x-ray beam longer because x-ray damage may be reduced during data collection and because the solvent may evaporate more slowly, which for example may be important for large biochemical molecules whose crystals often have a high percentage of water.
Other important compounds were studied by Lipscomb and his students.
Among these are
hydrazine, 
nitric oxide, 
metal-dithiolene complexes, 
methyl ethylene phosphate, 
mercury amides, 
(NO)2, 
crystalline hydrogen fluoride, 
Roussin's black salt,
(PCF3)5,
complexes of cyclo-octatetraene with iron tricarbonyl,
and leurocristine (Vincristine), which is used in several cancer therapies.
Positions, awards and honors.
Five books and published symposia are dedicated to Lipscomb.
A complete list of Lipscomb's awards and honors is in his Curriculum Vitae.

</doc>
<doc id="34122" url="http://en.wikipedia.org/wiki?curid=34122" title="Wacław Sierpiński">
Wacław Sierpiński

Wacław Franciszek Sierpiński (]) (March 14, 1882 – October 21, 1969) was a Polish mathematician. He was for outstanding contributions to set theory (research on the axiom of choice and the continuum hypothesis), number theory, theory of functions and topology. He published over 700 papers and 50 books.
Three well-known fractals are named after him (the Sierpinski triangle, the Sierpinski carpet and the Sierpinski curve), as are Sierpinski numbers and the associated Sierpiński problem.
Educations.
Sierpiński enrolled in the Department of Mathematics and Physics at the University of Warsaw in 1899 and graduated four years later. In 1903, while still at the University of Warsaw, the Department of Mathematics and Physics offered a prize for the best essay from a student on Voronoy's contribution to number theory. Sierpiński was awarded a gold medal for his essay, thus laying the foundation for his first major mathematical contribution. Unwilling for his work to be published in Russian, he withheld it until 1907, when it was published in Samuel Dickstein's mathematical magazine 'Prace Matematyczno-Fizyczne' (Polish: 'The Works of Mathematics and Physics').
After his graduation in 1904, Sierpiński worked as a school teacher of mathematics and physics in Warsaw. However, when the school closed because of a strike, Sierpiński decided to go to Kraków to pursue a doctorate. At the Jagiellonian University in Kraków he attended lectures by Stanisław Zaremba on mathematics. He also studied astronomy and philosophy. He received his doctorate and was appointed to the University of Lwów in 1908.
Contributions to mathematics.
In 1907 Sierpiński first became interested in set theory when he came across a theorem which stated that points in the plane could be specified with a single coordinate. He wrote to Tadeusz Banachiewicz (then at Göttingen), asking how such a result was possible. He received the one-word reply 'Cantor'. Sierpiński began to study set theory and, in 1909, he gave the first ever lecture course devoted entirely to the subject.
Sierpiński maintained an incredible output of research papers and books. During the years 1908 to 1914, when he taught at the University of Lwów, he published three books in addition to many research papers. These books were "The Theory of Irrational Numbers" (1910), "Outline of Set Theory" (1912), and "The Theory of Numbers" (1912). 
When World War I began in 1914, Sierpiński and his family were in Russia. To avoid the persecution that was common for Polish foreigners, Sierpiński spent the rest of the war years in Moscow working with Nikolai Luzin. Together they began the study of analytic sets. In 1916, Sierpiński gave the first example of an absolutely normal number.
When World War I ended in 1918, Sierpiński returned to Lwów. However shortly after taking up his appointment again in Lwów he was offered a post at the University of Warsaw, which he accepted. In 1919 he was promoted to a professor. He spent the rest of his life in Warsaw. 
During the Polish–Soviet War (1919–1921), Sierpiński helped break Soviet Russian ciphers for the Polish General Staff's cryptological agency.
In 1920, Sierpiński, together with Zygmunt Janiszewski and his former student Stefan Mazurkiewicz, founded an influential mathematical journal Fundamenta Mathematica. Sierpiński edited the journal, which specialized in papers on set theory. 
During this period, Sierpiński worked predominantly on set theory, but also on point set topology and functions of a real variable. In set theory he made contributions on the axiom of choice and on the continuum hypothesis. He proved that Zermelo–Fraenkel set theory together with the Generalized continuum hypothesis imply the Axiom of choice. He also worked on what is now known as the Sierpinski curve. Sierpiński continued to collaborate with Luzin on investigations of analytic and projective sets. His work on functions of a real variable includes results on functional series, differentiability of functions and Baire's classification. 
Sierpiński retired in 1960 as professor at the University of Warsaw, but continued until 1967 to give a seminar on the Theory of Numbers at the Polish Academy of Sciences. He also continued editorial work as editor-in-chief of "Acta Arithmetica", and as an editorial-board member of "Rendiconti del Circolo Matematico di Palermo", "Composito Matematica", and "Zentralblatt für Mathematik".
Sierpiński is interred at the Powązki Cemetery in Warsaw, Poland.
Honors received.
Honorary Degrees: Lwów (1929), St. Marks of Lima (1930), Amsterdam (1931), Tarta (1931), Sofia (1939), Prague (1947), Wrocław (1947), Lucknow (1949), and Moscow (1967). 
For high involvement with the development of mathematics in Poland, Sierpiński was honored with election to the Polish Academy of Learning in 1921 and that same year was made dean of the faculty at the University of Warsaw. In 1928, he became vice-chairman of the Warsaw Scientific Society, and that same year was elected chairman of the Polish Mathematical Society.
He was elected to the Geographic Society of Lima (1931), the Royal Scientific Society of Liège (1934), the Bulgarian Academy of Sciences (1936), the National Academy of Lima (1939), the Royal Society of Sciences of Naples (1939), the Accademia dei Lincei of Rome (1947), the Germany Academy of Sciences (1950), the United States National Academy of Sciences (1959), the Paris Academy (1960), the Royal Dutch Academy (1961), the Academy of Science of Brussels (1961), the London Mathematical Society (1964), the Romanian Academy (1965) and the Papal Academy of Sciences (1967). 
In 1949 Sierpiński was awarded Poland's Scientific Prize, first degree.
Publications.
Sierpiński authored 724 papers and 50 books (two of which, "Introduction to General Topology" (1934) and "General Topology" (1952) have been translated into English by Canadian mathematician Cecilia Krieger).

</doc>
<doc id="34131" url="http://en.wikipedia.org/wiki?curid=34131" title="Wilhelm Ostwald">
Wilhelm Ostwald

Friedrich Wilhelm Ostwald (Latvian: "Vilhelms Ostvalds"; 2 September 1853 – 4 April 1932) was a Baltic German chemist. He received the Nobel Prize in Chemistry in 1909 for his work on catalysis, chemical equilibria and reaction velocities. Ostwald, Jacobus Henricus van 't Hoff, and Svante Arrhenius are usually credited with being the modern founders of the field of physical chemistry.
Early life and education.
Ostwald was born ethnically Baltic German in Riga, to master-cooper Gottfried Wilhelm Ostwald (1824–1903) and Elisabeth Leuckel (1824–1903). He was the middle of two brothers, Eugen (1851–1932) and Gottfried (1855–1918). Ostwald graduated from the University of Tartu, Estonia, in 1875, received his Ph.D. there in 1878 under the guidance of Carl Schmidt, and taught at Co-Arc from 1875 to 1881 and at Riga Polytechnicum from 1881 to 1887.
Career and research.
Ostwald is usually credited with inventing the Ostwald process (patent 1902), used in the manufacture of nitric acid, although the basic chemistry had been patented some 64 years earlier by Kuhlmann, when it was probably of only academic interest due to the lack of a significant source of ammonia. That may have still been the state of affairs in 1902, although things were due to change dramatically in the second half of the decade as a result of Haber and Bosch's work on their nitrogen fixing process (completed by 1911 or 1913). The date 1908 (six years after the patent) is often given for the invention of the Ostwald process, and it may be that these developments motivated him to do additional work to commercialize the process in that time-frame. Alternatively, six years might simply have been the bureaucratic interval between filing the patent and the time it was granted.
The combination of these two breakthroughs soon led to more economical and larger-scale production of fertilizers and explosives, of which Germany was to find itself in desperate need during World War I. Ostwald also did significant work on dilution theory leading to his discovery of the law of dilution which is named after him. Ostwald's rule concerns the behaviour of polymorphs. The word mole, according to Gorin, was introduced into chemistry around 1900 by Ostwald. Ostwald defined one mole as the molecular weight of a substance in mass grams. The concept was linked to the ideal gas, according to Ostwald. Ironically, Ostwald's development of the mole concept was directly related to his philosophical opposition to the atomic theory, against which he (along with Ernst Mach) was one of the last holdouts. He explained in a conversation with Arnold Sommerfeld that he was converted by Jean Perrin's experiments on Brownian Motion.
In 1906 Ostwald was elected a member of the International Committee on Atomic Weights. As a consequence of World War I this membership ended in 1917 and was not resumed after the war. The 1917 Annual report of the committee ended with the unusual note: "Because of the European war the Committee has had much difficulty in the way of correspondence. The German member, Professor Ostwald, has not been heard from in connection with this report. Possibly the censorship of letters, either in Germany or en route, has led to a miscarriage".
In addition to his work in chemistry, Wilhelm Ostwald was very productive in an extremely broad range of fields. His published work, which includes numerous philosophical writings, contains about forty thousand pages. Ostwald was also engaged in the peace movement of Berta von Suttner.
Among his other interests, Ostwald was a passionate amateur painter who made his own pigments, and who developed a strong interest in color theory in the later decades of his life. He wrote several publications in the field, such as his "Malerbriefe" ("Letters to a Painter," 1904) and "Die Farbenfibel" ("The Color Primer," 1916). His work in color theory was influenced by that of Albert Henry Munsell, and in turn influenced Paul Klee and members of De Stijl, including Piet Mondrian. He was also interested in the international language movement, first learning Esperanto, then later supporting Ido. Ostwald donated half the proceddings of his 1909 Nobel prize to the Ido movement, funding the Ido magazine "Progreso" which he had proposed in 1908.
Ostwald adopted the philosophy of Monism as advanced by Ernst Haeckel and became President of the Monistic Alliance in 1911. He used the Alliance's forum to promote Social Darwinism, eugenics and euthanasia. Ostwald's Monism influenced Carl G. Jung's identification of psychological types.
He was one of the directors of the Die Brücke institute in München. The institute was sponsored, significantly, from Ostwald's Nobel Prize money.
Personal life.
On 24 April 1880 Ostwald married Helene von Reyher (1854 – 1946), with whom he had five children: 
In 1887, he moved to Leipzig where he worked for the rest of his life. Arthur Noyes was one of his students, as was Willis Rodney Whitney. On his religious views, Ostwald was an atheist. Ostwald died in a hospital in Leipzig on 4 April 1932, and was buried at his house in Großbothen, near Leipzig and then in the Great Cemetery of Riga.
In fiction.
He appears as a character in Joseph Skibell's 2010 novel, A Curable Romantic.

</doc>
<doc id="34139" url="http://en.wikipedia.org/wiki?curid=34139" title="Xenon">
Xenon

Xenon is a chemical element with symbol Xe and atomic number 54. It is a colorless, dense, odorless noble gas, that occurs in the Earth's atmosphere in trace amounts. Although generally unreactive, xenon can undergo a few chemical reactions such as the formation of xenon hexafluoroplatinate, the first noble gas compound to be synthesized.
Naturally occurring xenon consists of eight stable isotopes. There are also over 40 unstable isotopes that undergo radioactive decay. The isotope ratios of xenon are an important tool for studying the early history of the Solar System. Radioactive xenon-135 is produced from iodine-135 as a result of nuclear fission, and it acts as the most significant neutron absorber in nuclear reactors.
Xenon is used in flash lamps and arc lamps, and as a general anesthetic. The first excimer laser design used a xenon dimer molecule (Xe2) as its lasing medium, and the earliest laser designs used xenon flash lamps as pumps. Xenon is also being used to search for hypothetical weakly interacting massive particles and as the propellant for ion thrusters in spacecraft.
History.
Xenon was discovered in England by the Scottish chemist William Ramsay and English chemist Morris Travers on July 12, 1898, shortly after their discovery of the elements krypton and neon. They found xenon in the residue left over from evaporating components of liquid air. Ramsay suggested the name "xenon" for this gas from the Greek word "ξένον" [xenon], neuter singular form of "ξένος" [xenos], meaning 'foreign(er)', 'strange(r)', or 'guest'. In 1902, Ramsay estimated the proportion of xenon in the Earth's atmosphere as one part in 20 million.
During the 1930s, American engineer Harold Edgerton began exploring strobe light technology for high speed photography. This led him to the invention of the xenon flash lamp, in which light is generated by sending a brief electrical current through a tube filled with xenon gas. In 1934, Edgerton was able to generate flashes as brief as one microsecond with this method.
In 1939, American physician Albert R. Behnke Jr. began exploring the causes of "drunkenness" in deep-sea divers. He tested the effects of varying the breathing mixtures on his subjects, and discovered that this caused the divers to perceive a change in depth. From his results, he deduced that xenon gas could serve as an anesthetic. Although Russian toxicologist Nikolay V. Lazarev apparently studied xenon anesthesia in 1941, the first published report confirming xenon anesthesia was in 1946 by American medical researcher John H. Lawrence, who experimented on mice. Xenon was first used as a surgical anesthetic in 1951 by American anesthesiologist Stuart C. Cullen, who successfully operated on two patients.
Xenon and the other noble gases were for a long time considered to be completely chemically inert and not able to form compounds. However, while teaching at the University of British Columbia, Neil Bartlett discovered that the gas platinum hexafluoride (PtF6) was a powerful oxidizing agent that could oxidize oxygen gas (O2) to form dioxygenyl hexafluoroplatinate (O2+[PtF6]−). Since O2 and xenon have almost the same first ionization potential, Bartlett realized that platinum hexafluoride might also be able to oxidize xenon. On March 23, 1962, he mixed the two gases and produced the first known compound of a noble gas, xenon hexafluoroplatinate. Bartlett thought its composition to be Xe+[PtF6]−, although later work has revealed that it was probably a mixture of various xenon-containing salts. Since then, many other xenon compounds have been discovered, along with some compounds of the noble gases argon, krypton, and radon, including argon fluorohydride (HArF), krypton difluoride (KrF2), and radon fluoride. By 1971, more than 80 xenon compounds were known.
In November 1999 a team of IBM scientists demonstrated a technology capable of manipulating individual atoms. The program, called IBM in atoms, used a scanning tunneling microscope to arrange 35 individual xenon atoms on a substrate of chilled crystal of nickel to spell out the three letter company acronym. It was the first time atoms had been precisely positioned on a flat surface.
Characteristics.
Xenon has atomic number 54; that is, its nucleus contains 54 protons. At standard temperature and pressure, pure xenon gas has a density of 5.761 kg/m3, about 4.5 times the surface density of the Earth's atmosphere, 1.217 kg/m3. As a liquid, xenon has a density of up to 3.100 g/mL, with the density maximum occurring at the triple point. Under the same conditions, the density of solid xenon, 3.640 g/cm3, is higher than the average density of granite, 2.75 g/cm3. Using gigapascals of pressure, xenon has been forced into a metallic phase.
Solid xenon changes from face-centered cubic (fcc) to hexagonal close packed (hcp) crystal phase under pressure and begins to turn metallic at about 140 GPa, with no noticeable volume change in the hcp phase. It is completely metallic at 155 GPa. When metalized, xenon looks sky blue because it absorbs red light and transmits other visible frequencies. Such behavior is unusual for a metal and is explained by the relatively small widths of the electron bands in metallic xenon.
Xenon is a member of the zero-valence elements that are called noble or inert gases. It is inert to most common chemical reactions (such as combustion, for example) because the outer valence shell contains eight electrons. This produces a stable, minimum energy configuration in which the outer electrons are tightly bound.
In a gas-filled tube, xenon emits a blue or lavenderish glow when the gas is excited by electrical discharge. Xenon emits a band of emission lines that span the visual spectrum,
but the most intense lines occur in the region of blue light, which produces the coloration.
Occurrence and production.
Xenon is a trace gas in Earth's atmosphere, occurring at 87±1 parts per billion (nL/L), or approximately 1 part per 11.5 million, and is also found as a component in gases emitted from some mineral springs.
Xenon is obtained commercially as a by-product of the separation of air into oxygen and nitrogen. After this separation, generally performed by fractional distillation in a double-column plant, the liquid oxygen produced will contain small quantities of krypton and xenon. By additional fractional distillation steps, the liquid oxygen may be enriched to contain 0.1–0.2% of a krypton/xenon mixture, which is extracted either via absorption onto silica gel or by distillation. Finally, the krypton/xenon mixture may be separated into krypton and xenon via distillation. Worldwide production of xenon in 1998 was estimated at 5,000–7,000 m3. Because of its low abundance, xenon is much more expensive than the lighter noble gases—approximate prices for the purchase of small quantities in Europe in 1999 were 10 €/L for xenon, 1 €/L for krypton, and 0.20 €/L for neon; the much more plentiful argon costs less than a cent per liter.
Within the Solar System, the nucleon fraction of xenon is 1.56 × 10−8, for an abundance of approximately one part in 630 thousand of the total mass. Xenon is relatively rare in the Sun's atmosphere, on Earth, and in asteroids and comets. The planet Jupiter has an unusually high abundance of xenon in its atmosphere; about 2.6 times as much as the Sun. This high abundance remains unexplained and may have been caused by an early and rapid buildup of planetesimals—small, subplanetary bodies—before the presolar disk began to heat up. (Otherwise, xenon would not have been trapped in the planetesimal ices.) The problem of the low terrestrial xenon may potentially be explained by covalent bonding of xenon to oxygen within quartz, hence reducing the outgassing of xenon into the atmosphere.
Unlike the lower mass noble gases, the normal stellar nucleosynthesis process inside a star does not form xenon. Elements more massive than iron-56 have a net energy cost to produce through fusion, so there is no energy gain for a star when creating xenon. Instead, xenon is formed during supernova explosions, by the slow neutron capture process (s-process) of red giant stars that have exhausted the hydrogen at their cores and entered the asymptotic giant branch, in classical nova explosions and from the radioactive decay of elements such as iodine, uranium and plutonium.
Isotopes and isotopic studies.
Naturally occurring xenon is made of eight stable isotopes, the most of any element with the exception of tin, which has ten. Xenon and tin are the only elements to have more than seven stable isotopes. The isotopes 124Xe and 134Xe are predicted to undergo double beta decay, but this has never been observed so they are considered to be stable.
Besides these stable forms, there are over 40 unstable isotopes that have been studied. The longest lived of these isotopes is 136Xe, which has been observed to undergo double beta decay with a half-life of 2.11 x 1021yr. 129Xe is produced by beta decay of 129I, which has a half-life of 16 million years, while 131mXe, 133Xe, 133mXe, and 135Xe are some of the fission products of both 235U and 239Pu, and therefore used as indicators of nuclear explosions.
Nuclei of two of the stable isotopes of xenon, 129Xe and 131Xe, have non-zero intrinsic angular momenta (nuclear spins, suitable for nuclear magnetic resonance). The nuclear spins can be aligned beyond ordinary polarization levels by means of circularly polarized light and rubidium vapor. The resulting spin polarization of xenon nuclei can surpass 50% of its maximum possible value, greatly exceeding the thermal equilibrium value dictated by paramagnetic statistics (typically 0.001% of the maximum value at room temperature, even in the strongest magnets). Such non-equilibrium alignment of spins is a temporary condition, and is called "hyperpolarization". The process of hyperpolarizing the xenon is called "optical pumping" (although the process is different from pumping a laser).
Because a 129Xe nucleus has a spin of 1/2, and therefore a zero electric quadrupole moment, the 129Xe nucleus does not experience any quadrupolar interactions during collisions with other atoms, and thus its hyperpolarization can be maintained for long periods of time even after the laser beam has been turned off and the alkali vapor removed by condensation on a room-temperature surface. Spin polarization of 129Xe can persist from several seconds for xenon atoms dissolved in blood to several hours in the gas phase and several days in deeply frozen solid xenon. In contrast, 131Xe has a nuclear spin value of 3⁄2 and a nonzero quadrupole moment, and has t1 relaxation times in the millisecond and second ranges.
Some radioactive isotopes of xenon, for example, 133Xe and 135Xe, are produced by neutron irradiation of fissionable material within nuclear reactors. 135Xe is of considerable significance in the operation of nuclear fission reactors. 135Xe has a huge cross section for thermal neutrons, 2.6×106 barns, so it acts as a neutron absorber or "poison" that can slow or stop the chain reaction after a period of operation. This was discovered in the earliest nuclear reactors built by the American Manhattan Project for plutonium production. Fortunately the designers had made provisions in the design to increase the reactor's reactivity (the number of neutrons per fission that go on to fission other atoms of nuclear fuel).
135Xe reactor poisoning played a major role in the Chernobyl disaster. A shutdown or decrease of power of a reactor can result in buildup of 135Xe and getting the reactor into the iodine pit.
Under adverse conditions, relatively high concentrations of radioactive xenon isotopes may be found emanating from nuclear reactors due to the release of fission products from cracked fuel rods, or fissioning of uranium in cooling water.
Because xenon is a tracer for two parent isotopes, xenon isotope ratios in meteorites are a powerful tool for studying the formation of the solar system. The iodine-xenon method of dating gives the time elapsed between nucleosynthesis and the condensation of a solid object from the solar nebula. In 1960, physicist John H. Reynolds discovered that certain meteorites contained an isotopic anomaly in the form of an overabundance of xenon-129. He inferred that this was a decay product of radioactive iodine-129. This isotope is produced slowly by cosmic ray spallation and nuclear fission, but is produced in quantity only in supernova explosions. As the half-life of 129I is comparatively short on a cosmological time scale, only 16 million years, this demonstrated that only a short time had passed between the supernova and the time the meteorites had solidified and trapped the 129I. These two events (supernova and solidification of gas cloud) were inferred to have happened during the early history of the Solar System, as the 129I isotope was likely generated before the Solar System was formed, but not long before, and seeded the solar gas cloud with isotopes from a second source. This supernova source may also have caused collapse of the solar gas cloud.
In a similar way, xenon isotopic ratios such as 129Xe/130Xe and 136Xe/130Xe are also a powerful tool for understanding planetary differentiation and early outgassing. For example, The atmosphere of Mars shows a xenon abundance similar to that of Earth:
0.08 parts per million, however Mars shows a higher proportion of 129Xe than the Earth or the Sun. As this isotope is generated by radioactive decay, the result may indicate that Mars lost most of its primordial atmosphere, possibly within the first 100 million years after the planet was formed. In another example, excess 129Xe found in carbon dioxide well gases from New Mexico was believed to be from the decay of mantle-derived gases soon after Earth's formation.
Compounds.
See also: .
After Neil Bartlett's discovery in 1962 that xenon can form chemical compounds, a large number of xenon compounds have been discovered and described. Almost all known xenon compounds contain the electronegative atoms fluorine or oxygen.
Halides.
Three fluorides are known: XeF2, XeF4, and XeF6. XeF is theorized to be unstable. The fluorides are the starting point for the synthesis of almost all xenon compounds.
The solid, crystalline difluoride XeF2 is formed when a mixture of fluorine and xenon gases is exposed to ultraviolet light. Ordinary daylight is sufficient. Long-term heating of XeF2 at high temperatures under an NiF2 catalyst yields XeF6. Pyrolysis of XeF6 in the presence of NaF yields high-purity XeF4.
The xenon fluorides behave as both fluoride acceptors and fluoride donors, forming salts that contain such cations as XeF+ and XeF3+, and anions such as XeF5-, XeF7-, and XeF82-. The green, paramagnetic Xe2+ is formed by the reduction of XeF2 by xenon gas.
XeF2 is also able to form coordination complexes with transition metal ions. Over 30 such complexes have been synthesized and characterized.
Whereas the xenon fluorides are well-characterized, the other halides are not known, the only exception being the dichloride, XeCl2. Xenon dichloride is reported to be an endothermic, colorless, crystalline compound that decomposes into the elements at 80 °C, formed by the high-frequency irradiation of a mixture of xenon, fluorine, and silicon or carbon tetrachloride. However, doubt has been raised as to whether XeCl2 is a real compound and not merely a van der Waals molecule consisting of weakly bound Xe atoms and Cl2 molecules. Theoretical calculations indicate that the linear molecule XeCl2 is less stable than the van der Waals complex.
Oxides and oxohalides.
Three oxides of xenon are known: xenon trioxide (XeO3) and xenon tetroxide (XeO4), both of which are dangerously explosive and powerful oxidizing agents, and xenon dioxide (XeO2), which was reported in 2011 with a coordination number of four. XeO2 forms when xenon tetrafluoride is poured over ice. Its crystal structure may allow it to replace silicon in silicate minerals. The XeOO+ cation has been identified by infrared spectroscopy in solid argon.
Xenon does not react with oxygen directly; the trioxide is formed by the hydrolysis of XeF6:
XeO3 is weakly acidic, dissolving in alkali to form unstable "xenate" salts containing the HXeO4− anion. These unstable salts easily disproportionate into xenon gas and "perxenate" salts, containing the XeO64− anion.
Barium perxenate, when treated with concentrated sulfuric acid, yields gaseous xenon tetroxide:
To prevent decomposition, the xenon tetroxide thus formed is quickly cooled to form a pale-yellow solid. It explodes above −35.9 °C into xenon and oxygen gas.
A number of xenon oxyfluorides are known, including XeOF2, XeOF4, XeO2F2, and XeO3F2. XeOF2 is formed by the reaction of OF2 with xenon gas at low temperatures. It may also be obtained by the partial hydrolysis of XeF4. It disproportionates at −20 °C into XeF2 and XeO2F2. XeOF4 is formed by the partial hydrolysis of XeF6, or the reaction of XeF6 with sodium perxenate, Na4XeO6. The latter reaction also produces a small amount of XeO3F2. XeOF4 reacts with CsF to form the XeOF5− anion, while XeOF3 reacts with the alkali metal fluorides KF, RbF and CsF to form the XeOF4− anion.
Other compounds.
Recently, there has been an interest in xenon compounds where xenon is directly bonded to a less electronegative element than fluorine or oxygen, particularly carbon. Electron-withdrawing groups, such as groups with fluorine substitution, are necessary to stabilize these compounds. Numerous such compounds have been characterized, including:
Other compounds containing xenon bonded to a less electronegative element include F–Xe–N(SO2F)2 and F–Xe–BF2. The latter is synthesized from dioxygenyl tetrafluoroborate, O2BF4, at −100 °C.
An unusual ion containing xenon is the tetraxenonogold(II) cation, AuXe42+, which contains Xe–Au bonds. This ion occurs in the compound AuXe4(Sb2F11)2, and is remarkable in having direct chemical bonds between two notoriously unreactive atoms, xenon and gold, with xenon acting as a transition metal ligand.
The compound Xe2Sb2F11 contains a Xe–Xe bond, the longest element-element bond known (308.71 pm = 3.0871 Å).
In 1995, M. Räsänen and co-workers, scientists at the University of Helsinki in Finland, announced the preparation of xenon dihydride (HXeH), and later xenon hydride-hydroxide (HXeOH), hydroxenoacetylene (HXeCCH), and other Xe-containing molecules. In 2008, Khriachtchev "et al." reported the preparation of HXeOXeH by the photolysis of water within a cryogenic xenon matrix. Deuterated molecules, HXeOD and DXeOH, have also been produced.
Clathrates and excimers.
In addition to compounds where xenon forms a chemical bond, xenon can form clathrates—substances where xenon atoms are trapped by the crystalline lattice of another compound. An example is xenon hydrate (Xe•5.75 H2O), where xenon atoms occupy vacancies in a lattice of water molecules. This clathrate has a melting point of 24 °C. The deuterated version of this hydrate has also been produced. Such clathrate hydrates can occur naturally under conditions of high pressure,
such as in Lake Vostok underneath the Antarctic ice sheet. Clathrate formation can be used to fractionally distill xenon, argon and krypton.
Xenon can also form endohedral fullerene compounds, where a xenon atom is trapped inside a fullerene molecule. The xenon atom trapped in the fullerene can be monitored via 129Xe nuclear magnetic resonance (NMR) spectroscopy. Using this technique, chemical reactions on the fullerene molecule can be analyzed, due to the sensitivity of the chemical shift of the xenon atom to its environment. However, the xenon atom also has an electronic influence on the reactivity of the fullerene.
While xenon atoms are at their ground energy state, they repel each other and will not form a bond. When xenon atoms becomes energized, however, they can form an excimer (excited dimer) until the electrons return to the ground state. This entity is formed because the xenon atom tends to fill its outermost electronic shell, and can briefly do this by adding an electron from a neighboring xenon atom. The typical lifetime of a xenon excimer is 1–5 ns, and the decay releases photons with wavelengths of about 150 and 173 nm. Xenon can also form excimers with other elements, such as the halogens bromine, chlorine and fluorine.
Applications.
Although xenon is rare and relatively expensive to extract from the Earth's atmosphere, it has a number of applications.
Illumination and optics.
Gas-discharge lamps.
Xenon is used in light-emitting devices called xenon flash lamps, which are used in photographic flashes and stroboscopic lamps; to excite the active medium in lasers which then generate coherent light; and, occasionally, in bactericidal lamps. The first solid-state laser, invented in 1960, was pumped by a xenon flash lamp, and lasers used to power inertial confinement fusion are also pumped by xenon flash lamps.
Continuous, short-arc, high pressure xenon arc lamps have a color temperature closely approximating noon sunlight and are used in solar simulators. That is, the chromaticity of these lamps closely approximates a heated black body radiator that has a temperature close to that observed from the Sun. After they were first introduced during the 1940s, these lamps began replacing the shorter-lived carbon arc lamps in movie projectors. They are employed in typical 35mm, IMAX and the new digital projectors film projection systems, automotive HID headlights, high-end "tactical" flashlights and other specialized uses. These arc lamps are an excellent source of short wavelength ultraviolet radiation and they have intense emissions in the near infrared, which is used in some night vision systems.
The individual cells in a plasma display use a mixture of xenon and neon that is converted into a plasma using electrodes. The interaction of this plasma with the electrodes generates ultraviolet photons, which then excite the phosphor coating on the front of the display.
Xenon is used as a "starter gas" in high pressure sodium lamps. It has the lowest thermal conductivity and lowest ionization potential of all the non-radioactive noble gases. As a noble gas, it does not interfere with the chemical reactions occurring in the operating lamp. The low thermal conductivity minimizes thermal losses in the lamp while in the operating state, and the low ionization potential causes the breakdown voltage of the gas to be relatively low in the cold state, which allows the lamp to be more easily started.
Lasers.
In 1962, a group of researchers at Bell Laboratories discovered laser action in xenon, and later found that the laser gain was improved by adding helium to the lasing medium. The first excimer laser used a xenon dimer (Xe2) energized by a beam of electrons to produce stimulated emission at an ultraviolet wavelength of 176 nm.
Xenon chloride and xenon fluoride have also been used in excimer (or, more accurately, exciplex) lasers. The xenon chloride excimer laser has been employed, for example, in certain dermatological uses.
Medical.
Anesthesia.
Xenon has been used as a general anesthetic. Although it is expensive, anesthesia machines that can deliver xenon are about to appear on the European market, because advances in recovery and recycling of xenon have made it economically viable.
Xenon interacts with many different receptors and ion channels and like many theoretically multi-modal inhalation anesthetics these interactions are likely complementary. Xenon is a high-affinity glycine-site NMDA receptor antagonist. However, xenon distinguishes itself from other clinically used NMDA receptor antagonists in its lack of neurotoxicity and its ability to inhibit the neurotoxicity of ketamine and nitrous oxide. Unlike ketamine and nitrous oxide, xenon does not stimulate a dopamine efflux from the nucleus accumbens. Like nitrous oxide and cyclopropane, xenon activates the two-pore domain potassium channel TREK-1. A related channel TASK-3 also implicated in inhalational anesthetic actions is insensitive to xenon. Xenon inhibits nicotinic acetylcholine α4β2 receptors which contribute to spinally mediated analgesia. Xenon is an effective inhibitor of plasma membrane Ca2+ ATPase. Xenon inhibits Ca2+ ATPase by binding to a hydrophobic pore within the enzyme and preventing the enzyme from assuming active conformations.
Xenon is a competitive inhibitor of the serotonin 5-HT3 receptor. While neither anesthetic nor antinociceptive this activity reduces anesthesia-emergent nausea and vomiting.
Xenon has a minimum alveolar concentration (MAC) of 72% at age 40, making it 44% more potent than N2O as an anesthetic. Thus it can be used in concentrations with oxygen that have a lower risk of hypoxia. Unlike nitrous oxide (N2O), xenon is not a greenhouse gas and so it is also viewed as environmentally friendly. Xenon vented into the atmosphere is being returned to its original source, so no environmental impact is likely.
Neuroprotectant.
Xenon induces robust cardioprotection and neuroprotection through a variety of mechanisms of action. Through its influence on Ca2+, K+, KATP\HIF and NMDA antagonism xenon is neuroprotective when administered before, during and after ischemic insults. Xenon is a high affinity antagonist at the NMDA receptor glycine site. Xenon is cardioprotective in ischemia-reperfusion conditions by inducing pharmacologic non-ischemic preconditioning. Xenon is cardioprotective by activating PKC-epsilon & downstream p38-MAPK. Xenon mimics neuronal ischemic preconditioning by activating ATP sensitive potassium channels. Xenon allosterically reduces ATP mediated channel activation inhibition independently of the sulfonylurea receptor1 subunit, increasing KATP open-channel time and frequency.
Xenon upregulates hypoxia inducible factor 1 alpha (HIF1a).
Xenon gas was added as an ingredient of the ventilation mix for a newborn baby at St. Michael's Hospital, Bristol, England, whose life chances were otherwise very compromised, and was successful, leading to the authorisation of clinical trials for similar cases. The treatment is done simultaneously with cooling the body temperature to 33.5 °C.
Doping.
Inhaling a xenon/oxygen mixture activates production of the transcription factor HIF-1-alpha, which leads to increased production of erythropoietin. The latter hormone is known to increase red blood cell production and athletes' performance. Xenon inhalation has been used for this purpose in Russia since at least 2004. On August 31 2014 the World Anti Doping Agency (WADA) added Xenon (and Argon) to the list of prohibited substances and methods, although at this time there is no reliable test for abuse.
Imaging.
Gamma emission from the radioisotope 133Xe of xenon can be used to image the heart, lungs, and brain, for example, by means of single photon emission computed tomography. 133Xe has also been used to measure blood flow.
Xenon, particularly hyperpolarized 129Xe, is a useful contrast agent for magnetic resonance imaging (MRI). In the gas phase, it can be used to image empty space such as cavities in a porous sample or alveoli in lungs. Hyperpolarization renders 129Xe much more detectable via magnetic resonance imaging and has been used for studies of the lungs and other tissues. It can be used, for example, to trace the flow of gases within the lungs. Because xenon is soluble in water and also in hydrophobic solvents, it can be used to image various soft living tissues.
Xenon with its high nuclear mass is a useful contrast medium for x-ray photography. For this purpose it is supplemented by Krypton and used at concentrations below 35% as otherwise it would act as a narcotic.
NMR spectroscopy.
Because of the xenon atom's large, flexible outer electron shell, the NMR spectrum changes in response to surrounding conditions, and can therefore be used as a probe to measure the chemical circumstances around it. For instance xenon dissolved in water, in hydrophobic solvent, and xenon associated with certain proteins can be distinguished by NMR.
Hyperpolarized xenon can be used by surface chemists. Normally, it is difficult to characterize surfaces using NMR, because signals from the surface of a sample will be overwhelmed by signals from the far-more-numerous atomic nuclei in the bulk. However, nuclear spins on solid surfaces can be selectively polarized, by transferring spin polarization to them from hyperpolarized xenon gas. This makes the surface signals strong enough to measure, and distinguishes them from bulk signals.
Other.
In nuclear energy applications, xenon is used in bubble chambers, probes, and in other areas where a high molecular weight and inert nature is desirable. A by-product of nuclear weapon testing is the release of radioactive xenon-133 and xenon-135. The detection of these isotopes is used to monitor compliance with nuclear
test ban treaties, as well as
to confirm nuclear test explosions by states such as North Korea.
Liquid xenon is being used in calorimeters for measurements of gamma rays as well as a medium for detecting hypothetical weakly interacting massive particles, or WIMPs. When a WIMP collides with a xenon nucleus, it is predicted to impart enough energy to cause ionization and scintillation. Liquid xenon is useful for this type of experiment due to its high density which makes dark matter interaction more likely and permits a quiet detector due to self-shielding.
Xenon is the preferred propellant for ion propulsion of spacecraft because of its low ionization potential per atomic weight, and its ability to be stored as a liquid at near room temperature (under high pressure) yet be easily converted back into a gas to feed the engine. The inert nature of xenon makes it environmentally friendly and less corrosive to an ion engine than other fuels such as mercury or caesium. Xenon was first used for satellite ion engines during the 1970s. It was later employed as a propellant for JPL's Deep Space 1 probe, Europe's SMART-1 spacecraft and for the three ion propulsion engines on NASA's Dawn Spacecraft.
Chemically, the perxenate compounds are used as oxidizing agents in analytical chemistry. Xenon difluoride is used as an etchant for silicon, particularly in the production of microelectromechanical systems (MEMS). The anticancer drug 5-fluorouracil can be produced by reacting xenon difluoride with uracil. Xenon is also used in protein crystallography. Applied at pressures from 0.5 to 5 MPa (5 to 50 atm) to a protein crystal, xenon atoms bind in predominantly hydrophobic cavities, often creating a high-quality, isomorphous, heavy-atom derivative, which can be used for solving the phase problem.
Precautions.
Many oxygen-containing xenon compounds are toxic due to their strong oxidative properties, and explosive due to their tendency to break down into elemental xenon plus diatomic oxygen (O2), which contains much stronger chemical bonds than the xenon compounds.
Xenon gas can be safely kept in normal sealed glass or metal containers at standard temperature and pressure. However, it readily dissolves in most plastics and rubber, and will gradually escape from a container sealed with such materials. Xenon is non-toxic, although it does dissolve in blood and belongs to a select group of substances that penetrate the blood–brain barrier, causing mild to full surgical anesthesia when inhaled in high concentrations with oxygen.
At 169 m/s, the speed of sound in xenon gas is slower than that in air due to the slower average speed of the heavy xenon atoms compared to nitrogen and oxygen molecules. Hence, xenon lowers the rate of vibration in the vocal tract when exhaled. This produces a characteristic lowered voice timbre, an effect opposite to the high-timbred voice caused by inhalation of helium. Like helium, xenon does not satisfy the body's need for oxygen. Xenon is both a simple asphyxiant and an anesthetic more powerful than nitrous oxide; consequently, many universities no longer allow the voice stunt as a general chemistry demonstration. As xenon is expensive, the gas sulfur hexafluoride, which is similar to xenon in molecular weight (146 versus 131), is generally used in this stunt, and is an asphyxiant without being anesthetic.
It is possible to safely breathe dense gases such as xenon or sulfur hexafluoride when they are in a mixture of at least 20% oxygen. Xenon at 80% concentration along with 20% oxygen rapidly produces the unconsciousness of general anesthesia (and has been used for this, as discussed above). Breathing mixes gases of different densities very effectively and rapidly so that heavier gases are purged along with the oxygen, and do not accumulate at the bottom of the lungs. There is, however, a danger associated with any heavy gas in large quantities: it may sit invisibly in a container, and if a person enters a container filled with an odorless, colorless gas, they may find themselves breathing it unknowingly. Xenon is rarely used in large enough quantities for this to be a concern, though the potential for danger exists any time a tank or container of xenon is kept in an unventilated space.

</doc>
<doc id="34519" url="http://en.wikipedia.org/wiki?curid=34519" title="Żarnowiec">
Żarnowiec

Żarnowiec (Kashubian: "Żarnówc", German "Zarnowitz") is a village in the administrative district of Gmina Krokowa, within Puck County, Pomeranian Voivodeship, in northern Poland. It lies close to Żarnowieckie Lake, approximately 5 km west of Krokowa, 23 km north-west of Puck, and 59 km north-west of the regional capital Gdańsk. In 2005 the village had a population of 861.
Żarnowiec was the location for the first Polish nuclear power plant (Żarnowiec Nuclear Power Plant), but construction was stopped in 1990 due to protests of the local population and lack of funds. Recently, the construction plans are being reconsidered.
History.
The earliest evidence of settlement in the region dates from the 8th century BC: the inhabitants were apparently linked with the Lusatian and East Pomeranian cultures. There was a settlement near the Żarnowiec lake from the seventh to the tenth century AD. A village known alternately as "Sarnkow", "Sarnowitz", "Sarnowicz" or "Czarnowicz" is first mentioned in sources dating from the thirteenth century, when it was inhabited by the Kashubians.
In 1215, Żarnowiec belonged to the Cistercian order based in Oliwa Abbey, which founded a monastery for women there. In 1297 the monastery received special economic and juridical privileges from Mściwój II, Duke of Pomerania.
In fourteenth century Żarnowiec, together with all of Pomerelia was incorporated by the Teutonic Order. In 1433, it was raided by a Hussite army. In 1462, during the Thirteen Years' War, the Polish army under Piotr Dunin defeated the Teutonic Knights there (see Battle of Świecino, also known as the Battle of Żarnowiec). After the war Żarnowiec became a part of Royal Prussia ("Prusy Królewskie").
In 1590 it was taken over by a female order of Benedictines from Chełmno, who founded an abbey there in 1617. In 1772, after the first partition of Poland, it was taken over by Prussia, and in 1834 the abbey was liquidated. It was refounded in 1946 by a female order of Benedictines from Vilnius.
External links.
Żarnowiec Nuclear Power Plant
<br>

</doc>
<doc id="34542" url="http://en.wikipedia.org/wiki?curid=34542" title="Zhang Heng">
Zhang Heng

Zhang Heng (Chinese:  張衡,  张衡,  "Zhāng Héng"; AD 78–139), formerly romanized as Chang Hêng, was a Han Chinese polymath from Nanyang who lived during the Han dynasty. Educated in the capital cities of Luoyang and Chang'an, he achieved success as an astronomer, mathematician, scientist, engineer, inventor, geographer, cartographer, artist, poet, statesman, and literary scholar.
Zhang Heng began his career as a minor civil servant in Nanyang. Eventually, he became Chief Astronomer, Prefect of the Majors for Official Carriages, and then Palace Attendant at the imperial court. His uncompromising stance on historical and calendrical issues led to his becoming a controversial figure, preventing him from rising to the status of Grand Historian. His political rivalry with the palace eunuchs during the reign of Emperor Shun (r. 125–144) led to his decision to retire from the central court to serve as an administrator of Hejian in Hebei. Zhang returned home to Nanyang for a short time, before being recalled to serve in the capital once more in 138. He died there a year later, in 139.
Zhang applied his extensive knowledge of mechanics and gears in several of his inventions. He invented the world's first water-powered armillary sphere to assist astronomical observation; improved the inflow water clock by adding another tank; and invented the world's first seismometer, which discerned the cardinal direction of an earthquake 500 km away. He improved previous Chinese calculations for pi. In addition to documenting about 2,500 stars in his extensive star catalog, Zhang also posited theories about the Moon and its relationship to the Sun: specifically, he discussed the Moon's sphericity, its illumination by reflected sunlight on one side and the hidden nature of the other, and the nature of solar and lunar eclipses. His "fu" (rhapsody) and "shi" poetry were renowned in his time and studied and analyzed by later Chinese writers. Zhang received many posthumous honors for his scholarship and ingenuity; some modern scholars have compared his work in astronomy to that of the Greco-Roman Ptolemy (AD 86–161).
Life of Zhang Heng.
Early life.
Born in the town of Xi'e in Nanyang Commandery (north of the modern Nanyang City in Henan province), Zhang Heng came from a distinguished but not very affluent family. His grandfather Zhang Kan had been governor of a commandery and one of the leaders who supported the restoration of the Han by Emperor Guangwu (r. 25–57), following the death of the usurping Wang Mang of the Xin (AD 9–23). At age ten, Zhang's father died, leaving him in the care of his mother and grandmother. An accomplished writer in his youth, Zhang left home in the year 95 to pursue his studies in the capitals of Chang'an and Luoyang. While traveling to Luoyang, Zhang passed by a hot spring near Mount Li and dedicated one of his earliest "fu" poems to it. This work, entitled ""Fu" on the Hot Springs" ("Wēnquán fù" 溫泉賦), describes the throngs of people attending the hot springs, which later became famous as the "Huaqing Hot Springs", a favorite retreat of imperial concubine Yang Guifei during the Tang dynasty. After studying for some years at Luoyang's Taixue, he was well-versed in the classics and friends with several notable persons, including the mathematician and calligrapher Cui Yuan (78–143), the official and philosophical commentator Ma Rong (79–166), and the philosopher Wang Fu (78–163). Government authorities offered Zhang appointments to several offices, including a position as one of the Imperial Secretaries, yet he acted modestly and declined. At age twenty-three, he returned home with the title "Officer of Merit in Nanyang", serving as the master of documents under the administration of Governor Bao De (in office from 103–111). As he was charged with composing inscriptions and dirges for the governor, he gained experience in writing official documents. As Officer of Merit in the commandery, he was also responsible for local appointments to office and recommendations to the capital of nominees for higher office. He spent much of his time composing rhapsodies on the capital cities. When Bao De was recalled to the capital in 111 to serve as a minister of finance, Zhang continued his literary work at home in Xi'e. Zhang Heng began his studies in astronomy at the age of thirty and began publishing his works on astronomy and mathematics.
Official career.
In 112, Zhang was summoned to the court of Emperor An (r. 106–125), who had heard of his expertise in mathematics. When he was nominated to serve at the capital, Zhang was escorted by carriage—a symbol of his official status—to Luoyang, where he became a court gentleman working for the Imperial Secretariat. He was promoted to Chief Astronomer for the court, serving his first term from 115–120 under Emperor An and his second under the succeeding emperor from 126–132. As Chief Astronomer, Zhang was a subordinate of the Minister of Ceremonies, one of Nine Ministers ranked just below the Three Excellencies. In addition to recording heavenly observations and portents, preparing the calendar, and reporting which days were auspicious and which ill-omened, Zhang was also in charge of an advanced literacy test for all candidates to the Imperial Secretariat and the Censorate, both of whose members were required to know at least 9,000 characters and all major writing styles. Under Emperor An, Zhang also served as Prefect of the Majors for Official Carriages under the Ministry of Guards, in charge of receiving memorials to the throne (formal essays on policy and administration) as well as nominees for official appointments.
When the government official Dan Song proposed the Chinese calendar should be reformed in 123 to adopt certain apocryphal teachings, Zhang opposed the idea. He considered the teachings to be of questionable stature and believed they could introduce errors. Others shared Zhang's opinion and the calendar was not altered, yet Zhang's proposal that apocryphal writings should be banned was rejected. The officials Liu Zhen and Liu Taotu, members of a committee to compile the dynastic history "Dongguan Hanji" (東觀漢記), sought permission from the court to consult Zhang Heng. However, Zhang was barred from assisting the committee due to his controversial views on apocrypha and his objection to the relegation of Emperor Gengshi's (r. 23–25) role in the restoration of the Han Dynasty as lesser than Emperor Guangwu's. Liu Zhen and Liu Taotu were Zhang's only historian allies at court, and after their deaths Zhang had no further opportunities for promotion to the prestigious post of court historian.
Despite this setback in his official career, Zhang was reappointed as Chief Astronomer in 126 after Emperor Shun of Han (r. 125–144) ascended to the throne. His intensive astronomical work was rewarded only with the rank and salary of 600 bushels, or "shi", of grain (mostly commuted to coin cash or bolts of silk). To place this number in context, in a hierarchy of twenty official ranks, the lowest-paid official earned the rank and salary of 100 bushels and the highest-paid official earned 10,000 bushels during the Han. The 600-bushel rank was the lowest the emperor could directly appoint to a central government position; any official of lower status was overseen by central or provincial officials of high rank.
In 132, Zhang introduced an intricate seismometer to the court, which he claimed could detect the precise cardinal direction of a distant earthquake. On one occasion his device indicated that an earthquake had occurred in the northwest. As there was no perceivable tremor felt in the capital his political enemies were briefly able to relish the failure of his device, until a messenger arrived shortly afterwards to report that an earthquake had occurred about 400 km (248 mi) to 500 km (310 mi) northwest of Luoyang in Gansu province.
A year after Zhang presented his seismometer to the court, officials and candidates were asked to provide comments about a series of recent earthquakes which could be interpreted as signs of displeasure from Heaven. The ancient Chinese viewed natural calamities as cosmological punishments for misdeeds that were perpetrated by the Chinese ruler or his subordinates on earth. In Zhang's memorial discussing the reasons behind these natural disasters, he criticized the new recruitment system of Zuo Xiong which fixed the age of eligible candidates for the title "Filial and Incorrupt" at age forty. The new system also transferred the power of the candidates' assessment to the Three Excellencies rather than the Generals of the Household, who by tradition oversaw the affairs of court gentlemen. Although Zhang's memorial was rejected, his status was significantly elevated soon after to Palace Attendant, a position he used to influence the decisions of Emperor Shun. With this prestigious new position, Zhang earned a salary of 2,000 bushels and had the right to escort the emperor.
As Palace Attendant to Emperor Shun, Zhang Heng attempted to convince him that the court eunuchs represented a threat to the imperial court. Zhang pointed to specific examples of past court intrigues involving eunuchs, and convinced Shun that he should assume greater authority and limit their influence. The eunuchs attempted to slander Zhang, who responded with a "fu" rhapsody called ""Fu" on Pondering the Mystery", which vents his frustration. Rafe de Crespigny states that Zhang's rhapsody used imagery similar to Qu Yuan's (340–278 BC) poem "Li Sao" and focused on whether or not good men should flee the corrupted world or remain virtuous within it.
Literature and poetry.
While working for the central court, Zhang Heng had access to a variety of written materials located in the Archives of the Eastern Pavilion. Zhang read many of the great works of history in his day and claimed he had found ten instances where the "Records of the Grand Historian" by Sima Qian (145–90 BC) and the "Book of Han" by Ban Gu (AD 32–92) differed from other ancient texts that were available to him. His account was preserved and recorded in the 5th century text of the "Book of Later Han" by Fan Ye (398–445). His rhapsodies and other literary works displayed a deep knowledge of classic texts, Chinese philosophy, and histories. He also compiled a commentary on the "Taixuan" (太玄, "Great Mystery") by the Daoist author Yang Xiong (53 BC–AD 18).
Xiao Tong (501–531), a crown prince of the Liang Dynasty (502–557), immortalized several of Zhang's works in his literary anthology, "Selections of Refined Literature (Wen xuan" 文選")". Zhang's rhapsodies (賦, "fu") include "Western Metropolis Rhapsody" (西京賦), "Eastern Metropolis Rhapsody" (東京賦), "Southern Capital Rhapsody" (南都賦), "Rhapsody on Contemplating the Mystery" (思玄賦), and "Rhapsody on Returning to the Fields" (歸田賦). The latter fuses Daoist ideas with Confucianism and was a precursor to later Chinese metaphysical nature poetry, according to Liu Wu-chi. A set of four short lyric poems ("shi" 詩) entitled "Lyric Poems on Four Sorrows" (四愁詩), is also included with Zhang's preface. This set constitutes some of the earliest heptasyllabic "shi" Chinese poetry written. While still in Luoyang, Zhang became inspired to write his "Western Metropolis Rhapsody" and "Eastern Metropolis Rhapsody", which were based on the "Rhapsody on the Two Capitals" by the historian Ban Gu. Zhang's work was similar to Ban's, although the latter fully praised the contemporaneous Eastern Han regime while Zhang provided a warning that it could suffer the same fate as the Western Han if it too declined into a state of decadence and moral depravity. These two works satirized and criticized what he saw as the excessive luxury of the upper classes. Zhang's "Southern Capital Rhapsody" commemorated his home city of Nanyang, home of the restorer of the Han Dynasty, Guangwu.
In Zhang Heng's poem "Four Sorrows", he laments that he is unable to woo a beautiful woman due to the impediment of mountains, snows and rivers. Rafe de Crespigny, Tong Xiao, and David R. Knechtges claim that Zhang wrote this as an innuendo hinting at his inability to keep in contact with the emperor, hindered by unworthy rivals and petty men. This poem is one of the first in China to have seven words per line. His "Four Sorrows" reads:
In another poem of his called "Stabilizing the Passions" (定情賦) — preserved in a Tang Dynasty (618–907) encyclopedia, but referred to earlier by Tao Qian (365–427) in praise of Zhang's lyrical minimalism — Zhang displays his admiration for an attractive and exemplary woman. This simpler type of "fu" poem influenced later works by the prominent official and scholar Cai Yong (132–192). Zhang wrote:
Zhang's long lyrical poems also revealed a great amount of information on urban layout and basic geography. His rhapsody "Sir Based-On-Nothing" provides details on terrain, palaces, hunting parks, markets, and prominent buildings of Chang'an, the Western Han capital. Exemplifying his attention to detail, his rhapsody on Nanyang described gardens filled with spring garlic, summer bamboo shoots, autumn leeks, winter rape-turnips, perilla, evodia, and purple ginger. Zhang Heng's writing confirms the size of the imperial hunting park in the suburbs of Chang'an, as his estimate for the circumference of the park's encircling wall agrees with the historian Ban Gu's estimate of roughly 400 "li" (one li in Han times was equal to 415.8 m, or 1,364 ft, making the circumference of the park wall 166,320 m, or 545,600 ft). Along with Sima Xiangru (179–117 BC), Zhang listed a variety of animals and hunting game inhabiting the park, which were divided in the northern and southern portions of the park according to where the animals had originally came from: northern or southern China. Somewhat similar to the description of Sima Xiangru, Zhang described the Western Han emperors and their entourage enjoying boat outings, water plays, fishing, and displays of archery targeting birds and other animals with stringed arrows from the tops of along Chang'an's Kunming Lake. The focus of Zhang's writing on specific places and their terrain, society, people, and their customs could also be seen as early attempts of ethnographic categorization. In his poem "Xijing fu", Zhang shows that he was aware of the new foreign religion of Buddhism, introduced via the Silk Road, as well as the legend of the birth of Buddha with the vision of the white elephant bringing about conception. In his "Western Metropolis Rhapsody" (西京賦), Zhang described court entertainments such as "juedi" (角抵), a form of theatrical wrestling accompanied by music in which participants butted heads with bull horn masks.
With his "Responding to Criticism" ("Ying jian" 應間), a work modeled on Yang Xiong's "Justification Against Ridicule", Zhang was an early writer and proponent of the Chinese literary genre "shelun", or hypothetical discourse. Authors of this genre created a written dialogue between themselves and an imaginary person (or a real person of their entourage or association); the latter poses questions to the author on how to lead a successful life. He also used it as a means to criticize himself for failing to obtain high office, but coming to the conclusion that the true gentleman displays virtue instead of greed for power. In this work, Dominik Declercq asserts that the person urging Zhang to advance his career in a time of government corruption most likely represented the eunuchs or Empress Liang Na's (116–150) powerful relatives in the Liang clan. Declercq states that these two groups would have been "anxious to know whether this famous scholar could be lured over to their side", but Zhang flatly rejected such an alignment by declaring in this politically charged piece of literature that his gentlemanly quest for virtue trumped any desire of his for power.
Zhang wrote about the various love affairs of emperors dissatisfied with the imperial harem, going out into the city incognito to seek out prostitutes and sing-song girls. This was seen as a general criticism of the Eastern Han emperors and their imperial favorites, guised in the criticism of earlier Western Han emperors. Besides criticizing the Western Han emperors for lavish decadence, Zhang also pointed out that their behavior and ceremonies did not properly conform with the Chinese cyclical beliefs in yin and yang. In a poem criticizing the previous Western Han Dynasty, Zhang wrote:
Achievements in science and technology.
Astronomy and mathematics.
For centuries the Chinese approximated pi as 3; Liu Xin (d. AD 23) made the first known Chinese attempt at a more accurate calculation of 3.1457, but there is no record detailing the method he used to obtain this figure. In his work around 130, Zhang Heng compared the celestial circle to the diameter of the earth, proportioning the former as 736 and the latter as 232, thus calculating pi as 3.1724. In Zhang's day, the ratio 4:3 was given for the area of a square to the area of its inscribed circle and the volume of a cube and volume of the inscribed sphere should also be 42:32. In formula, with D as diameter and V as volume, D3:V = 16:9 or V=formula_1D3; Zhang realized that the value for diameter in this formula was inaccurate, noting the discrepancy as the value taken for the ratio. Zhang then attempted to remedy this by amending the formula with an additional formula_2D3, hence V=formula_1D3 + formula_2D3 = formula_5D3. With the ratio of the volume of the cube to the inscribed sphere at 8:5, the implied ratio of the area of the square to the circle is √8:√5. From this formula, Zhang calculated pi as the square root of 10 (or approximately 3.162). Zhang also calculated pi as formula_6 = 3.1466 in his book "Ling Xian" (靈憲). In the 3rd century, Liu Hui made the calculation more accurate with his π algorithm, which allowed him to obtain the value 3.14159. Later, Zu Chongzhi (429–500) approximated pi as formula_7 or 3.141592, the most accurate calculation for pi the ancient Chinese would achieve.
In his publication of AD 120 called "The Spiritual Constitution of the Universe" (靈憲, "Ling Xian", lit. "Sublime Model"), Zhang Heng theorized that the universe was like an egg "as round as a crossbow pellet" with the stars on the shell and the Earth as the central yolk. This universe theory is congruent with the geocentric model as opposed to the heliocentric model. Although the ancient Warring States (403–221 BC) Chinese astronomers Shi Shen and Gan De had compiled China's first star catalogue in the 4th century BC, Zhang nonetheless catalogued 2,500 stars which he placed in a "brightly shining" category (the Chinese estimated the total to be 14,000), and he recognized 124 constellations. In comparison, this star catalogue featured many more stars than the 850 documented by the Greek astronomer Hipparchus (c. 190–c.120 BC) in his catalogue, and more than Ptolemy (AD 83–161), who catalogued over 1,000. Zhang supported the "radiating influence" theory to explain solar and lunar eclipses, a theory which was opposed by Wang Chong (AD 27–97). In the "Ling Xian", Zhang wrote:
The Sun is like fire and the Moon like water. The fire gives out light and the water reflects it. Thus the moon's brightness is produced from the radiance of the Sun, and the Moon's darkness is due to (the light of) the sun being obstructed. The side which faces the Sun is fully lit, and the side which is away from it is dark. The planets (as well as the Moon) have the nature of water and reflect light. The light pouring forth from the Sun does not always reach the moon owing to the obstruction of the earth itself—this is called 'an-xu', a lunar eclipse. When (a similar effect) happens with a planet (we call it) an occultation; when the Moon passes across (the Sun's path) then there is a solar eclipse.
Zhang Heng viewed these astronomical phenomena in supernatural terms as well. The signs of comets, eclipses, and movements of heavenly bodies could all be interpreted by him as heavenly guides on how to conduct affairs of state. Contemporary writers also wrote about eclipses and the sphericity of heavenly bodies. The music theorist and mathematician Jing Fang (78–37 BC) wrote about the spherical shape of the Sun and Moon while discussing eclipses:
The Moon and the planets are Yin; they have shape but no light. This they receive only when the Sun illuminates them. The former masters regarded the Sun as round like a crossbow bullet, and they thought the Moon had the nature of a mirror. Some of them recognized the Moon as a ball too. Those parts of the Moon which the Sun illuminates look bright, those parts which it does not, remain dark.
The theory posited by Zhang and Jing was supported by later pre-modern scientists such as Shen Kuo (1031–1095), who expanded on the reasoning of why the Sun and Moon were spherical.
Extra tank for inflow clepsydra.
The outflow clepsydra was a timekeeping device used in China as long ago as the Shang Dynasty (c. 1600–c. 1050 BC), and certainly by the Zhou Dynasty (1122–256 BC). The inflow clepsydra with an indicator rod on a float had been known in China since the beginning of the Han Dynasty in 202 BC and had replaced the outflow type. The Han Chinese noted the problem with the falling pressure head in the reservoir, which slowed the timekeeping of the device as the inflow vessel was filled. Zhang Heng was the first to address this problem, indicated in his writings from 117, by adding an extra compensating tank between the reservoir and the inflow vessel. Zhang also mounted two statuettes of a Chinese immortal and a heavenly guard on the top of the inflow clepsydra, the two of which would guide the indicator rod with their left hand and point out the graduations with their right. Joseph Needham states that this was perhaps the ancestor of all clock jacks that would later sound the hours found in mechanical clocks by the 8th century, but he notes that these figures did not actually move like clock jack figurines or sound the hours. Many additional compensation tanks were added to later clepsydras in the tradition of Zhang Heng. In 610 the Sui Dynasty (581–618) engineers Geng Xun and Yuwen Kai crafted an unequal-armed steelyard balance able to make seasonal adjustments in the pressure head of the compensating tank, so that it could control the rate of water flow for different lengths of day and night during the year. Zhang mentioned a "jade dragon's neck", which in later times meant a siphon. He wrote of the floats and indicator-rods of the inflow clepsydra as follows:
Bronze vessels are made and placed one above the other at different levels; they are filled with pure water. Each has at the bottom a small opening in the form of a 'jade dragon's neck'. The water dripping (from above) enters two inflow receivers (alternately), the left one being for the night and the right one for the day. On the covers of each (inflow receiver) there are small cast statuettes in gilt bronze; the left (night) one is an immortal and the right (day) one is a policeman. These figures guide the indicator-rod (lit. arrow) with their left hands, and indicate the graduations on it with their right hands, thus giving the time.
Water-powered armillary sphere.
Zhang Heng is the first person known to have applied hydraulic motive power (i.e. by employing a waterwheel and clepsydra) to rotate an armillary sphere, an astronomical instrument representing the celestial sphere. The Greek astronomer Eratosthenes (276–194 BC) invented the first armillary sphere in 255 BC. The Chinese armillary sphere was fully developed by 52 BC, with the astronomer Geng Shouchang's addition of a permanently fixed equatorial ring. In AD 84  the astronomers Fu An and Jia Kui added the ecliptic ring, and finally Zhang Heng added the horizon and meridian rings. This invention is described and attributed to Zhang in quotations by Hsu Chen and Li Shan, referencing his book "Lou Shui Chuan Hun Thien I Chieh" (Apparatus for Rotating an Armillary Sphere by Clepsydra Water). It was likely not an actual book by Zhang, but a chapter from his "Hun I" or "Hun I Thu Chu", written in 117 AD. His water-powered armillary influenced the design of later Chinese water clocks and led to the discovery of the escapement mechanism by the 8th century. The historian Joseph Needham (1900–1995) states:
What were the factors leading to the first escapement clock in China? The chief tradition leading to Yi Xing (AD 725 ) was of course the succession of 'pre-clocks' which had started with Zhang Heng about 125. Reason has been given for believing that these applied power to the slow turning movement of computational armillary spheres and celestial globes by means of a water-wheel using clepsydra drip, which intermittently exerted the force of a lug to act on the teeth of a wheel on a polar-axis shaft. Zhang Heng in his turn had composed this arrangement by uniting the armillary rings of his predecessors into the equatorial armillary sphere, and combining it with the principles of the water-mills and hydraulic trip-hammers which had become so widespread in Chinese culture in the previous century.
Zhang did not initiate the Chinese tradition of hydraulic engineering, which began during the mid Zhou Dynasty (c. 6th century BC), through the work of engineers such as Sunshu Ao and Ximen Bao. Zhang's contemporary, Du Shi, (d. AD 38) was the first to apply the motive power of waterwheels to operate the bellows of a blast furnace to make pig iron, and the cupola furnace to make cast iron. Zhang provided a valuable description of his water-powered armillary sphere in the treatise of 125, stating:
The equatorial ring goes around the belly of the armillary sphere 91 and 5/19 (degrees) away from the pole. The circle of the ecliptic also goes round the belly of the instrument at an angle of 24 (degrees) with the equator. Thus at the summer solstice the ecliptic is 67 (degrees) and a fraction away from the pole, while at the winter solstice it is 115 (degrees) and a fraction away. Hence (the points) where the ecliptic and the equator intersect should give the north polar distances of the spring and autumn equinoxes. But now (it has been recorded that) the spring equinox is 90 and 1/4 (degrees) away from the pole, and the autumn equinox is 92 and 1/4 (degrees) away. The former figure is adopted only because it agrees with the (results obtained by the) method of measuring solstitial sun shadows as embodied in the Xia (dynasty) calendar.
Zhang Heng's water-powered armillary sphere had profound effects on Chinese astronomy and mechanical engineering in later generations. His model and its complex use of gears greatly influenced the water-powered instruments of later astronomers such as Yi Xing (683–727), Zhang Sixun (fl. 10th century), Su Song (1020–1101), Guo Shoujing (1231–1316), and many others. Water-powered armillary spheres in the tradition of Zhang Heng's were used in the eras of the Three Kingdoms (220–280) and Jin Dynasty (265–420), yet the design for it was temporarily out of use between 317 and 418, due to invasions of northern Xiongnu nomads. Zhang Heng's old instruments were recovered in 418, when Emperor Wu of Liu Song (r. 420–422) captured the ancient capital of Chang'an. Although still intact, the graduation marks and the representations of the stars, Moon, Sun, and planets were quite worn down by time and rust. In 436, the emperor ordered Qian Luozhi, the Secretary of the Bureau of Astronomy and Calendar, to recreate Zhang's device, which he managed to do successfully. Qian's water-powered celestial globe was still in use at the time of the Liang Dynasty (502–557), and successive models of water-powered armillary spheres were designed in subsequent dynasties.
Zhang's seismograph.
From the earliest times, the Chinese were concerned with the destructive force of earthquakes. It was recorded in Sima Qian's "Records of the Grand Historian" of 91 BC that in 780 BC an earthquake had been powerful enough to divert the courses of three rivers. It was not known at the time that earthquakes were caused by the shifting of tectonic plates in the Earth's crust; instead, the people of the ancient Zhou Dynasty explained them as disturbances with cosmic yin and yang, along with the heavens' displeasure with acts committed (or the common peoples' grievances ignored) by the current ruling dynasty. These theories were ultimately derived from the ancient text of the "Yijing" (Book of Changes), in its fifty-first hexagram. There were other early theories about earthquakes, developed by those such as the ancient Greeks. Anaxagoras (c. 500–428 BC) believed that they were caused by excess water near the surface crust of the earth bursting into the Earth's hollows; Democritus (c. 460–370 BC) believed that the saturation of the Earth with water caused them; Anaximenes (c. 585–c. 525 BC) believed they were the result of massive pieces of the Earth falling into the cavernous hollows due to drying; and Aristotle (384–322 BC) believed they were caused by instability of vapor ("pneuma") caused by the drying of the moist Earth by the Sun's rays.
During the Han Dynasty, many learned scholars—including Zhang Heng—believed in the "oracles of the winds". These oracles of the occult observed the direction, force, and timing of the winds, to speculate about the operation of the cosmos and to predict events on Earth. These ideas influenced Zhang Heng's views on the cause of earthquakes. Against the grain of earlier theories proposed by his fellow Chinese and contemporary Greeks, Zhang Heng believed that earthquakes were caused by wind and air, writing:
The chief cause of earthquake is air, an element naturally swift and shifting from place to place. As long as it is not stirred, but lurks in a vacant space, it reposes innocently, giving no trouble to objects around it. But any cause coming upon it from without rouses it, or compresses it, and drives it into a narrow space ... and when opportunity of escape is cut off, then 'With deep murmur of the Mountain it roars around the barriers', which after long battering it dislodges and tosses on high, growing more fierce the stronger the obstacle with which it has contended.
In 132, Zhang Heng presented to the Han court what many historians consider to be his most impressive invention, the first seismometer. It was named "earthquake weathervane" ("houfeng didongyi" 候風地動儀, lit. instrument for measuring the seasonal winds and the movements of the Earth), and it was able to roughly determine the direction (out of eight directions) where the earthquake came from. According to the "Book of Later Han" (compiled by Fan Ye in the 5th century), his bronze urn-shaped device, with a swinging pendulum inside, was able to detect the direction of an earthquake hundreds of miles/kilometers away. This was essential for the Han government in sending quick aid and relief to regions devastated by this natural disaster. The "Book of Later Han" records that, on one occasion, Zhang's device was triggered, though no observer had felt any seismic disturbance; several days later a messenger arrived from the west and reported that an earthquake had occurred in Longxi (modern Gansu Province), the same direction that Zhang's device had indicated, and thus the court was forced to admit the efficacy of the device.
To indicate the direction of a distant earthquake, Zhang's device dropped a bronze ball from one of eight tubed projections shaped as dragon heads; the ball fell into the mouth of a corresponding metal object shaped as a toad, each representing a direction like the points on a compass rose. His device had eight mobile arms (for all eight directions) connected with cranks having catch mechanisms at the periphery. When tripped, a crank and right angle lever would raise a dragon head and release a ball which had been supported by the lower jaw of the dragon head. His device also included a vertical pin passing through a slot in the crank, a catch device, a pivot on a projection, a sling suspending the pendulum, an attachment for the sling, and a horizontal bar supporting the pendulum. Wang Zhenduo (王振鐸) argued that the technology of the Eastern Han era was sophisticated enough to produce such a device, as evidenced by contemporary levers and cranks used in other devices such as crossbow triggers.
Later Chinese of subsequent periods were able to reinvent Zhang's seismometer. They included the 6th-century mathematician and surveyor Xindu Fang of the Northern Qi Dynasty (550–577) and the astronomer and mathematician Lin Xiaogong of the Sui Dynasty (581–618). Like Zhang, Xindu Fang and Lin Xiaogong were given imperial patronage for their services in craftsmanship of devices for the court. By the time of the Yuan Dynasty (1271–1368), it was acknowledged that all devices previously made were preserved, except for that of the seismometer. This was discussed by the scholar Zhou Mi around 1290, who remarked that the books of Xindu Fang and Lin Xiaogong detailing their seismological devices were no longer to be found. Horwitz, Kreitner, and Needham speculate if Tang Dynasty (618–907) era seismographs found their way to contemporary Japan; according to Needham, "instruments of apparently traditional type there in which a pendulum carries pins projecting in many directions and able to pierce a surrounding paper cylinder, have been described."
Hong-sen Yan states that modern replicas of Zhang's device have failed to reach the level of accuracy and sensitivity described in Chinese historical records. Wang Zhenduo presented two different models of the seismometer based on the ancient descriptions of Zhang's device. In his 1936 reconstruction, the central pillar ("du zhu") of the device was a suspended pendulum acting as a movement sensor, while the central pillar of his second model in 1963 was an inverted pendulum. According to Needham, while working in the Seismological Observatory of Tokyo University in 1939, Akitsune Imamura and Hagiwara made a reconstruction of Zhang's device. While it was John Milne and Wang Zhenduo who argued early on that Zhang's "central pillar" was a suspended pendulum, Imamura was the first to propose an inverted model. He argued that transverse shock would have rendered Wang's immobilization mechanism ineffective, as it would not have prevented further motion that could knock other balls out of their position. On June 13, 2005, modern Chinese seismologists announced that they had successfully created a replica of the instrument.
Anthony J. Barbieri-Low, a Professor of Early Chinese History at the University of California, Santa Barbara, names Zhang Heng as one of several high-ranking Eastern-Han officials who engaged in crafts that were traditionally reserved for artisans ("gong" 工), such as mechanical engineering. Barbieri-Low speculates that Zhang only designed his seismometer, but did not actually craft the device himself. He asserts that this would most likely have been the job of artisans commissioned by Zhang. He writes: "Zhang Heng was an official of moderately high rank and could not be seen sweating in the foundries with the "gong" artisans and the government slaves. Most likely, he worked collaboratively with the professional casters and mold makers in the imperial workshops."
Cartography.
The Wei (220–265) and Jin Dynasty (265–420) cartographer and official Pei Xiu (224–271) was the first in China to describe in full the geometric grid reference for maps that allowed for precise measurements using a graduated scale, as well as topographical elevation. However, map-making in China had existed since at least the 4th century BC with the Qin state maps found in Gansu in 1986. Pinpointed accuracy of the winding courses of rivers and familiarity with scaled distance had been known since the Qin and Han Dynasty, respectively, as evidenced by their existing maps, while the use of a rectangular grid had been known in China since the Han as well. Historian Howard Nelson states that, although the accounts of Zhang Heng's work in cartography are somewhat vague and sketchy, there is ample written evidence that Pei Xiu derived the use of the rectangular grid reference from the maps of Zhang Heng. Rafe de Crespigny asserts that it was Zhang who established the rectangular grid system in Chinese cartography. Needham points out that the title of his book "Flying Bird Calendar" may have been a mistake, and that the book is more accurately entitled "Bird's Eye Map". Historian Florian C. Reiter notes that Zhang's narrative "Guitian fu" contains a phrase about applauding the maps and documents of Confucius of the Zhou Dynasty, which Reiter suggests places maps ("tu") on a same level of importance with documents ("shu"). It is documented that a physical geography map was first presented by Zhang Heng in 116 AD, called a "Ti Hsing Thu".
Odometer and south-pointing chariot.
Zhang Heng is often credited with inventing the first odometer, an achievement also attributed to Archimedes (c. 287–212 BC) and Heron of Alexandria (fl. AD 10–70). Similar devices were used by the Roman and Han-Chinese empires at about the same period. By the 3rd century, the Chinese had termed the device the "ji li gu che", or "li-recording drum carriage" (the modern measurement of li = 500 m/1640 ft).
Ancient Chinese texts describe the mechanical carriage's functions; after one li was traversed, a mechanically driven wooden figure struck a drum, and after ten li had been covered, another wooden figure struck a gong or a bell with its mechanically operated arm. However, there is evidence to suggest that the invention of the odometer was a gradual process in Han Dynasty China that centered around the "huang men"—court people (i.e. eunuchs, palace officials, attendants and familiars, actors, acrobats, etc.) who followed the musical procession of the royal "drum-chariot". There is speculation that at some time during the 1st century BC the beating of drums and gongs was mechanically driven by the rotation of the road wheels. This might have actually been the design of Luoxia Hong (c. 110 BC), yet by at least 125 the mechanical odometer carriage was already known, as it was depicted in a mural of the Xiao Tang Shan Tomb.
The south-pointing chariot was another mechanical device credited to Zhang Heng. It was a non-magnetic compass vehicle in the form of a two-wheeled chariot. Differential gears driven by the chariot's wheels allowed a wooden figurine (in the shape of a Chinese state minister) to constantly point to the south, hence its name. The "Song Shu" (c. AD 500 ) records that Zhang Heng re-invented it from a model used in the Zhou Dynasty era, but the violent collapse of the Han Dynasty unfortunately did not allow it to be preserved. Whether Zhang Heng invented it or not, Ma Jun (200–265) succeeded in creating the chariot in the following century.
Legacy.
Science and technology.
Zhang Heng's mechanical inventions influenced later Chinese inventors such as Yi Xing, Zhang Sixun, Su Song, and Guo Shoujing. Su Song directly named Zhang's water-powered armillary sphere as the inspiration for his 11th-century clock tower. The cosmic model of nine points of Heaven corresponding with nine regions of earth conceived in the work of the scholar-official Chen Hongmou (1696–1771) followed in the tradition of Zhang's book "Spiritual Constitution of the Universe". The seismologist John Milne, who created the modern seismograph in 1876 alongside Thomas Gray and James A. Ewing at the Imperial College of Engineering in Tokyo, commented in 1886 on Zhang Heng's contributions to seismology. The historian Joseph Needham emphasized his contributions to pre-modern Chinese technology, stating that Zhang was noted even in his day for being able to "make three wheels rotate as if they were one." More than one scholar has described Zhang as a polymath. However, some scholars also point out that Zhang's writing lacks concrete scientific theories. Comparing Zhang with his contemporary, Ptolemy (83–161) of Roman Egypt, Jin Guantao, Fan Hongye, and Liu Qingfeng state:
Poetic literature.
Zhang's poetry was widely read during his life and after his death. In addition to the compilation of Xiao Tong mentioned above, the Eastern Wu official Xue Zong (d. 237) wrote commentary on Zhang's poems "Dongjing fu" and "Xijing fu". The influential poet Tao Qian wrote that he admired the poetry of Zhang Heng for its "curbing extravagant diction and aiming at simplicity", in regards to perceived tranquility and rectitude correlating with the simple but effective language of the poet. Tao wrote that both Zhang Heng and Cai Yong "avoided inflated language, aiming chiefly at simplicity", and adding that their "compositions begin by giving free expression to their fancies but end on a note of quiet, serving admirably to restrain undisciplined and passionate nature".
Posthumous honors.
Zhang was given great honors in life and in death. The philosopher and poet Fu Xuan (217–278) of the Wei and Jin dynasties once lamented in an essay over the fact that Zhang Heng was never placed in the Ministry of Works. Writing highly of Zhang and the 3rd-century mechanical engineer Ma Jun, Fu Xuan wrote, "Neither of them was ever an official of the Ministry of Works, and their ingenuity did not benefit the world. When (authorities) employ personnel with no regard to special talent, and having heard of genius neglect even to test it—is this not hateful and disastrous?"
In honor of Zhang's achievements in science and technology, his friend Cui Ziyu (Cui Yuan) wrote a memorial inscription on his burial stele, which has been preserved in the "Guwen yuan". Cui stated, "[Zhang Heng's] mathematical computations exhausted (the riddles of) the heavens and the earth. His inventions were comparable even to those of the Author of Change. The excellence of his talent and the splendour of his art were one with those of the gods." The minor official Xiahou Zhan (243–291) of the Wei Dynasty made an inscription for his own commemorative stele to be placed at Zhang Heng's tomb. It read: "Ever since gentlemen have composed literary texts, none has been as skillful as the Master [Zhang Heng] in choosing his words well ... if only the dead could rise, oh I could then turn to him for a teacher!"
Several things have been named after Zhang in modern times, including the lunar crater Chang Heng, the asteroid 1802 Zhang Heng, and the mineral Zhanghengite.
References.
Bibliography.
</dl>

</doc>
<doc id="34551" url="http://en.wikipedia.org/wiki?curid=34551" title="2001">
2001

2001 ()
will be .
2001 was designated as:

</doc>
<doc id="34569" url="http://en.wikipedia.org/wiki?curid=34569" title="17th century">
17th century

The 17th century was the century that lasted from January 1, 1601, to December 31, 1700, in the Gregorian calendar.
The 17th century falls into the Early Modern period of Europe and in that continent was characterized by the Dutch Golden Age, the Baroque cultural movement, the French "Grand Siècle" dominated by Louis XIV, the Scientific Revolution, and The General Crisis. This last is characterised in Europe most notably by the Thirty Years' War, the Great Turkish War, the end of the Dutch Revolt, the disintegration of the Polish–Lithuanian Commonwealth and the English Civil War.
Some historians extend the scope of the General Crisis to encompass the globe, as with the demographic collapse of the Ming Dynasty, China lost approximately 30% of its population. It was during this period also that European colonization of the Americas began in earnest, including the exploitation of the silver deposits of Potosí in Upper Peru and Mexico, which resulted in great bouts of inflation as wealth was drawn into Europe from the rest of the world.
In the midst of this global General Crisis, there were victory and triumph: In the Near East, the Ottoman, Safavid Persian and Mughal empires grew in strength and the Sikhs began to rise to power in the Punjab. And marathas established in western India. Farther east in Japan, Tokugawa Ieyasu established the Edo period at the beginning of the century, starting the isolationist Sakoku policy that was to last until the 19th century. In China, the collapsing Ming Dynasty was challenged by a series of conquests led by the Manchu warlord Nurhaci, which were consolidated by his son Hong Taiji and finally consummated by his grandson, the Shunzi Emperor, founder of the Qing Dynasty.
European politics during the Crisis were dominated by the France of Louis XIV, where royal power was solidified domestically in the civil war of the Fronde, in which the semi-feudal territorial French nobility was weakened and subjugated to the power of an absolute monarchy through the reinvention of the Palace of Versailles from a hunting lodge to a gilded prison in which a greatly expanded royal court could be more easily kept under surveillance. With domestic peace assured, Louis XIV caused the borders of France to be expanded to include, among other regions, Roussillon, Artois, Dunkirk, Franche-Comté, Strasbourg, Alsace and Lorraine. It was during this century that England's political system became unique in Europe – by the end of the century, the monarch was a symbolic figurehead and Parliament was the dominant force in government – a stark contrast to the rest of Europe, in particular Louis XIV's France.
By the end of the century, Europeans were also aware of logarithms, electricity, the telescope and microscope, calculus, universal gravitation, Newton's Laws of Motion, air pressure and calculating machines due to the work of the first scientists of the Scientific Revolution, including Galileo Galilei, Johannes Kepler, René Descartes, Pierre Fermat, Blaise Pascal, Robert Boyle, Christiaan Huygens, Antonie van Leeuwenhoek, Robert Hooke, Francesco Redi, Isaac Newton, and Gottfried Leibniz, among other luminaries.
Events.
1610s.
1610]]: Pedro de Peralta, governor of New Mexico, establishes the settlement of Santa Fe.
Significant people.
Science and philosophy.
 
Inventions, discoveries, introductions.
Major changes in philosophy and science take place, often characterized as the Scientific revolution.

</doc>
<doc id="34603" url="http://en.wikipedia.org/wiki?curid=34603" title="Election threshold">
Election threshold

The electoral threshold is the minimum share of the vote which a political party requires to secure any representation.
This limit can operate in various ways. For example:
In party-list proportional representation systems, an election threshold is a rule that requires that a party must receive a specified minimum percentage of votes (e.g. 5%), either nationally or within a particular district, to obtain any seats in the parliament.
If there are a number of multi-member constituencies, each constituency will have a quota, i.e. a minimum percentage of the votes in that constituency to be awarded one seat.
One can define two boundaries: a threshold of representation is the minimum vote share that might yield a party a seat (under the most favorable circumstances for the party), while the threshold of exclusion is the maximum vote share that could be insufficient to yield a party a seat (under the least favorable circumstances for the party). Lijphart suggested calculating the informal threshold as the mean of these.
The effect of an electoral threshold is to deny representation to small parties or to force them into coalitions, with the presumption of rendering the election system more stable by keeping out radical factions. It is also argued that in the absence of a ranked ballot system supporters of minor parties are effectively disenfranchised and denied the right of representation by someone of their choosing.
Legal election thresholds in various countries.
In Poland's Sejm, Germany's Bundestag and New Zealand's House of Representatives, the threshold is 5%. However, in Germany and New Zealand, if a party wins a minimum number of directly elected seats—three in Germany and one in New Zealand—the threshold does not apply (though the directly elected seats are kept regardless). The threshold is 3.25% in Israel's Knesset (it was 1% before 1992, 1.5% in 1992–2003 and 2% until March 2014), and 10% in the Turkish parliament. In Poland, ethnic minority parties do not have to reach the threshold level to get into the parliament, and so there is always a small German minority representation in the Sejm. In Romania, for the ethnic minority parties there is a different threshold than for the national parties that run for the Chamber of Deputies.
There are also countries such as Portugal, South Africa, Finland, the Netherlands, and the Republic of Macedonia, that have proportional representation systems without a legal threshold, although the Netherlands has a rule that the first seat can never be a remainder seat, which means that there is an effective threshold of 100% divided by the total number of seats. In the Slovenian parliamentary elections of 1992 and 1996 the threshold was set at 3 parliamentary seats. This meant that the parties needed to win about 3.2% of the votes in order to pass the threshold. In 2000 the threshold was raised to 4% of the votes.
In Sweden, there is a nationwide threshold of 4%, but if a party reaches 12% in one election district, it will take part in the seat allocation for that district. However, through the 2010 election, nobody has been elected based on the 12% rule. In Norway the nationwide electoral threshold of 4% applies only to leveling seats. A party with sufficient local support may still win the regular district seats, even if the party fails to meet the threshold. Following the 2009 election, the Liberal Party won two seats in this manner.
In Australia, which uses a single transferable vote proportional representation system, they avoided the need for a formal electoral threshold by establishing smaller electorates with each multi-member electorate returning fewer members of a Parliament and as such requiring a higher quota percentage in order to be elected. As Australia also uses a ranked voting system supporters of minor parties are not disenfranchised as their votes are redistributed to other candidates according to the voter's nominated order of preference which can then form part of another candidates winning quota.
In elections to the Danish Folketing, the electoral threshold is two percent of the valid votes cast in the election.
In the United States, as the majority of elections are conducted under the first-past-the-post system, legal election thresholds do not apply in the actual voting. However, several states have threshold requirements for parties to obtain automatic ballot access to the next general election without having to submit voter-signed petitions. The threshold requirements have no practical bearing on the two main political parties (the Republican and Democratic parties) as they easily meet the requirements, but have come into play for minor parties such as the Green and Libertarian parties. The threshold rules also apply for independent candidates to obtain ballot access.
Countries can have more than one threshold. Germany, as mentioned earlier, has a "regular" threshold of 5%, but a party winning three constituency seats in the Bundestag can gain additional representation even if it has achieved under 5% of the total vote. Most multiple-threshold systems are still in the proposal stage. For example, in Canada, one proposal to reform the electoral system would see a 5% national threshold, 1% of the vote and 1 seat in the House of Commons, or 2% nationally and 15% of the vote in any one province.
Election thresholds are often implemented with the intention of bringing stability to the political system.
The Parliamentary Assembly of the Council of Europe recommends for parliamentary elections a threshold not higher than 3%. However a 2007 European Court of Human Rights decision, "Yumak and Sadak v. Turkey", held that Turkey's 10% threshold did not violate Article 3 of Protocol 1 of the ECHR (right to free elections). Because Turkey has no limits for independent candidates, the 10% rule has to some extent been circumvented by parties running candidates as independents.
Notable failures to reach the threshold.
Examples of elections where established parties fall below the threshold are:
The amount of unrepresented vote.
Election thresholds can sometimes seriously affect the relationship between the percentages of the popular vote achieved by each party and the distribution of seats.
In the Russian parliamentary elections in 1995, with a threshold excluding parties under 5%, more than 45% of votes went to parties that failed to reach the threshold. In 1998, the Russian Constitutional Court found the threshold legal, taking into account limits in its use.
There has been a similar position in Turkey, which has a 10% threshold: easily higher than in any other country. The justification for such a high threshold was to prevent multi-party coalitions and put a stop to the endless fragmentation of political parties seen in the 1960s and 1970s. However, coalitions ruled between 1991 and 2002, mainstream parties continued to be fragmented and in the 2002 elections as many as 45% of votes were cast for parties which failed to reach the threshold and were thus unrepresented in the parliament.
In the Ukrainian elections of March 2006, for which there was a threshold of 3% (of the overall vote, i.e. including invalid votes), 22% of voters were effectively disenfranchised, having voted for minor candidates. In the parliamentary election held under the same system, fewer voters supported minor parties and the total percentage of disenfranchised voters fell to about 12%.
In Bulgaria, 24% of voters cast their ballots for parties that would not gain representation in the elections of 1991 and 2013.
In the Philippines where party-list seats are only contested in 20% of the 287 seats in the lower house, the effect of the 2% threshold is increased by the large number of parties participating in the election, which means that the threshold is harder to reach. This led to a quarter of valid votes being wasted, on average, and led to the 20% of the seats never being allocated due to the 3-seat cap. In 2007, the 2% threshold was altered to allow parties with less than 1% of first preferences to receive a seat each, the proportion of wasted votes reduced slightly to 21%, but it again increased to 29% in 2010 due to an increase in number of participating parties. These statistics take no account of the wasted votes for a party which is entitled to more than three seats but cannot claim those seats due to the three-seat cap.
Election thresholds can produce a spoiler effect, similar to that in the first-past-the-post voting system, in which minor parties unable to reach the threshold take votes away from other parties with similar ideologies. Fledgling parties in these systems often find themselves in a vicious circle: if a party is perceived as having no chance of meeting the threshold, it often cannot gain popular support, and if the party cannot gain popular support, it will continue to have little or no chance of meeting the threshold. As well as acting against extremist parties, it may also adversely affect moderate parties if the political climate becomes polarized between two major parties at opposite ends of the political spectrum: in such a scenario, moderate voters may abandon their preferred party in favour of a more popular party in the hope of keeping the even less desirable alternative out of power.
On occasion, election thresholds have resulted in a party winning an outright majority of seats without winning an outright majority of votes, the sort of outcome that a proportional voting system is supposed to prevent. For instance, the Turkish AK Party won a majority of seats with less than 50% of votes in three consecutive elections (2002, 2007 and 2011). 
In contrast, elections which use the ranked voting system can take account of each voter's complete indicated ranking preference. For example, the single transferable vote redistributes first preference votes for candidates below the threshold. This permits the continued participation in the election by those whose votes would otherwise be wasted. Minor parties can indicate to their supporters before the vote how they would wish to see their votes transferred. Ranked voting systems are widely used in Australia and Ireland.

</doc>
<doc id="34608" url="http://en.wikipedia.org/wiki?curid=34608" title="1931">
1931

1931 ()
will be .

</doc>
<doc id="34619" url="http://en.wikipedia.org/wiki?curid=34619" title="1900s (decade)">
1900s (decade)

The 1900s (pronounced "nineteen-hundreds" or "nineteen-aughts") was a decade that began on January 1, 1900 and ended on December 31, 1909. The term "nineteen-hundreds" can also equally be used for the years 1900–1999 (see 1900s). The Edwardian era (1901–1910) covers a similar span of time.
Pronunciation varieties.
There are several main varieties of how individual years of the decade are pronounced in English. Using 1906 as an example, they are "nineteen-oh-six", "nineteen-six", and "nineteen-ought-six". Which variety is most prominent depends somewhat on global region and generation. In American English, "nineteen-oh-six" is the most common; "nineteen-six" is less common; "nineteen-ought-six" is recognized but not much used. In British English, "nineteen-six" is more common than it is in American English. In the post–World War II era through the 1990s, mentions of "nineteen-ought-six" or "ought-six" often distinctly connoted old-fashioned speech; for example, it was once used to add to the geriatric-humor effect in the dialogue of the Grampa Simpson character. The strength of the comedic effect diminished during the aughts of the next century, as the public grew used to questioning how to refer to an "ohs" or "aughts" decade. 
Assassinations.
The 1900s were marked by several notable assassinations and assassination attempts:
Economics.
The cost of an American . The cost of a Swedish postage stamp was 100SEK.
People.
Sports.
The Tour de France starts for the first time in 1903.
The first Indianapolis 500 mile race is run in 1911
See also.
Timeline.
The following articles contain brief timelines which list the most prominent events of the decade:
1900 • 1901 • 1902 • 1903 • 1904 • 1905 • 1906 • 1907 • 1908 • 1909
Further reading.
</dl>

</doc>
<doc id="34630" url="http://en.wikipedia.org/wiki?curid=34630" title="1943">
1943

1943 ()
will be .
Events.
Below, events of World War II have the "WWII" prefix.
Births.
March.
 

</doc>
<doc id="34641" url="http://en.wikipedia.org/wiki?curid=34641" title="1863">
1863

Year 1863 (MDCCCLXIII) was a common year starting on Thursday (link will display the full calendar) of the Gregorian calendar and a common year starting on Tuesday of the 12-day slower Julian calendar.

</doc>
<doc id="34649" url="http://en.wikipedia.org/wiki?curid=34649" title="1907">
1907

1907 ()
will be .

</doc>
<doc id="34654" url="http://en.wikipedia.org/wiki?curid=34654" title="1974">
1974

 
1974 ()
will be .
Events.
April.
April – The world population reaches 4 billion people estimated by the United States Census Bureau.
Births.
November.
 
References.
 

</doc>
<doc id="34665" url="http://en.wikipedia.org/wiki?curid=34665" title="1919">
1919

1919 ()
will be .

</doc>
<doc id="34677" url="http://en.wikipedia.org/wiki?curid=34677" title="1916">
1916

1916 ()
will be .
Events.
Below, the events of World War I have the "WWI" prefix.

</doc>
<doc id="34697" url="http://en.wikipedia.org/wiki?curid=34697" title="1789">
1789

Year 1789 (MDCCLXXXIX) was a common year starting on Thursday (link will display the full calendar) of the Gregorian calendar and a common year starting on Monday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34708" url="http://en.wikipedia.org/wiki?curid=34708" title="1940">
1940

1940 ()
will be .
Events.
Below, the events of World War II have the "WWII" prefix.

</doc>
<doc id="34724" url="http://en.wikipedia.org/wiki?curid=34724" title="1921">
1921

1921 ()
will be .

</doc>
<doc id="34739" url="http://en.wikipedia.org/wiki?curid=34739" title="9th century">
9th century

The 9th century is the period from 801 to 900 in accordance with the Julian calendar in the Common Era.
West Africa.
Southeastern Nigeria.
Around the 9th century the Igbo people of what is now southeastern Nigeria developed bronze casts of humans, animals, and mythical creatures. These bronzes, which were used as vessels, amulets, pendants, and sacrificial tools, are among the earliest made bronzes ever found in Nigeria. Most items were part of a burial of a nobleman of the Nri-Igbo culture in the northern part of Igboland. Long distance trading was also discovered through the thousands of glass beads found at the sites. The discovery of the bronzes was made by locals from Igbo Ukwu, and they mark the start of the development of the Nri Kingdom.
Ghana Empire.
The Ghana (Wagadu) Empire (before c. 830 until c. 1235) was located in what is now southeastern Mauritania and western Mali. It is considered the first of the Sahelian Kingdoms, which would exist in some form until the early 20th century.
Western Europe.
British Isles.
Britain experienced a great influx of Viking peoples in the 9th century as the Viking Age continued from the previous century. The kingdoms of the Heptarchy were gradually conquered by the Danes, who set up Anglo-Saxon puppet rulers in each kingdom. This invasion was achieved by a huge military force known as the Great Heathen Army, which was supposedly led by Ivar the Boneless, Halfdan Ragnarsson, and Guthrum. This Danish army first arrived in Britain in 865 in East Anglia. After conquering that kingdom, the army proceeded to capture the city of York (Jorvik) and establish the kingdom of Jorvik. The Danes went on to subjugate the kingdom of Northumbria and to take all but the western portion of Mercia. The remaining kingdom of Wessex was the only kingdom of the Heptarchy left. Alfred the Great managed to maintain his kingdom of Wessex and push back the Viking incursions, relieving the neighbouring kingdoms from the threat of the Danes following his famous victory over them at the Battle of Ethandun in 878. Alfred re-established Anglo-Saxon rule over the western half of Mercia, and the Danelaw was established which separated Mercia into halves, the eastern half remaining under the control of the Danes.
Ireland was also affected by the Viking expansion across the North Sea. Extensive raids were carried out all along the coast and eventually permanent settlements were established, such as that of Dublin in 841. Particular targets for these raids were the monasteries on the western coast of Ireland, as they provided a rich source for loot. On such raids the Vikings set up impermanent camps, which were called longphorts by the Irish—this period of Viking raids on the coasts of Ireland has been named the longphort phase after these types of settlements. Ireland in the 9th century was organised into an amalgam of small kingdoms, called tuatha. These kingdoms were sometimes grouped together and ruled by a single, provincial ruler. If such a ruler could establish and maintain authority over a portion of these tuatha, he was sometimes granted the title of High King.
Scotland also experienced significant Viking incursions during the 9th century. The Vikings established themselves in coastal regions, usually in northern Scotland, and in the northern islands such as Orkney and Shetland. The Viking invasion and settlement in Scotland provided a contributing factor in the collapse of the kingdoms of the Picts, who inhabited most of Scotland at the time. Not only were the Pictish realms either destroyed or severely weakened, the Viking invasion and settlement may have been the reason for the movement of Kenneth MacAlpin, the king of Dál Riata at that time. The kingdom of Dál Riata was located on the western coast of Scotland, and Viking incursions destroyed it after the death of its previous king, Áed mac Boanta in 839, according to the Annals of Ulster. This may have caused the new king, MacAlpin, to move to the east, and conquer the remnants of the Pictish realms. MacAlpin became king of the Picts in 843 and later kings would be titled as the King of Alba or King of Scots.
Art of the 9th century.
Art in the 9th century was primarily dedicated to the Gospel and employed as basic tools of the Roman Catholic mass. Thousands of golden art objects were made: Sacred cups, vessels, reliqueries, crucifixes, rosaries, altar pieces, and statues of the Virgin and Child or Saints all kept the flame of art from dying out. Architecture began to revive to some extent by the 9th century, taking the form of Church facilities of all kinds, and the first castle fortifications since Roman times began to take form in simple "moat and baily" castles, or simple "strong point" tower structures, with little refinement.
See also.
Timeline of 9th-century Muslim history

</doc>
<doc id="34762" url="http://en.wikipedia.org/wiki?curid=34762" title="1857">
1857

Year 1857 (MDCCCLVII) was a common year starting on Thursday (link will display the full calendar) of the Gregorian calendar and a common year starting on Tuesday of the 12-day slower Julian calendar.

</doc>
<doc id="34772" url="http://en.wikipedia.org/wiki?curid=34772" title="1882">
1882

Year 1882 (MDCCCLXXXII) was a common year starting on Sunday (link will display the full calendar) of the Gregorian calendar and a common year starting on Friday of the 12-day slower Julian calendar.

</doc>
<doc id="34785" url="http://en.wikipedia.org/wiki?curid=34785" title="1862">
1862

Year 1862 (MDCCCLXII) was a common year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a common year starting on Monday of the 12-day later Julian calendar. This year was named by Mitchell Stephens as the greatest year to read newspapers.

</doc>
<doc id="34798" url="http://en.wikipedia.org/wiki?curid=34798" title="4th century BC">
4th century BC

The 4th century BC started the first day of 400 BC and ended the last day of 301 BC. It is considered part of the Classical era, epoch, or historical period.
This century marked the height of Classical Greek civilization in all of its aspects. By the year 400 Greek philosophy, art, literature and architecture had spread far and wide, with the numerous independent Greek colonies that had sprung up throughout the lands of the eastern Mediterranean.
Arguably the most important series of political events in this period were the conquests of Alexander, bringing about the collapse of the once formidable Persian Empire and spreading Greek culture far into the east. Alexander dreamed of an east/west union, but when his short life ended, his vast empire was plunged into civil war as his generals each carved out their own separate kingdoms. Thus began the Hellenistic age, a period characterized by a more absolute approach to rule, with Greek kings taking on royal trappings and setting up hereditary successions. While a degree of democracy still existed in some of the remaining independent Greek cities, many scholars see this age as marking the end of classical Greece.
In India, the Mauryan Empire was founded in 322 BCE by Chandragupta Maurya, who had overthrown the Nanda Dynasty and rapidly expanded his power westwards across central and western India, taking advantage of the disruptions of local powers in the wake of the withdrawal westward by the armies of Alexander.
China in the 4th century BCE entered an era of constant warfare known as the Warring States period. The period saw the rapid rise of large states (such as Chu) over smaller ones thanks to technological advancement. Though the period has usually been characterize by historians as being excessively violent compared to the Spring and Autumn period it was also punctured by several cultural and social growth through the expansion of several different sects of Confucianism and Taoism. 
Sovereign States.
See: List of sovereign states in the 4th century BC.

</doc>
<doc id="34847" url="http://en.wikipedia.org/wiki?curid=34847" title="1989">
1989

1989 ()
will be .
It was a historical turning point for the wave of revolutions that swept the Eastern Bloc, starting in Poland that summer with the beginning of a move towards private enterprise, coming to a head with opening of the Berlin Wall in November, embracing the overthrow of the communist dictatorship in Romania in December and ending in December 1991 with the dissolution of the Soviet Union. Collectively known as the Revolutions of 1989, they heralded the beginning of the post–Cold War period.
It was the year of the first Brazilian presidential elections in 29 years, since the end of the military government in 1985 which commanded the country for more than twenty years, and marked the redemocratization process's final point. F. W. de Klerk was elected in South Africa, and his regime gradually dismantled the apartheid system over the next five years. The Echo Boom in the United States reached its peak in 1989-1990.
The early Internet, by then 20 years old, was rapidly evolving in 1989. The first commercial Internet service providers surfaced in this year. The World Wide Web was first conceived of in Switzerland by Tim Berners-Lee and was opened to the public for free use in 1993 after several years of development at CERN. The first unofficial text message was also sent in 1989.
1989 marked the beginning of the current Heisei period in Japan. It is also, the latest year, when written in Roman numerals, to have an L. The upcoming year is as late as 2040.

</doc>
<doc id="34857" url="http://en.wikipedia.org/wiki?curid=34857" title="11th century BC">
11th century BC

The 11th century comprises all years from 1100 to 1001 . Although many human societies were literate in this period, some of the individuals mentioned below may be apocryphal rather than historically accurate.
Sovereign States.
See: List of sovereign states in the 11th century BC.

</doc>
<doc id="34893" url="http://en.wikipedia.org/wiki?curid=34893" title="1640">
1640

Year 1640 (MDCXL) was a leap year starting on Sunday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Wednesday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34906" url="http://en.wikipedia.org/wiki?curid=34906" title="1690s">
1690s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="34922" url="http://en.wikipedia.org/wiki?curid=34922" title="1842">
1842

Year 1842 (MDCCCXLII) was a common year starting on Saturday (link will display the full calendar) of the Gregorian calendar and a common year starting on Thursday of the 12-day slower Julian calendar.

</doc>
<doc id="34937" url="http://en.wikipedia.org/wiki?curid=34937" title="1858">
1858


</doc>
<doc id="34938" url="http://en.wikipedia.org/wiki?curid=34938" title="1878">
1878

Year 1878 (MDCCCLXXVIII) was a common year starting on Tuesday (link will display the full calendar) of the Gregorian calendar and a common year starting on Sunday of the 12-day slower Julian calendar.

</doc>
<doc id="34949" url="http://en.wikipedia.org/wiki?curid=34949" title="1812">
1812

Year 1812 (MDCCCXII) was a leap year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Monday of the 12-days-behind Julian calendar.

</doc>
<doc id="34961" url="http://en.wikipedia.org/wiki?curid=34961" title="1755">
1755

Year 1755 (MDCCLV) was a common year starting on Wednesday (link will display the full calendar) of the Gregorian calendar (or a common year starting on Sunday of the 11-day slower Julian calendar).
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="34972" url="http://en.wikipedia.org/wiki?curid=34972" title="1554">
1554

Year 1554 (MDLIV) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="34985" url="http://en.wikipedia.org/wiki?curid=34985" title="1135">
1135

Year 1135 (MCXXXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="34994" url="http://en.wikipedia.org/wiki?curid=34994" title="1520s">
1520s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35006" url="http://en.wikipedia.org/wiki?curid=35006" title="202 BC">
202 BC

Year 202 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Geminus and Nero (or, less frequently, year 552 "Ab urbe condita"). The denomination 202 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="35017" url="http://en.wikipedia.org/wiki?curid=35017" title="1600">
1600

Year 1600 (MDC) was a leap year starting on Saturday (link will display the full calendar) and a century leap year of the Gregorian calendar and a leap year starting on Tuesday of the 10-day slower Julian calendar. It was the last century leap year until the year 2000.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35027" url="http://en.wikipedia.org/wiki?curid=35027" title="820s">
820s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35037" url="http://en.wikipedia.org/wiki?curid=35037" title="770s">
770s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35051" url="http://en.wikipedia.org/wiki?curid=35051" title="1593">
1593

Year 1593 (MDXCIII) was a common year starting on Friday (link will display the full calendar) of the Gregorian calendar and a common year starting on Monday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35061" url="http://en.wikipedia.org/wiki?curid=35061" title="60 Minutes">
60 Minutes

60 Minutes is an American newsmagazine television program that is broadcast on the CBS television network. Debuting in 1968, the program was created by Don Hewitt, who chose to set it apart from other news programs by using a unique style of reporter-centered investigation. In 2002, "60 Minutes" was ranked at #6 on TV Guide's 50 Greatest TV Shows of All Time. "The New York Times" has called it "one of the most esteemed news magazines on American television".
Broadcast history.
Early years.
The program employed a magazine format, similar to that of the Canadian program "W5", which premiered two years earlier. It pioneered many of the most important investigative journalism procedures and techniques, including re-editing interviews, hidden cameras, and "gotcha journalism" visits to the home or office of an investigative subject. Similar programs sprang up in Australia and Canada during the 1970s, as well as on local television news.
Initially, "60 Minutes" aired as a bi-weekly show hosted by Harry Reasoner and Mike Wallace, debuting on September 24, 1968, and alternating weeks with other CBS News productions on Tuesday evenings at 10:00 p.m. Eastern Time. The first edition, described by Reasoner in the opening as a "kind of a magazine for television," featured the following segments:
The first "magazine-cover" chroma key was a photo of two helmeted policemen (for the Clark interview segment). Wallace and Reasoner sat in chairs on opposite sides of the set, which had a cream-colored backdrop; the more famous black backdrop (which is still used as of 2015[ [update]]) did not appear until the following year. The logo was in Helvetica type with the word "Minutes" spelled in all lower-case letters; the logo most associated with the show (rendered in Eurostile type with "Minutes" spelled in uppercase) did not appear until about 1974. Further, to extend the magazine motif, the producers added a "Vol. xx, No. xx" to the title display on the chroma key; modeled after the volume and issue number identifications featured in print magazines, this was used until about 1971. The trademark stopwatch, however, did not appear on the inaugural broadcast; it would not debut until several episodes later. Alpo dog food was the sole sponsor of the first program.
Don Hewitt, who had been a producer of the "CBS Evening News" with Walter Cronkite, sought out Wallace as a stylistic contrast to Reasoner. According to one historian of the show, the idea of the format was to make the hosts the reporters, to always feature stories that were of national importance but focused upon individuals involved with, or in conflict with, those issues, and to limit the reports' airtime to around 13 minutes. However, the initial season was troubled by lack of network confidence, as the program did not garner ratings much higher than that of other CBS News documentaries. As a rule, during that era, news programming during prime time lost money; networks mainly scheduled public affairs programs in prime time in order to bolster the prestige of their news departments, and thus boost ratings for the regular evening newscasts, which were seen by far more people than documentaries and the like. "60 Minutes" struggled under that stigma during its first three years.
Changes to "60 Minutes" came fairly early in the program's history. When Reasoner left CBS to co-anchor ABC's evening newscast (he would return to CBS and "60 Minutes" in 1978), Morley Safer joined the team in 1970, and he took over Reasoner's duties of reporting less aggressive stories. However, when Richard Nixon began targeting press access and reporting, even Safer, formerly the CBS News bureau chief in Saigon and London, began to do "hard" investigative reports, and during the 1970–71 season alone "60 Minutes" reported on cluster bombs, the South Vietnamese Army, draft dodgers, Nigeria, the Middle East, and Northern Ireland.
Effects from the Prime Time Access Rule.
By 1971, the Federal Communications Commission (FCC) introduced the Prime Time Access Rule, which freed local network affiliates in the top 50 markets (in practice, the entire network) to take a half-hour of prime time from the networks on Mondays through Saturdays and one full hour on Sundays. Because nearly all affiliates found production costs for the FCC's intended goal of increased public affairs programming very high and the ratings (and by association, advertising revenues) low, making it mostly unprofitable, the FCC created an exception for network-authored news and public affairs shows. After a six-month hiatus in late 1971, CBS found a prime place for "60 Minutes" in a portion of that displaced time, 6:00 to 7:00 p.m. (Eastern; 5:00 to 6:00 Central Time on Sundays, in January 1972.
This proved somewhat less than satisfactory, however, because in order to accommodate CBS' telecasts of late afternoon National Football League (NFL) football games, "60 Minutes" went on hiatus during the fall from 1972 to 1975 (and the summer of 1972). This took place because football telecasts were protected contractually from interruptions in the wake of the infamous "Heidi Bowl" incident on NBC in November 1968. Despite the irregular scheduling, the program's hard-hitting reports attracted a steadily growing audience, particularly during the waning days of the Vietnam War and the gripping events of the Watergate scandal; at that time, few if any other major network news shows did in-depth investigative reporting to the degree carried out by "60 Minutes." Eventually, during the summers of 1973 through 1975, CBS did allow the program back onto the prime time schedule proper, on Fridays in 1973 and Sundays the two years thereafter, as a replacement for programs aired during the regular television season.
It was only when the FCC returned an hour to the networks on Sundays (for children's/family or news programming), which had been taken away from them four years earlier, in a 1975 amendment to the Access Rule that CBS finally found a viable permanent timeslot for "60 Minutes." When a family-oriented drama, "Three for the Road", ended after a 12-week run in the fall, the newsmagazine took its place at 7:00 p.m. Eastern Time (6:00 Central) on December 7. It has aired at that time since, for 40 years as of 2015[ [update]], making "60 Minutes" not only the longest-running prime time program currently in production, but also the television program (excluding daily programs such as evening newscasts or morning news-talk shows) broadcasting for the longest length of time at a single time period each week in U.S. television history.
This move, and the addition of then-White House correspondent Dan Rather to the reporting team, made the program into a strong ratings hit and, eventually, a general cultural phenomenon. This was no less than a stunning reversal of the historically poor ratings performances of documentary programs on network television. By 1976, "60 Minutes" became the top-rated program on Sunday nights in the U.S. By 1979, it had achieved the #1 spot among all television programs in the Nielsen ratings, unheard of before for a news broadcast in prime time. This success translated into great profits for CBS; advertising rates went from $17,000 per 30-second spot in 1975 to $175,000 in 1982.
The program sometimes does not start until after 7:00 p.m. Eastern, due largely to CBS' live broadcast of NFL games. At the conclusion of an NFL game, "60 Minutes" will air in its entirety. However, on the West Coast (and all of the Mountain Time Zone), because the actual end of the live games is much earlier in the afternoon in comparison to the Eastern and Central time zones, "60 Minutes" is always able to start at its normal start time of 7:00 p.m. Pacific Time, leaving affiliates free to broadcast local news, the "CBS Evening News", and other local or syndicated programming leading up to "60 Minutes". The program's success has also led CBS Sports to schedule events (such as the Masters Tournament and daytime games of the NCAA Men's Basketball Tournament) leading into "60 Minutes" and the rest of the network's primetime lineup, thus (again, except on the West Coast) pre-empting the Sunday editions of the "CBS Evening News" and affiliates' local newscasts.
With complaints of late starts because of late NFL games, starting in the 2012-13 season, CBS officially changed the start time of "60 Minutes" to 7:30 p.m. Eastern time on Sundays when the network is scheduled to air an NFL doubleheader (there are nine during the NFL season – eight during the first 16 weeks of the season, and the final week).
Pre-emptions since 1978.
The program has rarely been pre-empted since 1978. Two notable pre-emptions occurred in 1976 and 1977, to make room for the annual telecast of "The Wizard of Oz", which had recently returned to CBS after having been shown on NBC for eight years. However, CBS would, in later years, schedule the film so that it would no longer pre-empt "60 Minutes". Another exception is on years when CBS airs the Super Bowl or since 2003, alternating years where the AFC Championship Game has the 6:30 p.m. Eastern start time, which is played into prime-time and followed by a special lead-out program.
On September 22, 2013, CBS chose to pre-empt "60 Minutes" as a result of carrying the 65th Primetime Emmy Awards after an NFL doubleheader.
Radio broadcast and Internet distribution.
"60 Minutes" is also simulcast on several CBS Radio stations (such as WCBS in New York City, KNX in Los Angeles, WBBM in Chicago, WWJ in Detroit and KCBS in San Francisco) when it airs locally on their sister CBS Television Network affiliate; even in the Central and Eastern time zones, the show is aired at the top of the hour at 7 p.m./6 p.m Central (barring local sports play-by-play pre-emptions and breaking news coverage) no matter how long the show is delayed on CBS Television, resulting in radio listeners often hearing the show on those stations ahead of the television broadcast. An audio version of each broadcast without advertising began to be distributed via podcast and the iTunes Store, starting with the September 23, 2007 broadcast. Video from "60 Minutes" (including full episodes) is also made available for streaming several hours after the program's initial broadcast on CBSNews.com and CBS Interactive property CNET TV.
Format.
"60 Minutes" consists of three long-form news stories, without superimposed graphics. There is a commercial break between two stories. Each story is introduced from a set with a backdrop resembling pages from a magazine story on the same topic. The program undertakes its own investigations and follows up on investigations instigated by national newspapers and other sources.
Reporting tone.
"60 Minutes" blends the probing journalism of the seminal 1950s CBS series "See It Now" with Edward R. Murrow (a show for which Hewitt served as the director for its first few years) and the personality profiles of another Murrow program, "Person to Person". In Hewitt's own words, "60 Minutes" blends "higher Murrow" and "lower Murrow".
"Point/Counterpoint" segment.
For most of the 1970s, the program included "Point/Counterpoint", in which a liberal and a conservative commentator debated a particular issue. This segment originally featured James J. Kilpatrick representing the conservative side and Nicholas von Hoffman for the liberal, with Shana Alexander taking over for von Hoffman after he departed in 1974. The segment was an innovation that caught the public imagination as a live version of competing editorials. "Point/Counterpoint" was also lampooned by the NBC comedy series "Saturday Night Live", which featured Jane Curtin and Dan Aykroyd as debaters, with Aykroyd typically beginning his remarks with, "Jane, you ignorant slut"; in the 1980 film "Airplane!", in which the "faux" Kilpatrick argues in favor of the plane crashing; and in the earlier sketch comedy film, "The Kentucky Fried Movie", where the segment was called "Count/Pointercount".
A similar concept was revived briefly in March 2003, this time featuring Bob Dole and Bill Clinton, former opponents in the 1996 presidential election. The pair agreed to do ten segments, called "Clinton/Dole" and "Dole/Clinton" in alternating weeks, but did not continue into the 2003–04 fall television season. Reports indicated that the segments were considered too gentlemanly, in the style of the earlier "Point/Counterpoint", and lacked the feistiness of "Crossfire".
Andy Rooney segment.
From 1978 to 2011, the program usually ended with a (usually light-hearted and humorous) commentary by Andy Rooney expounding on topics of wildly varying import, ranging from international politics, to economics, and to personal philosophy on everyday life. One recurring topic was measuring the amount of coffee in coffee cans.
Rooney's pieces, particularly one in which he referred to actor Mel Gibson as a "wacko," on occasion led to complaints from viewers. Rooney published several books documenting his contributions to the program, including "Years Of Minutes" and "A Few Minutes With Andy Rooney". Rooney retired from "60 Minutes", delivering his final commentary on October 2, 2011; it was his 1,097th commentary over his 34-year career on the program. He died one month later, on November 4, 2011. The November 13, 2011 edition of "60 Minutes" featured an hour-long tribute to Rooney and his career, and included a rebroadcast of his final commentary segment.
Opening sequence.
The opening sequence features a "60 Minutes" "magazine cover" with the show's trademark, an Aristo stopwatch, intercut with preview clips of the episode's stories. The sequence ends with each of the current correspondents and hosts introducing themselves. The last host who appears (currently Scott Pelley) then currently says, "Those stories tonight on "60 Minutes"". When Rooney was a prominent fixture, the final line was "Those stories and Andy Rooney, tonight on "60 Minutes"". Before that, and whenever Rooney did not appear, the final line was "Those stories and more, tonight on "60 Minutes"".
"60 Minutes" was the first, and remains the only, regularly scheduled program in the U.S. to never have used theme music. The only "theme" is the ticking of the stopwatch, which counts off each of the broadcast's titular 60 minutes, starting from zero at the beginning of each show. It is seen during the opening title sequence, before each commercial break, and at the tail-end of the closing credits, and each time it appears it displays (within reasonable accuracy) the elapsed time of the episode to that point.
On October 29, 2006, the opening sequence changed from a black background, which had been used for over a decade, to white. Also, the gray background for the Aristo stopwatch in the "cover" changed to red, the color for the title text changed to white, and the stopwatch itself changed from the diagonal position it had been oriented in for 31 years to an upright position.
Web content.
Videos and transcripts of "60 Minutes" editions, as well as clips that were not included in the broadcast are available on the program's website. In September 2010, the program launched a website called "60 Minutes Overtime", in which stories broadcast on-air are discussed in further detail.
iPad content.
CBS Interactive released a mobile app in 2013, "60 Minutes for iPad", which allows users to watch "60 Minutes" on iPad devices and access some of the show's archival footage.
Correspondents and hosts.
Former correspondents and hosts.
† = Deceased
Commentators.
Commentators for "60 Minutes" have included:
† = Deceased
Ratings and recognition.
Nielsen ratings.
Based on ratings, "60 Minutes" is the most successful program in U.S. television history, since it was moved into its present timeslot in 1975. For five of its seasons it has been that year's top program, a feat matched by the sitcoms "All in the Family" and "The Cosby Show", and surpassed only by the reality competition series "American Idol", which had been the #1 show for eight consecutive seasons from the 2003–04 television season up to the 2010–11 season. "60 Minutes" was a top ten show for 23 seasons in a row (1977–2000), an unsurpassed record.
"60 Minutes" first broke into the Nielsen Top 20 during the 1976–77 season. The following season, it was the fourth-most-watched program, and by 1979–80, it was the number one show. During the 21st century, it remains among the top 20 programs in the Nielsen ratings, and the highest-rated news magazine.
The November 16, 2008 edition, featuring an interview with President-Elect Barack Obama, earned a total viewership of 25.1 million viewers.
The October 6, 2013 edition (which was delayed by 44 minutes that evening due to a Denver Broncos-Dallas Cowboys NFL game) drew 17.94 million viewers; retaining 63% of the 28.32 million viewers of its lead-in, and making it the most watched "60 Minutes" broadcast since December 16, 2012.
The December 1, 2013 edition (delayed 50 minutes due to a Broncos-Kansas City Chiefs game) was watched by 18.09 million viewers, retaining 66% of its NFL lead-in (which earned 28.11 million viewers during the 7:00 p.m. hour).
Recognition.
Emmy Awards.
As of October 1, 2013, "60 Minutes" had won a total of 106 Emmy Awards, a record unsurpassed by any other primetime program on U.S. television.
Peabody Awards.
The program has won 20 Peabody Awards for segments including "All in the Family", an investigation into abuses by government and military contractors; "The CIA's Cocaine", which uncovered CIA involvement in drug smuggling; "Friendly Fire", a report on incidents of friendly fire in the Gulf War; "The Duke Rape Case", an investigation into accusations of rape at an off campus lacrosse team party in 2006; and "The Killings in Haditha", an investigation into the killing of Iraqi civilians by U.S. Marines.
Other awards.
The show received an Investigative Reporter and Editor medal for their segment "The Osprey", documenting a Marine cover-up of deadly flaws in the V-22 Osprey aircraft.
Impact on innocent victims.
In 1983, a report by Morley Safer, "Lenell Geter's in Jail", helped exonerate a Texas man who was wrongly convicted and imprisoned for armed robbery.
Record of longest-running show.
"60 Minutes" currently holds the record for the longest continuously running program of any genre scheduled during American network prime time; it has aired at 7:00 p.m. Eastern Time on Sundays since December 7, 1975 (although since 1998, it is officially scheduled for 7:30 p.m. Eastern Time on Sundays where a CBS affiliate has a late NFL game). The longer-running "Meet the Press" has also aired in prime time, but currently airs during the daytime, as it has for most of its history. The Walt Disney anthology television series, which premiered in 1954, and the "Hallmark Hall of Fame", which has aired since 1951, have aired longer, but none of them has aired in prime time continually, as "60 Minutes" has done.
Controversies.
The show has been praised for landmark journalism and received many awards. However, it has also become embroiled in some controversy, including (in order of appearance):
Unintended acceleration.
On November 23, 1986, "60 Minutes" aired a segment greenlit by Hewitt, concerning the Audi 5000 automobile, a popular German luxury car. The story covered a supposed problem of "unintended acceleration" when the brake pedal was pushed, with emotional interviews with six people who sued Audi (unsuccessfully) after they crashed their cars, including one woman whose 6-year-old boy had been killed. Footage was shown of an Audi 5000 with the accelerator moving down on its own, accelerating the car, after an expert witness employed by one of the plaintiffs modified it with a concealed device to cause it to do so. Independent investigators concluded that this was most likely due to driver error, where the driver let their foot slip off the brake and onto the accelerator. Tests by Audi and independent journalists showed that even with the throttle wide open, the car would simply stall if the brakes were actually being used.
The incident devastated Audi sales in the United States, which did not rebound for 15 years. The initial incidents which prompted the report were found by the National Highway Traffic Safety Administration and Transport Canada to have been attributable to operator error, where car owners had depressed the accelerator pedal instead of the brake pedal. CBS issued a partial retraction, without acknowledging the test results of involved government agencies. Years later, "Dateline NBC", a rival to "60 Minutes", was found guilty of similar tactics regarding the fuel tank integrity of General Motors pickup trucks.
Alar.
In February 1989, "60 Minutes" aired a report by the Natural Resources Defense Council claiming that the use of daminozide (Alar) on apples presented an unacceptably high health risk to consumers. Apple sales dropped and CBS was sued unsuccessfully by apple growers. Alar was subsequently banned for use on food crops in the U.S. by the Environmental Protection Agency (EPA).
Werner Erhard.
On March 3, 1991, "60 Minutes" broadcast "Werner Erhard," which dealt with controversies involving Erhard's personal and business life. One year after the "60 Minutes" piece aired, Erhard filed a lawsuit against CBS, claiming that the broadcast contained several "false, misleading and defamatory" statements about himself. One month after filing the lawsuit, Erhard filed for dismissal. Erhard later told Larry King in an interview that he dropped the suit after receiving legal advice telling him that in order to win it, it would not be sufficient to prove that CBS knew the allegations were false, but that he would also need to prove that CBS acted with malice. Because of factual inaccuracies, the segment was later removed by CBS from its archives, with a disclaimer: "This segment has been deleted at the request of CBS News for legal or copyright reasons."
Brown & Williamson.
In 1995, former Brown & Williamson Vice President for Research and Development Jeffrey Wigand provided information to "60 Minutes" producer Lowell Bergman that B&W had systematically hidden the health risks of their cigarettes (see ). Furthermore, it was alleged that B&W had introduced foreign agents (such as fiberglass and ammonia) with the intent of enhancing the effect of nicotine. Bergman began to produce a piece based upon the information, but ran into opposition from Don Hewitt who, along with CBS lawyers, feared a billion dollar lawsuit from Brown and Williamson for tortious interference for encouraging Wigand to violate his nondisclosure agreement. A number of people at CBS would benefit from a sale of CBS to Westinghouse Electric Corporation, including the head of CBS lawyers and CBS News. Also, because of the interview, the son of CBS President Laurence Tisch (who also controlled Lorillard Tobacco) was among the people from the big tobacco companies at risk of being caught having committed perjury. Due to Hewitt's hesitation, "The Wall Street Journal" instead broke Wigand's story. The "60 Minutes" piece was eventually aired with substantially altered content and minus some of the most damning evidence against B&W. The exposé of the incident was published in an article in "Vanity Fair" by Marie Brenner, entitled "The Man Who Knew Too Much".
"The New York Times" wrote: "the traditions of Edward R. Murrow were diluted in the process," though the newspaper revised the quote slightly, suggesting that "60 Minutes" and CBS had "betrayed the legacy of Edward R. Murrow". The incident was turned into a seven-times Oscar-nominated feature film entitled "The Insider", directed by Michael Mann and starring Russell Crowe as Wigand, Al Pacino as Bergman, and Christopher Plummer as Mike Wallace. Wallace denounced the portrayal of him as inaccurate to his stance on the issue.
U.S. Customs Service.
"60 Minutes" alleged in 1997 that agents of the U.S. Customs Service ignored drug trafficking across the Mexico – United States border at San Diego. The only evidence was a memorandum apparently written by Rudy Camacho, who was the head of the San Diego branch office. Based on this memo, CBS alleged that Camacho had allowed trucks belonging to a particular firm to cross the border unimpeded. Mike Horner, a former Customs Service employee, had passed the memos on to "60 Minutes", and even provided a copy with an official stamp. Camacho was not consulted about the piece, and his career was devastated in the immediate term as his own department placed suspicion on him. In the end, it turned out that Horner had forged the documents as an act of revenge for his treatment within the Customs Service. Camacho sued CBS and settled for an undisclosed amount of money in damages. Hewitt was forced to issue an on-air retraction.
Kennewick Man.
A legal battle between archaeologists and the Umatilla tribe over the remains of a skeleton, nicknamed Kennewick Man, was reported by "60 Minutes" on October 25, 1998, to which the Umatilla tribe reacted negatively. The tribe considered the segment heavily biased in favor of the scientists, cutting out important arguments, such as explanations of Native American Graves Protection and Repatriation Act. The report focused heavily on the racial politics of the controversy and also added inflammatory arguments, such as questioning the legitimacy of Native American sovereignty – much of the racial focus of the segment was later reported to have been either unfounded and/or misinterpreted.
Timothy McVeigh.
On March 12, 2000, "60 Minutes" aired an interview with Oklahoma City bomber, Timothy McVeigh. At the time, McVeigh had already been convicted and sentenced to death for the April 1995 bombing of the Alfred P. Murrah Federal Building and subsequent deaths of 168 people. On the program, McVeigh was given the opportunity to vent against the government. Following the program, a federal policy called the Special Confinement Unit Media Policy was enacted prohibiting face-to-face interviews with death row inmates. A federal inmate challenged the policy in "Hammer v. Ashcroft", under which the U.S. Court of Appeals for the Seventh Circuit upheld the prison policy. In March 2010, the United States Supreme Court declined to hear an appeal in the case, and the policy limiting media access to death row inmates remains in place. CBS refuses to show the entire interview, and has stated no reasons.
Viacom/CBS cross-promotion.
In recent years, the program has been accused of promoting books, films, and interviews with celebrities who are published or promoted by sister businesses of media conglomerate Viacom (which owned CBS from 2000 to 2005, and is now a separate company owned by National Amusements, which is also the parent of CBS Corporation) and publisher Simon & Schuster (which remains a part of CBS Corporation after the 2005 CBS/Viacom split), without disclosing the journalistic conflict-of-interest to viewers.
Killian documents controversy.
The Killian documents controversy (also referred to as Memogate or Rathergate) involved six documents critical of President George W. Bush's service in the Texas Air National Guard in 1972–73. Four of these documents were presented as authentic in a "60 Minutes Wednesday" broadcast aired by CBS on September 8, 2004, less than two months before the 2004 Presidential Election, but it was later found that CBS had failed to authenticate the documents. Subsequently, several typewriter and typography experts concluded the documents are forgeries, as have some media sources. No forensic document examiners or typography experts authenticated the documents, which may not be possible without original documents. The provider of the documents, Lt. Col. Bill Burkett, claimed to have burned the originals after faxing copies to CBS.
"The Internet Is Infected" episode and the false hacker photo.
A segment aired on the March 29, 2009 edition of "60 Minutes", "The Internet Is Infected", featured an interview with Don Jackson, a data protection professional for SecureWorks. Jackson himself declares in the program that: "A part of my job is to know the enemy". However, during the interview, Jackson showed a photo of Finnish upper-level comprehensive school pupils and misidentified them as Russian hackers. In the photo, one of the children is wearing a jacket with the Coat of Arms of Finland on it. Another one is wearing a cap which clearly has the logo of Karjala, a Finnish brand of beer, on it. The principal of the school in Taivalkoski confirmed that the photo was taken at the school about five years before the program was broadcast.
The photo's exact origins are unknown, but it is widely known in Finland, having been originally posted to a Finnish social networking site, IRC-Galleria, in the early 2000s. It spread all over Finnish internet communities, and even originated a couple of patriotically titled (but intentionally misspelled) mock sites. "60 Minutes" later issued a correction and on-air apology.
Benghazi report.
Subsequent to the 2012 Benghazi attack, "60 Minutes" aired report by correspondent Lara Logan on October 27, 2013 in which British military contractor, Dylan Davies, identified by CBS under the pseudonym “Morgan Jones,” described racing to the Benghazi compound several hours after the main assault was over, scaling a 12-foot wall and knocking out a lone fighter with the butt of a rifle. He also claimed to have visited a Benghazi hospital earlier that night where he saw Ambassador Christopher Stevens' body.
In the days following the report, Davies' personal actions were challenged. The FBI, which had interviewed Davies several times and considered him a credible source, said the account Davies had given them was different than what he told "60 Minutes". Davies stood by his story, but the inconsistency ultimately prompted "60 Minutes" to conclude it was a mistake to include Davies in their report and a correction was issued.
Following the correction, a journalistic review was conducted by Al Ortiz, CBS News' executive director of standards and practices. He determined that red flags about Davies' account were missed. Davies had said to the program and written in his book that he told an alternative version of his actions to his employer, who he said had demanded that he stay inside his Benghazi villa as the attack unfolded. That alternative version was shared with US authorities and 60 Minutes was unable to prove the story Davies had told them was true. The incident led CBS News to request that Logan and her producer, Max McLellan, take a leave of absence.
Davies' book, "The Embassy House", was published two days after the "60 Minutes" report, by Threshold Editions, part of the Simon and Schuster unit of CBS. It was pulled from shelves once "60 Minutes" issued its correction.
NSA report.
On December 15, 2013, "60 Minutes" aired a report on the National Security Agency (NSA) that was widely criticized as false and a "puff piece." The story was reported by John Miller, who once worked in the office of the Director of National Intelligence.
Tesla Automaker report.
On March 30, 2014, "60 Minutes" presented a story on the Tesla Model S luxury electric automobile in a segment, with Scott Pelley conducting an interview with CEO Elon Musk concerning the car brand as well as his SpaceX company. Within a day, the automotive blog site Jalopnik reported that the sounds accompanying footage of the car shown during the story were actually sounds from a traditional gasoline engine dubbed over the footage, when in reality the electric car is much quieter. CBS released a statement explaining that the sound was the result of an audio editing error, and subsequently removed the noise from the online version of the piece. However, several news outlets, as well as Jalopnik itself, have expressed doubt over the authenticity of this explanation, noting the similar scandal involving Tesla Motors and "The New York Times" in 2013.
Spin-offs.
The main "60 Minutes" show has created a number of spin-offs over the years.
"30 Minutes".
"30 Minutes" was a newsmagazine aimed at children that was patterned after "60 Minutes", airing as the final program in CBS's Saturday morning lineup from 1978 to 1982. It was hosted by Christopher Glenn (who also served as the voice-over for the interstitial program "In the News" and was an anchor on the CBS Radio Network), along with Betsy Aaron (1978–1980) and Betty Ann Bowser (1980–1982).
"60 Minutes More".
"60 Minutes More" was a spin-off that ran for one season from 1996 to 1997. The episodes featured popular stories from the past that were expanded with updates on the original story. Each episode featured three of these segments.
"60 Minutes II".
In 1999, a second edition of "60 Minutes" was started in the U.S., called "60 Minutes II". This edition was later renamed "60 Minutes" by CBS for the fall of 2004 in an effort to sell it as a high-quality program, since some had sarcastically referred to it as "60 Minutes, Jr." CBS News president Andrew Heyward said, "The Roman numeral II created some confusion on the part of the viewers and suggested a watered-down version". However, a widely known controversy which came to be known as "Rathergate", regarding a report that aired September 8, 2004, caused another name change. The program was retitled "60 Minutes Wednesday" both to differentiate itself and to avoid tarnishing the Sunday edition, as the editions were editorially independent from one another. It reverted to its original Roman numeral title on July 8, 2005, when the program moved to Fridays in an 8:00 p.m. Eastern Time slot to finish its run. The show's final broadcast was on September 2, 2005.
"60 Minutes on CNBC".
In 2011, CNBC began airing a "60 Minutes" spin-off of its own, called "60 Minutes on CNBC". Hosted by Lesley Stahl and Steve Kroft, it airs updated business-related reports seen on the original broadcasts and offers footage that were not included when the segments first aired.
"60 Minutes Sports".
CBS News began producing a sports-themed version of "60 Minutes" for corporate sister and premium channel Showtime in January 2013. The program, titled "60 Minutes Sports", includes two original segments along with a classic interview from the show's archives. Personalities from CBS Sports join the "60 Minutes" team in contributing.
25th anniversary edition.
For the "60 Minutes" 25th anniversary in 1993, Charles Kuralt interviewed Don Hewitt, the active correspondents, some former correspondents, and revisited notable stories and celebrities.
International versions.
Australia.
The Australian version of "60 Minutes" premiered on February 11, 1979. It still airs each Sunday night at 7:30 p.m. on the Nine Network and affiliates.
Reporter Richard Carleton suffered a heart attack on May 7, 2006. He asked a question at a news conference for the Beaconsfield Mine collapse, then walked out and suffered cardiac arrest. Paramedics tried to revive him for 20 minutes until an ambulance arrived, but was pronounced dead on arrival.
Although Nine Network has the rights to the format, as of 2007, it does not have rights to stories from the U.S. program. Nevertheless, stories from the flagship "60 Minutes" program in the U.S. often air on the Australian program by subleasing them from Network Ten. In 1980, "60 Minutes" won a Logie Award for their investigation of lethal abuses at the Chelmsford psychiatric hospital in Sydney.
Germany.
In the mid-1980s, an edited version (approx. 30 minutes in length) of the U.S. broadcast edition of "60 Minutes" was shown for a time on West German television. This version retained the English-language soundtrack of the original, but also featured German subtitles.
New Zealand.
The New Zealand version of "60 Minutes" has aired on national television since 1989, when it was originally launched on TV3. In 1992, the rights were acquired by TVNZ, who began broadcasting it in 1993. The network aired the program for nine years before dropping it in 2002 for its own program, entitled "Sunday", which is currently the highest-rated current affairs show broadcast on New Zealand television, followed by "20/20". "60 Minutes" was broadcast by rival network TV3, before switching to the Sky Television owned Prime channel in 2013, when the contract changed hands.
Portugal.
The original programs are shown in Portugal on SIC Notícias with introductory and closing remarks by journalist Mário Crespo.
Chile.
The news program of National Broadcasting of Chile (TVN), the public television network in that country, was named "60 Minutos" ("60 Minutes") from 1975 to 1988, but the program had no accusations of any kind and no investigative reporting.

</doc>
<doc id="35077" url="http://en.wikipedia.org/wiki?curid=35077" title="1473">
1473

Year 1473 (MCDLXXIII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35087" url="http://en.wikipedia.org/wiki?curid=35087" title="1607">
1607

Year 1607 (MDCVII) was a common year starting on Monday (link will display the full calendar) of the Gregorian calendar and a common year starting on Thursday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35099" url="http://en.wikipedia.org/wiki?curid=35099" title="1550">
1550

Year 1550 (MDL) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="35110" url="http://en.wikipedia.org/wiki?curid=35110" title="1621">
1621

Year 1621 (MDCXXI) was a common year starting on Friday (link will display the full calendar) of the Gregorian calendar and a common year starting on Monday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35121" url="http://en.wikipedia.org/wiki?curid=35121" title="754">
754

Year 754 (DCCLIV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. The denomination 754 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35131" url="http://en.wikipedia.org/wiki?curid=35131" title="660s">
660s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35142" url="http://en.wikipedia.org/wiki?curid=35142" title="1628">
1628

Year 1628 (MDCXXVIII) was a leap year starting on Saturday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Tuesday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35153" url="http://en.wikipedia.org/wiki?curid=35153" title="1380s">
1380s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35168" url="http://en.wikipedia.org/wiki?curid=35168" title="1100s (decade)">
1100s (decade)

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35179" url="http://en.wikipedia.org/wiki?curid=35179" title="991">
991

Year 991 (CMXCI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="35189" url="http://en.wikipedia.org/wiki?curid=35189" title="11">
11

Year 11 (XI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Lepidus and Taurus (or, less frequently, year 764 "Ab urbe condita"). The denomination 11 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. The Year XI (eleven) is also a year in the French Revolutionary Calendar.
Events.
<onlyinclude>
By Place.
Asia.
</onlyinclude>

</doc>
<doc id="35207" url="http://en.wikipedia.org/wiki?curid=35207" title="16">
16

Year 16 (XVI) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Taurus and Libo (or, less frequently, year 769 "Ab urbe condita"). The denomination 16 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="35218" url="http://en.wikipedia.org/wiki?curid=35218" title="30">
30

Year 30 (XXX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Vinicius and Longinus (or, less frequently, year 783 "Ab urbe condita"). The denomination 30 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="35234" url="http://en.wikipedia.org/wiki?curid=35234" title="520s">
520s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35245" url="http://en.wikipedia.org/wiki?curid=35245" title="490s">
490s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35257" url="http://en.wikipedia.org/wiki?curid=35257" title="40">
40

Year 40 (XL) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus without colleague (or, less frequently, year 793 "Ab urbe condita"). The denomination 40 for this year has been used since the Early Middle Ages, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35268" url="http://en.wikipedia.org/wiki?curid=35268" title="52">
52

Year 52 (LII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Sulla and Otho (or, less frequently, year 805 "Ab urbe condita"). The denomination 52 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35279" url="http://en.wikipedia.org/wiki?curid=35279" title="64">
64

Year 64 (LXIV) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Bassus and Crassus (or, less frequently, year 817 "Ab urbe condita"). The denomination 64 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="35290" url="http://en.wikipedia.org/wiki?curid=35290" title="78">
78

Year 78 (LXXVIII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Novius and Commodus (or, less frequently, year 831 "Ab urbe condita"). The denomination 78 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Philosophy.
</onlyinclude>

</doc>
<doc id="35301" url="http://en.wikipedia.org/wiki?curid=35301" title="91">
91

Year 91 (XCI) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Glabrio and Traianus (or, less frequently, year 844 "Ab urbe condita"). The denomination 91 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="35313" url="http://en.wikipedia.org/wiki?curid=35313" title="101">
101

Year 101 (CI) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Traianus and Paetus (or, less frequently, year 854 "Ab urbe condita"). The denomination 101 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="35323" url="http://en.wikipedia.org/wiki?curid=35323" title="451">
451

Year 451 (CDLI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Marcianus and Adelfius (or, less frequently, year 1204 "Ab urbe condita"). The denomination 451 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="35335" url="http://en.wikipedia.org/wiki?curid=35335" title="807">
807

Year 807 (DCCCVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Science.
</onlyinclude>

</doc>
<doc id="35351" url="http://en.wikipedia.org/wiki?curid=35351" title="160s">
160s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35363" url="http://en.wikipedia.org/wiki?curid=35363" title="111">
111

Year 111 (CXI) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Piso and Bolanus (or, less frequently, year 864 "Ab urbe condita"). The denomination 111 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="35375" url="http://en.wikipedia.org/wiki?curid=35375" title="123">
123

Year 123 (CXXIII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Paetinus and Apronius (or, less frequently, year 876 "Ab urbe condita"). The denomination 123 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="35395" url="http://en.wikipedia.org/wiki?curid=35395" title="629">
629

Year 629 (DCXXIX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 629 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35406" url="http://en.wikipedia.org/wiki?curid=35406" title="744">
744

Year 744 (DCCXLIV) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 744 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35417" url="http://en.wikipedia.org/wiki?curid=35417" title="756">
756

Year 756 (DCCLVI) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar. The denomination 756 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Japan.
</onlyinclude>

</doc>
<doc id="35428" url="http://en.wikipedia.org/wiki?curid=35428" title="769">
769

Year 769 (DCCLXIX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 769 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35439" url="http://en.wikipedia.org/wiki?curid=35439" title="70 BC">
70 BC

Year 70 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Magnus and Dives (or, less frequently, year 684 "Ab urbe condita"). The denomination 70 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Parthia.
</onlyinclude>

</doc>
<doc id="35450" url="http://en.wikipedia.org/wiki?curid=35450" title="770">
770

Year 770 (DCCLXX) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. The denomination 770 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="35461" url="http://en.wikipedia.org/wiki?curid=35461" title="784">
784

Year 784 (DCCLXXXIV) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar. The denomination 784 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35472" url="http://en.wikipedia.org/wiki?curid=35472" title="797">
797

Year 797 (DCCXCVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 797 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Britain.
</onlyinclude>

</doc>
<doc id="35483" url="http://en.wikipedia.org/wiki?curid=35483" title="153">
153

Year 153 (CLIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Rusticus and Rufinus (or, less frequently, year 906 "Ab urbe condita"). The denomination 153 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="35494" url="http://en.wikipedia.org/wiki?curid=35494" title="312">
312

Year 312 (CCCXII) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantinus and Licinianus (or, less frequently, year 1065 "Ab urbe condita"). The denomination 312 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35504" url="http://en.wikipedia.org/wiki?curid=35504" title="The 3DO Company">
The 3DO Company

The 3DO Company (formerly THDO on the NASDAQ stock exchange), also known as 3DO (short for three-dimensional operating system), was a video game company. It was founded in 1991 under the name SMSG, Inc. (for San Mateo Software Group) by Electronic Arts founder Trip Hawkins in a partnership with seven companies, including LG, Matsushita (now Panasonic), AT&T Corporation, MCA, Time Warner and Electronic Arts. After 3DO's flagship video game console, the 3DO Interactive Multiplayer, failed in the marketplace, the company exited the hardware business and became a third-party video game developer. It went bankrupt in 2003 due to poor sales of its games. Its headquarters were in Redwood City, California in the San Francisco Bay Area.
History.
Console developer.
When the company was first founded, its original objective was to create a next-generation CD-based video game system called the 3DO Interactive Multiplayer, which would be manufactured by various partners and licensees; 3DO would collect a royalty on each console sold and on each game manufactured. For game publishers, 3DO's $3 royalty per sold game was very low compared to the royalties Nintendo and Sega collected from game sales on their consoles. The launch of the console in October 1993 was well-promoted, with a great deal of attention in the mass media as part of the "multimedia wave" in the computer world.
The 3DO console launched in October 1993 at the price of $699. Poor console and game sales trumped the enticingly low royalty rate and proved a fatal flaw. While 3DO's business model attracted game publishers with its low royalty rates, it resulted in the console selling for a price higher than the Super Nintendo and Sega Genesis combined, hampering sales. While companies that manufactured and sold their own consoles could sell them, at a loss, for a competitive price, making up for lost profit through royalties collected from game publishers, the 3DO's manufacturers, not collecting any money from game publishers, and owing royalties to the 3DO Company, had to sell the console for a profit, resulting in high prices. As the console failed to compete with its cheaper competitors, game developers and publishers, while initially attracted by low royalties, dropped support for the console as its games failed to sell. Stock in the 3DO Company dropped from over $37 per share in November 1993 to $23 per share in late December. In October 1995, The 3DO Company sold its next generation console, M2, to Matsushita, and changed its business to develop and publish games for other game consoles and PC.
Third-party developer.
After abandoning the 3DO console, the company acquired Cyclone Studios, Archetype Interactive and New World Computing. The company's biggest hit was its series of "Army Men" games, featuring generic green plastic soldier toys. Its "Might and Magic" and especially "Heroes of Might and Magic" series from subsidiary New World Computing were perhaps the most popular among their games at the time of release. During the late 1990s, the company published one of the first 3D MMORPGs: "Meridian 59", which survives to this day in the hands of some of the game's original developers.
With the exception of its well-received "High Heat Baseball" franchise and "BattleTanx" games, most of the company's games were critically panned. After struggling for several years, the company filed for Chapter 11 bankruptcy in May 2003. Trip Hawkins' questionable policy of rushing games to the market in 6 months did not pay off. Employees were laid off without pay, and the company's game brands and other intellectual properties were sold to rivals like Microsoft, Namco, Crave and Ubisoft, and also to founder Trip Hawkins, who paid $405,000 for rights to some old brands and the company's "Internet patent portfolio". Hawkins went on to found Digital Chocolate, a mobile-based gaming company.

</doc>
<doc id="35519" url="http://en.wikipedia.org/wiki?curid=35519" title="United Kingdom general election, 1997">
United Kingdom general election, 1997

"* Indicates boundary change – so this is a nominal figure"
"^ Figure does not include the speaker"
 John Major
Tony Blair
The United Kingdom general election of 1997 was held on 1 May 1997, more than five years after the previous election on 9 April 1992, to elect 659 members to the British House of Commons. Under the leadership of Tony Blair, the Labour Party ended its 18 years in opposition and won the general election with a landslide victory, winning 418 seats, the most seats the party has ever held. Blair, as a result, became Prime Minister of the United Kingdom, a position he held until his resignation in 2007.
Under Blair's leadership, the Labour Party had adopted a more centrist policy platform under the name 'New Labour'. This was seen as moving away from the traditionally more left-wing stance of the Labour Party. Labour made several campaign pledges such as the creation of a National Minimum Wage, devolution referendums for Scotland and Wales and promised greater economic competence than the Conservatives, who were unpopular following the events of Black Wednesday in 1992. The Labour campaign was ultimately a success and the party returned an unprecedented 418 MPs and began the first of three consecutive terms for Labour in government. However, 1997 remains the last election in which Labour had a net gain of seats. A record number of women were elected to parliament, 120, of whom 101 were Labour MPs. This was in part thanks to Labour's policy of using all-women shortlists.
The Conservative Party was led by incumbent Prime Minister John Major and ran their campaign emphasising falling unemployment and a strong economic recovery following the early 1990s recession. However, a series of scandals, party disunity over the European Union, the events of Black Wednesday and a desire of the electorate for change after 18 years of Tory rule all contributed to the Conservatives' worst defeat since 1906, with only 165 MPs elected to Westminster, as well as their lowest percentage share of the vote since 1832. The party was left with no seats whatsoever in Scotland or Wales, and many key Conservative politicians, including Defence Secretary Michael Portillo, Foreign Secretary Malcolm Rifkind, Trade Secretary Ian Lang, Scottish Secretary Michael Forsyth and former ministers Edwina Currie, Norman Lamont, David Mellor and Neil Hamilton all lost their parliamentary seats. Following the defeat, the Conservatives began the longest continuous spell in opposition in the history of the present day (post-Tamworth Manifesto) Conservative Party, and indeed the longest such spell for any incarnation of the Tories/Conservatives since the 1760s, lasting 13 years.
The Liberal Democrats, under Paddy Ashdown, returned 46 MPs to parliament, the most for any third party since 1929 and more than double the seats they got in 1992, despite a drop in popular vote. The Scottish National Party (SNP) returned 6 MPs, double their total in 1992.
As with all general elections since the early 1950s, the results were broadcast live on the BBC; the presenters were David Dimbleby, Peter Snow and Jeremy Paxman.
Overview.
The British economy had been in recession at the time of the 1992 election, which the Conservatives had won, and although the recession had ended within a year, events such as Black Wednesday had tarnished the Conservative government's reputation for economic management. Labour had elected John Smith as its party leader in 1992, however his death from heart attack in 1994 led the way for Tony Blair to become Labour leader. Blair brought the party closer to the political centre and abolished the party's Clause IV in their constitution, which had committed them to mass nationalisation of industry. Labour also reversed its policy on unilateral nuclear disarmament and the events of Black Wednesday allowed Labour to promise greater economic management under the Chancellorship of Gordon Brown. A manifesto, entitled New Labour, New Life For Britain was released in 1996 and outlined 5 key pledges:
Disputes within the Conservative government over European Union issues, and a variety of "sleaze" allegations had severely affected the government's popularity. Despite the strong economic recovery and substantial fall in unemployment in the four years leading up to the election, the rise in Conservative support was only marginal with all of the major opinion polls having shown Labour in a comfortable lead since late 1992.
Timing.
The previous Parliament first sat on 29 April 1992. The Parliament Act 1911 required at the time that each Parliament to be dissolved before the 5th anniversary of its first sitting, therefore the latest date the dissolution and the summoning of the next parliament could have been held on was 28 April 1997. The 1985 amendment of the Representation of the People Act 1983 requires that the election must take place on the 11th working day after the deadline for nomination papers, which in turn must be no more than six working days after the next parliament was summoned. Therefore the latest date the election could have been held on was 22 May 1997 (which happened to be a Thursday). British elections (and referenda) have been held on Thursdays by convention since the 1930s, but can be held on other working days.
Campaign.
Prime Minister John Major called the election on Monday 17 March 1997, ensuring the formal campaign would be unusually long, at six weeks (Parliament was dissolved on 8 April). The election was scheduled for 1 May, to coincide with the local elections on the same day. This set a precedent, as the three subsequent general elections have also been held alongside the May local elections. The Conservatives argued that a long campaign would expose Labour and allow the Conservative message to be heard. However, Major was accused of arranging an early dissolution to protect Neil Hamilton from a pending parliamentary report into his conduct: a report that Major had earlier guaranteed would be published before the election.
Conservative campaign.
The Conservatives started low in the polls, and had experienced great difficulties over the past 5 years, with polling often putting it some 40 points adrift of Labour. The Conservative campaign emphasised stability, as did its manifesto title 'You can only be sure with the Conservatives'. However, the campaign was beset by deep set problems, such as the rise of James Goldsmith's Referendum Party, advocating a referendum on continued membership of the European Union. The party threatened to take away many right leaning votes from the Conservatives. Meanwhile, there was also division amongst the Conservative cabinet, with Chancellor Ken Clarke describing the views of Home Secretary Michael Howard on Europe as 'paranoid and xenophobic nonsense'. The Conservatives also struggled to come up with a definitive theme to attack the Labour Party, with some strategists arguing for an approach which castigated Labour for 'stealing Tory clothes' (copying their positions), with others making the case for a more confrontational approach, stating that New Labour was just a facade for 'old Labour'. The Tony Blair 'Demon Eyes' poster was an example of the latter strategy. In any case, the campaign failed to gain much traction, and the Conservatives went down to a landslide defeat at the polls.
Labour campaign.
Labour ran a slick campaign, which emphasised the splits within the Conservative government, and argued that the country needed a more centrist administration. Labour ran a centrist campaign that was good at picking up dissatisfied Tory voters, particularly moderate and suburban ones. Tony Blair, highly popular, was very much the centrepiece of the campaign, and proved a highly effective campaigner. The Labour campaign was reminiscent of those of Bill Clinton for the US Presidency, focusing on centrist themes, as well as adopting policies more commonly associated with the right, such as cracking down on crime and fiscal responsibility.
Liberal Democrat campaign.
The Liberal Democrats had suffered a disappointing performance in 1992, but they were very much strengthened in 1997 due to potential tactical voting between Labour and Lib Dem supporters in Tory marginal constituencies, particularly in the south. The Lib Dems promised to increase education funding.
Notional 1992 election.
The election was fought under new boundaries, with a net increase of eight seats compared to the 1992 election (651 to 659). Changes listed here are from the notional 1992 result, had it been fought on the boundaries established in 1997. These notional results were used by all media organisations at the time.
Results.
Labour won a landslide victory with their largest parliamentary majority (179) to date. On the BBC's election night programme Professor Anthony King described the result of the exit poll, which accurately predicted a Labour landslide, as being akin to "an asteroid hitting the planet and destroying practically all life on Earth". After years of trying the Labour Party had convinced the electorate that they would usher in a new age of prosperity—their policies, organisation and tone of optimism slotting perfectly into place.
Labour's victory was largely credited to the charisma of Tony Blair and a Labour public relations machine managed by Alastair Campbell. Between the 1992 election and the 1997 election there had also been major steps to modernise the party, including scrapping Clause IV that had committed the party to extending public ownership of industry. New Labour had suddenly seized the middle ground of the political spectrum, attracting voters much further to the right than their traditional working class or left-wing support. Famously, in the early hours of 2 May 1997 a party was held at the Royal Festival Hall, in which Blair stated triumphantly that "a new dawn has broken, has it not?".
The election was a crushing defeat for the Conservative Party, with the party having its lowest percentage share of the popular vote since 1832 under the Duke of Wellington's leadership, being wiped out in Scotland and Wales. A number of prominent Conservative MPs lost their seats in the election, including Michael Portillo, Malcolm Rifkind, Edwina Currie, David Mellor, Neil Hamilton and Norman Lamont. Such was the extent of Conservative losses at the election that Cecil Parkinson, speaking on the BBC's election night programme, remarked upon the Conservatives winning their second seat that he was pleased that the subsequent election for the leadership would be contested.
The election was a massive success for the Liberal Democrats, who more than doubled their number of seats thanks to the use of tactical voting against the Conservatives. Although their share of the vote fell slightly, their total of 46 MPs was the highest since Lloyd George got 59 seats in 1929.
The Referendum Party, which sought a referendum on the United Kingdom's relationship with the European Union, came fourth in terms of votes with 800,000 votes mainly from former Conservative voters , but won no seats in parliament. The six parties with the next highest votes stood only in either Scotland, Northern Ireland or Wales; in order, they were the Scottish National Party, the Ulster Unionist Party, the Social Democratic and Labour Party, Plaid Cymru, Sinn Féin, and the Democratic Unionist Party.
In the previously safe seat of Tatton, where incumbent Conservative MP Neil Hamilton was facing charges of having taken cash for questions, the Labour and Liberal Democrat Parties decided not to field candidates in order that an Independent candidate, Martin Bell, would have a better chance of winning the seat, which he duly did with a comfortable margin.
The result declared for the constituency of Winchester showed a margin of victory of just two votes for the Liberal Democrats. The defeated Conservative candidate mounted a successful legal challenge to the result on the grounds that errors by election officials (failures to stamp certain votes) had changed the result, the court ruled the result invalid and ordered a by-election on 20 November which was won by the Liberal Democrats with a much larger majority, causing much recrimination in the Conservative Party about the decision to challenge the original result in the first place.
This election would also mark the start of Labour government for the next 13 years until the formation of the Conservative-Liberal Democrat coalition in 2010.
"All parties with more than 500 votes shown. Labour total includes New Labour and "Labour Time for Change" candidates; Conservative total includes candidates in Northern Ireland (excluded in some lists) and "Loyal Conservative" candidate.
The Popular Unionist MP elected in 1992 died in 1995 and the party folded shortly afterwards.
There was no incumbent Speaker in the 1992 election."
Conservative MPs who lost their seats.
Constituencies given are those contested in 1997, rather than those held prior to the election - Norman Lamont, for example, had previously represented Kingston upon Thames in London.
Post election events.
The poor results for the Conservative Party led to infighting, with the One Nation, Tory Reform Group, and right wing Maastricht rebels blaming each other for the defeat. Party chairman Brian Mawhinney said on the night of the election, that it was due to disillusionment with 18 years of Conservative rule. John Major resigned as party leader, saying "When the curtain falls, it is time to leave the stage".
Despite receiving fewer votes than in 1992, the Liberal Democrats more than doubled their number of seats and won their best general election result since 1929 under David Lloyd George's leadership. Paddy Ashdown's continued leadership had been vindicated, despite a disappointing 1992 election, and they were in a position to build positively as a strong third party into the new millennium.
Internet coverage.
With the huge rise in internet use since the previous general election, BBC News created a special website covering the election as an experiment for the efficiency of an online news service which was due for a launch later in the year.

</doc>
<doc id="35591" url="http://en.wikipedia.org/wiki?curid=35591" title="398 BC">
398 BC

Year 398 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Tribunate of Potitus, Medullinus, Lactucinus, Fidenas, Camillus and Cornutus (or, less frequently, year 356 "Ab urbe condita"). The denomination 398 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Sicily.
</onlyinclude>

</doc>
<doc id="35602" url="http://en.wikipedia.org/wiki?curid=35602" title="14 BC">
14 BC

Year 14 BC was either a common year starting on Thursday or Friday or a leap year starting on Wednesday, Thursday or Friday (link will display the full calendar) of the Julian calendar (the sources differ, see leap year error for further information) and a common year starting on Tuesday of the Proleptic Julian calendar. At the time, it was known as the Year of the Consulship of Crassus and Lentulus (or, less frequently, year 740 "Ab urbe condita"). The denomination 14 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Empire.
</onlyinclude>

</doc>
<doc id="35613" url="http://en.wikipedia.org/wiki?curid=35613" title="415">
415

Year 415 (CDXV) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Honorius and Theodosius (or, less frequently, year 1168 "Ab urbe condita"). The denomination 415 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35631" url="http://en.wikipedia.org/wiki?curid=35631" title="403">
403

Year 403 (CDIII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Rumoridus (or, less frequently, year 1156 "Ab urbe condita"). The denomination 403 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35642" url="http://en.wikipedia.org/wiki?curid=35642" title="390s">
390s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="35653" url="http://en.wikipedia.org/wiki?curid=35653" title="284">
284

Year 284 (CCLXXXIV) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Carinus and Numerianus (or, less frequently, year 1037 "Ab urbe condita"). The denomination 284 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="35664" url="http://en.wikipedia.org/wiki?curid=35664" title="329">
329

Year 329 (CCCXXIX) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantinus and Constantinus (or, less frequently, year 1082 "Ab urbe condita"). The denomination 329 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35675" url="http://en.wikipedia.org/wiki?curid=35675" title="1771">
1771

Year 1771 (MDCCLXXI) was a common year starting on Tuesday (link will display the full calendar) of the Gregorian calendar and a common year starting on Saturday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35686" url="http://en.wikipedia.org/wiki?curid=35686" title="508">
508

Year 508 (DVIII) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Venantius and Celer (or, less frequently, year 1261 "Ab urbe condita"). The denomination 508 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="35697" url="http://en.wikipedia.org/wiki?curid=35697" title="526">
526

Year 526 (DXXVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Olybrius without colleague (or, less frequently, year 1279 "Ab urbe condita"). The denomination 526 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35708" url="http://en.wikipedia.org/wiki?curid=35708" title="550">
550

Year 550 (DL) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 550 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35719" url="http://en.wikipedia.org/wiki?curid=35719" title="584">
584

Year 584 (DLXXXIV) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 584 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="35730" url="http://en.wikipedia.org/wiki?curid=35730" title="488">
488

Year 488 (CDLXXXVIII) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Ecclesius and Sividius (or, less frequently, year 1241 "Ab urbe condita"). The denomination 488 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35741" url="http://en.wikipedia.org/wiki?curid=35741" title="475">
475

Year 475 (CDLXXV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Zeno without colleague (or, less frequently, year 1228 "Ab urbe condita"). The denomination 475 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35752" url="http://en.wikipedia.org/wiki?curid=35752" title="506">
506

Year 506 (DVI) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Messala and Dagalaiphus (or, less frequently, year 1259 "Ab urbe condita"). The denomination 506 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="35763" url="http://en.wikipedia.org/wiki?curid=35763" title="521">
521

Year 521 (DXXI) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Sabbatius and Valerius (or, less frequently, year 1274 "Ab urbe condita"). The denomination 521 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35774" url="http://en.wikipedia.org/wiki?curid=35774" title="541">
541

Year 541 (DXLI) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Basilius without colleague (or, less frequently, year 1294 "Ab urbe condita"). Basilius was the last person to be officially appointed Roman consul, since after this year, the office was permanently merged with the office of Roman/Byzantine emperor. Thus, from the next year forward, the consular year dating was abandoned. The denomination 541 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35785" url="http://en.wikipedia.org/wiki?curid=35785" title="557">
557

Year 557 (DLVII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. The denomination 557 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35796" url="http://en.wikipedia.org/wiki?curid=35796" title="579">
579

Year 579 (DLXXIX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 579 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35807" url="http://en.wikipedia.org/wiki?curid=35807" title="432 BC">
432 BC

Year 432 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Tribunate of Mamercus, Albinus and Medullinus (or, less frequently, year 322 "Ab urbe condita"). The denomination 432 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Architecture.
</onlyinclude>

</doc>
<doc id="35818" url="http://en.wikipedia.org/wiki?curid=35818" title="1772">
1772

Year 1772 (MDCCLXXII) was a leap year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Sunday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
July–December.
</onlyinclude>

</doc>
<doc id="35829" url="http://en.wikipedia.org/wiki?curid=35829" title="60s BC">
60s BC


</doc>
<doc id="35843" url="http://en.wikipedia.org/wiki?curid=35843" title="1744">
1744

Year 1744 (MDCCXLIV) was a leap year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Sunday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35854" url="http://en.wikipedia.org/wiki?curid=35854" title="1727">
1727

Year 1727 (MDCCXXVII) was a common year starting on Wednesday (link will display the full calendar) of the Gregorian calendar and a common year starting on Sunday of the 11-day slower Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="35866" url="http://en.wikipedia.org/wiki?curid=35866" title="236">
236

Year 236 (CCXXXVI) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Verus and Africanus (or, less frequently, year 989 "Ab urbe condita"). The denomination 236 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35878" url="http://en.wikipedia.org/wiki?curid=35878" title="167">
167

Year 167 (CLXVII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Aurelius and Quadratus (or, less frequently, year 920 "Ab urbe condita"). The denomination 167 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="35889" url="http://en.wikipedia.org/wiki?curid=35889" title="826">
826

Year 826 (DCCCXXVI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35900" url="http://en.wikipedia.org/wiki?curid=35900" title="1001">
1001

Year 1001 (MI) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. It is the first year of the 11th century and the 2nd millennium.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35911" url="http://en.wikipedia.org/wiki?curid=35911" title="838">
838

Year 838 (DCCCXXXVIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="35922" url="http://en.wikipedia.org/wiki?curid=35922" title="American Airlines Flight 587">
American Airlines Flight 587

American Airlines Flight 587 was a regularly scheduled passenger flight from John F. Kennedy International Airport in New York City to Santo Domingo's Las Américas International Airport in the Dominican Republic. On November 12, 2001, the Airbus A300-600 flying the route crashed into the Belle Harbor neighborhood of Queens, a borough of New York City, shortly after takeoff. All 260 people on board the flight were killed, along with five people on the ground. It is the second-deadliest aviation incident involving an Airbus A300, after Iran Air Flight 655 and the second-deadliest aviation incident to occur on U.S. soil, after American Airlines Flight 191. To date, no single-airplane crash incident that was ruled accidental and not criminal since then has surpassed that death toll, though before 2001 there had been deadlier incidents of this type.
The location of the accident and the fact that it took place two months and one day after the September 11 attacks on the World Trade Center in Manhattan initially spawned fears of another terrorist attack. However, terrorism was officially ruled out as the cause by the National Transportation Safety Board, which instead attributed the disaster to the first officer's overuse of rudder controls in response to wake turbulence or jet wash from a Japan Airlines Boeing 747-400 that took off minutes before it. According to the NTSB, this aggressive use of the rudder controls by the co-pilot caused the vertical stabilizer to snap off the plane. The plane's two engines also separated from the aircraft before it hit the ground.
Summary of the accident.
The accident aircraft, registration N14053, was an Airbus A300B4-605R delivered in 1988, with a seating configuration for 267 passengers (16 first-class seats and 251 economy-class seats); it was powered by two General Electric CF6-80C2A5 engines.
The accident flight had two flight crew members, seven cabin crew members and 251 passengers. The flight crew consisted of Captain Ed States (42) and First Officer Sten Molin (34), who was the pilot flying.
The plane pushed back from its gate at 9:00 AM. It taxied to Runway 31L behind a Japan Airlines Boeing 747-400 bound for Tokyo. At 9:11 AM, the 747 was cleared for takeoff. As the JAL flight climbed, the tower contacted Flight 587's pilots and warned them about potential wake turbulence from the 747.
At 9:13:28, the A300 was cleared for takeoff on Runway 31L. The aircraft left the runway at 9:14:29, about 1 minute and 40 seconds after the JAL flight. From takeoff, the plane climbed to an altitude of 500 feet above mean sea level (msl) and then entered a climbing left turn to a
heading of 220º. At 9:15:00, the pilot made initial contact with the departure controller, informing him that the airplane was at 1,300 feet and climbing to 5,000 feet. The departure controller instructed the aircraft to climb to and maintain a flight level of 13,000 feet.
Data from the flight data recorder (FDR) showed that the events leading into the crash began at 9:15:36, when the aircraft hit wake turbulence from the JAL flight just in front of it. The first officer attempted to stabilize the aircraft with alternating aggressive rudder inputs from left to right. This continued for at least 20 seconds, until 9:15:56, when the stress of the first officer's repeated rudder movements caused the lugs that attached the vertical stabilizer and rudder to fail. The stabilizer separated from the aircraft and fell into Jamaica Bay, about one mile north of the main wreckage site. Eight seconds later, the stall warning sounded on the cockpit voice recorder. The FDR cut off at 9:16:00, and the CVR (cockpit voice recorder) cut off at 9:16:15.
At the moment the stabilizer separated from the aircraft, the plane pitched downwards, headed straight for Belle Harbor. As the pilots struggled to control the aircraft, it went into a flat spin. The aerodynamic loads sheared both engines from the aircraft seconds before impact. The engines landed several blocks north and east of the main wreckage site. The fuselage slammed into Belle Harbor on Beach 131st Street, instantly destroying three houses (258, 262, and 266 Beach 131st Street) and spraying fiery debris along Beach 131st Street south of Newport Avenue. All 260 people aboard the plane and five people on the ground were killed instantly, and the impact forces and a post-crash fire destroyed the wreckage. Flight 587 operated under the provisions of 14 Code of Federal Regulations Part 121 on an instrument flight rules flight plan. Visual meteorological conditions (VMC) prevailed at the time of the accident.
Investigation.
The A300-600 took off immediately after a Japan Airlines Boeing 747-400 on the same runway. It flew into the larger jet's wake, an area of turbulent air. The first officer attempted to stabilize the aircraft with alternating aggressive rudder inputs. The strength of the air flowing against the moving rudder stressed the aircraft's vertical stabilizer, and eventually snapped it off entirely, causing the aircraft to lose control and crash. The National Transportation Safety Board (NTSB) concluded that the enormous stress on the rudder was due to the first officer's "unnecessary and excessive" rudder inputs, and not the wake turbulence caused by the 747. The NTSB further stated "if the first officer had stopped making additional inputs, the aircraft would have stabilized". Contributing to these rudder pedal inputs were characteristics of the Airbus A300-600 sensitive rudder system design and elements of the American Airlines Advanced Aircraft Maneuvering Training Program.
The manner in which the vertical stabilizer separated concerned investigators. The vertical stabilizer is connected to the fuselage with six attaching points. Each point has two sets of attachment lugs, one made of composite material, another of aluminum, all connected by a titanium bolt; damage analysis showed that the bolts and aluminum lugs were intact, but not the composite lugs. This, coupled with two events earlier in the life of the aircraft, namely delamination in part of the vertical stabilizer prior to its delivery from Airbus's Toulouse factory, and an encounter with heavy turbulence in 1994, caused investigators to examine the use of composites. The possibility that the composite materials might not be as strong as previously supposed was a cause of concern because they are used in other areas of the plane, including the engine mounting and the wings. Tests carried out on the vertical stabilizers from the accident aircraft, and from another similar aircraft, found that the strength of the composite material had not been compromised, and the NTSB concluded that the material had failed because it had been stressed beyond its design limit.
The official NTSB report of October 26, 2004, stated that the cause of the crash was the overuse of the rudder to counter wake turbulence.
The crash was witnessed by hundreds of people, 349 of whom gave accounts of what they saw to the NTSB. About half (52%) reported a fire or explosion before the plane hit the ground. Others stated that they saw a wing detach from the aircraft, when in fact it was the vertical stabilizer. Some witnesses reported seeing one of the engines burst into flames and break off the plane, and others reported hearing a loud sound like a sonic boom.
After the crash, Floyd Bennett Field's empty hangars were used as a makeshift morgue for the identification of crash victims.
Aftermath.
Since the NTSB's report, American Airlines has modified its pilot training program. Previous simulator training did not properly reflect "the actual large build-up in sideslip angle and sideloads that would accompany such rudder inputs in an actual airplane", according to the NTSB final report.
Cause.
Terrorist claims.
Because the crash was two months and one day after the September 11 attacks and occurred in New York, several major buildings including the Empire State Building and the United Nations Headquarters were evacuated. In the months after the crash, rumors suggested that it had been destroyed in a terrorist plot, with a shoe bomb similar to the one found on Richard Reid.
In May 2002, Mohammed Jabarah agreed to cooperate with investigators as part of a plea bargain. Among the details he gave authorities, was that Khalid Sheikh Mohammed's lieutenant had told him that Reid and Abderraouf Jdey had both been enlisted by the al-Qaeda chief to carry out identical shoe-bombing plots as part of a second wave of attacks against the United States, and that they had successfully blown up Flight 587, while Reid had been stymied.
NTSB findings.
The National Transportation Safety Board concluded that the cause of the crash was purely human error leading to mechanical failure, overuse of the rudder mechanism- not, as people had first feared- a type of terrorist attack.
According to the official accident report, after the first officer made his initial rudder pedal input, he made a series of alternating full rudder inputs. This led to increasing sideslip angles. The resulting hazardous sideslip angle led to extremely high aerodynamic loads that resulted in separation of the vertical stabilizer. If the first officer had stopped making these inputs at any time before the vertical stabilizer separation, the natural stability of the airplane would have returned the sideslip angle to near 0°, and the accident would have been avoided.
The airplane performance study indicated that when the vertical stabilizer separation began, the aerodynamic loads were about two times the loads defined by the design envelope. It can be determined that the vertical stabilizer's structural performance was consistent with design specifications and exceeded certification requirements.
Contributing factors include the following: First, the first officer's predisposition to overreact to wake turbulence; second, the training provided by American Airlines that could have encouraged pilots to make large flight control inputs; third, the first officer likely did not understand an airplane's response to large rudder inputs at high airspeeds or the mechanism by which the rudder rolls a transport-category airplane; finally, light rudder pedal forces and small pedal displacement of the A300-600 rudder pedal system increased the airplane's susceptibility to a rudder misuse.
Airbus and American are currently disputing the extent to which the two parties are responsible for the disaster. American charges that the crash was mostly Airbus' fault because the A300 was designed with unusually sensitive rudder controls. Most aircraft require increased pressure on the rudder pedals to achieve the same amount of rudder control at a higher speed. The Airbus A300 and later Airbus A310 do not operate on a fly-by-wire flight control system, instead using conventional mechanical flight controls. The NTSB determined that "because of its high sensitivity, the A300-600 rudder control system is susceptible to potentially hazardous rudder pedal inputs at higher speeds". The Allied Pilots Association, in its submission to the NTSB, argued that the unusual sensitivity of the rudder mechanism amounted to a design flaw which Airbus should have communicated to the airline. The main rationale for their position came from a 1997 report that referenced 10 incidents in which A300 tail fins had been stressed beyond their design limitation.
Airbus charges that the crash was mostly American’s fault because the airline did not train its pilots properly about the characteristics of the rudder. Aircraft tail fins are designed to withstand full rudder deflection in one direction when below maneuvering speed, but this does not guarantee that they can withstand an abrupt shift in rudder from one direction to the other. Most American Airlines pilots believed that the tail fin could withstand any rudder movement at maneuvering speed. The NTSB indicated that American Airlines' Advanced Aircraft Maneuvering Program tended to exaggerate the effects of wake turbulence on large aircraft. Therefore, pilots were being trained to react more aggressively than was necessary.
Victims.
All 260 people aboard the plane (251 passengers and the crew of 9) died, along with 5 Belle Harbor residents on the ground who were killed when their three homes on Beach 131st Street were destroyed in the crash. 
Relatives gathered at Las Américas International Airport. The airport created a private area for relatives wishing to receive news about Flight 587. Some relatives arrived at the airport to meet passengers, unaware that the flight had crashed. The authorities at John F. Kennedy International Airport used the JFK Ramada Plaza to house relatives and friends of the victims of the crash. Because of its role in housing friends and relatives of several plane crashes, the hotel became known as the "Heartbreak Hotel". Due to the fact that many families were ethnic Dominicans, the hotel prepared Dominican cuisine for them. The family crisis center later moved to the Javits Center in Manhattan.
One of the passengers killed on the flight was Hilda Yolanda Mayol, a 26-year-old American woman on her way to vacation in her native Dominican Republic. Two months earlier, on 9/11, Mayol was working at a restaurant on the ground floor of the World Trade Center and escaped before the tower collapsed.
Early on, some reports erroneously stated that Dominican native and then Yankees second baseman Alfonso Soriano had been aboard Flight 587. The flight was regularly used by Major League Baseball players and scouts heading to the Dominican Republic, but it turned out that Soriano was actually booked for a flight a few days later.
Memorial.
A memorial was constructed in Rockaway Park, Belle Harbor's neighboring community, in memory of the 265 victims of the crash at the south end of Beach 116th Street, a major commercial street in the area. It was dedicated on November 12, 2006, the fifth anniversary of the incident, in a ceremony attended by then Mayor of New York City Michael Bloomberg. A ceremony commemorating the disaster is held annually at the memorial every Nov. 12, featuring a reading of the names of those killed aboard the aircraft and on the ground, with a formal moment of silence observed at 9:16 a.m., the estimated time of the crash. 
The memorial wall, designed by Dominican artist Freddy Rodríguez and Situ Studio, has windows and a doorway looking toward the nearby Atlantic Ocean and angled toward the Dominican Republic. It is inscribed with the names of the victims. Atop the memorial is a quotation, in both Spanish and English, from Dominican poet Pedro Mir, reading "Después no quiero más que paz." (Translation: "Afterwards I want nothing more than peace.")
In a ceremony held on May 6, 2007, at Woodlawn Cemetery in the Bronx, 889 unidentified fragments of human remains of the victims of the crash were entombed in a group of four mausoleum crypts.
Cultural background.
In 2001, there were 51 weekly direct flights between JFK and the Dominican Republic, with additional flights offered in December. Most of the flights were offered by American Airlines, and the airline was described as having a virtual monopoly on the route. Around 90% of the passengers on the accident flight were of Dominican descent.
"The Guardian" described the flight as having "cult status" in Washington Heights, a Dominican area of Manhattan. Belkis Lora, a relative of a passenger on the crashed flight, said "Every Dominican in New York has either taken that flight or knows someone who has. It gets you there early. At home there are songs about it." Kugel said "For many Dominicans in New York, these journeys home are the defining metaphor of their complex push-pull relationship with their homeland; they embody, vividly and poignantly, the tug between their current lives and their former selves. That fact gave Monday's tragedy a particularly horrible resonance for New York's Dominicans." He also said, "Even before Monday's crash, Dominicans had developed a complex love-hate relationship with American Airlines, complaining about high prices and baggage restrictions even while favoring the carrier over other airlines that used to travel the same route." David Rivas, the owner of the New York City travel agency Rivas Travel, said, "For the Dominican to go to Santo Domingo during Christmas and summer is like the Muslims going to Mecca." In 1996 Kinito Mendez played the song "El avión" which mentions Flight 587.
The crash did not affect bookings for the JFK-Santo Domingo route. Dominicans continued to book travel on the flights. American Airlines announced that it would end services between JFK and Santo Domingo on April 1, 2013.
Television documentaries.
There have been two television documentaries made on the accident. An episode of the National Geographic Channel program "Seconds From Disaster", first aired on September 6, 2006, examined the Flight 587 accident in detail. The episode was titled "Plane Crash in Queens" (also known as "New York Air Crash"). The BBC program "Horizon" also created an episode about the crash.
A 2006 episode of" Modern Marvels" on The History Channel also aired an episode entitled "Engineering Disasters 20", which featured detailed information on Flight 587.
An episode of "Aircrash Confidential" on Discovery Channel also featured Flight 587. The episode was entitled "Pilot Error."
A 2010 episode of "Why Planes Crash" featured Flight 587. The episode was entitled "Human Error". It was aired on MSNBC.
The crash was also featured in "Air Crash Investigation" ("Mayday" in the US) in the episode 'Queens Catastrophe') first aired in January 2014.

</doc>
<doc id="35938" url="http://en.wikipedia.org/wiki?curid=35938" title="2nd millennium">
2nd millennium

The second millennium is a period of time that, according to official sources, such as the United States Naval Observatory, began on January 1, 1001, and ended on December 31, 2000, of the Gregorian calendar. It was the second period of one thousand years in Anno Domini or Common Era.
It encompassed the High Middle Ages, the Late Middle Ages, the Renaissance, the Early Modern Age, the age of colonialism, industrialization, the rise of nation states, and the 20th century with the impact of science, widespread education, and universal health care and vaccinations in many nations. The centuries of expanding large-scale warfare with high-tech weaponry (of the World Wars and nuclear bombs) were offset by growing peace movements from the United Nations, the Peace Corps, religious campaigns warning against violence, plus doctors and health workers crossing borders to treat injuries and disease and the return of the Olympics as contest without combat.
Scientists prevailed in explaining intellectual freedom; humans took their first steps on the Moon during the 20th century; and new technology was developed by governments, industry, and academia across the world, with education shared by many international conferences and journals. The development of movable type, radio, television, and the Internet spread information worldwide, within minutes, in audio, video, and print-image format to educate, entertain, and alert billions of people by the end of the 20th century.
The 15th century saw the beginning of the second migration of humans from Europe, Africa and Asia to The Americas, beginning the ever-accelerating process of globalization. The interwoven international trade led to the formation of multi-national corporations, with home offices in multiple countries. International business ventures reduced the impact of nationalism in popular thought.
The world population doubled over the first seven centuries of the millennium, (from 310 million in AD 1000 to 600 million in AD 1700), and later increased tenfold over its last three centuries, exceeding 6 billion in AD 2000.
Calendar.
The 2nd millennium was a period of time that commenced on January 1, 1001, and ended on December 31, 2000. It was the second period of one thousand years in Anno Domini or Common Era.
The Julian calendar was used in Europe at the beginning of the millennium, and all countries that once used the Julian calendar had adopted the Gregorian calendar by the end of it. So the end date is always calculated according to the Gregorian calendar, but the beginning date is usually according to the Julian calendar (or occasionally the Proleptic Gregorian calendar).
Stephen Jay Gould argued that it is not possible to decide if the millennium ended December 31, 1999, or December 31, 2000. The Associated Press reported that the third millennium began January 1, 2001, but also reported that celebrations in the US were generally more subdued at the beginning of 2001, compared to the beginning of 2000.
The second millennium is perhaps more popularly thought of as beginning and ending a year earlier, thus starting at the beginning of 1000 and finishing at the end of 1999. Many public celebrations for the end of the millennium were held on December 31, 1999 – January 1, 2000—with few on the actual date a year later.
Civilizations.
"The civilizations in this section are organized according to the UN geoscheme."
Events.
"The events in this section are organized according to the UN geoscheme."
Significant people.
"The people in this section are organized according to the UN geoscheme."
See also

</doc>
<doc id="35965" url="http://en.wikipedia.org/wiki?curid=35965" title="1 exametre">
1 exametre

This list includes distances between 1 and 10 exametres (1018 m). To help compare different distances this page lists lengths between 1018 m (1 Em or 105.7 light years) and 1019 m (1057 light years).
Distances shorter than 1 Em
Distances longer than 10 Em

</doc>
<doc id="35979" url="http://en.wikipedia.org/wiki?curid=35979" title="1 kilometre">
1 kilometre

To help compare different orders of magnitude this page lists lengths between 1 kilometre and 10 kilometres (103 and 104 metres).
Distances shorter than 1 kilometre
Conversions.
1 kilometre (unit symbol km) is equal to:
Astronomical.
Distances longer than 10 kilometres

</doc>
<doc id="36040" url="http://en.wikipedia.org/wiki?curid=36040" title="1084">
1084

Year 1084 (MLXXXIV) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="36048" url="http://en.wikipedia.org/wiki?curid=36048" title="1146">
1146

Year 1146 (MCXLVI) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Markets.
</onlyinclude>

</doc>
<doc id="36057" url="http://en.wikipedia.org/wiki?curid=36057" title="1209">
1209

Year 1209 (MCCIX) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36068" url="http://en.wikipedia.org/wiki?curid=36068" title="1232">
1232

Year 1232 (MCCXXXII) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="36082" url="http://en.wikipedia.org/wiki?curid=36082" title="872">
872

Year 872 (DCCCLXXII) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36116" url="http://en.wikipedia.org/wiki?curid=36116" title="1534">
1534

Year 1534 (MDXXXIV) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36126" url="http://en.wikipedia.org/wiki?curid=36126" title="1415">
1415

Year 1415 (MCDXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36164" url="http://en.wikipedia.org/wiki?curid=36164" title="2006">
2006

2006 ()
will be .
2006 was designated as:

</doc>
<doc id="36190" url="http://en.wikipedia.org/wiki?curid=36190" title="1459">
1459

Year 1459 (MCDLIX) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36207" url="http://en.wikipedia.org/wiki?curid=36207" title="946">
946

Year 946 (CMXLVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36216" url="http://en.wikipedia.org/wiki?curid=36216" title="1168">
1168

Year 1168 (MCLXVIII) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="36228" url="http://en.wikipedia.org/wiki?curid=36228" title="1580s">
1580s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="36237" url="http://en.wikipedia.org/wiki?curid=36237" title="715">
715

Year 715 (DCCXV) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. The denomination 715 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="36249" url="http://en.wikipedia.org/wiki?curid=36249" title="713">
713

Year 713 (DCCXIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 713 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36259" url="http://en.wikipedia.org/wiki?curid=36259" title="961">
961

Year 961 (CMLXI) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Art.
</onlyinclude>

</doc>
<doc id="36268" url="http://en.wikipedia.org/wiki?curid=36268" title="1116">
1116

Year 1116 (MCXVI) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Arts and technology.
</onlyinclude>

</doc>
<doc id="36279" url="http://en.wikipedia.org/wiki?curid=36279" title="1111">
1111

Year 1111 (MCXI) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36290" url="http://en.wikipedia.org/wiki?curid=36290" title="1055">
1055

Year 1055 (MLV) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36303" url="http://en.wikipedia.org/wiki?curid=36303" title="1461">
1461

Year 1461 (MCDLXI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36314" url="http://en.wikipedia.org/wiki?curid=36314" title="612">
612

Year 612 (DCXII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 612 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36325" url="http://en.wikipedia.org/wiki?curid=36325" title="654">
654

Year 654 (DCLIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 654 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36336" url="http://en.wikipedia.org/wiki?curid=36336" title="666">
666

Year 666 (DCLXVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. The denomination 666 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
Events pertaining to religion in the year 666.
</onlyinclude>

</doc>
<doc id="36347" url="http://en.wikipedia.org/wiki?curid=36347" title="886">
886

Year 886 (DCCCLXXXVI) was a common year starting on Saturday of the Julian calendar.
Events.
<onlyinclude>
By place.
Byzantine Empire.
</onlyinclude>

</doc>
<doc id="36359" url="http://en.wikipedia.org/wiki?curid=36359" title="1336">
1336

Year 1336 (MCCCXXXVI) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36370" url="http://en.wikipedia.org/wiki?curid=36370" title="1382">
1382

Year 1382 (MCCCLXXXII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="36383" url="http://en.wikipedia.org/wiki?curid=36383" title="679">
679

Year 679 (DCLXXIX) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 679 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36396" url="http://en.wikipedia.org/wiki?curid=36396" title="1214">
1214

Year 1214 (MCCXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36407" url="http://en.wikipedia.org/wiki?curid=36407" title="735">
735

Year 735 (DCCXXXV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 735 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36416" url="http://en.wikipedia.org/wiki?curid=36416" title="884">
884

Year 884 (DCCCLXXXIV) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="36423" url="http://en.wikipedia.org/wiki?curid=36423" title="926">
926

Year 926 (CMXXVI) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="36432" url="http://en.wikipedia.org/wiki?curid=36432" title="1330s">
1330s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="36442" url="http://en.wikipedia.org/wiki?curid=36442" title="1240s">
1240s

This is a list of events occurring in the 0s, ordered by year.
 

</doc>
<doc id="36455" url="http://en.wikipedia.org/wiki?curid=36455" title="10 yottametres">
10 yottametres

To help compare different orders of magnitude, this page lists distances starting at 10 Ym (1025 m or 1.1 billion light-years). At this scale, expansion of the universe becomes significant. Distance of these objects are derived from their measured redshifts, which depends on the cosmological models used.
Distances shorter than 10 Ym
Distances longer than 100 Ym

</doc>
<doc id="36460" url="http://en.wikipedia.org/wiki?curid=36460" title="First Book of Nephi">
First Book of Nephi

The First Book of Nephi () is the first book of the "Book of Mormon". Its full title is The First Book of Nephi: His Reign and Ministry. The book is usually referred to as First Nephi and abbreviated as "1 Ne." It is a first-person narrative, beginning around 600 BC, of a prophet named Nephi. The Second Book of Nephi is a continuation of this narrative.
Narrative.
At Jerusalem.
The book begins in Jerusalem at the time of King Zedekiah, where Nephi's father, Lehi, has a vision, wherein he sees God the Father, Christ, and the twelve apostles. Lehi is also made aware of the imminent Babylonian destruction of Jerusalem. As a result of this experience, Lehi begins to preach repentance to his people. They reject his teachings and attempt to kill him by stoning him as with the other prophets living at that time, such as Jeremiah.
In a dream, God commands Lehi to leave Jerusalem with his family (which include his wife, Sariah, and his four sons Laman, Lemuel, Sam, Nephi and an untold number of sisters). Yet almost immediately upon their exile, Lehi is commanded by God to send his sons back to Jerusalem to retrieve the brass plates, a record similar to the Old Testament which was kept by Laban, a powerful leader in Jerusalem. Nephi and his brothers return and become frustrated after several failed attempts, where Laban tries to steal Lehi's property and murder his sons. Laman and Lemuel take out their frustration by beating Nephi. An angel appears, commands Laman and Lemuel to stop beating Nephi, and tells the brothers to return to retrieve the plates. Nephi returns alone, finds Laban passed out from drunkenness, and kills him with his own sword as directed by the Spirit of God. He then disguises himself as Laban and fools Zoram, a servant of Laban, into taking the plates outside the city to his brothers. Zoram, discovering Nephi's trick, tries to flee, but Nephi persuades him to travel with Lehi and his family and they return together with the plates.
After receiving the brass plates, Lehi spends time studying them. He discovers it contains a genealogy of himself, and that he is a descendant of Joseph, the son of Jacob. It also the contains the five books of Moses and even some writings by the contemporary prophet Jeremiah.
Lehi's sons return to Jerusalem a second time, as directed by the Lord through the prophet Lehi, to retrieve the family of Ishmael. Meeting Ishmael, they convince him and his family to leave Jerusalem together with Lehi's family. On the journey back to camp, Laman and Lemuel and some members of Ishmael's family rebel. They want to return to Jerusalem. After a stern lecture by Nephi, where he reminds them of the prophecies and offers them a choice, they tie him up and leave him to die in the desert. However, Nephi is able to escape from the ropes. Laman and Lemuel try again to hurt him, but they are softened by the daughters of Ishmael and ask for forgiveness from Nephi.
Visions of Lehi and Nephi.
Returning again to camp, Lehi has a dream where he sees The Vision of the Tree of Life, with fruit that is "desirable to make one happy." Relating this vision to his children, he expounds on it by teaching about the Redeemer and the need to follow his counsel by keeping the commandments. Nephi desires to receive a similar vision, and asks the Lord for the vision and its interpretation. Nephi is swept away by the spirit and sees the vision his father had described. He is also given an explanation about its symbolism: the tree and its fruit represent the love of God, a "rod of iron" represents the word of God, and a "great and spacious building" represents the pride and mocking attitude of the world. Nephi is shown many past and future events, including the birth, ministry, and death of Jesus Christ. He also sees Christopher Columbus traveling across the Atlantic ocean, the American Revolutionary war, and the scattering of the "seed of my brethren", the American Indians. Other events include the coming forth of the Book of Mormon, it being a record made by the ancestors of the Indians, and also what is believed to be the founding of The Church of Jesus Christ of Latter-day Saints, although it is not mentioned by name, yet there are described several events which imply movements and church formation. Future events are also taken into account, such as the coming forth of scripture even after the Book of Mormon, that would make truths known which were taken out from the New Testament, a concept discussed in the several passages of this vision. Nephi begins to see wars and rumors of wars taking place in the future, and also other events. However, he is forbidden to write about such things because John the Apostle is to write about them in the Bible. Most importantly, Nephi sees the future of his generations and the generations of Laman and Lemuel. Whereas his people will have the gospel and will ultimately be destroyed for wickedness, the children of Laman and Lemuel will be raised without a knowledge of the gospel, survive the generations, and be taught by the Book of Mormon and the future church.
After Nephi's vision, he sees Laman and Lemuel, who are arguing over the meaning of Lehi's vision. Nephi chastises them for not asking the Lord for the interpretation, and explains the point they were disputing. He pleads with them to soften their hearts and submit themselves to the Lord.
Traveling the desert.
After the sons of Lehi marry the daughters of Ishmael, a "ball of curious workmanship" is discovered at Lehi's tent door. Using the directions on the ball, they begin journeying eastward along the Red Sea. As they travel along the banks of the Red Sea, Nephi's steel bow breaks while hunting. Upon hearing the news, the entire camp begins to complain and speak against the Lord for their misfortune, including Lehi. Nephi builds a new bow and arrows out of wood, and then enquires of Lehi where to hunt. Humbled, Lehi repents and turns to the Lord, and Nephi is able to find food for the camp. Looking on the ball, they discover a message that caused them to "fear and tremble exceedingly". The ball works by their faith. As long as they are faithful, it will lead them through the most fertile parts of the land. If they are not, it will not function properly and they will be left to their own devices.
Ishmael dies on their journey near a place called "Nahum". This is one of the few places listed in the Book of Mormon that relates to the Old World. In mourning, Ishmael's daughters complain against Lehi and Nephi, and desire to return to Jerusalem. Laman and Lemuel decide to kill Lehi and Nephi, but the voice of the Lord speaks many words to them and chastises them severely, causing them to change their minds and repent.
Building the ship.
Arriving in a place they call Bountiful, on the borders of the sea, they set up a camp. After many days, the voice of the Lord comes to Nephi and commands him to go up the mountain. Once on the mountain, the Lord instructs Nephi to build a ship, and describes how to build a ship and how to make the tools needed. Nephi returns to camp and begins working to build the ship. Laman and Lemuel see Nephi and mock him for trying to achieve an impossible task. Nephi lectures them about the strength of the Lord and how impossible things are possible when the Lord commands it. Being filled with the Spirit, he commands Laman and Lemuel not to touch him or they would die instantly. He also commands them to assist in building the ship. The Lord commands Nephi to touch them, saying it will not kill them. He touches them, causing a shock, whereafter Laman and Lemuel proclaim to Nephi, "We know of a surety that the Lord is with thee". Laman and Lemuel repent again, and begin helping Nephi build the ship.
After completing the ship, the voice of the Lord commands Lehi to load his family and supplies on the ship. They depart on the ocean. Many days later, Laman and Lemuel and the sons of Ishmael begin partying, dancing, singing, and being rude. Nephi, fearing that the Lord would be angry with them, speaks to them. They get upset with Nephi and tie him up. The compass—the ball of curious workmanship—ceases to function, and they get caught in a terrible storm. At the pleading of their wives, and for fear of sinking, Laman and Lemuel release Nephi. Nephi prays, and the storm stops, leaving a great calm. Many days later, they arrive in the promised land, on the American continent.
Promised land.
Upon arriving in America, they begin building farms and raising livestock that they find native to the land. Nephi is commanded by the Lord to make the small and large plates of Nephi. On the small plates, he is to write the spiritual record of the people, and the more plain and precious parts. On the large plates, he writes the history of the people, including their genealogy.
The last parts of First Nephi contain some teachings from Nephi to his people concerning Jesus Christ. He quotes Isaiah chapters 48 and 49 from the Old Testament. He ends the first book showing how all the ancient prophets have testified of Christ, and how it is only through Christ that one can be redeemed from the fall of Adam and their sins.
Internal prophecies.
Many prophecies are made that are fulfilled within the pages of the Book of Mormon. Nephi sees the land of America, and so many cities built there by his seed that he could not count them. Nephi also sees in his vision that the descendants of his brothers will become unbelievers, and transform into a "dark, and loathsome, and a filthy people, full of idleness and all manner of abominations."
External prophecies.
There are several prophecies that are made that can only be fulfilled outside of the Book of Mormon. Key among these are the prophecies concerning the birth, life, and death of the Savior. Nephi records that Jesus would be born to a virgin, conceived by the Spirit; that Jesus would be baptized; that there would be twelve disciples; that he would heal the sick and bedeviled; that he would be judged by the world; and that he would be crucified.
Although in the book of First Nephi Jesus is never identified by name, he is identified as the son of God, a great prophet, the messiah, and the savior of the world.
In First Nephi, Nephi's father Lehi prophesied that Jerusalem would be destroyed by the Babylonians.
Nephi also prophesies concerning the future of the American continent. He sees people flee from Europe to settle in America; that the Bible (a record from the Jews) would travel among the people; that the people settling in America would drive the indigenous people out of the land; that the settlers would overpower Europe; the discovery and translation of the Book of Mormon; and that the apostle John the Revelator would write concerning the final days.
Interpretations of Nephi's vision.
Perhaps the most potentially controversial part of the First Book of Nephi is the misunderstanding that has resulted from descriptions trying to define the "great and abominable church" that Nephi sees among the "nations and kingdoms of the Gentiles"(I Nephi 13:3-8). It may be of interest to note how this particular vision has had an impact on the way of Mormonism's view toward the rest of Christianity.
The Great and Abominable Church
Nephi sees the persecution of the apostles and their followers by the "house of Israel", then later sees a "great church" that is, according to the description of the angel, "...most abominable above all other churches, which slayeth the saints of God, yea, and tortureth them and bindeth them down, and yoketh them with a yoke of iron...". The description almost immediately appears to be describing the persecution of Reformers and Protestant or pre-Protestant groups of people who suffered persecution and execution before, during, and after the Middle Ages, and this view seems to have been held by Bruce R. McConkie in his first edition of "Mormon Doctrine". While Mormons do not believe such groups had the fullness of the gospel (often meaning priesthood authority of the Aaronic and Melchizedek Priesthoods), and neither considering themselves Protestant, they do however believe that such groups had many righteous leaders and members who could be considered saints because they followed the light of Christ and sought to follow Him. Such people would include Wycliffe and Tyndale, who have been brought up most recently in an LDS General Conference.
The majority of members of The Church of Jesus Christ of Latter-day Saints believe that this great and abominable church includes any organized group of people who fight against God and His divine purposes by means of persecution, false teachings and belief systems, and oppression. Bruce R. McConkie, who was later ordained an apostle of the LDS Church, originally wrote his opinion that the Catholic Church was the great and abominable church, but very few in the LDS Church share his view today. But we must note that his meaning of the Catholic Church was the church in its original state when it was executing and plundering and hiding its crimes as a government and a church. Scriptures indicate organizations and nations during periods of times, but not necessarily throughout the span of history in its entirety.
The idea that the great and abominable church includes all evil institutions may appear completely accurate to some, since there were also immoral institutions even before the death of the Apostles. However, when Christ was crucified and his apostles and the saints were killed, the proper leadership of his church was lost. There was no longer authority directed from Christ on the earth and therefore man began to lead the church based on their own beliefs. This eventually led to changes away from true doctrine and the formation of many churches holding different viewpoints. Other churches had existed during the time of the apostles, but from the church Christ set up, after the corruption and mystification of doctrine in the Bible, the Catholic Church began to rise. Precisely what "The Great and Abomninable Church" is, however, remains vague.
The Taking Away of Truth from the Bible
Besides the persecution of the saints, Nephi sees that people who comprise the great and abominable church among the Gentiles would also be involved with taking "away from the gospel of the Lamb many parts which are plain and most precious; and also many covenants of the Lord have they taken away."
They would do this by taking the record of the Jews that would go forth "in purity" "by the hand of the twelve apostles of the Lamb" to the Gentiles, and taking away "plain and precious things" from the "book of the Lamb of God." Latter-day Saints believe that the Bible lost some of its originally intended meaning and doctrine as spoken by the ancient prophets and apostles because of this taking away of plain and precious truths by some Gentile teachers and compilers soon after the death of the apostles, though the Bible remained an important source of truth as attested by Nephi. This could be looked upon when the council of Nicaea voted on which parts of the Bible were "true doctrine."
Future Events and Books of Scripture 
Nephi sees in his vision that the record and testimony of his people in the Book of Mormon is brought forth "unto the Gentiles, by the gift and power of the Lamb." He sees this book taken to the descendants of the Lamanites to teach them the fullness of the gospel, and that other books are also brought by the Gentiles to them and to all the house of Israel to convince them "that the records of the twelve apostles of the Lamb are true." He sees that these books have the important role of making known the "plain and precious things" that had been lost from the Bible, but also "establish the truth of the first" (the Bible).
Nephi teaches of the fulfillment of the Abrahamic covenant when scattered Israel will be gathered through believing in Jesus Christ and his divine mission and atonement. He sees that there will be "wars and rumors of wars among all the nations and kindreds of the earth," but that "the covenant people of the Lord, ...scattered upon all the face of the earth," ... "were armed with righteousness and with the power of God in great glory." He foresees the eventual destruction of the great and abominable church, and the triumph of Christ's reign on earth during the Millennium.

</doc>
<doc id="36511" url="http://en.wikipedia.org/wiki?curid=36511" title="Milan">
Milan

Milan (, Italian: "Milano" ], Lombard: "Milan" ], Latin: "Mediolanum") is the second-most populous municipality in Italy and the capital of Lombardy. The municipality has a population of 1.353.882, while its urban area is the 5th largest in the EU with an estimated population of about 5.257.000. The massive suburban sprawl that followed the post-war boom of the 1950s–60s and the growth of a vast commuter belt, suggest that socio-economic linkages have expanded well beyond the boundaries of its administrative limits and its agglomeration, creating a polycentric metropolitan area of between 7 and 10 million people, stretching over the former provinces of Milan, Bergamo, Como, Lecco, Lodi, Monza and Brianza, Pavia, Varese, Novara. The Milan metropolitan region is part of the so-called "Blue Banana", the area of Europe with the highest population and industrial density, and one of the Four Motors for Europe.
Milan was founded by the Insubres, a Celtic people. The city was later conquered by the Romans as Mediolanum, eventually becoming the capital of the Western Roman Empire. During the Middle Ages, Milan flourished as a commercial and banking center. In the course of the following centuries, it had been alternatively dominated by France, Habsburg Spain, and Austria, until 1859 when the city joined the rising Kingdom of Italy. During the early 1900s, Milan led the industrialization process of the young nation, being at the very center of the economic, social, and political debate. Badly affected by World War II, and suffering a harsh Nazi occupation, the city became the main centre of the Italian Resistance. In post-war years, the city enjoyed a prolonged economic boom, attracting large flows of workers from Southern Italy. During the last decades, Milan has seen a dramatic rise in the number of international immigrants, and in 2011 more than one sixth of its population is foreign born.
Milan is the main industrial, commercial, and financial centre of Italy and a leading global city. Its business district hosts the Borsa Italiana (Italy's main stock exchange) and the headquarters of the largest national banks and companies. The city is a major world fashion and design capital. Milan's museums, theatres and landmarks (including the Milan Cathedral, the fifth largest cathedral in the world, and Santa Maria delle Grazie, decorated with Leonardo da Vinci paintings, a UNESCO World Heritage Site) attracts over 8 million annual visitors. It hosts numerous cultural institutions and universities, with 185,000 enrolled students in 2011, i.e. 11 percent of the national total. The city is also well known for several international events and fairs, including Milan Fashion Week and the Milan Furniture Fair, the largest of its kind in the world, and hosted for the second time an Universal Exposition, the Expo 2015. Milan is home to two of the world's major football teams, A.C. Milan and F.C. Internazionale Milano.
History.
Toponymy.
The etymology of Milan is very uncertain. While the modern name of the city is clearly derived from its Latin name "Mediolanum", apparently from the Latin words "medio", meaning "in the middle", and "planus", "plain", it has been suggested that its original roots could lie more deeply in the city's Celtic heritage. Indeed, the name "Mediolanum" is borne by about sixty Gallo-Roman sites all over France, such as Saintes ("Mediolanum Santonum") and Évreux ("Mediolanum Aulercorum"), as every Celtic community had its sacred assembly place of law and justice, usually placed at the midpoint of their territory. In addition, some scholars have suggested that the second element of the Latin name, "lanum", could be identified with the Celtic root "lan", signifying an enclosure or demarcated territory (source of the Welsh word 'llan', meaning a sanctuary or church) in which Celtic communities used to build shrines. Hence, "Mediolanum" could signify the central town or sanctuary of a particular Celtic tribe.
Another theory links the origin of the name to the boar sow (the "Scrofa semilanuta") an ancient emblem of the city, fancifully accounted for in Andrea Alciato's "Emblemata" (1584), beneath a woodcut of the first raising of the city walls, where a boar is seen lifted from the excavation, and the etymology of "Mediolanum" given as "half-wool", explained in Latin and in French. The foundation of Milan is credited to two Celtic peoples, the Bituriges and the Aedui, having as their emblems a ram and a boar; therefore "The city's symbol is a wool-bearing boar, an animal of double form, here with sharp bristles, there with sleek wool." Alciato credits Ambrose for his account.
Antiquity.
Around 400 BC, the Celtic Insubres settled Milan and the surrounding region. In 222 BC, the Romans conquered the settlement, which was then renamed Mediolanum. After several centuries of Roman control, Milan was declared the capital of the Western Roman Empire by Emperor Diocletian in 286 AD. Diocletian chose to stay in the Eastern Roman Empire (capital Nicomedia) and his colleague Maximianus ruled the Western one. Immediately Maximian built several gigantic monuments, like a large circus 470 x, the Thermae Herculeae, a large complex of imperial palaces and several other services and buildings.
With the Edict of Milan of 313, Emperor Constantine I guaranteed freedom of religion for Christians. After city was besieged by the Visigoths in 402, the imperial residence was moved to the more strategic city of Ravenna. In 452, the Huns overran the city. In 539, the Ostrogoths conquered and destroyed Milan in the course of the Gothic War against Byzantine Emperor Justinian I. In the summer of 569, the Lombards (from which the name of the Italian region Lombardy derives), a Teutonic tribe conquered Milan, overpowering the small Byzantine army left for its defence. Some Roman structures remained in use in Milan under Lombard rule. Milan surrendered to the Franks in 774 when Charlemagne, in an utterly novel decision, took the title of "King of the Lombards" as well (before then the Germanic kingdoms had frequently conquered each other, but none had adopted the title of King of another people). The Iron Crown of Lombardy dates from this period. Subsequently, Milan became part of the Holy Roman Empire.
Middle Ages.
During the Middle Ages, Milan prospered as a centre of trade due to its command of the rich plain of the Po and routes from Italy across the Alps. The war of conquest by Frederick I Barbarossa against the Lombard cities brought the destruction of much of Milan in 1162. After the founding of the Lombard League in 1167, Milan took the leading role in this alliance. The war between the German emperor and the Italian communes went on with mixed fortunes for years, ending with the Italian victory at the battle of Legnano. As a result of the independence that the Lombard cities gained in the Peace of Constance in 1183, Milan became a duchy. In 1208 Rambertino Buvalelli served a term as podestà of the city, in 1242 Luca Grimaldi, and in 1282 Luchetto Gattilusio. The position could be fraught with personal dangers in the violent political life of the medieval commune: in 1252 Milanese heretics assassinated the Church's Inquisitor, later known as Saint Peter Martyr, at a ford in the nearby "contado"; the killers bribed their way to freedom, and in the ensuing riot the "podestà" was very nearly lynched. In 1256 the archbishop and leading nobles were expelled from the city. In 1259 Martino della Torre was elected "Capitano del Popolo" by members of the guilds; he took the city by force, expelled his enemies, and ruled by dictatorial powers, paving streets, digging canals, successfully taxing the countryside. His policy, however, brought the Milanese treasury to collapse; the use of often reckless mercenary units further angered the population, granting an increasing support for the Della Torre's traditional enemies, the Visconti. It is worthy of note that the most important industries throughout the period were major armaments and wool production, a whole catalogue of activities and trades is given in Bonvesin della Riva's "de Magnalibus Urbis Mediolani".
On 22 July 1262 Ottone Visconti was created archbishop of Milan by Pope Urban IV, against the Della Torre candidate, Raimondo della Torre, Bishop of Como. The latter thus started to publicize allegations of the Visconti's closeness to the heretic Cathars and charged them of high treason: the Visconti, who accused the Della Torre of the same crimes, were then banned from Milan and their properties confiscated. The ensuing civil war caused more damage to Milan's population and economy, lasting for more than a decade. Ottone Visconti unsuccessfully led a group of exiles against the city in 1263, but after years of escalating violence on all sides, finally, after the victory in the Battle of Desio (1277), he won the city for his family. The Visconti succeeded in ousting the della Torre forever, ruling the city and its possession until the 15th century.
Much of the prior history of Milan was the tale of the struggle between two political factions: the Guelphs and the Ghibellines. Most of the time the Guelphs were successful in the city of Milan. However, the Visconti family were able to seize power (signoria) in Milan, based on their "Ghibelline" friendship with the German Emperors. In 1395, one of these emperors, Wenceslas (1378–1400), raised the Milanese to the dignity of a duchy. Also in 1395, Gian Galeazzo Visconti became duke of Milan. The Ghibelline Visconti family was to retain power in Milan for a century and a half from the early 14th century until the middle of the 15th century.
In 1447 Filippo Maria Visconti, Duke of Milan, died without a male heir; following the end of the Visconti line, the Ambrosian Republic was enacted. The Ambrosian Republic took its name from St. Ambrose, popular patron saint of the city of Milan. Both the Guelph and the Ghibelline factions worked together to bring about the Ambrosian Republic in Milan. However, the Republic collapsed when in 1450, Milan was conquered by Francesco Sforza, of the House of Sforza, which made Milan one of the leading cities of the Italian Renaissance.
Early modern.
Milan's last independent ruler, Lodovico il Moro, called French king Charles VIII into Italy in the expectation that France might be an ally in inter-Italian wars. The future king of France, Louis of Orléans, took part in the expedition and realized Italy was virtually defenceless. This prompted him to come back a few years later and claim the Duchy of Milan for himself, his grandmother having been a member of the ruling Visconti family. At that time, Milan was also defended by Swiss mercenaries. After the victory of Louis's successor François I over the Swiss at the Battle of Marignan, the duchy was promised to the French king François I. When the Spanish Habsburg Charles V defeated François I at the Battle of Pavia in 1525, northern Italy, including Milan, passed to Habsburg Spain.
In 1556, Charles V abdicated in favour of his son Philip II and his brother Ferdinand I. Charles's Italian possessions, including Milan, passed to Philip II and remained with the Spanish line of Habsburgs, while Ferdinand's Austrian line of Habsburgs ruled the Holy Roman Empire.
The Great Plague of Milan in 1629–31 killed an estimated 60,000 people out of a population of 130,000. This episode is considered one of the last outbreaks of the centuries-long pandemic of plague that began with the Black Death.
In 1700 the Spanish line of Habsburgs was extinguished with the death of Charles II. After his death, the War of the Spanish Succession began in 1701 with the occupation of all Spanish possessions by French troops backing the claim of the French Philippe of Anjou to the Spanish throne. In 1706, the French were defeated in Ramillies and Turin and were forced to yield northern Italy to the Austrian Habsburgs. In 1713, the Treaty of Utrecht formally confirmed Austrian sovereignty over most of Spain's Italian possessions including Lombardy and its capital, Milan.
Napoleon invaded Italy in 1796, and Milan was declared capital of the Cisalpine Republic. Later, he declared Milan capital of the Kingdom of Italy and was crowned in the Duomo. Once Napoleon's occupation ended, the Congress of Vienna returned Lombardy, and Milan, along with Veneto, to Austrian control in 1815. During this period, Milan became a centre of lyric opera. Here in the 1770s Mozart had premiered three operas at the Teatro Regio Ducal. Later La Scala became the reference theatre in the world, with its premières of Bellini, Donizetti, Rossini and Verdi. Verdi himself is interred in the Casa di Riposo per Musicisti, his present to Milan. In the 19th century other important theatres were "La Cannobiana" and the "Teatro Carcano".
On 18 March 1848, the Milanese rebelled against Austrian rule, during the so-called "Five Days" (Italian: "Le Cinque Giornate"), and Field Marshal Radetzky was forced to withdraw from the city temporarily. The Kingdom of Sardinia stepped in to help the insurgents; a plebiscite held in Lombardy decided in favor of unification with Sardinia. However, after defeating the Sardinian forces at Custoza on 24 July, Radetzky was able to reassert Austrian control over Milan and northern Italy. A few years on, however, Italian nationalists again called for the removal of Austria and Italian unification. Sardinia and France formed an alliance and defeated Austria at the Battle of Solferino in 1859. Following this battle, Milan and the rest of Lombardy were incorporated into the Kingdom of Sardinia, which soon gained control of most of Italy and in 1861 was rechristened as the Kingdom of Italy.
The political unification of Italy cemented Milan's commercial dominance over northern Italy. It also led to a flurry of railway construction that had started under Austrian partronage (Venice–Milan; Milan–Monza) that made Milan the rail hub of northern Italy. Thereafter with the opening of the Gotthard (1881) and Simplon (1906) railway tunnels, Milan became the major South European rail focus for business and passenger movements e.g. the Simplon Orient Express. Rapid industrialization and market expansion put Milan at the centre of Italy's leading industrial region, though in the 1890s Milan was shaken by the Bava-Beccaris massacre, a riot related to a high inflation rate. Meanwhile, as Milanese banks dominated Italy's financial sphere, the city became the country's leading financial centre.
Late modern and contemporary.
In 1919, Fascist leader Benito Mussolini organized his Blackshirts in Milan, that rallied for the first time in Piazza San Sepolcro, a small square near Milan Cathedral. Subsequently, Mussolini led his March on Rome starting from the city. During the Second World War Milan suffered extensive damage from Allied bombings. When Italy surrendered in 1943, German forces occupied most of Northern Italy until 1945. As a result, antifascist resistance groups formed and started guerilla warfare against Nazi and Italian Social Republic's troops. As the war came to an end, the American 1st Armored Division advanced on Milan as part of the Po Valley Campaign. But before they arrived, members of the resistance seized control of the city and executed Mussolini along with several members of his collaborationist government. On 29 April 1945, the corpses of Mussolini, his mistress Clara Petacci and other Fascist leaders were hanged in Piazzale Loreto, where a year before fifteen partisans had been executed.
During the post-war economic boom, a large wave of internal migration (especially from rural areas of Southern Italy), moved to the city, bringing the population from 1.3 million in 1951 to 1.7 million in 1967. During this period, Milan saw a quick reconstruction of most of its destroyed facilities, with the building of several innovative and modernist skyscrapers, such as the Torre Velasca and the Pirelli Tower, that soon became symbols of the boom. The economic prosperity was however overshadowed in the late 1960s and early 1970s during the so-called Years of Lead, when Milan witnessed an unprecedented wave of street violence, labour strikes and political terrorism. The apex of this period of turmoil occurred on 12 December 1969, when a bomb exploded at the National Agrarian Bank in Piazza Fontana, killing seventeen people and injuring eighty-eight.
In the 1980s, as several fashion firms based in the city became internationally successful (such as Armani, Versace and Dolce & Gabbana), Milan became one of the world's fashion capitals. The city saw also a marked rise in international tourism, notably from America and Japan, while the stock exchange increased its market capitalization more than five-fold. This short-lived period of collective euphoria and the new international image of the city led the mass media to nickname the metropolis "Milano da bere", literally "Milan to drink". However, in the 1990s, Milan was badly affected by Tangentopoli, a large political scandal in which many local and national politicians and businessmen were tried for alleged corruption. The city was also affected by a severe financial crisis and a steady decline in textiles, automobile and steel production, that led to a deep reorganization of its economy.
In the early 21st century, Milan underwent a series of massive redevelopments, with the moving of its exhibition centre to a much larger site in the satellite town of Rho, and the construction of new business districts such as Porta Nuova and CityLife. Despite the decline in Milan's manufacturing production, the city has found alternative and successful sources of revenue, including publishing, finance, banking, fashion design, information technology, logistics, transport and tourism. The 2010 official announcement of Milan hosting Expo 2015 has brightened prospects for the city's future, with several new plans of regeneration and the planned construction of numerous futuristic structures. In addition, the city's decades-long population decline seems to have come to an end in recent years, with signs of recovery as it grew by seven percent since the last census.
Geography.
Topography.
Milan is located in the north-western section of the Po Valley, approximately half-way between the river Po to the south and the first reliefs of the Alps with the great lakes (Lake Como, Lake Maggiore, Lake Lugano) to the north, the Ticino river to the west and the Adda to the east. The municipal territory is entirely flat, the highest point being at 122 m above sea level. The administrative commune covers an area of about 181 km2, with a population, in 2013, of 1,324,169 and a population density of 7315 PD/km2. A larger urban area, comprising parts of the provinces of Milan, Monza e Brianza, Como, Lecco and Varese is 1891 km2 wide and has a population of 5,264,000 with a density of 2783 PD/km2.
Modern Milan has a central area focused on residential and tertiary activities, with a financial district that hosts the stock exchange and the headquarters of banks and insurance companies, shopping centres and educational institutions. In the concentric layout of the city centre is still evident the influence of "Navigli", an ancient system of navigable and interconnected canals, now mostly covered. Around the city proper, and beyond its railway and motorway rings, lies a vast urbanized valley that expands mainly to the north, engulfing many communes in a continuous urban landscape. The contiguous built-up area trespass by far the city limits, forming a vast urban agglomeration that stretches northeast and northwest to reach Varese, Como, Lecco and Bergamo.
Climate.
According to the Köppen climate classification, Milan has a humid subtropical climate (Cfa). Milan's climate is similar to much of northern Italy's inland plains, where hot, sunny summers and moderately cold, wet and foggy winters prevail. However the mean number of days with precipitation per year is one of the lowest in Europe outside the regions with mediterranean climates. Actually, the Alps and Apennines mountains form a natural barrier that protects the city from the major circulations coming from northern Europe and the sea.
During winter, average temperatures can fall below freezing levels (-2 °C) and significant accumulations of snow can occur: the historic average of Milan's area is 21 cm in the period between 1950 and 2007, with a record of 90 cm in January, 1985. In the stereotypical image, the city is often shrouded in the heavy fog typical of cold seasons in the Po Basin, although the removal of rice paddies from the southern neighborhoods and the urban heat island effect have reduced this occurrence in recent decades, at least in the city centre. Occasionally, bursts of Foehn winds cause the temperatures to rise unexpectedly: on 22 January 2012 the daily high reached 16 °C while on 22 February 2012 it reached 21 °C. The city receives on average seven days of snow per year. Air pollution levels rise significantly in wintertime when cold air clings to the soil, causing Milan to be one of Europe’s most polluted cities.
Summers can be quite sultry, when humidity levels are high and peak temperatures can reach 34 °C. Usually this season enjoys clearer skies and more than 13 hours of daylight on average; when precipitations occur though, there is a higher likelihood of them being thunderstorms and hailstorms. Springs and autumns are well marked and generally pleasant, with temperatures ranging between 10 and; these seasons are characterised by higher precipitation averages, especially in April and May. Relative humidity typically ranges between 45% (comfortable) and 95% (very humid) throughout the year, rarely dropping below 27% (dry) and reaching as high as 100% Wind is generally absent: over the course of the year typical wind speeds vary from 0 mph to 9 mph (calm to gentle breeze), rarely exceeding 18 mph (fresh breeze), except during summer thunderstorms when winds can blow strong. In the spring, gale-force windstorms may happen, generated either by Tramontane blowing from the Alps or by Bora-like winds from the north. 
Government.
Municipal government.
The legislative body of the municipality is the City Council ("Consiglio Comunale"), which is composed by 48 councillors elected every five years with a proportional system, contextually to the mayoral elections. The executive body is the City Committee ("Giunta Comunale"), composed by 16 assessors, that is nominated and presided over by a directly elected Mayor. The current mayor of Milan is Giuliano Pisapia, a left-wing independent leading a progressive alliance composed by the Democrats, Left Ecology Freedom, the Greens and Federation of the Left.
The municipality of Milan is subdivided into nine administrative Borough Councils ("Consigli di Zona"), down from the former twenty districts before the 1999 administrative reform. Each Borough Council is governed by a Council ("Consiglio") and a President, elected contextually to the city Mayor. The urban organization is governed by the Italian Constitution (art. 114), the Municipal Statute and several laws, notably the Legislative Decree 267/2000 or Unified Text on Local Administration ("Testo Unico degli Enti Locali"). The Borough Councils have the power to advise the Mayor with nonbinding opinions on a large spectrum of topics (environment, construction, public health, local markets) and exercise the functions delegated to them by the City Council; in addition they are supplied with an autonomous funding in order to finance local activities.
Provincial and Regional government.
Milan is the capital of the eponymous administrative province and of Lombardy, one of the twenty regions of Italy. While the Province of Milan has a population of 3,195,211, making it the second most populated province of Italy after Rome, Lombardy is by far the most populated region of Italy, with more than ten million inhabitants, almost one sixth of the national total. The seat of the regional government is Palazzo Lombardia that, standing at 161.3 m, is the second tallest building in Milan.
According to the last governmental dispositions concerning administrative reorganization, the urban area of Milan is one of the 15 Metropolitan municipalities ("città metropolitane"), new administrative bodies fully operative since 1 January 2015. The new Metro municipalities, giving large urban areas the administrative powers of a province, are conceived for improving the performance of local administrations and to slash local spending by better coordinating the municipalities in providing basic services (including transport, school and social programs) and environment protection. In this policy framework, the Mayor of Milan is designated to exercise the functions of Metropolitan mayor ("Sindaco metropolitano"), presieding over a Metropolitan Council formed by 16 mayors of municipalities within the Metro municipality.
The Metropolitan City of Milan is headed by the Metropolitan Mayor ("Sindaco metropolitano") and by the Metropolitan Council ("Consiglio metropolitano"). Since 1 January 2015 Giuliano Pisapia, as mayor of the capital city, has been the first mayor of the Metropolitan City.
Cityscape.
Architecture.
There are only few remains of the ancient Roman colony, notably the well-preserved Colonne di San Lorenzo. During the second half of the 4th century, Saint Ambrose, as bishop of Milan, had a strong influence on the layout of the city, reshaping the centre (although the cathedral and baptistery built in Roman times are now lost) and building the great basilicas at the city gates: Sant'Ambrogio, San Nazaro in Brolo, San Simpliciano and Sant'Eustorgio, which still stand, refurbished over the centuries, as some of the finest and most important churches in Milan. Milan's Cathedral, built between 1386 and 1577, is the fifth largest cathedral in the world and the most important example of Gothic architecture in Italy. The gilt bronze statue of the Virgin Mary, placed in 1774 on the highest pinnacle of the Duomo, soon became one of the most enduring symbols of Milan.
In the 15th century, when the Sforza ruled the city, an old Viscontean fortress was enlarged and embellished to become the Castello Sforzesco, the seat of an elegant Renaissance court surrounded by a walled hunting park. Notable architects involved in the project included the Florentine Filarete, who was commissioned to build the high central entrance tower, and the military specialist Bartolomeo Gadio. The alliance between Francesco Sforza and Florence's Cosimo de' Medici bore to Milan Tuscan models of Renaissance architecture, apparent in the Ospedale Maggiore and Bramante's work in the city, which includes Santa Maria presso San Satiro (a reconstruction of a small 9th-century church), the tribune of Santa Maria delle Grazie and three cloisters for Sant'Ambrogio. The Counter-Reformation in the 16th-17th century was also the period of Spanish domination and was marked by two powerful figures: Saint Charles Borromeo and his cousin, Cardinal Federico Borromeo. Not only did they impose themselves as moral guides to the people of Milan, but they also gave a great impulse to culture, with the creation of the Biblioteca Ambrosiana, in a building designed by Francesco Maria Ricchino, and the nearby Pinacoteca Ambrosiana. Many notable churches and Baroque mansions were built in the city during this period by the architects, Pellegrino Tibaldi, Galeazzo Alessi and Ricchino himself.
Empress Maria Theresa of Austria was responsible for the significant renovations carried out in Milan during the 18th century. This profound urban and artistic renewal included the establishment of Teatro alla Scala, inaugurated in 1778 and today one of the world's most famous opera houses, and the renovation of the Royal Palace. The late 1700s Palazzo Belgioioso by Giuseppe Piermarini and Royal Villa of Milan by Leopoldo Pollack, later the official residence of Austrian vice-roys, are often regarded among the best examples of Neoclassical architecture in Lombardy. The Napoleonic rule of the city in 1805-1814, having established Milan as the capital of a satellite Kingdom of Italy, took steps in order to reshape it accordingly to its new status, with the construction of large boulevards, new squares (Porta Ticinese by Luigi Cagnola and Foro Bonaparte by Giovanni Antonio Antolini) and cultural institutions (Art Gallery and the Academy of Fine Arts). The massive Arch of Peace, situated at the bottom of Corso Sempione, are often compared to the Arc de Triomphe in Paris. In the second half of the 19th century, Milan quickly became the main industrial center in of the new Italian nation, drawing inspiration from the great European capitals that were hubs of the second industrial revolution. The great Galleria Vittorio Emanuele II, realized by Giuseppe Mengoni between 1865 and 1877 to celebrate Vittorio Emanuele II, is a covered passage with a glass and cast iron roof, inspired by the Burlington Arcade in London. Another late 19th century eclectic monument in the city is the Cimitero Monumentale graveyard, built in a Neo-Romanesque style between 1863 and 1866.
The tumultuous period of early 20th century brought several, radical innovations in Milanese architecture. Art Nouveau, also known as "Liberty" in Italy, is recognisable in Palazzo Castiglioni, built by architect Giuseppe Sommaruga between 1901 and 1904. Other remarkable examples include Hotel Corso and Berri-Meregalli house, the latter built in a traditional Milanese Art Nouveau style combined with elements of neo-Romanesque and Gothic revival architecture, regarded as one of the last such types of architecture in the city. A new, more eclectic form of architecture can be seen in buildings such as Castello Cova, built the 1910s in a distinctly neo-medioeval style, evoking the architectural trends of the past. An important example of Art Deco, which blended such styles with Fascist architecture, is the massive Central railway station inaugurated in 1931.
The post–World War II period saw rapid reconstruction and fast economic growth, accompanied by a nearly two-fold increase in population. In the 1950s and 1960s, a strong demand for new residential and commercial areas drove to extreme urban expansion, that has produced some of the major milestones in the city's architectural history, including Gio Ponti's Pirelli Tower (1956–60), Velasca Tower (1956–58), and the creation of brand new residential satellite towns, as well as huge amounts of low quality public housings. In recent years, de-industrialization, urban decay and gentrification led to a massive urban renewal of former industrial areas, that have been transformed into modern residential and financial districts, notably Porta Nuova in downtown Milan and FieraMilano in the suburb of Rho. In addition, the old exhibition area is being completely reshaped according to the Citylife regeneration project, featuring residencial areas, museums, an urban park and three skyscrapers designed by international architects, and after whom they are named: the 202m Isozaki Tower – when completed, the tallest building in Italy, the twisted Hadid Tower, and the curved Libeskind Tower.
Parks and gardens.
The largest parks in the central area of Milan are Sempione Park, at the north-western edge, and Montanelli Gardens, situated northeast of the city. English-style Sempione Park, built in 1890, contains a Napoleonic Arena, the Milan City Aquarium, a steel lattice panoramic tower, an art exhibition centre, a Japanese garden and a public library. The Montanelli gardens, created in the 18th century, hosts the Natural History Museum of Milan and a planetarium. Slightly away from the city centre, heading east, Forlanini Park is characterized by a large pond and a few preserved shacks which remind of the area's agricultural past.
In addition, even though Milan is located in one of the most urbanised regions of Italy, it's surrounded by a belt of green areas and features numerous gardens even in its very centre. Since 1990, the farmlands and woodlands north (Parco Nord Milano) and south (Parco Agricolo Sud Milano) of the urban area have been protected as regional parks. West of the city, the Parco delle Cave (Sand pit park,) has been established on a neglected site where gravel and sand used to be extracted, featuring artificial lakes and woods.
Demographics.
With rapid industrialization in post-war years, the population of Milan peaked at 1,743,427 in 1973. Thereafter, during the following thirty years, almost one third of the population moved to the outer belt of new suburbs and satellite settlements that grew around the city proper. There were an estimated 1,324,169 official residents in the commune of Milan at the end of 2013. However, Milan's continuous urban area extends beyond the borders of its administrative commune and was home to 5,264,000 people in 2013, while its wider metropolitan area has a population of between 7 and 10 million depending on the definition used.
Ethnic groups.
As of 2011, the Italian national institute of statistics Istat estimated that 236,855 foreign-born residents lived in Milan, representing almost 20% of the total resident population, a rapid increase from recent years levels. After World War II, Milan experienced two main distinct epochs of massive immigration: the first period, dating from the 1950s to the early 1970s, saw a large influx of immigrants from poorer and rural areas within Italy; the second period, starting from the late 1970s, has been characterized by the preponderance of foreign-born immigrants.
The early period coincided with the so-called Italian economic miracle of postwar years, an era of extraordinary growth based on rapid industrial expansion and massive public works, that brought to the city a large influx of over 400,000 people, mainly from rural and overpopulated Southern Italy. In the last three decades, the foreign born share of the population soared. Immigrants came mainly from Africa (in particular Egyptian, Moroccans, Senegalese, and Nigerian), and the former socialist countries of Eastern Europe (notably Albania, Romania, Ukraine, Macedonia, Moldova), in addition to a growing number of Asians (in particular Chinese, Sri Lankans and Filipinos) and Latin Americans (Mainly South Americans). At the beginning of the 1990s, Milan already had a population of foreign-born residents of approximately 58,000 (or 4% of the then population), that rose rapidly to over 117,000 by the end of the decade (about 9% of the total).
Decades of continuing massive immigration have made the city the most cosmopolitan and multi-cultural in Italy. Milan notably hosts the oldest and largest Chinese community in Italy, with almost 21,000 people in 2011. Situated in the 9th district, and centered on Via Paolo Sarpi, an important commercial avenue, the Milanese Chinatown was originally established in the 1920s by immigrants from Wencheng County, in the Zhejiang province, and used to operate small textile and leather workshops. Milan has also a substantial English-speaking community (more than 3,000 American, British and Australian expatriates), and several English schools and language publications, such as Hello Milano, Where Milano and Easy Milano.
Religion.
Milan's population, like that of Italy as a whole, is mostly Catholic. It is the seat of the Roman Catholic Archdiocese of Milan. The city is also home to sizeable Orthodox, Buddhist, Jewish, Muslim, and Protestant communities.
Milan has its own historic Catholic rite known as the Ambrosian Rite (Italian: "Rito ambrosiano"). It varies slightly from the typical Catholic rite (the "Roman", used in all other western regions), with some differences in the liturgy and mass celebrations, in the Canons are Easter and Lent, in the colour of liturgical vestments, peculiar use of incense, marriage form, office for the dead, baptism by immersion, and in the calendar (for example, the date for the beginning of lent is celebrated some days after the common date, so the carnival has different date). The season of Advent is of six weeks duration and starts on the Sunday after the feast of Saint Martin (11 November). The Ambrosian rite is also practiced in other surrounding locations in Lombardy, parts of Piedmont and in the Swiss canton of Ticino. The sounding of church bells uses a peculiar technique. Another important difference concerns the liturgical music. The Gregorian chant was completely unused in Milan and surrounding areas, because the official one was its own Ambrosian chant, definitively established by the Council of Trent (1545–1563) and earlier than the Gregorian. To preserve this music there has developed the unique "schola cantorum", a college, and an Institute called PIAMS (Pontifical Ambrosian Institute of Sacred Music), in partnership with the Pontifical Institute of Sacred Music (PIMS) in Rome.
The Milan Synagogue was designed by Luca Beltrami in 1892. The Anglican Episcopal Church of All Saints Milan was built in 1896. In 2014 the City Council agreed on the construction of a mosque next to the area of the former sport venue Palatrussardi.
Economy.
While Rome is Italy's political capital, Milan is the country's economic and financial heart. With a 2010 GDP estimated at €132.5 billion, the province of Milan generates approximately 9% of the national GDP; while the economy of the Lombardy region generates approximately 20% of the Italy's GDP (or an estimated €325 billion in 2010, roughly the size of Belgium).
The province of Milan is home to about 45% of businesses in the Lombardy region and more than 8 percent of all businesses in Italy, including three Fortune 500 companies. Milan is home to a large number of media and advertising agencies, national newspapers and telecommunication companies, including both the public service broadcaster RAI and private television companies like Mediaset, Telecom Italia Media and Sky Italia. In addition, it has also seen a rapid increase in internet companies with both domestic and international companies such as Altervista, Google, Lycos, Virgilio and Yahoo! establishing their Italian operations in the city. Milan is a major world fashion centre, where the sector can count on 12,000 companies, 800 show rooms, and 6,000 sales outlets (with brands such as Armani, Versace and Valentino), while four weeks a year are dedicated to top shows and other fashion events. The city is also an important manufacturing centre, especially for the automotive industry, with companies such as Alfa Romeo and Pirelli having a significant presence in the city. Other important products made in Milan include chemicals, machinery, pharmaceuticals and plastics.
Other key sectors in the city's economy are advanced research in health and biotechnologies, chemicals and engineering, banking and finance. Milan is the home to Italy's main banking groups (198 companies), including Banca Popolare di Milano, Mediobanca, Mediolanum and UniCredit and over forty foreign banks. The Associazione Bancaria Italiana representing the Italian banking system and Milan Stock Exchange (225 companies listed on the stock exchange) are both located in the city. The city can boast one of Europe's largest trade fair systems of over 1600000 m² and about 4.5 million visitors flock to the around 75 major events every year from all over the world as well as to the high-tech conference centres. Tourism is an increasingly important part of the city's economy: in 2010, the city registered more than 2.3 million international arrivals, up 10% on the previous year.
Milan is presently undergoing a massive urban renewal. FieraMilano, the historical city trade fair operator, owned a fairground known as "FieraMilanoCity", which was dismantled to be house for a major urban development, CityLife district. The new trade exhibition center, built in the north-western suburb of Rho and inaugurated in April 2005, makes FieraMilano one of the largest expo areas in the world. Along with CityLife, many other construction projects are under way to rehabilitate disused industrial areas. Several famous architects take part in the projects, such as Renzo Piano, Norman Foster, Arata Isozaki, Zaha Hadid, Massimiliano Fuksas and Daniel Libeskind. Many of these projects are in preparation contextually to Expo 2015.
Culture.
Museums and art galleries.
Milan is home to many cultural institutions, museums and art galleries, that account for about a tenth of the national total of visitors and recepits. The Pinacoteca di Brera is one of Milan's most important art galleries. It contains one of the foremost collections of Italian painting, including masterpieces such as the "Brera Madonna" by Piero della Francesca. The Castello Sforzesco hosts numerous art collections and exhibitions, especially statues, ancient arms and furnitures, as well as the Pinacoteca del Castello Sforzesco, with an art collection including Michelangelo's last sculpture, the "Rondanini Pietà", Andrea Mantegna's "Trivulzio Madonna" and Leonardo da Vinci's "Codex Trivulzianus" manuscript. The Castello complex also includes The Museum of Ancient Art, The Furniture Museum, The Museum of Musical Instruments and the Applied Arts Collection, The Egyptian and Prehistoric sections of the Archaeological Museum and the Achille Bertarelli Print Collection.
Milan's figurative art flourished in the Middle-Ages, and with the Visconti family being major patrons of the arts, the city became an important centre of Gothic art and architecture (Milan Cathedral being the city's most formidable work of Gothic architecture). Leonardo worked in Milan from 1482 until 1499. He was commissioned to paint the "Virgin of the Rocks" for the Confraternity of the Immaculate Conception and "The Last Supper" for the monastery of Santa Maria delle Grazie.
The city was affected by the Baroque in the 17th and 18th centuries, and hosted numerous formidable artists, architects and painters of that period, such as Caravaggio and Francesco Hayez, which several important works are hosted in Brera Academy. The Museum of Risorgimento is specialized on the history of Italian unification Its collections include iconic paintings like Baldassare Verazzi's "Episode from the Five Days" and Francesco Hayez's 1840 " of Emperor Ferdinand I of Austria". The Triennale is a design museum and events venue located in Palazzo dell'Arte, in Sempione Park. It hosts exhibitions and events highlighting contemporary Italian design, urban planning, architecture, music, and media arts, emphasizing the relationship between art and industry.
Milan in the 20th century was the epicenter of the Futurist artistic movement. Filippo Marinetti, the founder of Italian Futurism wrote in his 1909 ""Futurist Manifesto" (in Italian, "Manifesto Futuristico"), that Milan was "grande...tradizionale e futurista" ("grand...traditional and futuristic"", in English). Umberto Boccioni was also an important Futurism artist who worked in the city.Today, Milan remains a major international hub of modern and contemporary art, with numerous modern art galleries. The Modern Art Gallery, situated in the Royal Villa, hosts collections of Italian and European painting from the 18th to the early 20th centuries.
The Museo del Novecento, situated in the Palazzo dell'Arengario, is one of the most important art galleries in Italy about 20th-century art; of particular relevance are the sections dedicated to Futurism, Spatialism and Arte povera.
Milan is home to many public art projects, with a variety of works that range from sculptures to murals to pieces by internationally renowned artists, including Arman, Francesco Barzaghi, Alberto Burri, Pietro Cascella, Maurizio Cattelan, Leonardo Da Vinci, Giorgio de Chirico, Claes Oldenburg, Igor Mitoraj, Arnaldo Pomodoro, Aldo Rossi, Domenico Trentacoste.
Music.
Milan is a major national and international centre of the performing arts, most notably opera. Milan hosts La Scala operahouse, considered one of the most prestigious operahouses in the world, and throughout history has hosted the premieres of numerous operas, such as "Nabucco" by Giuseppe Verdi in 1842, "La Gioconda" by Amilcare Ponchielli, "Madama Butterfly" by Giacomo Puccini in 1904, "Turandot" by Giacomo Puccini in 1926, and more recently "Teneke", by Fabio Vacchi in 2007. Other major theatres in Milan include the Teatro degli Arcimboldi, Teatro Dal Verme, Teatro Lirico and formerly the Teatro Regio Ducal. The city also has a renowned symphony orchestra and musical conservatory, and has been, throughout history, a major centre for musical composition: numerous famous composers and musicians such as Gioseppe Caimo, Simon Boyleau, Hoste da Reggio, Verdi, Giulio Gatti-Casazza, Paolo Cherici and Alice Edun are or were from, or call or called Milan their home. The city is also the birthplace of many modern ensembles and bands, including Camaleonti, Camerata Mediolanense, Dynamis Ensemble, Elio e le Storie Tese, Krisma, Premiata Forneria Marconi, Quartetto Cetra, Stormy Six and Le Vibrazioni.
Fashion and design.
Milan is widely regarded as a global capital in industrial design, fashion and architecture. In the 1950s and 60s, as the main industrial centre of Italy and one of Europe's most dynamic cities, Milan became a world capital of design and architecture. There was such a revolutionary change that Milan’s fashion exports accounted for $726 million (U.S. currency) in 1952, and by 1955 that number grew to $2.5 billion. Modern skyscrapers, such as the Pirelli Tower and the Torre Velasca were built, and artists such as Bruno Munari, Lucio Fontana, Enrico Castellani and Piero Manzoni gathered in the city. Today, Milan is still particularly well known for its high-quality furniture and interior design industry. The city is home to FieraMilano, Europe's largest permanent trade exhibition, and Salone Internazionale del Mobile, one of the most prestigious international furniture and design fairs.
Milan is also regarded as one of the fashion capitals of the world, along with New York City, Paris, and London. Milan is synonymous with the Italian prêt-à-porter industry, as many of the most famous Italian fashion brands, such as Valentino, Gucci, Versace, Prada, Armani and Dolce & Gabbana, are headquartered in the city. Numerous international fashion labels also operate shops in Milan. Furthermore, the city hosts the Milan Fashion Week twice a year, one of the most important events in the international fashion system. Milan's main upscale fashion district, "quadrilatero della moda", is home to the city's most prestigious shopping streets (Via Monte Napoleone, Via della Spiga, Via Sant'Andrea, Via Manzoni and Corso Venezia), in addition to Galleria Vittorio Emanuele II, one of the world's oldest shopping malls.
Language and literature.
In the late 18th century, and throughout the 19th, Milan was an important centre for intellectual discussion and literary creativity. The Enlightenment found here a fertile ground. Cesare, Marquis of Beccaria, with his famous "Dei delitti e delle pene", and Count Pietro Verri, with the periodical "Il Caffè" were able to exert a considerable influence over the new middle-class culture, thanks also to an open-minded Austrian administration. In the first years of the 19th century, the ideals of the Romantic movement made their impact on the cultural life of the city and its major writers debated the primacy of Classical versus Romantic poetry. Here, too, Giuseppe Parini, and Ugo Foscolo published their most important works, and were admired by younger poets as masters of ethics, as well as of literary craftsmanship. Foscolo's poem "Dei sepolcri" was inspired by a Napoleonic law that—against the will of many of its inhabitants—was being extended to the city. In the third decade of the 19th century, Alessandro Manzoni wrote his novel "I Promessi Sposi", considered the manifesto of Italian Romanticism, which found in Milan its centre, and Carlo Porta wrote his poems in Lombard Language. The periodical "Il Conciliatore" published articles by Silvio Pellico, Giovanni Berchet, Ludovico di Breme, who were both Romantic in poetry and patriotic in politics. After the Unification of Italy in 1861, Milan lost its political importance; nevertheless it retained a sort of central position in cultural debates. New ideas and movements from other countries of Europe were accepted and discussed: thus Realism and Naturalism gave birth to an Italian movement, "Verismo". The greatest "verista" novelist, Giovanni Verga, was born in Sicily but wrote his most important books in Milan.
In addition to Italian, approximately 2 million people in the Milan metropolitan area can speak the Milanese dialect or one of its Western Lombard variations.
Cuisine.
Like most cities in Italy, Milan has developed his own local culinary tradition, which, as it is typical for North Italian cuisines, uses more frequently rice than pasta, butter than vegetable oil and features almost no tomato or fish. Milanese traditional dishes includes "cotoletta alla milanese," a breaded veal (pork and turkey can be used) cutlet pan-fried in butter (similar to Viennese "Wienerschnitzel"). Other typical dishes are "cassoeula" (stewed pork rib chops and sausage with Savoy cabbage), "ossobuco" (braised veal shank served with a condiment called "gremolata"), risotto alla milanese (with saffron and beef marrow), "busecca" (stewed tripe with beans), and "brasato" (stewed beef or pork with wine and potatoes). Season-related pastries include "chiacchiere" (flat fritters dusted with sugar) and "tortelli" (fried spherical cookies) for Carnival, "colomba" (glazed cake shaped as a dove) for Easter, "pane dei morti" ("Deads' Day bread", cookies aromatized with cinnamon) for All Souls' Day and panettone for Christmas. The "salame Milano", a salami with a very fine grain, is widespread throughout Italy. Renown Milanese cheeses are gorgonzola (from the namesake village nearby), mascarpone, used in pastry-making, taleggio and quartirolo.
Milan is well known for its world-class restaurants and cafés, characterized by innovative cuisine and design. As of 2014, Milan has 157 Michelin-selected places, including three 2-Michelin-starred restaurants; these include Cracco, Sadler and il Luogo di Aimo e Nadia. Many historical restaurants and bars are found in the historic centre, the Brera and Navigli districts. One of the city's oldest surviving cafés, Caffè Cova, was established in 1817. In total, Milan has 15 cafés, bars and restaurants registered among the Historical Places of Italy, continuously operating since at least 70 years.
Sport.
Milan hosted the FIFA World Cup in 1934 and 1990, the UEFA European Football Championship in 1980 and most recently the 2003 World Rowing Championships, the 2009 World Boxing Championships, and some games of the FIVB World Championship in 2010 and the final games of the FIVB World Championship in 2014.
Milan is the only city in Europe that is home to two European Cup/Champions League winning teams - Serie A renewed football clubs A.C. Milan and F.C. Internazionale Milano. Both teams have also won the Intercontinental Cup (now FIFA Club World Cup). With a combined ten Champions League titles, Milan is tied with Madrid as one of the cities that have won more European Cups. They are the most successful clubs in the world of football in terms of international trophies. Both teams play at the UEFA 5-star rated Giuseppe Meazza Stadium, more commonly known as the San Siro, that is one of the biggest stadiums in Europe, with a seating capacity of over 80,000. The Meazza Stadium will host the 2016 UEFA Champions League Final. A third team, Brera Calcio F.C. plays in Seconda Categoria.
There are currently four professional Lega Basket clubs in Milan: Olimpia Milano, Pallacanestro Milano 1958, Società Canottieri Milano and A.S.S.I. Milano. Olimpia Milano won 26 Italian Championships as well as 3 European Champions Cups. The team play at the Mediolanum Forum, with a capacity of 12,000. Milan is also home to Italy's oldest American football team: Rhinos Milano, that won 4 Italian Super Bowls. The team play at the Velodromo Vigorelli, with a capacity of 8,000. Milan has also two cricket teams, Milano Fiori (currently competing in the second division) and Kingsgrove Milan, who won the Serie A championship in 2014. The world famous Monza Formula One circuit is located near the city, inside a suburban park. It is one of the world's oldest car racing circuits. The capacity for the F1 races is currently of over 113,000. It has hosted an F1 race nearly every year since the first year of competition, with the exception of 1980.
Education.
Milan is home to some of Italy's most prominent educational institutions. Milan's higher education system includes 7 universities, 48 faculties and 142 departments, with 185,000 university students in 2011 (approximately 11 percent of the national total) and the largest number of university graduates and postgraduate students (34,000 and more than 5,000, respectively) in Italy.
Founded in 1863, the Politecnico di Milano is the oldest university in Milan. The Politecnico is organized in 16 departments and a network of 9 Schools of engineering, architecture and industrial design spread over 7 campuses in the Lombardy region. The number of students enrolled in all campuses is approximately 38,000, which makes Politecnico the largest technical university in Italy. The University of Milan, founded in 1923, is the largest public teaching and research university in the city, with 9 faculties, 58 departments, 48 institutes and a teaching staff of 2,500 professors. A leading institute in Italy and Europe in scientific publication, the University of Milan is the sixth largest university in Italy, with approximately 60,000 enrolled students.
Other prominent universities in Milan include: the "Università Cattolica del Sacro Cuore", a private institute founded in 1921 and located in the Basilica of Sant'Ambrogio, famous for its law and economics teaching, currently the largest Catholic university in the world with 42,000 enrolled students; the Bocconi University, a private management and finance school established in 1902, ranking as the seventh best business school in Europe; the University of Milan Bicocca, a multidisciplinary public university with more than 30,000 enrolled students; the IULM University of Milan, specializing in marketing, information and communications technology, tourism and fashion; the Università Vita Salute San Raffaele, linked to the San Raffaele hospital, is home to research laboratories in neurology, neurosurgery, diabetology, molecular biology, AIDS studies and cognitive science.
Milan is also well known for its fine arts and music schools. The Milan Academy of Fine Arts (Brera Academy) is a public academic institution founded in 1776 by Empress Maria Theresa of Austria; the New Academy of Fine Arts is the largest private art and design university in Italy; the European Institute of Design is a private university specialized in fashion, industrial and interior design, audio/visual design including photography, advertising and marketing and business communication; the Marangoni Institute, is a fashion institute with campuses in Milan, London, and Paris; the Domus Academy is a private postgraduate institution of design, fashion, architecture, interior design and management; the Pontifical Ambrosian Institute of Sacred Music, a college of music founded in 1931 by the blessed cardinal A.I. Schuster, archbishop of Milan, and raised according to the rules by the Holy See in 1940, is - similarly to the Pontifical Institute of Sacred Music in Rome, which is consociated with - an Institute "ad instar facultatis" and is authorized to confer university qualifications with canonical validity and the Milan Conservatory, a college of music established in 1807, currently Italy's largest with more than 1,700 students and 240 music teachers.
Transport.
Milan is one of southern Europe's key transport nodes and one of Italy's most important railway hubs. Its five major railway stations, among which the Milan Central station, are among Italy's busiest. Since the end of 2009, two high speed train lines link Milan to Rome, Naples and Turin, considerably shortening travel times with other major cities in Italy.
The Azienda Trasporti Milanesi (ATM) operates within the metropolitan area, managing a public transport network consisting of an underground rapid transit network and tram, trolley-bus and bus lines. Overall the network covers nearly 1400 km reaching 86 municipalities. Besides public transport, ATM manages the interchange parking lots and other transportation services including bike sharing and car sharing systems.
Milan Metro is the rapid transit system serving the city, with 4 lines and a total length of more than 90 km. The recently opened M5 line is undergoing further expansion and the construction of the M4 line has been approved. The Milan suburban railway service comprises 10 lines and connects the metropolitan area with the city centre through the Milan Passerby underground railway. Commonly referred to as "Il Passante", it has a train running every 6 minutes (and in the city functions as a subway line with full transferability to the Milan Metro).
The city tram network consists of approximately 160 km of track and 17 lines. Bus lines cover over 1070 km. Milan has also taxi services operated by private companies and licensed by the City council of Milan. The city is also a key node for the national road network, being served by all the major highways of Northern Italy.
Milan is served by three international airports. Malpensa International Airport, the second busiest in Italy (about 19 million passengers in 2010), is 45 km from central Milan and connected to the city by the "Malpensa Express" railway service. Linate Airport, which lies within the city limits, is mainly used for domestic and short-haul international flights, and served over 9 million passengers in 2010. The airport of Orio al Serio, near the city of Bergamo, serves the low-cost traffic of Milan (8 million passengers in 2010). Milano Bresso Airport, operated by Aero Club Milano, is a general aviation airport.
International relations.
Twin towns – Sister cities.
Milan has fifteen official sister cities as reported on the city's website. The date column indicates the year in which the relationship was established. São Paulo was Milan's first sister city.
The partnership with the city of St. Petersburg, Russia, that started in 1967, was suspended in 2012 (a decision taken by the city of Milan), because of the prohibition of the Russian government on "homosexual propaganda".
Other forms of cooperation, partnership and city friendship.
Milan has the following collaborations:
References.
Bibliography.
</dl>

</doc>
<doc id="36610" url="http://en.wikipedia.org/wiki?curid=36610" title="1097">
1097

Year 1097 (MXCVII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="36645" url="http://en.wikipedia.org/wiki?curid=36645" title="Bob Welch">
Bob Welch

Bob Welch may refer to:

</doc>
<doc id="36646" url="http://en.wikipedia.org/wiki?curid=36646" title="Lindsey Buckingham">
Lindsey Buckingham

Lindsey Adams Buckingham (born October 3, 1949) is an American musician, singer and songwriter, best known as guitarist and male vocalist of the musical group Fleetwood Mac from 1975 to 1987, and 1997 to the present day. Aside from his tenure with Fleetwood Mac, Buckingham has also released six solo albums and three live albums. As a member of Fleetwood Mac, he was inducted into the Rock and Roll Hall of Fame in 1998. In 2011, Buckingham was ranked 100th in "Rolling Stone " magazine's 2011 list of "The 100 Greatest Guitarists of All Time". In Fleetwood Mac's heyday, Buckingham was known for his fingerpicking guitar style and wide vocal range as well as the sometimes tense chemistry between himself and former girlfriend and member Stevie Nicks.
Early years.
Born in Palo Alto, California, Buckingham was the third and youngest child of Rutheda (née Elliott) and Morris Buckingham. He had two older brothers, Jeff and Greg. Growing up in the San Francisco Bay Area community of Atherton, Buckingham and his brothers were encouraged to swim competitively. Though Buckingham dropped out of athletics to pursue music, his brother Greg went on to win a silver medal at the 1968 Olympics in Mexico City. Buckingham attended San Jose State University but did not graduate.
Buckingham's first forays into guitar playing took place on a toy Mickey Mouse guitar, playing along to his brother Jeff's extensive collection of 45s. Noticing his talent, Buckingham's parents bought their son a $35 Harmony guitar.
Buckingham never took guitar lessons and does not read music. By age 13, he became interested in folk music and, influenced by banjo methods, practiced the energetic style of The Kingston Trio. From 1966-1971, Buckingham performed psychedelic and folk rock with the band Fritz in the San Francisco Bay as a bassist and vocalist. Shortly after joining Fritz, Buckingham invited friend Nicks to join Fritz as a second vocalist. Their romantic relationship would begin after both left Fritz five years later.
Buckingham Nicks.
Buckingham and his then-girlfriend Stevie Nicks recorded seven demos in 1972 on a 4-track recorder, and drove to Los Angeles to pursue a record deal. In 1973, Polydor Records signed the pair. Their album, produced by Keith Olsen and second engineer Richard Dashut, "Buckingham Nicks", was released in September 1973; soon after its release, however, Polydor dropped the duo because of poor sales.
Buckingham and Nicks began a short tour promoting the "Buckingham Nicks" album, shortly after joining Fleetwood Mac. The touring band included drummers Bob Aguirre (from Fritz) and Gary Hodges playing simultaneously and bassist Tom Moncrieff, who later played bass on Stevie Nicks' 1981 album "Bella Donna". Bootlegs of two concerts in Birmingham and Tuscaloosa exist and are widely distributed on peer-to-peer networks and fansites. 
To help make ends meet, Buckingham toured with Don Everly's backing band, singing Phil Everly's parts.
"Buckingham Nicks" has never been released on CD (a bootleg version does exist), although both Buckingham and Nicks have hinted at a possible remix and re-release on CD in the near future. Buckingham has also suggested a tour in support of the collection could be something the two may be interested in. Moncrieff and Hodges from the original Buckingham Nicks touring band have also expressed interest.
Fleetwood Mac.
While investigating Sound City recording studio in California, Mick Fleetwood heard the song "Frozen Love" from the "Buckingham Nicks" album. Impressed, he asked who the guitarist was. By chance, Buckingham and Nicks were also in Sound City recording demos, and Buckingham and Fleetwood were introduced. When Bob Welch left Fleetwood Mac in December 1974, Fleetwood immediately contacted Buckingham and offered him the vacant guitar slot in his band. Buckingham told Fleetwood that he and Nicks were a team and that he didn't want to work without her. Fleetwood agreed to hire both of them, without an audition.
Fleetwood Mac released their eponymously titled album in 1975, which reached number one in the American charts. However it was the second album of this new line-up, "Rumours", that propelled the band to superstar status, when it became one of the best-selling albums of all time. Buckingham's "Go Your Own Way" was the lead-off single, soaring into the US Top Ten. After the resounding commercial success of "Rumours" (during the making of which Buckingham and Nicks famously split), Buckingham was determined to avoid falling into repeating the same musical pattern. The result was "Tusk" (1979), a double album that Buckingham primarily directed. Once again, Buckingham wrote the lead-off single, the title track that would peak at #8 on Billboard's Hot 100. It was during this time that Buckingham moved in with record company secretary and aspiring model, Carol Ann Harris, with whom he lived until 1984. Though by most standards a hit, "Tusk" failed to come close to "Rumours" record sales, and the album was followed by a hiatus in the band's studio recording efforts.
After a large world tour that ended in 1980, Fleetwood Mac took a year-long break before reconvening to record their next album "Mirage", a more pop-friendly work that returned the band to the top of the US album chart. However, by this time various members of the band were enjoying success as solo artists (particularly Nicks) and it would be five years before the release of the next Fleetwood Mac album. By the time "Tango in the Night" was released in 1987, Buckingham had already released two solo albums and had given up much of the material for what would have been his third solo album for the project, including "Big Love", "Tango in the Night", "Family Man", "You and I" and "Caroline". On several of these tracks Buckingham played every instrument . "Big Love", released as the first single from the album, became a top ten hit in the US and the UK.
Propelled by a string of hit singles, "Tango in the Night" became the band's biggest album since "Rumours" a decade earlier. However, following its release, Buckingham left Fleetwood Mac largely because of his desire not to tour and the strain he was feeling within the band. "I needed to get some separation from Stevie especially, because I don't think I'd ever quite gotten closure on our relationship," he said. "I needed to get on with the next phase of my creative growth and my emotional growth. When you break up with someone and then for the next 10 years you have to be around them and do for them and watch them move away from you, it's not easy." Fleetwood Mac continued without him, and Buckingham was replaced by guitarists Rick Vito and Billy Burnette.
Solo projects.
During the time he worked on "Tusk", Buckingham also produced albums for Walter Egan and John Stewart in the late 1970s as well as beginning work on his own solo album.
In 1981, Buckingham released his first solo album "Law and Order", playing nearly every instrument and featuring guest appearances by bandmates Mick Fleetwood and Christine McVie. The album pursued the quirky, eclectic, often lo-fi and new-wave-influences of "Tusk" and spawned the hit single "Trouble" (inspired by Richard Dashut), which reached #9 on the Hot 100 and #1 in Australia (for three weeks). Two years later he wrote and performed the songs "Holiday Road" and "Dancin' Across the U. S. A." for the film "National Lampoon's Vacation". "Holiday Road" was released as a single, and reached #82 on the "Billboard"'s Hot 100. He did other soundtrack work, including the song "Time Bomb Town" from "Back to the Future" (1985). Buckingham played all of the instruments on the track except drums, which were played by Michael Huey.
In 1984, after ending his 7-year relationship with Carol Ann Harris, he released his second solo album, "Go Insane". The title track was a modest hit, reaching #23 on the Hot 100. In 2008 he revealed the title track was about his post break-up relationship with Stevie Nicks. The last track of the album, D. W. Suite, was a tribute to the late Beach Boys drummer Dennis Wilson. The next year, Buckingham performed on USA for Africa's fundraising single, "We Are the World".
Following his split with Fleetwood Mac in 1987, Buckingham spent much of the next five years in the studio, working on his third solo album, "Out of the Cradle", which was released in 1992. Many of the songs deal with his relationship with Nicks and his decision to leave the band. "There were things lingering for years having to do with relationships and the band, hurtful things, that were impossible to deal with until I left. If you were in a relationship and split up, then had to see that person every day for the next 15 years, it might keep you from dealing with some of those things. While we made ′Rumours′ (in 1977) there were two couples breaking up in the band(Buckingham and Nicks, and John and Christine McVie), and we had to say, ′This is an important thing we′re doing, so we've got to put this set of feelings on this side of the room and get on with it.' And when you do that long enough you forget that those feelings are even there. On this album, I'm putting all these feelings in the healthiest possible perspective and that, looking at it broadly, is a lot of what the album is dealing with. It′s a catharsis, absolutely." "Wrong" was a gentle rebuke of former bandmate Mick Fleetwood's tell-all biography. "Out of the Cradle" received some favorable reviews but did not achieve the sales levels associated with Fleetwood Mac. However, Buckingham toured throughout 1992–93 for the first time as a solo artist; his band included an army of seven other guitarists (Buckingham himself calls them "the crazy band" on his "Soundstage" DVD), each of whom he individually taught the entire two-and-a-half hours of music from the concert ("Lindsey Buckingham: Behind the Music" documentary for VH-1, 2001).
A subsequent solo album, entitled "Gift of Screws", was recorded between 1995–2001 and presented to Warner Bros./Reprise for release. Executives at the label managed to persuade Buckingham to hold the album back and instead take several tracks from "Gift of Screws" and re-record them with Fleetwood Mac. Thus, seven songs from "Gift of Screws" appear on the Fleetwood Mac album "Say You Will", in substantially the same form as Buckingham had recorded them for his solo release. Excellent bootleg copies of "Gift of Screws"—taken from an original CD-R presented to Warner Bros/Reprise—are known to exist and have been widely distributed among fans through the use of torrent sites and other peer-to-peer networks.
On his 57th birthday, (October 3, 2006) Buckingham's fourth solo album, an acoustic album entitled "Under the Skin", was released. "Under The Skin" features Buckingham on almost all instruments, with the exception of two tracks that feature Fleetwood Mac's rhythm section of John McVie and Mick Fleetwood. The album includes a cover of The Rolling Stones classic "I Am Waiting". Three days after the album's release, Buckingham embarked on a tour in support the album that lasted until the end of June 2007. A live album and DVD, "Live at the Bass Performance Hall", was released documenting the Fort Worth, Texas show from this tour.
In 2008 the "Gift of Screws" album was finally released, containing three tracks from the originally planned album, as well as seven new recordings. Buckingham then commenced a short tour to promote "Gift of Screws" in September and October, opening in Saratoga, California and closing in New York City.
On November 3, 2010, Buckingham's website announced that he was working on an untitled album with release planned in early 2011. Buckingham had finished recording the album, titled "Seeds We Sow" in April, and on April 22, 2011, he filmed a concert for DVD release to support the album. "Seeds We Sow" was released on September 6, 2011. On September 10, Buckingham kicked off the "Seeds We Sow" Tour in Reno, Nevada; the tour ended in Tulsa, Oklahoma, on November 14. Buckingham had planned to conduct his first solo tour of the United Kingdom and Ireland in December. However, in early December, Buckingham postponed all UK dates due to his guitarist suffering a back injury. The UK dates were subsequently cancelled.
Buckingham began a "solo" (no backing band) tour of the United States on May 3, 2012, in Solana Beach, California. and in November 2012 released a completely solo live album "One Man Show" via download at iTunes that was recorded from a single night in Des Moines, Iowa. "One Man Show" was released on Buckingham's own label Buckingham Records LLC.
Rejoining Fleetwood Mac.
In 1992, newly elected president Bill Clinton asked Fleetwood Mac to come together to perform the song he had chosen for his campaign, "Don't Stop", at his inaugural ceremony. Buckingham agreed to be part of the performance, but the experience was something of a one-off for the band, who were still very much at odds with one another and had no plans to reunite officially.
While assembling material for a planned fourth solo album in the mid-1990s, Buckingham contacted Mick Fleetwood for assistance on a song. Their collaboration lasted much longer than anticipated, and the two eventually decided to call upon Stevie Nicks, John and Christine McVie. The band's old chemistry was clearly still there, and plans for a reunion tour were soon in the works. In 1997, Buckingham and all four of his bandmates from the "Rumours"-era line-up of Fleetwood Mac went on the road for the first time together since 1982 in a reunion tour titled "The Dance". The tour was hugely successful and did much to heal the damage that had been done between Buckingham and his bandmates. However, Christine McVie left the band in 1998, now making the band a foursome. In 2003, the reformed band released the first studio album involving Buckingham and Nicks in 15 years, "Say You Will". Buckingham's song "Peacekeeper" was the first single from the album, and the band went on a world concert tour that would last almost a year and a half.
The band toured in 2009, with the first date of the "UNLEASHED" Tour as March 1, 2009, in Mellon Arena (Pittsburgh). Christine McVie was not involved with this project. As of 2013, Fleetwood Mac is again touring as a four-piece band throughout North America, Europe, and the UK; the "Live World" tour commenced on April 4, 2013, in Columbus, Ohio. On April 30, the band released their first new studio material since 2003's "Say You Will" via digital download on iTunes with the four-track EP containing three new songs from Buckingham and one new song from the "Buckingham Nicks" sessions ("Without You").
Christine McVie announced that she would rejoin Fleetwood Mac and the five-piece band would return to the studio and record new tracks for a possible 2015 studio album, in addition Christine would join Lindsey and the rest of the band on the "On With The Show Tour" that kicks off in Minneapolis, Minnesota on 30 September 2014.
Musical style.
Unlike most rock guitarists, Buckingham does not play with a pick; instead, he picks the strings with his fingers and fingernails. Initially after joining Fleetwood Mac, Buckingham used a Gibson Les Paul. Before the band, a Fender Telecaster was his main guitar, and was used on his first Fleetwood Mac album. In 1979, he worked with Rick Turner, owner of Renaissance Guitars to create the Model One. He has used it extensively since, both with Fleetwood Mac and for his solo efforts. He uses a Taylor Guitar 814ce for most of his acoustic performances and has also used an Ovation Celebrity in the past.
In the 80s, he also extensively used the Fairlight CMI.
His influences include The Beach Boys and The Kingston Trio.
In an interview with "Guitar World Acoustic Magazine", Buckingham said:
I've always believed that you play to highlight the song, not to highlight the player. The song is all that matters. There are two ways you can choose to go. You can try to be someone like Eddie Van Halen, who is a great guitarist, a virtuoso. Yet he doesn't make good records because what he plays is totally lost in the context of this band's music. Then there are guitar players like Chet Atkins, who weren't out there trying to show themselves off as guitarists per se, but were using the guitar as a tool to make good records. I remember loving Chet's work when I was a kid, but it was only later, when I really listened to his guitar parts, that I realized how much they were a part of the song's fabric, and how much you'd be going 'Oh, that song just isn't working' if they weren't there.
And in another interview to "Guitar World", he said about using his fingers rather than a plectrum:
Well, it's not really a choice at all. It's just, you know, I started playing very young and from early on, the people I was listening to had some element of finger style. Probably the first guitarist I was emulating was Scotty Moore, when I was maybe 6 or 7. And he played with a pick, but he also used fingers. And a lot of the session players, like Chet Atkins, they played with fingers or a pick. Then I listened to a certain amount of light classical guitar playing. And of course later on, when the first wave of rock 'n' roll kind of fell away, folk music was very popular and very influential in my style. So it was really less of a choice than what I fell into. I use a pick occasionally. I certainly use it more in the studio when you want to get a certain tone. But it's just the way I came up. I wasn't taught. I just sort of figured things out on my own terms. I guess that was one of the ways that I became comfortable and it just kind of set in.
Personal life.
On July 8, 1998, Buckingham's girlfriend, Kristen Messner, gave birth to their son, William Gregory Buckingham. Buckingham and Messner subsequently married in 2000, when Messner was 30 and Buckingham was 51; she gave birth to a daughter, Leelee, the same year. Their third child, Stella, was born on April 20, 2004. The song "It Was You" from his "Under the Skin" album pays homage to all three children by using their names.
In popular culture.
Buckingham has been portrayed by Bill Hader in a recurring sketch titled "What Up with That" on NBC's "Saturday Night Live". He appeared as himself on the May 14, 2011 episode during this sketch.

</doc>
<doc id="36647" url="http://en.wikipedia.org/wiki?curid=36647" title="Christine McVie">
Christine McVie

Christine Anne Perfect (born 12 July 1943), professionally known as Christine McVie, is an English singer-songwriter and keyboardist. Her fame came as a member of rock band Fleetwood Mac, joining the band in 1970 while married to bassist John McVie. Eight songs she had written and sung are on the band's "Greatest Hits" album, including "Don't Stop", "Little Lies", "Everywhere", "Over My Head", and "You Make Loving Fun". She has also released three solo albums. AllMusic critic Steve Leggett noted McVie's "naturally smoky low alto vocal style", describing her as an "Unabashedly easy-on-the-ears singer/songwriter, and the prime mover behind some of Fleetwood Mac's biggest hits."
In 1998, as a member of Fleetwood Mac, McVie was inducted into the Rock and Roll Hall of Fame and received the Brit Award for Outstanding Contribution to Music. Since retiring from the band, she has worked on solo material in her converted barn at her home in Wickhambreaux in Kent. McVie appeared on stage with Fleetwood Mac at London's O2 Arena in September 2013, and rejoined the band in January 2014. Her first full shows since her return came during Fleetwood Mac's On with the Show tour in October 2014. In 2014 she received the British Academy's Ivor Novello Award for Lifetime Achievement.
Early life.
McVie was born in the small Lake District village of Bouth, England (then in Lancashire, now Cumbria) and grew up in the Bearwood area of Smethwick near Birmingham, where her father, Cyril P.A. Perfect, was a concert violinist and music lecturer at St Peter's College of Education, Saltley, Birmingham and taught violin at St Philip's Grammar School, Birmingham. McVie's mother Beatrice E.M. (called Tee) née Reece, claimed to be a medium, psychic, and faith healer. McVie's grandfather had been an organist at Westminster Abbey.
Although McVie was introduced to the piano at age four, it was not until age 11 that she studied music seriously, when she was re-introduced to piano by Philip Fisher, a local musician and school friend of McVie's older brother, John. Continuing her classical training until age 15, McVie radically shifted her musical focus to rock & roll when John brought home a Fats Domino songbook. Other early influences included The Everly Brothers.
Early music.
McVie studied sculpture at an art college in Birmingham for five years, with the goal of becoming an art teacher. During that time she met a number of budding musicians in Britain's blues scene. Her first foray into the music field didn't come until she met two friends, Stan Webb and Andy Silvester in a pub one night. At the time, they were playing in a band called "Sounds Of Blue" which had a few dates booked, but no bass guitarist. Knowing that McVie had musical talent, they asked her to join. Also, during that time, she often sang with Spencer Davis. After five years, McVie graduated from art college with a teaching degree, but by that time "Sounds of Blue" had split up.
Fresh out of art college, McVie found she did not have enough money to launch herself into the art world, so she moved to London, where she worked briefly as a department store window dresser.
Chicken Shack.
In 1967 McVie learned that her ex-band mates, Andy Silvester and Stan Webb, were forming a blues band, Chicken Shack, and were looking for a pianist. She wrote to them asking to join, and they invited her to play keyboards/piano and to sing background vocals. Chicken Shack's debut release was "It's Okay With Me Baby", written by and featuring McVie. She stayed with Chicken Shack for two albums, during which her genuine feel for the blues became evident, not only in her Sonny Thompson-style piano playing, but through her authentic "bluesy" voice. Chicken Shack had a hit with "I'd Rather Go Blind", which featured McVie on lead vocals. Perfect received a Melody Maker award for female vocalist in both 1969 and 1970. McVie left Chicken Shack in 1969 after marrying Fleetwood Mac bassist John McVie a year earlier.
Fleetwood Mac.
McVie was a fan of Fleetwood Mac at the time (continuing from 'Chicken Shack' under "Early music" above); and while touring with Chicken Shack, the two bands often would meet. They also were "label mates" at Blue Horizon, and Fleetwood Mac had asked McVie to play piano as a session musician for Peter Green's songs on the band's second album, "Mr. Wonderful".
Encouraged to continue her career, McVie recorded a solo album, "Christine Perfect"; following her success as a member of Fleetwood Mac, the album was reissued under the name "The Legendary Christine Perfect Album". After marrying Fleetwood Mac bassist John McVie, McVie joined Fleetwood Mac in 1970. She had already contributed backup vocals and painted the cover for "Kiln House". The band had just lost founding member Peter Green, and its members were nervous about touring without him. McVie had been a huge fan of the Peter Green-era Fleetwood Mac; and since she knew all the lyrics to their songs, she went along.
McVie went on to become an integral member of the group and the first album with her as a full-fledged band member was Future Games, recorded at London's Advision Studios and also the first with American-born member Bob Welch in place of founding member Jeremy Spencer. Danny Kirwan was still in the band at this point, but he was fired in 1972 after an incident on tour where he smashed his guitar prior to a gig after a row with Welch.
The early 1970s was a rocky time for the band, with a revolving door of musicians; and only the albums "Bare Trees" and "Mystery to Me", scored any successes. Furthermore, a group impersonating Fleetwood Mac (which later became Stretch) was touring the United States with encouragement from the band's manager, Clifford Davis. The tour collapsed, but it led to a protracted lawsuit between Davis and Fleetwood Mac (or Stretch?).
In 1974 McVie reluctantly agreed to move with the rest of Fleetwood Mac to the U.S. and make a fresh start. Within a year Stevie Nicks and Lindsey Buckingham of Buckingham Nicks joined the band, giving it an added dimension. Their first album together, 1975's "Fleetwood Mac", had several hit songs, with McVie's "Over My Head" and "Say You Love Me", both reaching Billboard's top-20 singles chart. It was "Over My Head" which first put Fleetwood Mac on American radio and into the national Top 20.
In 1976 McVie began an on-the-road affair with the band's lighting director, which inspired her to write "You Make Loving Fun", a top-10 hit on the landmark smash "Rumours", one of the best-selling albums of all-time. Her biggest hit was "Don't Stop", which climbed all the way to number three. The "Rumours" tour also included McVie's "Songbird", a ballad played as the encore of many Fleetwood Mac concerts.
By the end of the "Rumours" tour, the McVies were divorced. The 1979 double album "Tusk" produced three more US top-20 hits ("Tusk", which is also the band's first "conceptual" music video, "Sara", and Christine's "Think About Me"), but it came nowhere near to matching the success of the "Rumours" album. The "Tusk" tour continued into 1980 after which the band took time apart. They reunited in 1981 to record the album "Mirage" at the Château d'Hérouville's studio in France. The album, released in 1982, returned the band to the top of the US charts and also contained the top-5 hit "Hold Me", co-written by McVie. McVie's inspiration for the song was her tortured relationship with Beach Boys drummer Dennis Wilson. Her song, "Love in Store", became the third single from the album peaking at #22 in early 1983.
In 1984 McVie recorded another solo album. She created hits with the songs "Got a Hold on Me" (Top 10 pop and #1 adult contemporary) and "Love Will Show Us How" (#30). A Third single, "I'm The One", was released but did not chart. McVie is quoted in "The Billboard Book of Number One Adult Contemporary Hits" as saying of her solo album, "Maybe it isn't the most adventurous album in the world, but I wanted to be honest and please my own ears with it."
McVie also met keyboardist Eddy Quintela (12 years her junior), whom she married on 18 October 1986. Quintela went on to co-write many songs with her that were featured on subsequent Fleetwood Mac albums. The couple divorced in the mid-1990s. She rejoined Fleetwood Mac to record the "Tango in the Night" album, which went on to become the band's biggest success since "Rumours" 10 years earlier. The biggest hit from the album, which was top 5 in both the UK and U.S., was McVie's "Little Lies", co-written with her husband Quintela. Another McVie single from the album, "Everywhere", reached #4 in the UK, which would be the band's third highest ever chart peak there and their final top 40 UK hit to date (the single peaked at #14 in the U.S.).
In 1990 the band (now without Lindsey Buckingham) recorded "Behind the Mask", but the album only reached Gold status in the U.S., and only McVie's song "Save Me" made the U.S. Top 40. The album did, however, enter the UK album chart at #1 and reached Platinum status there. The second US single release from the album, McVie's "Skies the Limit" did not make the top
100, but did chart the A/C at number 10. McVie had always been reluctant to go on concert tours, preferring to stay close to home with friends and family. Upon the death of her father, Cyril Perfect, while she was touring for "Behind the Mask", McVie made the decision to retire from touring altogether. Despite the departure of Stevie Nicks, McVie remained loyal to Mick Fleetwood and her former husband John McVie, writing and recording a new track ("Love Shines") for the 1992 boxed set "25 Years - The Chain", and five songs for the band's 1995 album "Time".
The members of the band seemed to have gone their separate ways until Mick Fleetwood, John McVie, and Lindsey Buckingham got together again for one of Buckingham's solo projects. Christine was soon asked to sing and play on some of the tracks. The four of them decided a full reunion was possible and Stevie Nicks was called back into the fold and the resulting live album, 1997's "The Dance", went to #1 on the U.S. album charts. Despite her reservations, McVie complied with the band's touring schedule, and then performed for the group's 1998 induction into the Rock and Roll Hall of Fame, as well as the Grammy Awards show, and the BRIT Awards in the UK. McVie later revealed (in a 2014 "Rolling Stone" interview) that she had developed a phobia about flying, which was later treated with psychotherapy. This phobia was the reason she decided not to continue with Fleetwood Mac after 1998.
In 2006 "Paste" magazine named McVie, together with bandmates Lindsey Buckingham and Stevie Nicks, as the 83rd greatest living songwriter or songwriting team.
Hiatus from Fleetwood Mac (1998-2014).
After "The Dance", McVie returned to England to be near her family and stepped out of public view until 2000 when she appeared in public to receive an Honorary Doctorate in music from the University of Greenwich. Sometime after leaving Fleetwood Mac, she and Quintela divorced.
In a 2004 interview, McVie admitted to not listening much to pop music anymore and stated instead a preference for Classic FM. In December 2003, McVie went to see Fleetwood Mac's last UK performance on the "Say You Will" tour in London, but did not join her former bandmates on the stage. Mid-2004 saw the release of McVie's new solo album, "In the Meantime", her third in a career spanning five decades. Recording in her converted barn in Kent, she worked on the project with her nephew, Dan Perfect, who contributed guitar-playing, backing vocals, and songwriting. No tour was organized to promote this album; instead McVie conducted several press interviews in both Britain and America.
In 2006, McVie was awarded the British Academy of Songwriters, Composers and Authors' Gold Badge of Merit at a ceremony held in London's Savoy Hotel. In November 2009, McVie went to see Fleetwood Mac's last UK performance on their "Unleashed" tour in London, but did not join her former band mates on the stage. During the announcement of Fleetwood Mac's 2012 world tour, Stevie Nicks downplayed the likelihood of McVie ever rejoining the group: "She went to England and she has never been back since 1998 [...] as much as we would all like to think that she'll just change her mind one day, I don't think it'll happen [...] We love her, so we had to let her go."
Return to Fleetwood Mac (2014–present).
In 2013, McVie appeared on stage in Maui, Hawaii performing with the Mick Fleetwood Blues Band which included Mick Fleetwood and ex-Fleetwood Mac guitarist Rick Vito. This was her first appearance on stage in 15 years. Later in September, Christine joined Fleetwood Mac on stage for the first time in 15 years to play 'Don't Stop' at The O2 Arena, London. She played on two dates and her appearance on stage was received with rapturous applause.
On 11 January 2014, Mick Fleetwood announced during a concert he performed in Maui that McVie would be rejoining the band, and it was officially announced two days later that she had rejoined. The band's most popular lineup (Lindsey Buckingham, Mick Fleetwood, Christine McVie, John McVie and Stevie Nicks) performed together for the first time since 1998 in its On with the Show tour beginning in Minneapolis at Target Center on 30 September 2014.
Collaborations.
McVie sang with Dennis Wilson on his song "Love Surrounds Me" for The Beach Boys' 1979 album "L.A. (Light Album)". She also sang with Christopher Cross on the song "Never Stop Believing", on his 1988 album "Back of My Mind".

</doc>
<doc id="36670" url="http://en.wikipedia.org/wiki?curid=36670" title="Soma cube">
Soma cube

The Soma cube is a solid dissection puzzle invented by Piet Hein in 1933 during a lecture on quantum mechanics conducted by Werner Heisenberg. Seven pieces made out of unit cubes must be assembled into a 3x3x3 cube. The pieces can also be used to make a variety of other 3D shapes.
The pieces of the Soma cube consist of all possible combinations of three or four unit cubes, joined at their faces, such that at least one inside corner is formed. There is one combination of three cubes that satisfies this condition, and six combinations of four cubes that satisfy this condition, of which two are mirror images of each other (see Chirality). Thus, 3 + (6 x 4) is 27 which is exactly the number of cells in a 3 x 3 x 3 cube.
The Soma cube has been discussed in detail by Martin Gardner and John Horton Conway, and the book "Winning Ways for your Mathematical Plays" contains a detailed analysis of the Soma cube problem. There are 240 distinct solutions of the Soma cube puzzle, excluding rotations and reflections: these are easily generated by a simple recursive backtracking search computer program similar to that used for the eight queens puzzle.
The seven Soma pieces are six polycubes of order four and one of order three: 
Trivia.
Similar to Soma cube is the 3D pentomino puzzle, which can fill boxes of 2×3×10, 2×5×6 and 3×4×5 units.
Piet Hein authorized a finely crafted rosewood version of the Soma cube manufactured by Theodor Skjøde Knudsen's company Skjøde Skjern (of Denmark). Beginning in about 1967, it was marketed in the U.S. for several years by the game manufacturer Parker Brothers. Plastic Soma cube sets were also commercially produced by Parker Brothers in several colors (blue, red, and orange) during the 1970s.
Solving the Soma cube has been used by as a task to measure individuals' performance and effort in a series of psychology experiments. In these experiments, test subjects are asked to solve a soma cube as many times as possible within a set period of time. For example, In 1969, Edward Deci, a Carnegie Mellon University graduate assistant at the time, asked his research subjects to solve a soma cube under conditions with varying incentives in his dissertation work on intrinsic and extrinsic motivation establishing the social psychological theory of crowding out.
In each of the 240 solutions to the cube puzzle, there is only one place that the "T" piece can be placed. Each solved cube can be rotated such that the "T" piece is on the bottom with its long edge along the front and the "tongue" of the "T" in the bottom center cube (this is the normalized position of the large cube). This can be proven as follows: If you consider all the possible ways that the "T" piece can be placed in the large cube (without regard to any of the other pieces), it will be seen that it will always fill either two corners of the large cube or zero corners. There is no way to orient the "T" piece such that it fills only one corner of the large cube. The "L" piece can be oriented such that it fills two corners, or one corner, or zero corners. Each of the other five pieces have no orientation that fills two corners; they can fill either one corner or zero corners. Therefore, if you exclude the "T" piece, the maximum number of corners that can be filled by the remaining six pieces is seven (one corner each for five pieces, plus two corners for the "L" piece). A cube has eight corners. But the "T" piece cannot be oriented to fill just that one remaining corner, and orienting it such that it fills zero corners will obviously not make a cube. Therefore, the "T" must always fill two corners, and there is only one orientation (discounting rotations and reflections) in which it does that. It also follows from this that in all solutions, five of the remaining six pieces will fill their maximum number of corners and one piece will fill one fewer than its maximum (this is called the deficient piece).

</doc>
<doc id="36673" url="http://en.wikipedia.org/wiki?curid=36673" title="Lhotshampa">
Lhotshampa

Lhotshampa or Lhotsampa (Nepali: ल्होत्साम्पा; Tibetan: ལྷོ་མཚམས་པ་, Wylie: "lho-mtshams-pa"
) means "southerners" in Dzongkha, the national language of Bhutan. The term refers to the heterogeneous ethnic Nepalese population of Bhutan.
History.
The first small groups of Nepalese emigrated primarily from eastern Nepal under Indian auspices in the late nineteenth and early twentieth centuries. 
The beginning of Nepalese immigration largely coincided with Bhutan's political development: in 1885, Druk Gyalpo Ugyen Wangchuck consolidated power after a period of civil unrest and cultivated closer ties with the British in India.
In 1910, the government of Bhutan signed a treaty with the British in India, granting them control over Bhutan's foreign relations. 
Immigrants from Nepal and India continued to enter Bhutan with a spurt from the 1960s when Bhutan's first modern 5-year plan began, many arriving as construction workers.
By the late 1980s, the Bhutanese government estimated 28 percent of the Bhutanese population were of Nepalese origin. Unofficial estimates of the ethnic Nepalese population ran as high as 30 to 40 percent, constituting a majority in the south. The number of legal permanent Nepalese residents in the late 1980s may have been as few as 15 percent of the total population.
The government traditionally attempted to limit immigration and restrict residence and employment of Nepalese to the southern region. Liberalization measures in the 1970s and 1980s encouraged intermarriage and provided increasing opportunities for public service. The government allowed more internal migration by Nepalese seeking better education and business opportunities. However, the most divisive issue in Bhutan in the 1980s and early 1990s was the accommodation of the Nepalese Hindu minority. 
In 1988, the government census led to the branding of many ethnic Nepalis as illegal immigrants. Local Lhotshampa leaders responded with anti-government rallies demanding citizenship and attacks against government institutions. 
In 1989, the Bhutanese government enacted reforms that directly impacted the Lhotshampa. First, it elevated the status of the national dress code of the Driglam namzha from recommended to mandatory. All citizens including the Lhotshampa were required to observe the dress code in public during business hours. This decree was resented by the Lhotshampa who voiced complaints about being forced to wear the clothing of the Ngalong majority. Second, the government removed Nepali as a language of instruction in schools, in favor of Dzongkha, the national language. This alienated the Lhotshampa, many of whom knew no Dzongkha at all.
Expulsion.
Since the late 1980s, over 100,000 Lhotshampa have been forced out of Bhutan, accused by the government of being illegal aliens. Between 1988-1993, thousands of others left alleging ethnic and political repression. In 1990, violent ethnic unrest and anti-government protests in southern Bhutan pressing for greater democracy and respect for minority rights. That year, the Bhutan Peoples' Party, whose members are mostly Lhotshampa, began a campaign of violence against the Bhutanese government. In the wake of this unrest, thousands fled Bhutan. Many of them have either entered Nepal's seven refugee camps (on 20 January 2010, 85,544 refugees resided in the camps) or are working in India. According to U.S. State Department estimates, about 35% of the population of Bhutan is Lhotshampa if the displaced refugees are counted as citizens.
Culture.
Traditionally, the Lhotshampa have been involved mostly in sedentary agriculture, although some have cleared forest cover and conducted "tsheri" and slash and burn agriculture. The Lhotshampa are generally classified as Hindus. However, this is an oversimplification as many groups that include Tamang and the Gurung are largely Buddhist; the Kiranti groups that include the Rai and Limbu are largely animist followers of Mundhum (these latter groups are mainly found in eastern Bhutan). Whether they are Hindu or Tibetan Buddhist, most of them abstain from beef, notably those belonging to the orthodox classes who are vegetarians. Their main festivals include Dashain and Tihar, a festival superficially similar to the Indian Diwali.
The oversimplification also carries over into "when" Lhotshampa immigrated into Bhutan. The government had accepted all those citizens of Nepalese origin who were there prior to 1958. However, settlers continued to enter Bhutan with a spurt from the 1960s when Bhutan's first modern five-year plan began. These numbers swelled significantly and later led to a crackdown by the government.

</doc>
<doc id="36679" url="http://en.wikipedia.org/wiki?curid=36679" title="Milan Kundera">
Milan Kundera

Milan Kundera (]; born 1 April 1929) is a Czech-born writer who lives in exile in France since 1975, having become a naturalised citizen in 1981. He "sees himself as a French writer and insists his work should be studied as French literature and classified as such in book stores."
Kundera's best-known work is "The Unbearable Lightness of Being." His books were banned by the Communist regime of Czechoslovakia until the downfall of the regime in the Velvet Revolution of 1989. He lives virtually incognito and rarely speaks to the media. A perennial contender for the Nobel Prize in Literature, he has been nominated on several occasions.
Biography.
Kundera was born in 1929 at Purkyňova ulice, 6 (6 Purkyňova Street) in Brno, Czechoslovakia, to a middle-class family. His father, Ludvík Kundera (1891–1971) was an important Czech musicologist and pianist who served as the head of the Janáček Music Academy in Brno from 1948 to 1961. His mother was Milada Kunderová (born Janošíková). Milan learned to play the piano from his father; he later studied musicology and musical composition. Musicological influences and references can be found throughout his work; he has even included musical notation in the text to make a point. Kundera is a cousin of Czech writer and translator Ludvík Kundera. He belonged to the generation of young Czechs who had had little or no experience of the pre-war democratic Czechoslovak Republic. Their ideology was greatly influenced by the experiences of World War II and the German occupation. Still in his teens, he joined the Communist Party of Czechoslovakia which seized power in 1948. He completed his secondary school studies in Brno at Gymnázium třída Kapitána Jaroše in 1948. He studied literature and aesthetics at the Faculty of Arts at Charles University in Prague. After two terms, he transferred to the Film Faculty of the Academy of Performing Arts in Prague where he first attended lectures in film direction and script writing.
In 1950, his studies were briefly interrupted by political interferences. He and writer Jan Trefulka were expelled from the party for "anti-party activities." Trefulka described the incident in his novella "Pršelo jim štěstí" ("Happiness Rained On Them", 1962). Kundera also used the incident as an inspiration for the main theme of his novel "Žert" ("The Joke", 1967). After Kundera graduated in 1952, the Film Faculty appointed him a lecturer in world literature. In 1956 Milan Kundera was readmitted into the Party. He was expelled for the second time in 1970. Kundera, along with other reform communist writers such as Pavel Kohout, was partly involved in the 1968 Prague Spring. This brief period of reformist activities was crushed by the Soviet invasion of Czechoslovakia in August 1968. Kundera remained committed to reforming Czech communism, and argued vehemently in print with fellow Czech writer Václav Havel, saying, essentially, that everyone should remain calm and that "nobody is being locked up for his opinions yet," and "the significance of the Prague Autumn may ultimately be greater than that of the Prague Spring." Finally, however, Kundera relinquished his reformist dreams and moved to France in 1975. He taught for a few years in the University of Rennes. He was stripped of Czechoslovak citizenship in 1979; he has been a French citizen since 1981.
He maintains contact with Czech and Slovak friends in his homeland, but rarely returns and always does so incognito.
Work.
Although his early poetic works are staunchly pro-communist, his novels escape ideological classification. Kundera has repeatedly insisted on being considered a novelist, rather than a political or dissident writer. Political commentary has all but disappeared from his novels (starting specifically after "The Unbearable Lightness of Being") except in relation to broader philosophical themes. Kundera's style of fiction, interlaced with philosophical digression, is greatly inspired by the novels of Robert Musil and the philosophy of Nietzsche, is also used by authors Alain de Botton and Adam Thirlwell. Kundera takes his inspiration, as he notes often enough, not only from the Renaissance authors Giovanni Boccaccio and Rabelais, but also from Laurence Sterne, Henry Fielding, Denis Diderot, Robert Musil, Witold Gombrowicz, Hermann Broch, Franz Kafka, Martin Heidegger, and perhaps most importantly, Miguel de Cervantes, to whose legacy he considers himself most committed.
Originally, he wrote in Czech. From 1993 onwards, he has written his novels in French. Between 1985 and 1987 he undertook the revision of the French translations of his earlier works. As a result, all of his books exist in French with the authority of the original. His books have been translated into many languages.
"The Joke".
In his first novel, "The Joke" (1967), he gave a satirical account of the nature of totalitarianism in the Communist era. Kundera was quick to criticize the Soviet invasion in 1968. This led to his blacklisting in Czechoslovakia and his works being banned there.
"Life is Elsewhere".
Kundera's second novel was first published in French as "" in 1973 and in Czech as "Život je jinde" in 1979. Set in Czechoslovakia before, during and after the Second World War, "Life Is Elsewhere" is a satirical portrait of the fictional poet Jaromil, a young and very naive idealist who becomes involved in political scandals.
"The Book of Laughter and Forgetting".
In 1975, Kundera moved to France. There he published "The Book of Laughter and Forgetting" (1979) which told of Czech citizens opposing the communist regime in various ways. An unusual mixture of novel, short story collection and author's musings, the book set the tone for his works in exile. Critics have noted the irony that the country that Kundera seemed to be writing about when he talked about Czechoslovakia in the book, "is, thanks to the latest political redefinitions, no longer precisely there" which is the "kind of disappearance and reappearance" Kundera explores in the book. Published in Czech (Kniha smíchu a zapomnění) in April 1981 by 68 Publishers Toronto.
"The Unbearable Lightness of Being".
Kundera's most famous work, "The Unbearable Lightness of Being", was published in 1984. The book chronicles the fragile nature of an individual's fate, theorizing that a single lifetime is insignificant in the scope of Nietzsche's concept of eternal return. In an infinite universe, everything is guaranteed to recur infinitely. In 1988, American director Philip Kaufman released a film adaptation.
"Immortality".
In 1990, Kundera published "Immortality". The novel, his last in Czech, was more cosmopolitan than its predecessors, as well as more explicitly philosophical and less political. It would set the tone for his later novels.
Writing style and philosophy.
Kundera often explicitly identifies his characters as figments of his imagination, commenting in the first-person on the characters in entirely third-person stories. Kundera is more concerned with the words that shape or mold his characters than with their physical appearance. In his non-fiction work, "The Art of the Novel", he says that the reader's imagination automatically completes the writer's vision. He, as the writer, thus wishes to focus on the essential, arguing that the physical is not critical to understanding a character. Indeed, for him the essential may not even include the interior world (the psychological world) of his characters. Still, at times, a specific feature or trait may become the character's idiosyncratic focus.
François Ricard suggested that Kundera conceives with regard to an overall oeuvre, rather than limiting his ideas to the scope of just one novel at a time. His themes and meta-themes exist across the entire oeuvre. Each new book manifests the latest stage of his personal philosophy. Some of these meta-themes include exile, identity, life beyond the border (beyond love, beyond art, beyond seriousness), history as continual return, and the pleasure of a less "important" life. (François Ricard, 2003) Many of Kundera's characters seem to develop as expositions of one of these themes at the expense of their full humanity. Specifics in regard to the characters tend to be rather vague. Often, more than one main character is used in a novel; Kundera may even completely discontinue a character, resuming the plot with somebody new. As he told Philip Roth in an interview in "The Village Voice": "Intimate life [is] understood as one's personal secret, as something valuable, inviolable, the basis of one's originality."
Kundera's early novels explore the dual tragic and comic aspects of totalitarianism. He does not view his works, however, as political commentary. "The condemnation of totalitarianism doesn't deserve a novel," he has said. According to the Mexican novelist Carlos Fuentes, "What he finds interesting is the similarity between totalitarianism and "the immemorial and fascinating dream of a harmonious society where private life and public life form but one unity and all are united around one will and one faith." In exploring the dark humor of this topic, Kundera seems deeply influenced by Franz Kafka.
Kundera considers himself a writer without a message. In "Sixty-three Words," a chapter in "The Art of the Novel", Kundera recounts an episode when a Scandinavian publisher hesitated about going ahead with "The Farewell Party" because of its apparent anti-abortion message. Not only was the publisher wrong about the existence of such a message, Kundera explains, but, "I was delighted with the misunderstanding. I had succeeded as a novelist. I succeeded in maintaining the moral ambiguity of the situation. I had kept faith with the essence of the novel as an art: irony. And irony doesn't give a damn about messages!"
Kundera also ventures often into musical matters, analyzing Czech folk music, quoting from Leoš Janáček and Bartók. Further in this vein, he interpolates musical excerpts into the text (for example, in "The Joke"), or discusses Schoenberg and atonality.
Miroslav Dvořáček controversy.
On 13 October 2008, the Czech weekly "Respekt" prominently publicised an investigation carried out by the Czech Institute for Studies of Totalitarian Regimes, which alleged that Kundera had denounced a young Czech pilot, Miroslav Dvořáček, to the police in 1950. The accusation was based on a police station report which named "Milan Kundera, student, born 1.4.1929" as the informant in regard to Dvořáček's presence at a student dormitory; information about his defection from military service and residence in Germany was attributed in the report to Iva Militká. Dvořáček had fled Czechoslovakia after being ordered to join the infantry in the wake of a purge of the flight academy; he returned to Czechoslovakia as an agent of a spy agency organised by Czechoslovak exiles. The police report does not mention his activity as an agent. Dvořáček returned secretly to the student dormitory of a friend's former sweetheart, Iva Militká. Militká was dating (and later married) a fellow student Ivan Dlask, and Dlask knew Kundera. The police report states that Militká told Dlask of Dvořáček's presence, and that Dlask told Kundera, who told the police. Although the Communist prosecutor sought the death penalty, Dvořáček was sentenced to 22 years (as well as being charged 10,000 crowns, forfeiting property, and being stripped of civic rights). He ended up serving 14 years in a labor camp, some of it working in a uranium mine, before he was released.
After "Respekt"‍ '​s report (which states that Kundera did not know Dvořáček), Kundera denied turning Dvořáček in to the police, stating he did not know him at all, and could not even recollect "Militská". On 14 October 2008, the Czech Security Forces Archive ruled out the possibility that the document could be a fake, but refused to make any interpretation about it. (Vojtech Ripka, of the Institute for the Study of Totalitarian Regimes, said, "There are two pieces of circumstantial evidence [the police report and its sub-file], but we, of course, cannot be one hundred percent sure. Unless we find all survivors, which is unfortunately impossible, it will not be complete." Ripka added that the signature on the police report matches the name of a man who worked in the corresponding National Security Corps section and that a police protocol is missing.)
Many critics in the Czech Republic condemned Kundera as a "police informer," but many other voices sharply criticised "Respekt" for publishing a badly researched piece. The short police report does not contain Kundera's signature, nor does it contain any information from his ID card. Kundera was the student representative of the dorm Dvořáček visited, and it cannot be ruled out that anyone could have reported him to the police using Kundera's name. Contradictory statements by Kundera's fellow students were carried by the Czech newspapers in the wake of this "scandal." "Historian" Tomáš Hradílek was criticised for an undeclared conflict of interest: one of the protagonists of the incident was his relative. The Prague-based Institute for the Study of Totalitarian Regimes is regarded by many as a propagandistic institution. It states on its website that its task is to "impartially study the crimes of the former communist regime." Critics also accused "Respekt" of using Kundera's name to boost its failing circulation. 
On 3 November 2008, eleven internationally recognized writers came to Kundera's defence: these included four Nobel laureates -– J. M. Coetzee, Gabriel García Márquez, Nadine Gordimer and Orhan Pamuk -– as well as Carlos Fuentes, Juan Goytisolo, Philip Roth, Salman Rushdie and Jorge Semprún.
Awards and honors.
In 1985, Kundera received the Jerusalem Prize. His acceptance address is printed in his essay collection "The Art of the Novel." He won The Austrian State Prize for European Literature in 1987. In 2000, he was awarded the international Herder Prize. In 2007, he was awarded the Czech State Literature Prize. In 2010, he was made an honorary citizen of his hometown, Brno. In 2011, he received the Ovid Prize. The asteroid 7390 Kundera, discovered at the Kleť Observatory in 1983, is named in his honor.

</doc>
<doc id="36682" url="http://en.wikipedia.org/wiki?curid=36682" title="Linear predictive coding">
Linear predictive coding

Linear predictive coding (LPC) is a tool used mostly in audio signal processing and speech processing for representing the spectral envelope of a digital signal of speech in compressed form, using the information of a linear predictive model. It is one of the most powerful speech analysis techniques, and one of the most useful methods for encoding good quality speech at a low bit rate and provides extremely accurate estimates of speech parameters.
Overview.
LPC starts with the assumption that a speech signal is produced by a buzzer at the end of a tube (voiced sounds), with occasional added hissing and popping sounds (sibilants and plosive sounds). Although apparently crude, this model is actually a close approximation of the reality of speech production. The glottis (the space between the vocal folds) produces the buzz, which is characterized by its intensity (loudness) and frequency (pitch). The vocal tract (the throat and mouth) forms the tube, which is characterized by its resonances, which give rise to formants, or enhanced frequency bands in the sound produced. Hisses and pops are generated by the action of the tongue, lips and throat during sibilants and plosives.
LPC analyzes the speech signal by estimating the formants, removing their effects from the speech signal, and estimating the intensity and frequency of the remaining buzz. The process of removing the formants is called inverse filtering, and the remaining signal after the subtraction of the filtered modeled signal is called the residue.
The numbers which describe the intensity and frequency of the buzz, the formants, and the residue signal, can be stored or transmitted somewhere else. LPC synthesizes the speech signal by reversing the process: use the buzz parameters and the residue to create a source signal, use the formants to create a filter (which represents the tube), and run the source through the filter, resulting in speech.
Because speech signals vary with time, this process is done on short chunks of the speech signal, which are called frames; generally 30 to 50 frames per second give intelligible speech with good compression.
Early history of LPC.
According to Robert M. Gray of Stanford University, the first ideas leading to LPC started in 1966 when S. Saito and F. Itakura of NTT described an approach to automatic phoneme discrimination that involved the first maximum likelihood approach to speech coding. In 1967, John Burg outlined the maximum entropy approach. In 1969 Itakura and Saito introduced partial correlation, May Glen Culler proposed realtime speech encoding, and Bishnu S. Atal presented an LPC speech coder at the Annual Meeting of the Acoustical Society of America. In 1971 realtime LPC using 16-bit LPC hardware was demonstrated by Philco-Ford; four units were sold.
In 1972 Bob Kahn of ARPA, with Jim Forgie (Lincoln Laboratory, LL) and Dave Walden (BBN Technologies), started the first developments in packetized speech, which would eventually lead to Voice over IP technology. In 1973, according to Lincoln Laboratory informal history, the first realtime 2400 bit/s LPC was implemented by Ed Hofstetter. In 1974 the first realtime two-way LPC packet speech communication was accomplished over the ARPANET at 3500 bit/s between Culler-Harrison and Lincoln Laboratories. In 1976 the first LPC conference took place over the ARPANET using the Network Voice Protocol, between Culler-Harrison, ISI, SRI, and LL at 3500 bit/s. And finally in 1978, B. S. Atal and Vishwanath "et al." of BBN developed the first variable-rate LPC algorithm.
LPC coefficient representations.
LPC is frequently used for transmitting spectral envelope information, and as such it has to be tolerant of transmission errors. Transmission of the filter coefficients directly (see linear prediction for definition of coefficients) is undesirable, since they are very sensitive to errors. In other words, a very small error can distort the whole spectrum, or worse, a small error might make the prediction filter unstable.
There are more advanced representations such as log area ratios (LAR), line spectral pairs (LSP) decomposition and reflection coefficients. Of these, especially LSP decomposition has gained popularity, since it ensures stability of the predictor, and spectral errors are local for small coefficient deviations.
Applications.
LPC is generally used for speech analysis and resynthesis. It is used as a form of voice compression by phone companies, for example in the GSM standard. It is also used for secure wireless, where voice must be digitized, encrypted and sent over a narrow voice channel; an early example of this is the US government's Navajo I.
LPC synthesis can be used to construct vocoders where musical instruments are used as excitation signal to the time-varying filter estimated from a singer's speech. This is somewhat popular in electronic music.
Paul Lansky made the well-known computer music piece notjustmoreidlechatter using linear predictive coding.
A 10th-order LPC was used in the popular 1980s Speak & Spell educational toy.
LPC predictors are used in Shorten, MPEG-4 ALS, FLAC, SILK audio codec, and other lossless audio codecs.
LPC is receiving some attention as a tool for use in the tonal analysis of violins and other stringed musical instruments.

</doc>
<doc id="36685" url="http://en.wikipedia.org/wiki?curid=36685" title="Emperor Go-Ichijō">
Emperor Go-Ichijō

Emperor Go-Ichijō (後一条天皇, Go-Ichijō-tennō, October 12, 1008 – May 15, 1036) was the 68th emperor of Japan, according to the traditional order of succession.
Go-Ichijō's reign spanned the years from 1016 through 1036.
This 11th century sovereign was named after Emperor Ichijō and "go-" (後), translates literally as "later;" and thus, he is sometimes called the "Later Emperor Ichijō". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Ichijō, the second."
Traditional narrative.
Before his ascension to the Chrysanthemum Throne, his personal name ("imina") was Atsuhira "-shinnō" (敦成親王). He was also known as Atsunari"-shinnō".
Atsuhira was the second son of Emperor Ichijō. His mother, Fujiwara no Akiko/Shōshi (藤原彰子) (988–1074), was a daughter of Fujiwara no Michinaga. In her later years, Ichijō's "chūgo" consort was known as Jōtō-mon In (上東門院).
Events of Go-Ichijō's life.
Atsuhira"-shinnō" was used as a pawn in Imperial court politics when he was only a child.
Atsuhira became emperor at the age of 8, upon the abdication of his first cousin once removed, Emperor Sanjō.
During the initial years of Go-Ichijō's reign, Fujiwara no Michinaga actually ruled from his position as "sesshō" (regent).
The actual site of Go-Ichijō's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Kyoto.
The Imperial Household Agency designates this location as Go-Ichijō's mausoleum. It is formally named "Bodaijuin no misasagi".
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Ichijō's reign, this apex of the "Daijō-kan included:
Eras of Go-Ichijō's reign.
The years of Go-Ichijō's reign are more specifically identified by more than one era name or "nengō".
Consort and children.
Go-Ichijō had one Empress and two Imperial daughters.
Empress ("chūgū"): Fujiwara no "Ishi" (藤原威子) (999–1036), third daughter of Fujiwara no Michinaga

</doc>
<doc id="36703" url="http://en.wikipedia.org/wiki?curid=36703" title="Hex (board game)">
Hex (board game)

Hex is a strategy board game played on a hexagonal grid, theoretically of any size and several possible shapes, but traditionally as an 11×11 rhombus. Other popular dimensions are 13×13 and 19×19 as a result of the game's relationship to the older game of Go. According to the book "A Beautiful Mind", John Nash (one of the game's inventors) advocated 14×14 as the optimal size.
History.
The game was invented by the Danish mathematician Piet Hein, who introduced it in 1942 at the Niels Bohr Institute. It was independently re-invented in 1947 by the mathematician John Nash at Princeton University. It became known in Denmark under the name "Polygon" (though Hein called it CON-TAC-TIX); Nash's fellow players at first called the game "Nash". According to Martin Gardner, some of the Princeton University students also referred to the game as "John" (according to some sources this was because they played the game using the mosaic of the bathroom floor). However, according to Sylvia Nasar's biography of John Forbes Nash "A Beautiful Mind", the game was referred to as "Nash" or "John" after its apparent creator. John Nash was said to have thought of this game, independent of Hein's, during his graduate years at Princeton. In 1952, Parker Brothers marketed a version. They called their version "Hex" and the name stuck.
Hex is an abstract strategy game that belongs to the general category of "connection" games. Other connection games include Omni, Y and Havannah. All of these games bear varying degrees of similarity to the ancient Asian game of Go.
Rules.
Each player has an allocated color, conventionally Red and Blue or White and Black. Players take turns placing a stone of their color on a single cell within the overall playing board. The goal for each player is to form a connected path of their own stones linking the opposing sides of the board marked by their colors, before their opponent connects his or her sides in a similar fashion. The first player to complete his or her connection wins the game. The four corner hexagons each belong to both adjacent sides.
Since the first player to move in Hex has a distinct advantage, the pie rule is generally implemented for fairness. This rule allows the second player to choose whether to switch positions with the first player after the first player makes the first move.
Strategy.
The game can never end in a tie, a fact proved by John Nash: the only way a player can prevent an opponent from forming a connecting path is to form their own path. In other words, Hex is a "determined" game.
When the sides of the grid are equal, the game favors the first player. A standard non-constructive strategy-stealing argument proves that the first player has a winning strategy as follows:
One might attempt to compensate for the second player's disadvantage by making the second player's sides closer together, playing on a parallelogram rather than a rhombus. However, using a simple pairing strategy, this has been proven to result in an easy win for the second player.
Bridges and connections.
Two (groups of) stones are safely connected if nothing can stop them from being connected even if the opponent has the next move. One example of this is the bridge. Let A, B, C and D be the hexes that make up a rhombus, with A and C being the non-touching pair.
To form a bridge, a player places stones at A and C, leaving B and D empty. If the opponent places a stone at B or D, the remaining hex can be filled to join the original two stones into a single group. This strategy is very useful throughout the game.
Paths.
Two groups of stones are said to be "n"-connected if they can be safely connected in "n" moves (or, more precisely, the number of moves a player must make in order to safely connect the two groups minus the number of moves their opponent makes is "n"). Safely connected stones, such as adjacent stones are 0-connected. Bridges are also 0-connected. The lower the value of "n", the better for the player.
A path consists of two (or more) groups of stones and an empty-point set, which is the set of empty hexes that are required for the given connections. For example, the bridge path consists of the (one-member) group of stones at A and another (one-member) group of stones at C. The empty-point set is made up of the hexes B and D. For two paths to coexist and maintain the level of connectivity they have while independent, their empty-point sets must not contain any of the same hexes (otherwise the opponent could play there).
Two 1-connected paths can be consolidated together if the two groups of stones they start and end in are the same and their empty-point sets do not overlap.
Templates.
An important concept in the theory of Hex is the template. Templates can be considered a special type of 0-connected path where one of the groups of stones is the edge that the player is trying to connect to.
Ladders.
Ladders are sequences of forcing moves where stones are placed in two parallel lines. They can be considered normal edge templates and can be analyzed using path analysis in the same way that bridges, paths, and templates can.
Theory and proofs.
Hex is a connection game, and can be classified as a Maker-Breaker game, a particular type of positional game.
John Nash proved in 1952 that a game of Hex cannot end in a tie, and that for a symmetric board there exists a winning strategy for the player who makes the first move (by the strategy-stealing argument). However, the argument is non-constructive: it only shows the existence of a winning strategy, without describing it explicitly. Finding an explicit strategy has been the main subject of research since then.
An explicit winning strategy with a pairing argument exists on non-symmetric "n"×"m" boards, which leaves only symmetric "n"×"n" boards as the center of interest.
In 1976, Shimon Even and Robert Tarjan proved that determining whether a position in the game of Hex is a winning position is
PSPACE-complete.
A generalization of this result was proved by Reisch.
In the computational complexity theory, it is widely conjectured that PSPACE-complete problems cannot be solved with efficient (polynomial time) algorithms. This result limits the efficiency of the best possible algorithms when considering arbitrary positions on boards of unbounded size, but it doesn't rule out the possibility of a simple winning strategy for the initial position (on boards of unbounded size), or a simple winning strategy for all positions on a board of a particular size.
In 2002, Jing Yang, Simon Liao and Mirek Pawlak found an explicit winning strategy for the first player on Hex boards of size 7×7. They extended the method to the 8×8 and 9×9 boards in 2003.
In 2009, Philip Henderson, Broderick Arneson and Ryan B. Hayward completed the analysis of the 8×8 board with a computer search, solving all the possible openings. In 2013, Jakub Pawlewicz
and Ryan B. Hayward solved all openings for 9x9 boards, and one opening move on the 10x10 board.
The determinacy of Hex has other mathematical consequences: it can be used to prove the two-dimensional Brouwer fixed point theorem, as David Gale showed in 1979, and the determinacy of higher-dimensional variants proves the fixed-point theorem in general.
Variants.
"Blockbusters".
Hex had an incarnation as the question board from the television game show "Blockbusters". In order to play a "move", contestants had to answer a question correctly. The board had 5 alternating columns of 4 hexagons; the solo player could connect top-to-bottom in 4 moves, while the team of two could connect left-to-right in 5 moves.
The game of Y.
The game of Y is a generalization of Hex to the extent that any position on a Hex board can be represented as an equivalent position on a larger Y board.
Havannah.
Havannah has some similarities to Hex, but the winning structures (game objectives) are different.
Mind Ninja.
Mind Ninja is another game that is a generalization of Hex, albeit a rather broad one. As in Hex, two players vie to create mutually exclusive patterns by filling in cells of a tiled surface. In Mind Ninja, however, the players themselves define the patterns, subject to certain constraints. Mind Ninja differs from Hex also in that it can be played on any tiled surface, and each player may fill in a cell with any available color, rather than just one.
Chameleon.
Utilizing the same board and pieces as Hex, Chameleon gives the players the option of placing a piece of either color on the board. One player is attempting to connect the north and south edges, and the other is attempting to connect the east and west edges. The game is won when a connection between a player's goal edges is formed using "either" color. If a piece is placed that creates a connection between both players' goal edges (i.e. all edges are connected), the winner is the player who placed the final piece.
Chameleon is described in Cameron Browne's book "Connection Games: Variations on a Theme" (2005) and was independently discovered by Randy Cox and Bill Taylor.
The Shannon switching game.
The Shannon switching game involves two players coloring the edges of an arbitrary graph, one player attempting to connect two distinguished vertices with edges of their color, and the other erasing edges to prevent this. It was invented by "the father of information theory", Claude Shannon.
Unlike Hex, this game is not known to be PSPACE hard, unless played on a directed graph or in the variant where play is along vertices rather than edges.
Gale.
In this game invented by David Gale (also known as Game of Gale, Bridg-It, or Bird Cage), two grids of differently-colored dots are overlaid at an offset. One player links orthogonally adjacent dots on one grid, and the other player uses the other. One player attempts to link the top of their grid to the bottom, while the other tries to link their left side to the right.
The game is equivalent to the Shannon switching game played on a rectangular grid.
Pex.
Pex is nearly identical to Hex, except that it's played on a rhombus-shaped tiling of irregular pentagons, instead of regular hexagons. Pex's tiling is notable for the fact that half of the pentagons each connect to seven adjacent neighbors, while the other half each connect to only to five neighbors. Pex's tactics are said to be much sharper than those of Hex.
Hecks.
Hecks is yet another variant of Hex in which the tiles of the square board are irregular polygons and the graph formed by polygon edges is trivalent, i.e. each node has precisely three incident arcs. The trivalence condition is meant to avoid the decision about the validity of the contact between two tiles that share only a vertex. An interesting aspect of Hecks is that the sides of the board have no predefined color: the black and white players do not have to declare in advance which pair of sides they attempt to connect, and the first player who completes a path across the board wins.
Nex.
Players take turns to place a stone of their color and a neutral stone on empty cells; or replace two neutral stones with stones of their color, and replace a different stone of their color on the board to neutral stone.

</doc>
<doc id="36717" url="http://en.wikipedia.org/wiki?curid=36717" title="Kingdom of Northumbria">
Kingdom of Northumbria

The Kingdom of Northumbria (; Old English: "Norþhymbra rīce", "kingdom of the Northumbrians") was a medieval Anglian kingdom in what is now northern England and south-east Scotland, which subsequently became an earldom in a unified English kingdom. The name reflects the approximate southern limit to the kingdom's territory, the Humber estuary.
Northumbria was formed by Æthelfrith in central Great Britain in Anglo-Saxon times. At the beginning of the 7th century, the two kingdoms of Bernicia and Deira were unified. (In the 12th century writings of Henry of Huntingdon the kingdom was defined as one of the Heptarchy of Anglo-Saxon kingdoms). At its height, the kingdom extended at least from just south of the Humber to the River Mersey and to the Forth (roughly, Sheffield to Runcorn to Edinburgh)—and there is some evidence that it may have been much greater (see map).
The later (and smaller) earldom came about when the southern part of Northumbria (ex-Deira) was lost to the Danelaw. The northern part (ex-Bernicia) at first retained its status as a kingdom but when it became subordinate to the Danish kingdom, it had its powers curtailed to that of an earldom and retained that status when England was reunited by the Wessex-led reconquest of the Danelaw. The earldom was bounded by the River Tees in the south and the River Tweed in the north (broadly similar to the modern North East England).
Much of this land was "debated" between England and Scotland, but the Earldom of Northumbria was eventually recognised as part of England by the Anglo-Scottish Treaty of York in 1237. On the northern border, Berwick-upon-Tweed, which is north of the Tweed but had changed hands many times, was defined as subject to the laws of England by the Wales and Berwick Act 1746. The land once part of Northumbria at its peak is now divided by modern administrative boundaries:
Northumbria is also used in the names of some regional institutions, particularly the police force (Northumbria Police, which covers Northumberland and Tyne and Wear) and a university (Northumbria University) based in Newcastle upon Tyne. The local Environment Agency office, located in Newcastle Business Park, also uses the term Northumbria to describe its patch. Otherwise, the term is not used in everyday conversation and is not the official name for the UK and EU region of North East England.
Kingdom (654–954).
Northumbria was originally composed of the union of two independent kingdoms, Bernicia and Deira. Bernicia covered lands north of the Tees, while Deira corresponded roughly to modern-day Yorkshire. Bernicia and Deira were first united by Aethelfrith, a king of Bernicia who conquered Deira around the year 604. He was defeated and killed around the year 616 in battle at the River Idle by Raedwald of East Anglia, who installed Edwin, the son of Ælla, a former king of Deira, as king.
Edwin, who accepted Christianity in 627, soon grew to become the most powerful king in England: he was recognised as Bretwalda and conquered the Isle of Man and Gwynedd in northern Wales. He was, however, himself defeated by an alliance of the exiled king of Gwynedd, Cadwallon ap Cadfan and Penda, king of Mercia, at the Battle of Hatfield Chase in 633.
King Oswald.
After Edwin's death, Northumbria was split between Bernicia, where Eanfrith, a son of Aethelfrith, took power, and Deira, where a cousin of Edwin, Osric, became king. Cumbria tended to remain a country frontier with the Britons. Both of these rulers were killed during the year that followed, as Cadwallon continued his devastating invasion of Northumbria. After the murder of Eanfrith, his brother, Oswald, backed by warriors sent by Domnall Brecc of Dál Riata, defeated and killed Cadwallon at the Battle of Heavenfield in 634.
Oswald expanded his kingdom considerably. He incorporated Gododdin lands northwards up to the Firth of Forth and also gradually extended his reach westward, encroaching on the remaining Cumbric speaking kingdoms of Rheged and Strathclyde. Thus, Northumbria became not only part of modern England's far north, but also covered much of what is now the south-east of Scotland. King Oswald re-introduced Christianity to the Kingdom by appointing St. Aidan, an Irish monk from the Scottish island of Iona to convert his people. This led to the introduction of the practices of Celtic Christianity. A monastery was established on Lindisfarne. 
War with Mercia continued, however. In 642, Oswald was killed by the Mercians under Penda at the Battle of Maserfield. In 655, Penda launched a massive invasion of Northumbria, aided by the sub-king of Deira, Aethelwald, but suffered a crushing defeat at the hands of an inferior force under Oswiu, Oswald's successor, at the Battle of Winwaed. This battle marked a major turning point in Northumbrian fortunes: Penda died in the battle, and Oswiu gained supremacy over Mercia, making himself the most powerful king in England.
Religious consolidation.
In the year 664 the Synod of Whitby was held to discuss the controversy regarding the timing of the Easter festival. Dispute had arisen between the practices of the Celtic church influencing Northumbria and those of the Roman church predominate throughout southern England and Western Europe. Eventually, Northumbria was persuaded to move to the Roman practice and the Celtic Bishop Colman of Lindisfarne returned to Iona. The episcopal seat of Northumbria was transferred from Lindisfarne to York, which indicates the growing religious, political and economic importance of connections to southern Great Britain.
Decline of the Kingdom of Northumbria.
Northumbria lost control of Mercia in the late 650s, after a successful revolt under Penda's son Wulfhere, but it retained its dominant position until it suffered a disastrous defeat at the hands of the Picts at the Battle of Dun Nechtain in 685; Northumbria's king, Ecgfrith (son of Oswiu), was killed, and its power in the north was gravely weakened. The peaceful reign of Aldfrith, Ecgfrith's half-brother and successor, did something to limit the damage done, but it is from this point that Northumbria's power began to decline, and chronic instability followed Aldfrith's death in 704. 
In 867 Northumbria became the northern kingdom of the Danelaw, after its conquest by the brothers Halfdan Ragnarsson and Ivar the Boneless who installed an Englishman, Ecgberht, as a puppet king. Despite the pillaging of the kingdom, Viking rule brought lucrative trade to Northumbria, especially at their capital York. The kingdom passed between English, Norse and Norse-Gaelic kings until it was finally absorbed by King Eadred after the death of the last independent Northumbrian monarch, Erik Bloodaxe, in 954.
Ealdormanships and earldoms in Northumbria (954–1217).
After the English regained the territory of the former kingdom, Scots invasions reduced Northumbria to an earldom stretching from the Humber to the Tweed. Northumbria was disputed between the emerging kingdoms of England and Scotland. The land north of the Tweed was finally ceded to Scotland in 1018 as a result of the battle of Carham. Yorkshire and Northumberland were first mentioned as separate in the "Anglo-Saxon Chronicle" in 1065.
Norman invasion and partition of the earldom.
William the Conqueror became king of England in 1066. He realised he needed to control Northumbria, which had remained virtually independent of the Kings of England, to protect his kingdom from Scottish invasion. In 1067, William appointed Copsi (sometimes Copsig) as Earl. However, just five weeks into his reign as earl, Copsi was murdered by Osulf II of Bamburgh. 
To acknowledge the remote independence of Northumbria and ensure England was properly defended from the Scots, William gained the allegiance of both the Bishop of Durham and the Earl and confirmed their powers and privileges. However, anti-Norman rebellions followed. William therefore attempted to install Robert Comine, a Norman noble, as the Earl of Northumbria, but before Comine could take up office, he and his 700 men were massacred in the city of Durham. In revenge, the Conqueror led his army in a bloody raid into Northumbria, an event that became known as the "Harrying of the North". Ethelwin, the Anglo-Saxon Bishop of Durham, tried to flee Northumbria at the time of the raid, with Northumbrian treasures. The bishop was subsequently caught, imprisoned, and later died in confinement; his seat was left vacant.
Rebellions continued, and William's son William Rufus decided to partition Northumbria. William of St. Carilef was made Bishop of Durham, and was also given the powers of Earl for the region south of the rivers Tyne and Derwent, which became the County Palatine of Durham. 
The remainder, to the north of the rivers, became Northumberland, where the political powers of the Bishops of Durham were limited to only certain districts, and the earls continued to rule as clients of the English throne. The city of Newcastle was founded by the Normans in 1080 to control the region by holding the strategically important crossing point of the river Tyne. 
Subsequent history.
The Northumbrian region continued a history of revolt and rebellion against the government, as seen in the Rising of the North in Tudor times. A major reason was the strength of Catholicism in the area after the Reformation. Rural, thinly populated, and sharing a border with an often hostile Scotland, the region became a wild place where reivers raided across the border and outlaws took refuge from justice. However, after the union of the crowns of Scotland and England under King James VI and I peace was largely established. After the Restoration, many inhabitants of the Northumbrian region supported the Jacobite cause.
Culture.
Northumbria during its "golden age" was the most important centre of religious learning and arts in the British Isles. Initially the kingdom was evangelised by Irish monks from the Celtic Church, based at Iona in modern Scotland, which led to a flowering of monastic life. Lindisfarne on the east coast was founded from Iona by Saint Aidan in about 635, and was to remain the major Northumbrian monastic centre, producing figures like Wilfrid and Saint Cuthbert. The nobleman Benedict Biscop had visited Rome and headed the monastery at Canterbury in Kent and his twin-foundation Monkwearmouth-Jarrow Abbey added a direct Roman influence to Northumbrian culture, and produced figures such as Ceolfrith and Bede. Northumbria played an important role in the formation of Insular art, a unique style combining Anglo-Saxon, Celtic, Pictish, Byzantine and other elements, producing works such as the Lindisfarne Gospels, St Cuthbert Gospel, the Ruthwell Cross and Bewcastle Cross, and later the Book of Kells, which was probably created at Iona. After the Synod of Whitby in 664 Roman church practices officially replaced the Celtic ones but the influence of the Celtic style continued, the most famous examples of this being the Lindisfarne Gospels. The Venerable Bede (673–735) wrote his "Historia ecclesiastica gentis Anglorum" (Ecclesiastical History of the English People, completed in 731) in Monkwearmouth-Jarrow, and much of it focuses on the kingdom. The devastating Viking raid on Lindisfarne in 793 marked the beginning of a century of Viking invasions that severely checked all Anglo-Saxon culture, and heralded the end of Northumbria's position as a centre of influence, although in the years immediately following confident works like the Easby Cross were still being produced. A bullion-weight economy was still in use at the end of the 9th-century AD, as demonstrated in the twenty-nine silver ingots found in the Bedale Hoard, along with sword fittings and necklaces in gold and silver.
Northumbria has its own "check" or "tartan", which is similar to many ancient tartans (especially those from Northern Europe, such as one found near Falkirk and those discovered in Jutland that date from Roman times (and even earlier). Modern Border Tartans are almost invariably a bold black and white check, but historically the light squares were the yellowish colour of untreated wool, with the dark squares any of a range of dark greys, blues, greens or browns; hence the alternative name of "Border Drab". At a distance the checks blend together making the fabric ideal camouflage for stalking game.
Language.
Apart from standard English, Northumbria has a series of closely related but distinctive dialects, descended from the early Germanic languages of the Angles, of which 80% of its vocabulary is derived, and Vikings with a few Celtic and Latin loanwords. The Scots language began to diverge from early Northumbrian Middle English, which was called "Ynglis" as late as the early 16th century (until the end of the 15th century the name "Scottis" (modern form: Scots) referred to Scottish Gaelic). There are many similarities between Modern Scots dialects and those of Northumbria. 
The major Northumbrian dialects are Geordie (Tyneside), Northern (north of the River Coquet), Western (from Allendale through Hexham up to Kielder), Southern or Pitmatic (the mining towns such as Ashington and much of Durham), Mackem (Wearside), Smoggie (Teesside) and Tyke (Yorkshire). To an outsider's ear the similarities far outweigh the differences between the dialects. As an example of the differences, in the softer South County Durham/Wearside the English 'book' is pronounced 'bewk', in Geordie it becomes 'bouk', while in the Northumbrian it is 'byuk'.

</doc>
<doc id="36762" url="http://en.wikipedia.org/wiki?curid=36762" title="Ilona Staller">
Ilona Staller

Ilona Staller (born 26 November 1951), widely known by her stage name, Cicciolina, is a Hungarian-born Italian porn star, politician, and singer. 
Early life.
Ilona was born in Budapest, Hungary. Her father, László Staller, left the family when she was young. She was raised by her mother, who was a midwife, and her stepfather who was an official in the Hungarian Ministry of the Interior.
In 1964, she began working as a model for the Hungarian news agency, M.T.I. In her memoirs and in a 1999 TV interview, she claimed that she had provided the Hungarian authorities with information on American diplomats staying at a Budapest luxury hotel where she worked as a maid the 1960s. By the age of 25, and during her hotel work, she met an older Italian national named Salvatore Martini whom she later married.
Pornography and show business.
Naturalized by marriage and settled in Italy, Staller met pornographer Riccardo Schicchi in the early 1970s, and, beginning in 1973, achieved fame with a radio show called "Voulez-vous coucher avec moi?" on Radio Luna. For that program she adopted the name "Cicciolina". She referred to her male fanbase, and later the male members of the Italian parliament, as "cicciolini", translating loosely as "little tubby boys". Although she appeared in several films from 1970, she made her debut under her own name in 1975 with "La liceale" (aka "The Teasers") by playing with Gloria Guida as her lesbian classmate.
In 1978, on the RAI show "C'era due Volte", her breasts were the first to be bared live on Italian TV. Staller appeared in her first hardcore pornographic film, "Telefono rosso" (Red telephone) in 1983. She produced the film together with Schicchi's company Diva Futura. Her memoirs were published as "Confessioni erotiche di Cicciolina" (Erotic confessions of Cicciolina) by Olympia Press of Milan in 1987. That same year she appeared in "Carne bollente," called "The Rise and Fall of the Roman Empress" in the United States, co-starring John Holmes. The film would later create a furor when it was revealed that Holmes had tested positive for HIV prior to appearing in it. Staller has appeared nude in "Playboy's" editions in several countries. Her first "Playboy" appearance was in Argentina in March 1988. Other appearances for the magazine were in the U.S. (September 1990), Hungary (June 2005), Serbia (July 2005) and Mexico (September 2005).
In 1994, she appeared in the film "Replikator" and, in 1996, she had a role in the Brazilian soap opera "Xica da Silva (TV series)" as Princess Ludovica di Castelgandolfo di Genova. In 2008, she was a contestant on the Argentine version of "Strictly Come Dancing" named "Bailando por un Sueño".
Political life.
In 1979, Staller was presented as a candidate to the Italian parliament by the Lista del Sole, Italy's first Green party. In 1985, she switched to the Partito Radicale, campaigning on a libertarian platform against nuclear energy and NATO membership, as well as for human rights. She was elected to the Italian parliament in 1987, with approximately 20,000 votes. While in office, and before the outset of the Gulf War, she offered to have sex with Iraqi leader Saddam Hussein in return for peace in the region. She was not re-elected at the end of her term in 1991.
In 1991, Staller was among the founders of another Italian political movement, called Partito dell'Amore ("Party of Love"), which was spearheaded by friend and fellow porn star Moana Pozzi. In January 2002, she began exploring the possibility of campaigning in Hungary, her country of birth, to represent Budapest's industrial Kőbánya district in the Hungarian parliament. However, she failed to collect enough petition signatures for a non-partisan candidacy. In the same year, she ran in local elections in Monza, Italy, promising to convert a prominent building into a gambling casino, but she attracted few votes. In 2004, she announced plans to run for mayor of Milan with a similar promise. She renewed her offer to have sex with Saddam Hussein in October 2002, when Iraq was resisting international pressure to allow inspections for weapons of mass destruction, and in April 2006, she made the same offer to Osama bin Laden.
In September 2011, it was revealed that Staller was eligible for and would be receiving a yearly pension of 39,000 euros from the Italian state as a result of her five years in the country's parliament. Reacting to the controversy raised by the news, the former porn star, who started receiving the pension in November 2011, when she turned 60, stated: "I earned it and I'm proud of it."
In 2012, Staller founded the Democracy, Nature and Love Party (DNA) with her partner Luca di Carlo, a criminal defense lawyer. Its objectives included: the legalization of same-sex marriage; the reopening of former brothels ("closed houses"); a guaranteed minimum wage for young people; a properly functioning judicial system for every Italian; and the elimination of the privileges of the rich political "caste".
The party is ready to stand for the upcoming elections on 24 February 2013.
In popular culture.
British band Pop Will Eat Itself released a song called Touched By The Hand Of Cicciolina as an unofficial World Cup single in July 1990. The song reached number 28 in the UK singles charts.
Musical career.
Staller has recorded several songs, mostly from live performances, with explicit lyrics being sung to a children's melody. Her most famous song is "Muscolo rosso", a song entirely dedicated to "il cazzo", which means "the dick" in Italian. Because of its extensive use of swear words, the song could not be released in Italy, but became a hit in other countries, especially in France. The song gained considerable popularity in the internet era, when many Italian speakers were able to hear it for the first time.
Several unreleased songs were recorded during her RCA period and the Diva Futura agency period. Some of these unreleased songs were subsequently used during her TV shows, live performances or as soundtracks in her porn movies.
Personal life.
Staller married American artist Jeff Koons in 1991. Koons produced a series of sculptures and photographs of them having sex in many positions, settings and costumes, which were exhibited under the title "Made In Heaven." 
Following the end of their marriage in 1994, as she wanted to continue to perform as a porn actress and Koons wanted them to be monogamous, Staller, in violation of a US court order, left the US with their then-two-year-old son, Ludwig and took the child to Italy. A lengthy custody battle ensued. Koons won custody in 1998 but Ludwig remains with Staller in Italy. In 2008, Staller filed suit against Koons for failing to pay child support.
In 2015 she declared to be a practicing Catholic and to be ready for an "artistic pornographic film", the last of her career, a gift for all her fans.

</doc>
<doc id="36869" url="http://en.wikipedia.org/wiki?curid=36869" title="Cell division">
Cell division

Cell division is the process by which a "parent cell" divides into two or more "daughter cells". Cell division usually occurs as part of a larger cell cycle. In eukaryotes, there are two distinct types of cell division: a vegetative division, whereby each daughter cell is genetically identical to the parent cell (mitosis), and a reductive cell division, whereby the number of chromosomes in the daughter cells is reduced by half, to produce haploid gametes (meiosis). Meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions: homologous chromosomes are separated in the first division, and sister chromatids are separated in the second division. Both of these cell division cycles are in sexually reproducing organisms at some point in their life cycle, and both are believed to be present in the last eukaryotic common ancestor Prokaryotes also undergo a vegetative cell division known as binary fission, where their genetic material is segregated equally into two daughter cells. All cell divisions, regardless of organism, are preceded by a single round of DNA replication.
For simple unicellular organisms such as the amoeba, one cell division is equivalent to reproduction – an entire new organism is created. On a larger scale, mitotic cell division can create progeny from multicellular organisms, such as plants that grow from cuttings. Cell division also enables sexually reproducing organisms to develop from the one-celled zygote, which itself was produced by cell division from gametes. And after growth, cell division allows for continual construction and repair of the organism. A human being's body experiences about 10,000 trillion cell divisions in a lifetime.
Cell division has been modeled by finite subdivision rules.
The primary concern of cell division is the maintenance of the original cell's genome. Before division can occur, the genomic information that is stored in chromosomes must be replicated, and the duplicated genome must be separated cleanly between cells. A great deal of cellular infrastructure is involved in keeping genomic information consistent between "generations".
Daughter cells of cell divisions, in early embryonic development, contribute unequally to the generation of adult tissues.
Variants.
Cells are classified into two main categories: simple, non-nucleated prokaryotic cells, and complex, nucleated eukaryotic cells. By dint of their structural differences, eukaryotic and prokaryotic cells do not divide in the same way. Also, the pattern of cell division that transforms eukaryotic stem cells into gametes (sperm cells in males or ova – egg cells – in females) is different from that of the somatic cell division in the cells of the body.
Degradation.
Multicellular organisms replace worn-out cells through cell division. In some animals, however, cell division eventually halts. In humans this occurs on average, after 52 divisions, known as the Hayflick limit. The cell is then referred to as senescent. Cells stop dividing because the telomeres, protective bits of DNA on the end of a chromosome required for replication, shorten with each copy, eventually being consumed. Cancer cells, on the other hand, are not thought to degrade in this way, if at all. An enzyme called telomerase, present in large quantities in cancerous cells, rebuilds the telomeres, allowing division to continue indefinitely.

</doc>
<doc id="36871" url="http://en.wikipedia.org/wiki?curid=36871" title="Show jumping">
Show jumping

Show jumping, also known as "stadium jumping", "open jumping", or "jumpers", is a member of a family of English riding equestrian events that also includes dressage, eventing, hunters, and equitation. Jumping classes are commonly seen at horse shows throughout the world, including the Olympics. Sometimes shows are limited exclusively to jumpers, sometimes jumper classes are offered in conjunction with other English-style events, and sometimes show jumping is but one division of very large, all-breed competitions that include a very wide variety of disciplines. Jumping classes may be governed by various national horse show sanctioning organizations, such as the United States Equestrian Federation in the USA. International competitions are governed by the rules of the International Federation for Equestrian Sports (FEI, from the body's French name of "Fédération Équestre Internationale").
Hunters or jumpers.
People unfamiliar with horse shows may be confused by the difference between hunter classes and jumper classes. Hunters are judged subjectively on the degree to which they meet an ideal standard of manners, style, and way of going. Conversely, jumper classes are scored objectively, based entirely on a numerical score determined only by whether the horse attempts the obstacle, clears it, and finishes the course in the allotted time. Jumper courses often are colorful, and at times, quite creatively designed. Jumper courses tend to be much more complex and technical than hunter courses, because riders and horses are not being judged on style. Hunters have meticulous turnout and tend toward very quiet, conservative horse tack and rider attire. Hunter bits, bridles, crops, spurs, and martingales are tightly regulated. Jumpers, while caring for their horses and grooming them well, are not scored on turnout, are allowed a wider range of equipment, and riders may wear less conservative attire, so long as it stays within the rules. Formal turnout always is preferred; a neat rider gives a good impression at shows.
In addition to hunters and jumpers, there are equitation classes, sometimes called hunt seat equitation, which judges the ability of the rider. The equipment, clothing, and fence styles used in equitation more closely resemble hunter classes, although the technical difficulty of the courses may more closely resemble jumping events.
Courses and rules.
Jumper classes are held over a course of show jumping obstacles, including verticals, spreads, double and triple combinations, usually with many turns and changes of direction. The intent is to jump cleanly over a set course within an allotted time. Time faults are assessed for exceeding the time allowance. Jumping faults are incurred for knockdowns and blatant disobedience, such as refusals (when the horse stops before a fence or "runs out") ("see" "Modern Rules" "below"). Horses are allowed a limited number of refusals before being disqualified. A refusal may lead to a rider exceeding the time allowed on course. Placings are based on the lowest number of points or "faults" accumulated. A horse and rider who have not accumulated any jumping faults or penalty points are said to have scored a "clear round." Tied entries usually have a jump-off over a raised and shortened course, and the course is timed; if entries are tied for faults accumulated in the jump-off, the fastest time wins.
In most competitions, riders are allowed to walk the initial course, but not the jump-off course (usually the same course with missing jumps, e.g., 1, 3, 5, 7, 8 instead of 1, 2, 3, 4, 5, 6, 7, 8, 9) before competition to plan their ride. Walking the course before the event is a chance for the rider to walk the lines he or she will have to ride, in order to decide how many strides the horse will need to take between each jump and from which angle. Going off course will cost time if minor errors are made and major departures may result in disqualification.
The higher levels of competition, such as "A" or "AA" rated shows in the United States, or the international "Grand Prix" circuit, present more technical and complex courses. Not only is the height and width ("spread") of an obstacle increased to present a greater challenge, technical difficulty also increases with tighter turns and shorter or unusual distances between fences. Horses sometimes also have to jump fences from an angle rather than straight on. For example, a course designer might set up a line so that there are six and a half strides (the standard measure for a canter stride is twelve feet) between the jumps, requiring the rider to adjust the horse's stride dramatically in order to make the distance.
Unlike show hunter classes, which reward calmness and style, jumper classes require boldness, scope, power, accuracy, and control; speed also is a factor, especially in jump-off courses and speed classes (when time counts even in the first round). A jumper must jump big, bravely, and fast, but also must be careful and accurate to avoid knockdowns and must be balanced and rideable in order to rate and turn accurately. The rider must choose the best line to each fence, saving ground with well-planned turns and lines and must adjust the horse's stride for each fence and distance. In a jump-off, a rider must balance the need to go as fast as possible and turn as tightly as possible against the horse's ability to jump cleanly with good scope.
History.
Show jumping is a relatively new equestrian sport. Until the Inclosure Acts, which came into force in England in the 18th century, there had been little need for horses to jump fences routinely, but with this act of Parliament came new challenges for those who followed fox hounds. The Inclosure Acts brought fencing and boundaries to many parts of the country as common ground was dispersed amongst wealthy landowners. This meant that those wishing to pursue their sport now needed horses that were capable of jumping these obstacles.
In the early horse shows held in France, there was a parade of competitors who then took off across country for the jumping. This sport was, however, not popular with spectators as they could not follow to watch the jumping. Thus, it was not long before fences began to appear in an arena for the competitions. This became known as "Lepping". 1869 was the year ‘horse leaping’ came to prominence at Dublin horse show. Fifteen years later, "Lepping" competitions were brought to Britain and by 1900 most of the more important shows had "Lepping" classes. Separate classes were held for women riding sidesaddle.
At this time, the principal cavalry schools of Europe at Pinerolo and Tor-di-Quinto in Italy, the French school in Saumur, and the Spanish school in Vienna all preferred to use a very deep seat with long stirrups when jumping. While this style of riding may have felt more secure for the rider, it also impeded the freedom of the horse to use its body to the extent needed to clear large obstacles.
An Italian riding instructor, Captain Federico Caprilli, heavily influenced the world of jumping with his ideas promoting a forward position with shorter stirrups. This style placed the rider in a position that did not interfere with the balance of the horse while negotiating obstacles. This style, now known as the forward seat, is commonly used today. The deep, Dressage-style seat, while useful for riding on the flat and in conditions where control of the horse is of greater importance than freedom of movement, is less suitable for jumping.
The first major show jumping competition held in England was at Olympia in 1907. Most of the competitors were members of the military and it became clear at this competition and in the subsequent years, that there was no uniformity of rules for the sport. Judges marked on their own opinions. Some marked according to the severity of the obstacle and others marked according to style. Before 1907 there were no penalties for a refusal and the competitor was sometimes asked to miss the fence to please the spectators. The first courses were built with little imagination; many consisting of only a straight bar fence and a water jump. A meeting was arranged in 1923 which led to the formation of the BSJA in 1925. In the United States, a similar need for national rules for jumping and other equestrian activities led to the formation of the American Horse Shows Association in 1917, which now is known as the United States Equestrian Federation.
An early form of show jumping first was incorporated into the Olympic Games in 1900. Show jumping in its current format appeared in 1912, and has thrived ever since, its recent popularity due in part to its suitability as a spectator sport which is well adapted for viewing on television.
Original scoring tariff.
The original list of faults introduced in Great Britain in 1925 was as follows:
Water jumps were once at least fifteen feet (5 m) wide, although the water often had drained out of them by the time the last competitor jumped. High jumping would start with a pole at around five feet high, but this was later abandoned, as many horses went under the pole. It was for this reason that more poles were added and fillers came into use. Time penalties were not counted until 1917.
Modern rules.
Rules have evolved since then, with different national federations having different classes and rules. The international governing body for most major show jumping competitions is the Fédération Équestre Internationale (FEI). The two most common types of penalties are jumping penalties and time penalties.
Tack.
Show jumping competitors use a very forward style of English saddle, most often the "close contact" design, which has a forward flap and a seat and cantle that is flatter than saddles designed for general all-purpose English riding or dressage. This construction allows greater freedom of movement for the rider when in jumping position, and allows a shorter stirrup, allowing the rider to lighten the seat on the horse. Other saddles, such as those designed for dressage, are intended for riders with a deep seat, can hinder a rider over large fences, forcing them into a position that limits the horse's movement and may put the rider dangerously behind the movement of the horse.
At international levels, saddle pads are usually white and square in shape, allowing the pair to display a sponsorship, national flag, or breeding affiliation. In contrast, riders in show hunters and equitation often use "fitted" fleece pads that are the same shape as the saddle. Girths vary in type, but usually have a contour to give room for the horse's elbows, and many have belly guards to protect the underside of the horse from its shoe studs when the front legs are tightly folded under.
Bridles may be used with any style of cavesson noseband, and there are few rules regarding the severity of this equipment. The figure-8 cavesson is the most popular type. Bits may also vary in severity, and competitors may use any bit, or even a "bitless bridle" or a mechanical hackamore. The ground jury at the show has the right, however, based on veterinary advice, to refuse a bit or bridling scheme if it could cause harm to the horse.
Boots and wraps are worn by almost all horses, due to the fact that they may easily injure their legs when landing or when making tight turns at speed. Open-fronted tendon boots usually are worn on the forelegs, because they provide protection for the delicate tendons that run down the back of the leg, but still allow the horse to feel a rail should it get careless and hang its legs. Fetlock boots are sometimes seen on the rear legs, primarily to prevent the horse from hitting itself on tight turns.
Martingales are very common, especially on horses used at the Grand Prix level. The majority of jumpers are ridden in running martingales, as these provide the most freedom over fences. Although a standing martingale (a strap connecting directly to the horse's noseband) is commonly seen on show hunters and may be helpful in keeping a horse from throwing its head up, it also may be quite dangerous in the event of a stumble, restricting a horse from using its head to regain its balance. For this reason, standing martingales are not used in show jumping or eventing. Breastplates also are common, used to keep the saddle in place as the horse goes over large fences.
Rider attire.
Rider attire may be somewhat less formal than that used in hunter riding. An approved ASTM/SEI equestrian helmet with a harness is always required, however, and is a practical necessity to protect the rider's head in the event of a fall. Tall boots are required, usually black. Spurs are optional, but commonly used. Breeches are traditional in color, usually white, tan, or beige. At approved competitions, depending on sanctioning organization, a dark-colored coat usually is worn (although under the rules of the USEF tweed or wash jackets are allowed in the summer and lighter colors are currently in fashion), with a light-colored (usually white) ratcatcher-style shirt and either a choker or stock tie. In hot summer weather, many riders wear a simple short-sleeved "polo" style shirt with helmet, boots and breeches, and even where coats are required, the judges may waive the coat rule in extremely hot weather. Gloves, usually black, are optional, as is the plaiting of the horse's mane and tail.
At FEI Grand Prix levels, dress is more strictly controlled. Riders must wear white or light-colored shirts, white ties or chokers, black or brown boots, white or light fawn breeches, and red or black jackets. Members of the military, police forces, and national studs, however, retain the right to wear their service uniforms instead of FEI-prescribed dress. In some circumstances, members of international teams may wear jackets in their country's respective colors or add national insignia.
Types of show jumps.
Show jumping fences often are colorful, sometimes very elaborate and artistic in design, particularly at the highest levels of competition. Types of jumps used include the following: 
At international level competitions that are governed by FEI rules, fence heights begin at 1.50 m. Other competition levels are given different names in different nations, but are based primarily on the height and spread of fences
In the United States, jumping levels range from 0–9 as follows:
USEF Jumper Levels
In Germany, competition levels are denoted by the letters E, A, L, M, S, and correspond to heights ranging from 0.80 to 1.55 meters.
The horses.
A show jumper must have the scope and courage to jump large fences as well as the athletic ability to handle the sharp turns and bursts of speed necessary to navigate the most difficult courses. Many breeds of horses have been successful show jumpers, and even some grade horses of uncertain breeding have been champions. Most show jumpers are tall horses, over  hands , usually of Warmblood or Thoroughbred breeding, though horses as small as  hands have been on the Olympic teams of various nations and carried riders to Olympic and other international medals. There is no correlation between the size of a horse and its athletic ability, nor do tall horses necessarily have an advantage when jumping. Nonetheless, a taller horse may make a fence appear less daunting to the rider.
Ponies also compete in show jumping competitions in many countries, usually in classes limited to youth riders, defined as those under the age of 16 or 18 years, depending on the sanctioning organization. Pony-sized horses may, on occasion, compete in open competition with adult riders. The most famous example was Stroller, who only stood  hands but was nonetheless an Individual silver medal winner and part the Great Britain show jumping team in the 1968 Summer Olympics, jumping one of the few clean rounds in the competition. Significant jumpers from the United States are included in the Show Jumping Hall of Fame.

</doc>
<doc id="36877" url="http://en.wikipedia.org/wiki?curid=36877" title="Bucharest">
Bucharest

Bucharest (; Romanian: "București", ]) is the capital municipality, cultural, industrial, and financial centre of Romania. It is the largest city in Romania and located in the southeast of the country, at , lies on the banks of the Dâmbovița River, less than 70 km north of the Danube River.
Bucharest was first mentioned in documents in 1459. It became the capital of Romania in 1862 and is the centre of Romanian media, culture and art. Its architecture is a mix of historical (neo-classical), interbellum (Bauhaus and art deco), communist-era and modern. In the period between the two World Wars, the city's elegant architecture and the sophistication of its elite earned Bucharest the nickname of "Little Paris" ("Micul Paris"). Although buildings and districts in the historic city centre were heavily damaged or destroyed by war, earthquakes, and above all Nicolae Ceaușescu's program of systematization, many survived. In recent years, the city has been experiencing an economic and cultural boom.
According to 2011 census, 1,883,425 inhabitants live within the city limits, a decrease from the figure recorded at the 2002 census. The urban area extends beyond the limits of Bucharest proper and has a population of about 1.9 million people. Adding the satellite towns around the urban area, the proposed metropolitan area of Bucharest would have a population of 2.27 million people. According to Eurostat, Bucharest has a larger urban zone of 2,151,880 residents. According to unofficial data, the population is more than 3 million. Bucharest is the 6th largest city in the European Union by population within city limits, after London, Berlin, Madrid, Rome, and Paris.
Economically, Bucharest is the most prosperous city in Romania and is one of the main industrial centres and transportation hubs of Eastern Europe. The city has big convention facilities, educational institutes, cultural venues, traditional "shopping arcades" and recreational areas.
The city proper is administratively known as the "Municipality of Bucharest" ("Municipiul București"), and has the same administrative level as that of a national county, being further subdivided into six sectors, each governed by a local mayor.
Etymology.
The name of "București" has an uncertain origin: tradition connects the founding of Bucharest with the name of "Bucur" who was either a prince, an outlaw, a fisherman, a shepherd, or a hunter, according to different legends. In Romanian the word stem "bucurie" means 'joy', ("happiness") and it is believed to be of Dacian origin.
There are other etymologies given by early scholars, including the one of an Ottoman traveler, Evliya Çelebi, who said that Bucharest was named after a certain "Abu-Kariș", from the tribe of "Bani-Kureiș". In 1781, Franz Sulzer claimed that it was related to "bucurie" (joy), "bucuros" (joyful) or "a se bucura" (to become joyful), while an early 19th-century book published in Vienna assumed its name has been derived from "Bukovie", a beech forest.
The official city name in full is "The Municipality of Bucharest" (Romanian: "Municipiul București").
A native or resident of Bucharest is called a "Bucharester" (Romanian: "bucureștean").
History.
Bucharest's history alternated periods of development and decline from the early settlements in antiquity until its consolidation as the national capital of Romania late in the 19th century.
First mentioned as the "Citadel of București" in 1459, it became the residence of the famous Wallachian prince Vlad III the Impaler.:23
The Ottomans appointed Greek administrators (Phanariotes) to run the town from the 18th century. A short-lived revolt initiated by Tudor Vladimirescu in 1821 led to the end of the rule of Constantinople Greeks in Bucharest.
The Old Princely Court ("Curtea Veche") was erected by Mircea Ciobanul in the mid-16th century. Under subsequent rulers, Bucharest was established as the summer residence of the royal court. During the years to come it competed with Târgoviște on the status of capital city after an increase in the importance of southern Muntenia brought about by the demands of the suzerain power – the Ottoman Empire.
Bucharest became finally the permanent location of the Wallachian court after 1698 (starting with the reign of Constantin Brâncoveanu).
Partly destroyed by natural disasters and rebuilt several times during the following 200 years, and hit by "Caragea's plague" in 1813–14, the city was wrested from Ottoman control and occupied at several intervals by the Habsburg Monarchy (1716, 1737, 1789) and Imperial Russia (three times between 1768 and 1806). It was placed under Russian administration between 1828 and the Crimean War, with an interlude during the Bucharest-centred 1848 Wallachian revolution. Later on an Austrian garrison took possession after the Russian departure (remaining in the city until March 1857). On 23 March 1847, a fire consumed about 2,000 buildings, destroying a third of the city.
In 1862, after Wallachia and Moldavia were united to form the Principality of Romania, Bucharest became the new nation's capital city. In 1881, it became the political centre of the newly proclaimed Kingdom of Romania under King Carol I. During the second half of the 19th century the city's population increased dramatically, and a new period of urban development began. During this period, gas lighting, horse-drawn trams and limited electrification were introduced. The Dâmbovița river was also massively channelled in 1883, thus putting a stop to previously endemic floods like the 1865 flooding of Bucharest. The Fortifications of Bucharest were built. The extravagant architecture and cosmopolitan high culture of this period won Bucharest the nickname of "Little Paris" ("Micul Paris") of the east, with Calea Victoriei as its Champs-Élysées.
Between 6 December 1916 and November 1918, the city was occupied by German forces as a result of the Battle of Bucharest, with the official capital temporarily moved to Iași, in the Moldavia region. After World War I, Bucharest became the capital of Greater Romania. In the interwar years continued its urban development, with the city gaining an average of 30,000 new residents each year. Also, some of the city's main landmarks were built in this period, including Arcul de Triumf and Palatul Telefoanelor. However, the Great Depression took its toll on Bucharest's citizens, culminating in the Grivița Strike of 1933.
In January 1941, the city was the scene of the Legionnaires' rebellion and Bucharest pogrom. As capital of an Axis country and a major transit point for Axis troops en route to the Eastern Front, Bucharest suffered heavy damage during World War II due to Allied bombings. On 23 August 1944 it was the site of the royal coup which brought Romania into the Allied camp, suffering a short period of Nazi Luftwaffe bombings as well as a failed attempt by German troops to regain the city by force.
After the establishment of communism in Romania, the city continued growing. New districts were constructed, most of them dominated by tower blocks. During Nicolae Ceaușescu's leadership (1965–89), much of the historic part of the city was demolished and replaced by "Socialist realism" style development: (1) the Centrul Civic (the Civic Centre); (2) the Palace of the Parliament, where an entire historic quarter was razed to make way for Ceaușescu's megalomaniac plans. On 4 March 1977, an earthquake centered in Vrancea, about 135 km away, claimed 1,500 lives and caused further damage to the historic centre.
The Romanian Revolution of 1989 began with massive anti-Ceaușescu protests in Timișoara in December 1989 and continued in Bucharest, leading to the overthrow of the Communist regime. Dissatisfied with the post-revolutionary leadership of the National Salvation Front, some student leagues and opposition groups organized large-scale protests in 1990 (the "Golaniad"), which were violently repressed by the miners of Valea Jiului called in by the authorities (the "Mineriad"). Several other "Mineriads" followed, which finally caused political changes.
Since 2000, the city has been continuously modernized and is still undergoing urban renewal. Residential and commercial developments are underway, particularly in the northern districts, and Bucharest's old historic centre is being restored.
Geography.
General.
Bucharest is situated on the banks of the Dâmbovița River, which flows into the Argeș River, a tributary of the Danube. Several lakes – the most important of which are Lake Herăstrău, Lake Floreasca, Lake Tei, and Lake Colentina – stretch across the northern parts of the city, along the Colentina River, a tributary of the Dâmbovița. In addition, in the centre of the capital there is a small artificial lake – Lake Cișmigiu – surrounded by the Cișmigiu Gardens. The Cișmigiu Gardens have a rich history, being frequented by poets and writers. Opened in 1847 and based on the plans of German architect Carl F.W. Meyer, the gardens are the main recreational facility in the city centre.
Besides Cișmigiu, Bucharest parks and gardens include Herăstrău Park and the Botanical Garden. Herăstrău Park is located in the northern part of the city, around Lake Herăstrău, and includes the site the Village Museum. The Botanical Garden, located in the Cotroceni neighborhood a bit west of the city centre, is the largest of its kind in Romania and contains over 10,000 species of plants (many of them exotic); it originated as the pleasure park of the royal family.
Bucharest is situated in the south eastern corner of the Romanian Plain, in an area once covered by the Vlăsiei forest, which, after it was cleared, gave way for a fertile flatland. As with many cities, Bucharest is traditionally considered to be built upon seven hills, similar to the seven hills of Rome. Bucharest's seven hills are: Mihai Vodă, Dealul Mitropoliei, Radu Vodă, Cotroceni, Spirei, Văcărești and Sf. Gheorghe Nou.
The city has an area of 226 km2. The altitude varies from 55.8 m at the Dâmbovița bridge in Cățelu, south-eastern Bucharest and 91.5 m at the Militari church. The city has an approximately round shape, with the centre situated in the cross-way of the main north-south/east-west axes at University Square. The milestone for Romania's Kilometre Zero is placed just south of University Square in front of the New St. George Church (Sfântul Gheorghe Nou) at St. George Square (Piața Sfântul Gheorghe). Bucharest's radius, from University Square to the city limits in all directions, varies from about 10 to.
Until recently, the regions surrounding Bucharest were largely rural, but after 1989, suburbs started to be built around Bucharest, in the surrounding Ilfov county. Further urban consolidation is expected to take place in the late 2010s, when the "Bucharest Metropolitan Area" plan will become operational, incorporating additional communes and cities from the Ilfov and other neighbouring counties.
Climate.
Bucharest has a Humid continental climate "Dfa". Owing to its position on the Romanian Plain, the city's winters can get windy, even though some of the winds are mitigated due to urbanisation. Winter temperatures often dip below 0 °C, sometimes even to -20 °C. In summer, the average temperature is 23 °C (the average for July and August) Temperatures frequently reach 35 to in mid-summer in the city centre. Although average precipitation and humidity during summer are low, there are occasional heavy storms. During spring and autumn, daytime temperatures vary between 17 to, and precipitation during spring tends to be higher than in summer, with more frequent yet milder periods of rain.
Law and government.
Administration.
Bucharest has a unique status in Romanian administration, since it is the only municipal area that is not part of a county. Its population, however, is larger than that of any other Romanian county, hence the power of the Bucharest General Municipality ("Primăria Generală"), which is the capital's local government body, is the same as any other Romanian County Council.
The city government is headed by a general mayor (Primar General), as of 2008 Sorin Oprescu. Decisions are approved and discussed by the capital's General Council (Consiliu General) made up of 55 elected councilors. Furthermore, the city is divided into six administrative sectors (sectoare), each of which has their own 27-seat sectoral council, town hall and mayor. The powers of the local government over a certain area are therefore shared both by the Bucharest Municipality and the local sectoral councils with little or no overlapping of authority. The general rule is that the main Capital Municipality is responsible for citywide utilities such as the water and sewage system, the overall transport system and the main boulevards, while sectoral town halls manage the contact between individuals and the local government, secondary streets and parks maintenance, schools administration and cleaning services.
The six sectors are numbered from one to six and are disposed radially so that each one has under its administration a certain area of the city centre. They are numbered clockwise and are further divided into sectoral quarters ("cartiere") which are not part of the official administrative division:
Like all other local councils in Romania, the Bucharest sectoral councils, the capital's General Council and the mayors are elected every four years by the population. Additionally, Bucharest has a prefect, who is appointed by Romania's national government. The prefect is not allowed to be a member of a political party and his role is to represent the national government at the municipal level. The prefect is acting as a liaison official facilitating the implementation of National Development Plans and governing programs at local level. The prefect of Bucharest (as of 2012) is Georgeta Gavrilă.
The Municipality of Bucharest, along with the surrounding Ilfov County and several other neighbouring counties are part of the Bucharest development region project, which is equivalent to NUTS-II regions in the European Union and is used both by the Union and the Romanian government for statistical analysis and regional development planning. The Bucharest development region is not, however, an administrative entity yet.
Justice system.
Bucharest's judicial system is similar to that of the Romanian counties. Each of the six sectors has its own local first instance court ("judecătorie"), while more serious cases are directed to the Bucharest Tribunal (Tribunalul Bucureşti), the city's municipal court. The Bucharest Court of Appeal (Curtea de Apel Bucureşti) judges appeals against decisions taken by first instance courts and tribunals in Bucharest and in five surrounding counties (Teleorman, Ialomița, Giurgiu, Călărași and Ilfov). Bucharest is also home to Romania's supreme court, the High Court of Cassation and Justice, as well as to the Constitutional Court of Romania.
Bucharest has a municipal police force, the Bucharest Police ("Poliția București"), which is responsible for policing of crime within the whole city, and operates a number of divisions. The Bucharest Police are headquartered on Ștefan cel Mare Blvd. in the city centre, and at precincts throughout the city. From 2004 onwards, each Sector City Hall also has under its administration a Community Police force ("Poliția Comunitară"), dealing with local community issues. Bucharest also houses the General Inspectorates of the Gendarmerie and the National Police.
Crime.
Bucharest's crime rate is rather low in comparison to other European capital cities, with the number of total offenses declining by 51% between 2000 and 2004, and by 7% between 2012 and 2013. The violent crime rate in Bucharest remains very low, with 11 murders and 983 other violent offenses taking place in 2007. Although violent crimes fell by 13% in 2013 compared to 2012, there were 19 recorded murders (18 of which the suspects were arrested).
Although in the 2000s, there were a number of police crackdowns on organized crime gangs, such as the Cămătaru clan, organized crime generally has little impact on public life. Petty crime, however, is more common, particularly in the form of pickpocketing, which occurs mainly on the city's public transport network. Confidence tricks were common in the 1990s, especially in regards to tourists, but the frequency of these incidents has since declined. However, in general, theft was reduced by 13.6% in 2013 compared to 2012. Levels of crime are higher in the southern districts of the city, particularly in Ferentari, a socially disadvantaged area.
Although the presence of street children was a problem in Bucharest in the 1990s, their numbers have declined in recent years, now lying at or below the average of major European capital cities. A documentary called Children Underground depicted the life of Romanian street kids in 2001. There are still an estimated 1,000 street children in the city, some of whom engage in petty crime and begging.
Quality of life.
As stated by the Mercer international surveys for quality of life in cities around the world, Bucharest occupied the 94th place in 2001 and slipped lower, to the 108th place in 2009 and the 107th place in 2010. Compared to it, Vienna occupied No. 1 worldwide in 2011 and 2009. Budapest ranked 73rd (2010) and Sofia 114th (2010). Mercer Human Resource Consulting issues yearly a global ranking of the world's most livable cities based on 39 key quality-of-life issues. Among them: political stability, currency-exchange regulations, political and media censorship, school quality, housing, the environment, public safety. Mercer collects data worldwide, in 215 cities. The difficult situation of the quality of life in Bucharest is confirmed also by a vast urbanism study, done by the Ion Mincu University of Architecture and Urbanism.
Demographics.
As per the 2011 census, 1,883,425 inhabitants live within the city limits, a decrease from the figure recorded at the 2002 census. This decrease is due to low natural increase, but also to a shift in population from the city itself to neighboring small towns like Voluntari, Buftea or Otopeni. In a study published by the United Nations, Bucharest placed 19th in among 28 cities that recorded sharp declines in population from 1990 to the mid-2010s. In particular, the population fell by 3.77%.
The city's population, according to the 2002 census, was 1,926,334 inhabitants, or 8.9% of the total population of Romania. A significant number of people commute to the city every day, mostly from the surrounding Ilfov county, however official statistics regarding their numbers do not exist.
Bucharest's population experienced two phases of rapid growth, the first beginning in the late 19th century when the city was consolidated as the national capital and lasting until the Second World War, and the second during the Ceaușescu years (1965–1989), when a massive urbanization campaign was launched and many people migrated from rural areas to the capital. At this time, due to Ceaușescu's decision to ban abortion and contraception, natural increase was also significant.
Approximately 96.6% of the population of Bucharest are Romanians. Other significant ethnic groups are Roma Gypsies, Hungarians, Jews, Turks, Chinese and Germans. A relatively small number of Bucharesters are of Greek, North American, French, Armenian, Lippovan and Italian descent. One of the predominantly Greek neighborhoods was Vitan – where a Jewish population also lived; the latter was more present in Văcărești and areas around Unirii Square.
In terms of religious affiliation, 96.1% of the population are Romanian Orthodox, 1.2% are Roman Catholic, 0.5% are Muslim and 0.4% are Romanian Greek Catholic. Despite this, only 18% of the population, of any religion, attend a place of worship once a week or more. The life expectancy of residents of Bucharest in 2003–2005 was 74.14 years, around 2 years higher than the Romanian average. Female life expectancy was 77.41 years, in comparison to 70.57 years for males.
Economy.
Bucharest is the center of the Romanian economy and industry, accounting for around 22.7% (2010) of the country's GDP and about one-quarter of its industrial production, while being inhabited by 9% of the country's population. Almost one third of national taxes are paid by Bucharest's citizens and companies. In 2011, at purchasing power parity, Bucharest had a per-capita GDP of €30,700, or 122% that of the European Union average and more than twice the Romanian average. After relative stagnation in the 1990s, the city's strong economic growth has revitalized infrastructure and led to the development of shopping malls, residential estates and high-rise office buildings. In January 2013, Bucharest had an unemployment rate of 2.1%, significantly lower than the national unemployment rate of 5.8%.
Bucharest's economy is centered on industry and services, with services particularly growing in importance in the last ten years. The headquarters of 186,000 firms, including nearly all large Romanian companies are located in Bucharest. An important source of growth since 2000 has been the city's rapidly expanding property and construction sector. Bucharest is also Romania's largest centre for information technology and communications and is home to several software companies operating offshore delivery centres.
Romania's largest stock exchange, the Bucharest Stock Exchange, which was merged in December 2005 with the Bucharest-based electronic stock exchange Rasdaq, plays a major role in the city's economy.
There are international supermarket chains such as Carrefour, Cora and METRO operating in Bucharest. The city is undergoing a retail boom, with supermarkets and hypermarkets opened every year (see supermarkets in Romania). Bucharest hosts a lot of luxury brands such as Louis Vuitton, Hermes, Gucci, Armani, Hugo Boss, Prada, Calvin Klein, Rolex, Burberry and many others. Malls and large shopping centres have been built since the late 1990s, such as AFI Palace Cotroceni, Sun Plaza, Băneasa Shopping City, Plaza Romania, Unirea Shopping Center and Liberty Center. There are traditional retail arcades and markets such as the one at Obor.
Transport.
Public transport.
Bucharest's public transport system is the largest in Romania and one of the largest in Europe. It is made up of the Bucharest Metro, run by Metrorex, as well as a surface transport system run by RATB (Regia Autonomă de Transport București), which consists of buses, trams, trolleybuses, and light rail. In addition, there is a private minibus system. As of 2007, there is a limit of 10,000 taxicab licenses.
Railways.
Bucharest is the hub of Romania's national railway network, run by "Căile Ferate Române". The main railway station is Gara de Nord ("North Station"), which provides connections to all major cities in Romania as well as international destinations: Belgrade, Sofia, Varna, Chișinău, Kiev, Chernivtsi, Lviv, Thessaloniki, Vienna, Budapest, Istanbul, Moscow, etc.
The city has five other railway stations run by CFR, of which the most important are Basarab (adjacent to North Station), Obor, Băneasa and Progresu. These are in the process of being integrated into a commuter railway serving Bucharest and the surrounding Ilfov County. Seven main lines radiate out of Bucharest.
Air.
Bucharest has two international airports:
Roads.
Bucharest is a major intersection of Romania's national road network. A few of the busiest national roads and motorways, link the city to all of Romania's major cities as well as to neighbouring countries such as Hungary, Bulgaria and Ukraine. The A1 to Pitești, the A2 Sun Motorway to the Dobrogea region and Constanta and the A3 to Ploieşti all start from Bucharest.
The city's municipal road network is centred around a series of high-capacity boulevards, which generally radiate out from the city centre to the outskirts. The main axes, which run north-south, east-west and northwest-southeast, as well as one internal and one external ring road, support the bulk of the traffic. The city's roads are usually very crowded during rush hours, due to an increase in car ownership in recent years. In 2013, the number of cars registered in Bucharest amounted to 1,125,591. This results in wear and potholes appearing on busy roads, particularly secondary roads, this being identified as one of Bucharest's main infrastructural problems. There has been a comprehensive effort on behalf of the City Hall to boost road infrastructure and according to the general development plan, 2,000 roads have been repaired by 2008. On 17 June 2011, the Basarab Overpass was inaugurated and opened to traffic, thus completing the inner city traffic ring. The overpass took 5 years to build and is the longest cable-stayed bridge in Romania and the widest such bridge in Europe; upon completion, traffic on the Grant Bridge and in the Gara de Nord area became noticeably more fluid.
Water.
Although it is situated on the banks of a river, Bucharest has never functioned as a port city, with other Romanian cities such as Constanța and Galați acting as the country's main ports. The unfinished Danube-Bucharest Canal, which is 73 km long and approximately 70% completed, could link Bucharest to the Danube River and, via the Danube-Black Sea Canal, to the Black Sea. Works on the canal were suspended in 1989, but there have been proposals to resume construction as part of the European Strategy for the Danube Region.
Culture.
Bucharest has a growing cultural scene, in fields including the visual arts, performing arts and nightlife. Unlike other parts of Romania, such as the Black Sea coast or Transylvania, Bucharest's cultural scene has no defined style, and instead incorporates elements of Romanian and international culture.
Landmarks.
Bucharest has landmark buildings and monuments. Perhaps the most prominent of these is the Palace of the Parliament, built in the 1980s during the reign of Communist dictator Nicolae Ceaușescu. The largest Parliament building in the world, the Palace houses the Romanian Parliament (the Chamber of Deputies and the Senate), as well as the National Museum of Contemporary Art. The building boasts one of the largest convention centres in the world.
Another landmark in Bucharest is Arcul de Triumf (The Triumphal Arch), built in its current form in 1935 and modeled after the Arc de Triomphe in Paris. A newer landmark of the city is the Memorial of Rebirth, a stylized marble pillar unveiled in 2005 to commemorate the victims of the Romanian Revolution of 1989, which overthrew Communism. The abstract monument sparked controversy when it was unveiled, being dubbed with names such as "the olive on the toothpick", ("măslina-n scobitoare"), as many argued that it does not fit in its surroundings and believed that its choice was based on political reasons.
The Romanian Athenaeum building is considered to be a symbol of Romanian culture and since 2007 is on the list of the Label of European Heritage sights.
InterContinental Bucharest is a highrise five star hotel situated near University Square and is also a landmark of the city. The building is designed so that each room has a unique panorama of the city.
Other cultural venues include the National Museum of Art of Romania, Museum of Natural History "Grigore Antipa", Museum of the Romanian Peasant ("Muzeul țăranului Român"), National History Museum, and the Military Museum.
Visual arts.
In terms of visual arts, the city has museums featuring both classical and contemporary Romanian art, as well as selected international works. The National Museum of Art of Romania is perhaps the best-known of Bucharest museums. It is located in the royal palace and features collections of medieval and modern Romanian art, including works by sculptor Constantin Brâncuși, as well as an international collection assembled by the Romanian royal family.
Other, smaller, museums contain specialised collections. The Zambaccian Museum, which is situated in the former home of art collector Krikor H. Zambaccian contains works by well-known Romanian artists as well as international artists such as Paul Cézanne, Eugène Delacroix, Henri Matisse, Camille Pissarro and Pablo Picasso.
The Gheorghe Tattarescu Museum contains portraits of Romanian revolutionaries in exile such as Gheorghe Magheru, ștefan Golescu, Nicolae Bălcescu and allegorical compositions with revolutionary ("Romania's rebirth", 1849) and patriotic ("The Principalities' Unification", 1857) themes.
The Theodor Pallady Museum is situated in one of the oldest surviving merchant houses in Bucharest and includes works by Romanian painter Theodor Pallady as well as European and oriental furniture pieces.
The Museum of Art Collections contains the collections of Romanian art aficionados, including Krikor Zambaccian and Theodor Pallady.
Despite the classical art galleries and museums in the city, there is also a contemporary arts scene. The National Museum of Contemporary Art (MNAC), situated in a wing of the Palace of the Parliament, was opened in 2004 and contains Romanian and international contemporary art. The MNAC also manages the Kalinderu MediaLab, which caters to multimedia and experimental art. There are also private art galleries throughout the city centre.
The palace of the National Bank of Romania houses the national numismatic collection. Exhibits include banknotes, coins, documents, photographs, maps, silver and gold bullion bars, bullion coins, dies and moulds. The building was constructed between 1884 and 1890. The thesaurus room contains notable marble decorations.
Performing arts.
Performing arts are one of the strongest cultural elements of Bucharest. The most famous symphony orchestra is National Radio Orchestra of Romania. One of the most prominent buildings is the neoclassical Romanian Athenaeum, which was founded in 1852, and hosts classical music concerts, the George Enescu Festival, and is home to the George Enescu Philharmonic Orchestra.
Bucharest is home to the Romanian National Opera, as well as the I.L. Caragiale National Theatre. Another well-known theatre in Bucharest is the State Jewish Theatre, which features plays starring world-renowned Romanian-Jewish actress Maia Morgenstern. Smaller theatres throughout the city cater to specific genres, such as the Comedy Theatre, the Nottara Theatre, the Bulandra Theatre, the Odeon Theatre, and the revue theatre of Constantin Tănase.
Music and nightlife.
Bucharest is home to Romania's largest recording labels, and is often the residence of Romanian musicians. Romanian rock bands of the 1970s and 1980s, such as Iris and Holograf, continue to be popular, particularly with the middle-aged, while since the beginning of the 1990s the hip hop/rap scene has developed. Hip-hop bands and artists from Bucharest such as B.U.G. Mafia, Paraziții, La Familia enjoy national and international recognition.
The pop-rock band Taxi have been gaining international respect, as has Spitalul de Urgență's raucous updating of traditional Romanian music. While many neighbourhood discos play manele, an Oriental- and Roma-influenced genre of music that is particularly popular in Bucharest's working class districts, the city has a rich jazz and blues scene, and, to an even larger extent, house music/trance and heavy metal/punk scenes. Bucharest's jazz profile has especially risen since 2002, with the presence of two venues, Green Hours and Art Jazz, as well as an American presence alongside established Romanians.
There is no central nightlife strip, with entertainment venues dispersed throughout the city, with clusters in Lipscani and Regie. The city hosts some of the best electronic music clubs in Europe such as Kristal Glam Club and Studio Martin. Some other notable venues are Gaia, Bamboo, Fratelli, Kulturhaus and Fabrica.
Cultural events and festivals.
There are a number of cultural festivals in Bucharest throughout the year but most festivals take place in the summer months of June, July and August. The National Opera organises the International Opera Festival every year in May and June, which includes ensembles and orchestras from all over the world.
The Romanian Athaeneum Society hosts the George Enescu Festival at locations throughout the city in September every two years (odd years). The Museum of the Romanian Peasant and the Village Museum organise events throughout the year showcasing Romanian folk arts and crafts.
In the 2000s, due to the growing prominence of the Chinese community in Bucharest, Chinese cultural events took place. The first officially organised Chinese festival was the Chinese New Year's Eve Festival of February 2005 which took place in Nichita Stănescu Park and was organised by the Bucharest City Hall.
In 2005, Bucharest was the first city in Southeastern Europe to host the international CowParade, which resulted in dozens of decorated cow sculptures being placed across the city.
In 2004, Bucharest imposed in the circle of important festivals in Eastern Europe with "BIFF" (abbreviation for "Bucharest International Film Festival"), event widely acknowledged in Europe, having as guests of honor huge names from the world cinema: Andrei Konchalovsky, Danis Tanović, Nikita Mikhalkov, Rutger Hauer, Jerzy Skolimowski, Jan Harlan, Radu Mihăileanu and many others.
Since 2005 Bucharest has its own contemporary art biennale, the Bucharest Biennale.
Traditional culture.
Traditional Romanian culture continues to have a major influence in arts such as theatre, film and music. Bucharest has two internationally renowned ethnographic museums, the Museum of the Romanian Peasant and the open-air Village Museum.
The Dimitrie Gusti National Village Museum, in Herăstrău Park, contains 272 authentic buildings and peasant farms from all over Romania.
The Museum of the Romanian Peasant was declared the European Museum of the Year in 1996. Patronized by the Ministry of Culture, the museum preserves and exhibits numerous collections of objects and monuments of material and spiritual culture. The Museum of the Romanian Peasant holds one of the richest collections of peasant objects in Romania, its heritage being nearly 90,000 pieces, those being divided into several collections: ceramics, costumes, textiles, wooden objects, religious objects, customs etc.
The Museum of Romanian History is another important museum in Bucharest, containing a collection of artefacts detailing Romanian history and culture from the prehistoric times, Dacian era, medieval times and the modern era.
Religious life.
Bucharest is the seat of the Patriarch of the Romanian Orthodox Church, one of the Eastern Orthodox churches in communion with the Patriarch of Constantinople, and also of its subdivisions, the Metropolis of Muntenia and Dobrudja and the Archbishopric of Bucharest. Orthodox believers consider Demetrius Basarabov to be the patron saint of the city.
The city is a center for other religious organizations in Romania, including the Roman Catholic Archdiocese of Bucharest, established in 1883, and the Romanian Greek-Catholic Eparchy of Saint Basil the Great, founded in 2014.
Architecture.
The city centre is a mixture of medieval, neoclassical and art nouveau buildings, as well as 'neo-Romanian' buildings dating from the beginning of the 20th century and a collection of modern buildings from the 1920s and 1930s. The mostly utilitarian Communist-era architecture dominates most southern boroughs. Recently built contemporary structures such as skyscrapers and office buildings complete the landscape.
Historical architecture.
Of the city's medieval architecture, most of what survived into modern times was destroyed by Communist systematization, fire and military incursions. Some medieval and renaissance edifices remain, the most notable are in the Lipscani area. This precinct contains notable buildings such as Manuc's Inn ("Hanul lui Manuc") and the ruins of the Old Court ("Curtea Veche"), during the late Middle Ages this area was the heart of commerce in Bucharest. From the 1970s onwards, the area went through urban decline, and many historical buildings fell into disrepair. In 2005, the Lipscani area was pedestrianised and is undergoing restoration. 
The city centre has retained architecture from the late 19th and early 20th centuries, particularly the interwar period, which is often seen as the "golden age" of Bucharest architecture. During this time, the city grew in size and wealth therefore seeking to emulate other large European capitals such as Paris. Much of the architecture of the time belongs to a Modern (rationalist) Architecture current, led by Horia Creangă and Marcel Iancu.
In Romania, the tendencies of innovation in the architectural language met the need of valorisation and affirmation of the national cultural identity. The Art Nouveau movement finds expression through new architectural style initiated by Ion Mincu and taken over by other prestigious architects that capitalize important references of Romanian laic and medieval ecclesiastical architecture (for example the Mogoșoaia Palace, the Stavropoleos Church or the disappeared church of Văcărești Monastery) and Romanian folk motifs.
Two notable buildings from this time are the Crețulescu Palace, housing cultural institutions including UNESCO's European Centre for Higher Education, and the Cotroceni Palace, the residence of the Romanian President. Many large-scale constructions such as Gara de Nord, the busiest railway station in the city, National Bank of Romania's headquarters and the Telephone Palace date from these times. In the 2000s, historic buildings in the city centre underwent restoration. In some residential areas of the city, particularly in high-income central and northern districts, there are turn of the 20th century villas, most of which were restored beginning in the late 1990s.
Communist architecture.
A major part of Bucharest's architecture is made up of buildings constructed during the Communist era replacing the historical architecture with high density apartment blocks – significant portions of the historic center of Bucharest were demolished in order to construct one of the largest buildings in the world, the Palace of the Parliament (then officially called the House of the Republic). In Nicolae Ceaușescu's project of systematization new buildings were built in previously historical areas, which were razed and then built upon.
One of the singular examples of this type of architecture is Centrul Civic, a development that replaced a major part of Bucharest's historic city centre with giant utilitarian buildings, mainly with marble or travertine façades, inspired by North Korean architecture. Communist-era architecture can also be found in Bucharest's residential districts, mainly in "blocuri", which are high-density apartment blocks that house the majority of the city's population.
Contemporary architecture.
Since the fall of Communism in 1989, several Communist-era buildings have been refurbished, modernised and used for other purposes. Perhaps the best example of this is the conversion of obsolete retail complexes into shopping malls and commercial centres. These giant circular halls, which were unofficially called hunger circuses due to the food shortages experienced in the 1980s, were constructed during the Ceaușescu era to act as produce markets and refectories, although most were left unfinished at the time of the Revolution.
Modern shopping malls like Unirea Shopping Center, Bucharest Mall, Plaza Romania and City Mall emerged on pre-existent structures of former hunger circuses. Another example is the conversion of a large utilitarian construction in Centrul Civic into a Marriott Hotel. This process was accelerated after 2000, when the city underwent a property boom, and many Communist-era buildings in the city centre became prime real estate due to their location. Many Communist-era apartment blocks have also been refurbished to improve urban appearance.
The newest contribution to Bucharest's architecture took place after the fall of Communism, particularly after 2000, when the city went through a period of urban renewal – and architectural revitalization – on the back of Romania's economic growth. Buildings from this time are mostly made of glass and steel, and often have more than ten storeys. Examples include shopping malls (particularly the Bucharest Mall, a conversion and extension of an abandoned building), office buildings, bank headquarters, etc. 
As of 2005, there are office buildings under construction, particularly in the northern and eastern parts of the city. Additionally, there has been a trend to add modern wings and façades to historic buildings, the most prominent example of which is the Bucharest Architects' Association Building, which is a modern glass-and-steel construction built inside a historic stone façade. In 2013, the Bucharest skyline enriched with a 137 meters high office building ("SkyTower" of Floreasca City Center), currently the tallest building in Romania. Despite this development on vertical, the Romanian architects avoid designing tall buildings due to vulnerability to earthquakes.
Aside from buildings used for business and institutions, residential developments are underway, many of which consist of high-rise office buildings and suburban residential communities. These developments are increasingly prominent in northern Bucharest, which is less densely populated and is home to middle- and upper-class Bucharesters due to the process of gentrification.
Media.
Bucharest is headquarters of most of the national television networks as well as national newspapers, radio stations and online news websites. The largest daily newspapers in Bucharest include "Evenimentul Zilei", "Jurnalul Național", "Cotidianul", "România Liberă", "Adevărul", while the biggest news websites are Hotnews.ro (with an English and Spanish version), and "Gândul". During the rush hours, tabloid newspapers "Click!", "Libertatea" and "Cancan" are popular for commuters.
A number of newspapers and media publications are based in Casa Presei Libere (The House of the Free Press), a landmark of northern Bucharest, originally named Casa Scânteii after the Communist Romania-era official newspaper "Scînteia". Casa Presei Libere is not the only Bucharest landmark that grew out of the media and communications industry. Palatul Telefoanelor ("The Telephone Palace") was the first major modernist building on Calea Victoriei in the city's centre, and the massive, unfinished communist-era Casa Radio looms over a park a block away from the Opera.
English-language newspapers first became available in the early 1930s and reappeared in the 1990s. There are two daily English-language newspapers, "Bucharest Daily News" and "Nine O' Clock", as well as magazines. Publications in other languages are available, such as the Hungarian-language daily "Új Magyar Szó".
"Observator Cultural" covers the city's arts, and the free weekly magazines "șapte Seri" ("Seven Evenings") and "B24FUN", list entertainment events. The city is home to the intellectual journal "Dilema veche" and the satire magazine "Academia Cațavencu". Bucharest was the host city of the fourth edition of the Junior Eurovision Song Contest in 2006.
Education.
There are 16 public universities in Bucharest, the largest of which are the University of Bucharest, the Bucharest Academy of Economic Studies, the Carol Davila University of Medicine and Pharmacy, and the Politehnica University of Bucharest. These are supplemented by 19 private universities, such as the Romanian-American University and Spiru Haret University, the latter being the largest in Europe with some 302,000 enrolled students in 2009.
Overall, there are 159 faculties in 34 universities. Private universities, however, have a mixed reputation due to irregularities in the educational process as well as perceived corruption. As in the rest of Romania, universities in Bucharest are lower rated internationally, in comparison to their American and Western European counterparts.
Nevertheless, the University of Bucharest was included in the 2012 QS World University Rankings Top 200 universities of the world (151–200 band). Also, in recent years the city has seen increasing numbers of foreign students enrolling in its universities.
The first modern educational institution was the Princely Academy of Bucharest, founded in 1694 and divided in 1864 to form the present-day University of Bucharest and the Saint Sava National College, both of which are among the most prestigious of their kind in Romania.
There are over 450 public primary and secondary schools in the city, all of which are administered by the Bucharest Municipal Schooling Inspectorate. Each sector also has its own Schooling Inspectorate, subordinated to the municipal one.
Healthcare.
One of the most modern hospitals in the capital is Colțea that has been re-equipped after a 90-million-euro investment in 2011. It specializes in oncological and cardiac disorders. Also the oldest hospital in Bucharest, Coltea Hospital was built by Mihai Cantacuzino between 1701 and 1703, composed of many buildings, each with 12 to 30 beds, a church, three chapels, a school, and doctors' and teachers' houses.
Another conventional hospital is Pantelimon which was established in 1733 by Grigore II Ghica. The surface area of the hospital land property was 400000 m². The hospital had in its inventory a house for infectious diseases and a house for persons with disabilities.
Other hospitals or clinics are Bucharest Emergency Hospital, Floreasca Emergency Clinic Hospital, Bucharest University Emergency Hospital and Fundeni Clinical Institute or Biomedica International and Euroclinic, which are private.
Sports.
Football is the most widely followed sport in Bucharest, with the city having numerous club teams, some of them being known throughout Europe: Steaua, Dinamo or Rapid.
Arena Națională, a new stadium inaugurated on 6 September 2011, hosted the 2012 Europa League Final. and has a 55,600 seats capacity, making it one of the largest stadiums in Southeastern Europe.
There are sport clubs for ice hockey, rugby union, basketball, handball, water polo and volleyball. The majority of Romanian track and field athletes and most gymnasts are affiliated with clubs in Bucharest. The Athletics and many Gymnastics National Championships are held in Bucharest at the Polyvalent Hall, which is also used for other indoor sports such as volleyball and handball.
The largest indoor arena in Bucharest is the Romexpo Dome with a seating capacity of 10,000. It is used for tennis, boxing and kickboxing.
Starting in 2007 Bucharest has hosted annual races along a temporary urban track surrounding the Palace of the Parliament, called Bucharest Ring. The competition is called the Bucharest City Challenge, and has hosted FIA GT, FIA GT3, British F3, and Logan Cup races in 2007 and 2008. The 2009 and 2010 edition have not been held in Bucharest due to a lawsuit. Bucharest GP, owned by the controversial businessman Nicolae șerbu, won the lawsuit that it initiated and will host city races around the Parliament starting 2011 with the Auto GP.
Every year, Bucharest hosts BRD Năstase Țiriac Trophy international tennis tournament, which is included in the ATP Tour. The outdoors tournament is hosted by the tennis complex BNR Arenas. The ice hockey games are held at the Mihai Flamaropol Arena, which holds 8,000 spectators. The rugby games are held in different locations, but the most modern stadium is Arcul de Triumf Stadium, where also the Romanian national rugby team plays.
Twin towns and sister cities.
The twin towns and sister cities of Bucharest are listed below:

</doc>
<doc id="36947" url="http://en.wikipedia.org/wiki?curid=36947" title="Indemnity">
Indemnity

An indemnity is an obligation by a person (indemnitor) to provide compensation for a particular loss suffered by another person (indemnitee).
Indemnities form the basis of many insurance contracts; for example, a car owner may purchase different kinds of insurance as an indemnity for various kinds of loss arising from operation of the car, such as damage to the car itself, or medical expenses following an accident. In an agency context, a principal may be obligated to indemnify their agent for liabilities incurred while carrying out responsibilities under the relationship. While the events giving rise to an indemnity may be specified by contract, the actions that must be taken to compensate the injured party are largely unpredictable, and the maximum compensation is often expressly limited.
Common law.
Indemnity clauses.
Under section 4 of the Statute of Frauds (1677), a "guarantee" (an undertaking of secondary liability; to answer for another's default) must be evidenced in writing. No such formal requirement exists in respect of indemnities (involving the assumption of primary liability; to pay irrespective of another's default) which are enforceable even if made orally. (Ref: Peel E: "Treitel, The Law of Contract") 
Under current English law, indemnities must be clearly and precisely worded in the contract in order to be enforceable. Under the Unfair Contract Terms Act 1977 s4, a consumer cannot be made to unreasonably indemnify another for their breach of contract or negligence.
Contract award.
In England and Wales an "indemnity" monetary award may form part of rescission during an action of Restitutio in integrum. The property and funds are exchanged, but indemnity may be granted for costs necessarily incurred to the innocent party pursuant to the contract. The leading case is "Whittington v Seale-Hayne", in which a contaminated farm was sold. The contract made the buyers renovate the real estate and, the contamination incurred medical expenses for their manager, who had fallen ill. Once the contract was rescinded, the buyer could be indemnified for the cost of renovation as this was necessary to the contract, but not the medical expenses as the contract did not require them to hire a manager. Were the sellers at fault, damages would clearly be available.
The distinction between indemnity and damages is subtle may be differentiated by considering the roots of the law of obligations: how can money be paid where the defendant is not at fault? The contract before rescission is voidable but not void, so, for a period of time, there is a legal contract. During that time, both parties have legal obligation. If the contract is to be voided "ab initio" the obligations performed must also be compensated. Therefore, the costs of indemnity arise from the (transient and performed) obligations of the claimant rather than a breach of obligation by the defendant.
Distinction from guarantees.
An indemnity is distinct from a guarantee, which is the promise of a third party to honor the obligation of a party to a contract should that party be unable or unwilling to do so (usually a guarantee is limited to an obligation to pay a debt). This distinction between indemnity and guarantee was discussed as early as the eighteenth century in "Birkmya v Darnell". In that case, concerned with a guarantee of payment for goods rather than payment of rent, the presiding judge explained that a guarantee effectively says "Let him have the goods; if he does not pay you, I will." 
Distinction from warranties.
An indemnity is distinct from a warranty in that:
Insurance.
Indemnity insurance compensates the beneficiaries of the policies for their actual economic losses, up to the limiting amount of the insurance policy. It generally requires the insured to prove the amount of its loss before it can recover. Recovery is limited to the amount of the provable loss even if the face amount of the policy is higher. This is in contrast to, for example, life insurance, where the amount of the beneficiary's economic loss is irrelevant. The death of the person whose life is insured for reasons not excluded from the policy obligate the insurer to pay the entire policy amount to the beneficiary.
Most business interruption insurance policies contain an Extended Period of Indemnity Endorsement, which extends coverage beyond the time that it takes to physically restore the property. This provision covers additional expenses that allow the business to return to prosperity and help the business restore revenues to pre-loss levels.
Historical examples.
Freeing of slaves and indentured servants.
Slave owners suffered a loss whenever their slaves or indentured servants were granted their freedom. Slave owners might have been paid to cover their losses.
When the slaves of Zanzibar were freed in 1897, it was by compensation since the prevailing opinion was that the slave owners suffered the loss of an asset whenever a slave was freed.
In the 1860s in the United States, U.S. President Abraham Lincoln had requested many millions of dollars from Congress with which to compensate slave owners for the loss of their slaves. On July 9, 1868, Section IV of the Fourteenth Constitutional Amendment dismissed all of the claims that slave owners had been injured by the freeing of the slaves.
In 1807-08, in Prussia, statesman Baron Heinrich vom Stein introduced a series of reforms, the principal of which was the abolition of serfdom with indemnification to territorial lords.
Haiti was required to pay an indemnity of 150,000,000 francs to France in order to atone for the loss suffered by the French slave owners.
Costs of war.
The nation that wins a war may insist on being paid compensations for the costs of the war, even after having been the instigator of the war. 

</doc>
<doc id="36951" url="http://en.wikipedia.org/wiki?curid=36951" title="Ritual Entertainment">
Ritual Entertainment

Ritual Entertainment was a video game developer established in 1996 by Robert Atkins, Mark Dochtermann, Jim Dosé, Richard 'Levelord' Gray, Michael Hadwin, Harry Miller and Tom Mustaine. Based in Dallas, Texas, Ritual was formerly known as Hipnotic Interactive, during which period they began development of their signature video game "SiN".
Members of the Ritual development team have contributed game assets to other titles such as "American McGee's Alice", ', ' and "25 to Life" and are also the creators of "Übertools" for id Tech 3, which has been licensed for a number of other games.
On January 24, 2007, developer MumboJumbo announced the acquisition of Ritual Entertainment. With this acquisition, Ritual's focus on traditional action-oriented games was changed to casual games, essentially "stalling" Ritual's latest game series, "SiN Episodes", after releasing only one episode out of a planned nine.
The purchase followed months of departures of several key employees including chief executive officer Steve Nix who became director of business development at id Software, vice president and co-founder Tom Mustaine who left to found Escalation Studios. Several months after the acquisition, community relations manager Steve Hessel left the company to join Splash Damage.
Prior to the announcement, on December 6, 2006, Ritual announced the appointment of Ken Harward as the company's new studio director.

</doc>
<doc id="36956" url="http://en.wikipedia.org/wiki?curid=36956" title="Cappuccino">
Cappuccino

A cappuccino (; ]) is an Italian coffee drink which is traditionally prepared with espresso, hot milk and steamed milk foam. Cream may be used instead of milk and is often topped with cinnamon. It is typically smaller in volume than a caffe latte, with a thicker layer of micro foam. 
The name comes from the Capuchin friars, referring to the colour of their habits, and in this context referring to the colour of the beverage when milk is added in small portion to dark, brewed coffee (today mostly espresso). The physical appearance of a modern cappuccino with espresso créma and steamed milk is a result of a long evolution of the drink.
The Viennese bestowed the name "Kapuziner" possibly in the 18th century on a version that included whipped cream and spices of unknown origin. The Italian cappuccino was unknown until the 1930s, and seems to be born out of Viennese-style cafés in Trieste and other cities in the former Austria in the first decades of the 20th Century.
Definition.
Cappuccino is a coffee drink which today is composed of espresso and hot milk, with the surface topped with foamed milk. Cappuccinos are most often prepared with an espresso machine. The espresso is poured into the bottom of the cup, followed by a similar amount of hot milk, which is prepared by heating and texturing the milk using the espresso machine steam wand. The top third of the drink consists of milk foam; this foam can be decorated with artistic drawings made with the same milk, called "latte art". 
In a traditional cappuccino, as served in Europe and artisan coffee houses in the United States, the total of espresso and milk/foam make up between approximately 150 -. Commercial coffee restaurant chains in the US more often serve the cappuccino as a 360 ml drink or larger.
Cappuccino is traditionally small (max 180 ml) with a thick layer of foam, while 'latte' traditionally is larger (200 ml-300 ml). Caffè latte is often served in a large glass; cappuccino mostly in a 150 - 180 ml cup with a handle. Cappuccino traditionally has a layer of textured milk micro foam exceeding 1 cm in thickness; micro foam is frothed/steamed milk in which the bubbles are so small and so numerous that they are not seen, but it makes the milk lighter and thicker. As a result the micro foam will remain partly on top of the mug when the espresso is poured in correctly as well as mix well with the rest of the cappuccino.
The World Barista Championships have been arranged annually since 2000, and during the course of the competition, the competing barista must produce -for four sensory judges- among other drinks four cappuccinos, defined in WBC Rules and Regulations as [...] a coffee and milk beverage that should produce a harmonious balance of rich, sweet milk and espresso [...] The cappuccino is prepared with one (1) single shot of espresso, textured milk and foam. A minimum of 1 centimeter of foam depth [...] A cappuccino is a beverage between 150 ml and 180 ml in total volume [...] 
Etymology.
'Cappuccino' comes from Latin "Caputium", later borrowed from German/Austrian and modified into 'kapuziner'. It is the diminutive form of cappuccio in Italian, meaning 'hood' or something that covers the head, thus 'cappuccino' reads 'small capuchin'.
The coffee beverage has its name not from the "hood" but from the colour of the hooded robes worn by monks and nuns of the capuchin order. This colour is quite distinctive and 'capuchin' was a common description of the colour of red-brown in 17th-century Europe. The capuchin monks chose the particular design of their orders' robes both in colour and shape of the hood back in the 16th century, inspired by Francis of Assisi's preserved 13th century vestments. The long and pointed hood was characteristic and soon gave the brothers the nickname 'capuchins' (hood-wearing). It was, however the choice of red-brown as the order's vestment colour that, as early as the 17th century, saw 'capuchin' used also as a term for a specific colour. While Francis of Assisi humbly used uncoloured and un-bleached wool for his robes, the capuchins coloured their vestments to differ from Franciscans, Benedictines, Augustinians and other orders.
The word 'Cappuccino' in its Italian form is not known in Italian writings until the 20th century, but the German-language 'Kapuziner' is mentioned as a coffee beverage in the 18th century in Austria, and is described as 'coffee with sugar, egg yolks and cream' in dictionary entries from 1800 onwards. "Kapuziner" was by World War I a common coffee drink in Cafés in the parts of northern Italy which at that time still belonged to Austria.
Although it seems the 'Kapuziner' may have had whipped cream on top, it seems likely the name comes from the specific capuchin-colour of the beverage's mix of coffee, cream and eggs.
The use of fresh milk in coffee in cafés and restaurants is a newer phenomenon (from the 20th century) when fridges became common. The use of full cream is known much further back in time (but not in the use as whipped cream [chantilly] ), as this was a product more easily stored and frequently used also in cooking and baking. Thus, a 'Kapuziner' was prepared with a very small amount of cream to get the 'capuchin' colour. Today, 'Kapuziner' is still served in viennese traditional cafés: still black coffee with only a few drops of cream (in some establishments developed into a capå of whipped cream, but that's another story).
History and evolution.
The consumption of coffee in Europe was initially based on the traditional Ottoman preparation of the drink, by bringing to boil the mixture of coffee and water together, sometimes by adding sugar. The British seem to have started filtering and steeping coffee already in the 2nd part of the 17th century and France and continental Europe followed suit. By the 19th century coffee was brewed in different devices designed for both home and public cafés.
Adding milk to coffee is mentioned by Europeans already in the 1600s, and sometimes advised.
'Cappuccino' originated as the coffee beverage "Kapuziner" in the Viennese coffee houses in the 1700s at the same time as the counterpart coffee beverage named "Franziskaner": 'Kapuziner' shows up on coffee house menus all over the Austro-Hungarian Empire around this time, and is in 1805 described in a "Wörterbuch" (dictionary) as 'coffee with cream and sugar' (although it does not say how it is composed). 'Kapuziner' is mentioned again in writings in the 1850s, described as 'coffee with cream, spices and sugar'. Around the same time, the coffee beverage 'Melange' is mentioned in writings, explained as a blend of coffee and milk presumably similar to present days 'Caffè Latte'
Other coffees containing cream surfaced in Vienna, and outside Austria these are referred to as 'Viennese Coffee' or 'Café Viennois', coffee with whipped cream. 
Predecessors of Irish Coffee, sweetened coffee with different alcohols, topped with whipped cream also spreads out from Vienna.
The 'Kapuziner' took its name from the colour of coffee with a few drops of cream, nicknamed so because the capuchin monks in Vienna and elsewhere wore vestments with this colour. Another popular coffee was Franziskaner, with more cream, referring to the somewhat 'lighter' brown colour of the robes of monks of the Franciscan order.
Cappuccino as we write it today (in Italian) is first mentioned in northern Italy in the 1930s, and photographs from that time shows the drink to resemble a 'viennese' —a coffee topped with whipped cream sprinkled with cinnamon or chocolate. The Italian cappuccino evolved and developed in the following decades: The steamed milk atop is a later addition, and in the US a slight misunderstanding has led to this 'cap' of milk foam being named 'monk's head' -although it originally had nothing to do with the name of the beverage.
Though coffee was brewed differently all over Europe after WW2, in Italy, the real espresso machines became widespread only during the 1950s, and 'cappuccino' was redefined, now made from espresso and frothed milk (though far from the quality of "micro foam" steamed milk today). As the espresso machines improved, so did the dosing of coffee and the heating of the milk. Outside Italy, 'cappuccino' spread, but was generally made from dark coffee with whipped cream, as it still is in large parts of Europe even in 2014.
The 'Kapuziner' remained unchanged on the Austrian coffee menu, even in Trieste, which by 1920 belonged to Italy and in Budapest, Prague, Bratislava and other cities of the former Empire.
Espresso machines were introduced at the beginning of the 20th century, after Luigi Bezzera of Milan filed the first patent in 1901, and although the first generations of machines certainly did not make espresso the way we define it today. 
Coffee making in cafés changed in the first decades of the 20th century. These first machines made it possible to serve coffee 'espresso' -specifically to each customer. The cups were still the same size, and the dose of beans were ground coarse as before. The too high temperature of the boilers scalded the coffee and several attempts at improving this came in the years after the 1st World War.
By the end of the Second World War, the Italians launched the 'age of crema' as the new coffee machines could create a higher pressure, leading to a finer grind and the now classic 'crema'.
The first small cups appear in the 1950s, and the machines could by now also heat milk. The modern 'cappuccino' was born. In Vienna, the espresso bars were introduced in the 1950s, leading to both the 'kapuziner' and the by now new-born Italian 'cappuccino' being served as two different beverages alongside each other.
In the United Kingdom, espresso coffee initially gained popularity in the form of the cappuccino, influenced by the British custom of drinking coffee with milk, the desire for a longer drink to preserve the café as a destination, and the exotic texture of the beverage.
Ingredients.
The way the cappuccino is defined today, in addition to a shot of espresso a most important factor in preparing a cappuccino is the texture and temperature of the milk. When a barista steams the milk for a cappuccino, microfoam is created by introducing very tiny bubbles of air into the milk, giving the milk a velvety texture. The traditional cappuccino consists of an espresso, on which the barista pours the hot foamed milk, resulting in a 2 cm (¾ inch) thick milk foam on top. Variations of the mixtures are usually called "cappuccino chiaro" (light cappuccino, also known as a "wet cappuccino") with more steamed milk to froth than normal, and "cappuccino scuro" (dark cappuccino, also known as a "dry cappuccino") with more froth to steamed milk than normal.
Attaining the correct ratio of foam requires close attention while steaming the milk, thus making the cappuccino one of the most difficult espresso-based beverages to make properly. A skilled barista may obtain artistic shapes while pouring the milk on the top of the espresso coffee.
Popularity.
Cappuccino was traditionally a taste largely appreciated in Europe, Australia, South America and some of North America. By the mid-1990s cappuccino was made much more widely available to North Americans, as upscale coffee houses sprang up.
In Italy, and throughout continental Europe, cappuccino was traditionally consumed early in the day as part of the breakfast, with some kind of sweet pastry. Generally, Europeans did not drink cappuccino with meals other than breakfast, preferring espresso throughout the day and following dinner. However, in recent years Europeans have started to drink cappuccino throughout the entire day. Especially in Australia and Western Europe (The UK, Ireland, The Netherlands, Germany, Belgium, France and Spain) cappuccino is popular at cafés and terraces during the afternoon and in restaurants after dinner. In modern day Italy, cappuccino is consumed only up to 11 a.m., and Italians consider it very "strange" to ask for a cappuccino after that hour. If the beverage is requested in the evening, although not common, it should only be consumed after desert, as the final part of the meal. Espresso is usually ordered after a meal, the lack of milk aids in digestion. In the United States, cappuccinos have become popular concurrent with the boom in the American coffee industry through the late 1990s and early 2000s, especially in the urban Pacific Northwest.
Cappuccino is traditionally served in 150 - cups. By the start of the 21st century, a modified "short-cut" version was being served by fast-food chains in servings up to 600 ml.
Preparation.
Traditional and latte art.
Although size is what varies most among different cappuccinos, there are two main ways of preparing cappuccino: one is the traditional or classical way with a cap of milk foam; the other is the 'Latte Art' way. The former follows the traditional idea of the cappuccino being prepared by 1/3 espresso, 1/3 steamed milk and 1/3 milk foam. The latter follows the same recipe, but is served more often in smaller cups, and the textured milk is gently poured in and finished with a pattern in the surface crèma. The illustrations in this article show the two ways of preparing.
Iced coffee.
Cappuccino Freddo is the cold version of a cappuccino, where the drink usually has a small amount of cold frothed milk atop it. This drink is widely available in Greece, Cyprus and parts of Italy. In Rome, for example, each bar has the drink already prepared. In cities of Northern Italy, like Milan, however, it is almost impossible to find "cappuccino freddo". Instead, "gelato da bere" (a thick blend of gelato and espresso) or "shakerato" (espresso and ice shaken together) are more popular. The term has also spread throughout the Mediterranean region where foam is added to the drink just before serving, often varying from the Italian original. In North America, however, the terms "Cappuccino Freddo" or "Iced cappuccino", if offered, may be somewhat of a misnomer if the characteristic frothed milk is omitted in the iced variation. For example, at Starbucks, without the frothed milk the drink is called an "iced latte".
Iced cappuccino.
In Canada, Tim Hortons's coffee chain sells iced coffee cappuccino under the brand name Ice Capps. The coffee drink mix comes to the Tim Hortons stores as a thick black syrup which is mixed at three parts water to one part syrup in a slurpee machine. The frozen coffee drink is then blended with cream at the time of service (or blended with milk, or chocolate milk upon customer request). The Ice Capp can also be prepared as a Supreme, which includes a flavour shot, whipped topping, and either caramel or chocolate syrup. There is also a Brownie Supreme, which is made with chocolate milk, and mixed with bits of brownie. This is then topped with whipped topping, and finished off with more bits of brownie scattered on top. The chain has also recently introduced traditional iced coffee to its Canadian menu in addition to its U.S. menu.
In Greece and Cyprus, the iced cappuccino is widespread, known locally as Freddo Cappuccino, as opposed to Cappuccino Freddo. The coffee is topped with foam known as aphrogala (Greek: αφρόγαλα), which is created using cold milk and an electric frother. These frothers are commonplace in Greek coffeeshops due to their necessity for making the national iced coffee known as the Frappé. The foam is then added to espresso poured over ice. Despite the Italian name, the drink is not common in Italy or most places outside of Greece.
Similar drinks.
Other milk and espresso drinks similar to the cappuccino include:

</doc>
<doc id="36969" url="http://en.wikipedia.org/wiki?curid=36969" title="Bread">
Bread

Bread is a staple food prepared from a dough of flour and water, usually by baking. Throughout recorded history it has been popular around the world and is one of the oldest artificial foods, having been of importance since the dawn of agriculture.
There are many combinations and proportions of types of flour and other ingredients, and also of different traditional recipes and modes of preparation of bread. As a result, there are wide varieties of types, shapes, sizes, and textures of breads in various regions. Bread may be leavened by many different processes ranging from the use of naturally occurring microbes (for example in sourdough recipes) to high-pressure artificial aeration methods during preparation or baking. However, some products are left unleavened, either for preference, or for traditional or religious reasons. Many non-cereal ingredients may be included, ranging from fruits and nuts to various fats. Commercial bread in particular, commonly contains additives, some of them non-nutritional, to improve flavor, texture, color, shelf life, or ease of manufacturing.
Depending on local custom and convenience, bread may be served in various forms at any meal of the day. It also is eaten as a snack, or used as an ingredient in other culinary preparations, such as fried items coated in crumbs to prevent sticking, or the bland main component of a bread pudding, or stuffings designed to fill cavities or retain juices that otherwise might drip away.
Partly because of its importance as a basic foodstuff, bread has a social and emotional significance beyond its importance in nutrition; it plays essential roles in religious rituals and secular culture. Its prominence in daily life is reflected in language, where it appears in proverbs, colloquial expressions ("He stole the bread from my mouth"), in prayer ("Give us this day our daily bread") and even in the etymology of words, such as " and " (literally those who eat/share bread with you).
Etymology.
The word itself, Old English "bread", is most common in various forms to many Germanic languages, such as Frisian "brea", Dutch "brood", German "Brot", Swedish "bröd", and Norwegian and Danish "brød"; it has been claimed to be derived from the root of "brew". It may be connected with the root of "break", for its early uses are confined to "broken pieces" or "bits" of bread, the Latin "crustum", and it was not until the 12th century that it took the place—as the generic name for bread—of "hlaf" ("hlaifs" in Gothic: modern English "loaf"), which appears to be the oldest Teutonic name. Old High German "hleib" and modern German "Laib" derive from this Proto-Germanic word for "loaf", which was borrowed into Slavic (Polish "chleb", Russian "khleb") and Finnic (Finnish "leipä", Estonian "leib") languages as well.
In many cultures, bread is a metaphor for basic necessities and living conditions in general. For example, a "bread-winner" is a household's main economic contributor and has little to do with actual bread-provision. This is also seen in the phrase "putting bread on the table". The Roman poet Juvenal satirized superficial politicians and the public as caring only for "panem et circenses" (bread and circuses). In Russia in 1917, the Bolsheviks promised "peace, land, and bread." The term "breadbasket" denotes an agriculturally productive region. In Slavic cultures bread and salt is offered as a welcome to guests. In India, life's basic necessities are often referred to as "roti, kapra aur makan" (bread, cloth, and house). In Israel, the most usual phrase in work-related demonstrations is "lekhem, avoda" ("bread, work").
The word "bread" is commonly used around the world in English-speaking countries as a synonym for money (as is the case with the word "dough"). A remarkable or revolutionary innovation is often referred to in North America and the United Kingdom as "the greatest thing since sliced bread" or "the best thing since sliced bread". In Cockney rhyming slang, "bread" means money; this usage is derived from the phrase "bread and honey".
History.
Bread is one of the oldest prepared foods. Evidence from 30,000 years ago in Europe revealed starch residue on rocks used for pounding plants. It is possible that during this time, starch extract from the roots of plants, such as cattails and ferns, was spread on a flat rock, placed over a fire and cooked into a primitive form of flatbread. Around 10,000 BC, with the dawn of the Neolithic age and the spread of agriculture, grains became the mainstay of making bread. Yeast spores are ubiquitous, including the surface of cereal grains, so any dough left to rest will become naturally leavened.
There were multiple sources of leavening available for early bread. Airborne yeasts could be harnessed by leaving uncooked dough exposed to air for some time before cooking. Pliny the Elder reported that the Gauls and Iberians used the foam skimmed from beer to produce "a lighter kind of bread than other peoples." Parts of the ancient world that drank wine instead of beer used a paste composed of grape juice and flour that was allowed to begin fermenting, or wheat bran steeped in wine, as a source for yeast. The most common source of leavening was to retain a piece of dough from the previous day to use as a form of sourdough starter.
In 1961 the Chorleywood bread process was developed, which used the intense mechanical working of dough to dramatically reduce the fermentation period and the time taken to produce a loaf. The process, whose high-energy mixing allows for the use of lower protein grain, is now widely used around the world in large factories. As a result, bread can be produced very quickly and at low costs to the manufacturer and the consumer. However there has been some criticism of the effect on nutritional value.
Recently, domestic bread machines that automate the process of making bread have become popular.
Types.
Bread is the staple food of the Middle East, North Africa, Europe, and in European-derived cultures such as those in the Americas, Australia, and Southern Africa, in contrast to East Asia where rice is the staple. Bread is usually made from a wheat-flour dough that is cultured with yeast, allowed to rise, and finally baked in an oven. Owing to its high levels of gluten (which give the dough sponginess and elasticity), common wheat (also known as bread wheat) is the most common grain used for the preparation of bread.
Bread is also made from the flour of other wheat species (including durum, spelt and emmer), rye, barley, maize (corn), and oats, usually, but not always, in combination with wheat flour. Spelt bread (Dinkelbrot) continues to be widely consumed in Germany, and emmer bread was a staple food in ancient Egypt. Canadian bread is known for its heartier consistency due to high protein levels in Canadian flour.
Preparation.
Doughs are usually baked, but in some cuisines breads are steamed (e.g., mantou), fried (e.g., puri), or baked on an unoiled frying pan (e.g., tortillas). It may be leavened or unleavened (e.g. matzo). Salt, fat and leavening agents such as yeast and baking soda are common ingredients, though bread may contain other ingredients, such as milk, egg, sugar, spice, fruit (such as raisins), vegetables (such as onion), nuts (such as walnuts) or seeds (such as poppy). Referred to colloquially as the "staff of life", bread has been prepared for at least 30,000 years. The development of leavened bread can probably also be traced to prehistoric times. Sometimes, the word "bread" refers to a sweetened loaf cake, often containing appealing ingredients like dried fruit, chocolate chips, nuts or spices, such as pumpkin bread, banana bread or gingerbread.
Fresh bread is prized for its taste, aroma, quality, appearance and texture. Retaining its freshness is important to keep it appetizing. Bread that has stiffened or dried past its prime is said to be stale. Modern bread is sometimes wrapped in paper or plastic film or stored in a container such as a breadbox to reduce drying. Bread that is kept in warm, moist environments is prone to the growth of mold. Bread kept at low temperatures, in a refrigerator for example, will develop mold growth more slowly than bread kept at room temperature, but will turn stale quickly due to retrogradation.
The soft, inner part of bread is known to bakers and other culinary professionals as the "crumb", which is not to be confused with small bits of bread that often fall off, called "crumbs". The outer hard portion of bread is called the "crust". The crumb's texture is greatly determined by the quality of the pores in the bread.
Formulation.
Professional baker recipes are stated using a notation called baker's percentage. The amount of flour is denoted to be 100%, and the amounts of the other ingredients are expressed as a percentage of that amount by weight. Measurement by weight is more accurate and consistent than measurement by volume, particularly for dry ingredients.
The proportion of water to flour is the most important measurement in a bread recipe, as it affects texture and crumb the most. Hard US wheat flours absorb about 62% water, while softer wheat flours absorb about 56%. Common table breads made from these doughs result in a finely textured, light bread. Most artisan bread formulas contain anywhere from 60 to 75% water. In yeast breads, the higher water percentages result in more CO2 bubbles and a coarser bread crumb. One pound (450 g) of flour will yield a standard loaf of bread or two French loaves.
Calcium propionate is commonly added by commercial bakeries to retard the growth of molds.
Flour.
Flour is a product made from grain that has been ground to a powdery consistency. Flour provides the primary structure to the final baked bread. While wheat flour is most commonly used for breads, flours made from rye, barley, maize, and other grains are also commonly available. Each of these grains provides the starch and protein needed to form bread.
The protein content of the flour is the best indicator of the quality of the bread dough and the finished bread. While bread can be made from all-purpose wheat flour, a specialty bread flour, containing more protein (12–14%), is recommended for high-quality bread. If one uses a flour with a lower protein content (9–11%) to produce bread, a shorter mixing time will be required to develop gluten strength properly. An extended mixing time leads to oxidization of the dough, which gives the finished product a whiter crumb, instead of the cream color preferred by most artisan bakers.
Wheat flour, in addition to its starch, contains three water-soluble protein groups (albumin, globulin, and proteoses) and two water-insoluble protein groups (glutenin and gliadin). When flour is mixed with water, the water-soluble proteins dissolve, leaving the glutenin and gliadin to form the structure of the resulting bread. When relatively dry dough is worked by kneading, or wet dough is allowed to rise for a long time (see no-knead bread), the glutenin forms strands of long, thin, chainlike molecules, while the shorter gliadin forms bridges between the strands of glutenin. The resulting networks of strands produced by these two proteins are known as gluten. Gluten development improves if the dough is allowed to autolyse.
Liquids.
Water, or some other liquid, is used to form the flour into a paste or dough. The weight of liquid required varies between recipes, but a ratio of 3 parts liquid to 5 parts flour is common for yeast breads. Recipes that use steam as the primary leavening method may have a liquid content in excess of 1 part liquid to 1 part flour. Instead of water, other types of liquids, such as dairy products, fruit juices, or beer, may be used; they contribute additional sweeteners, fats, or leavening components, as well as water.
Leavening.
Leavening is the process of adding gas to a dough before or during baking to produce a lighter, more easily chewed bread. Most bread consumed in the West is leavened. Unleavened breads have symbolic importance in Judaism and Christianity: Jews consume unleavened bread called matzo during Passover, and Roman Catholic and some Protestant Christians consume unleavened bread when celebrating the Eucharist, a rite derived from the narrative of the Last Supper when Jesus broke bread with his disciples, perhaps during a Passover Seder. In contrast, Orthodox Christians always use leavened bread during their liturgy.
Chemical leavening.
A simple technique for leavening bread is the use of gas-producing chemicals. There are two common methods. The first is to use baking powder or a self-rising flour that includes baking powder. The second is to include an acidic ingredient such as buttermilk and add baking soda; the reaction of the acid with the soda produces gas.
Chemically leavened breads are called "quick breads" and "soda breads". This method is commonly used to make muffins, pancakes, American-style biscuits, and quick breads such as banana bread.
Yeast.
Many breads are leavened by yeast. The yeast most commonly used for leavening bread is "Saccharomyces cerevisiae", the same species used for brewing alcoholic beverages. This yeast ferments some of the carbohydrates in the flour, including any sugar, producing carbon dioxide. Most bakers in the U.S. leaven their dough with commercially produced baker's yeast. Baker's yeast has the advantage of producing uniform, quick, and reliable results, because it is obtained from a pure culture. Many artisan bakers produce their own yeast by preparing a growth culture that they then use in the making of bread. When this culture is kept in the right conditions, it will continue to grow and provide leavening for many years.
Both the baker's yeast and the sourdough methods of baking bread follow the same pattern. Water is mixed with flour, salt and the leavening agent (baker's yeast or sourdough starter). Other additions (spices, herbs, fats, seeds, fruit, etc.) are not needed to bake bread, but are often used. The mixed dough is then allowed to rise one or more times (a longer rising time results in more flavor, so bakers often "punch down" the dough and let it rise again), then loaves are formed, and (after an optional final rising time) the bread is baked in an oven.
Many breads are made from a "straight dough", which means that all of the ingredients are combined in one step, and the dough is baked after the rising time; others are made from a "pre-ferment" in which the leavening agent is combined with some of the flour and water a day or so ahead of baking and allowed to ferment overnight. On the day of the baking, the rest of the ingredients are added, and process continues as with straight dough. This produces a more flavorful bread with better texture.
Many bakers see the starter method as a compromise between the highly reliable results of baker's yeast and the flavor and complexity of a longer fermentation. It also allows the baker to use only a minimal amount of baker's yeast, which was scarce and expensive when it first became available. Most yeasted pre-ferments fall into one of three categories: "poolish" or "pouliche", a loose-textured mixture composed of roughly equal amounts of flour and water (by weight); "biga", a stiff mixture with a higher proportion of flour; and "pâte fermentée", which is simply a portion of dough reserved from a previous batch. Sourdough (also known as "levain" or "natural leaven") takes the pre-ferment method a step further, mixing flour and water to allow naturally occurring yeast and bacteria to propagate (usually "Saccharomyces exiguus", which is more acid-tolerant than "S. cerevisiae" and various species of "Lactobacillus").
Sourdough.
Sourdough is a type of bread produced by a long fermentation of dough using naturally occurring yeasts and lactobacilli. In comparison with breads made with cultivated yeast, it usually has a mildly sour taste because of the lactic acid produced by the lactobacilli.
Sourdough breads are made with a sourdough starter (which differs from starters made with baker's yeast). The starter cultivates yeast and lactobacilli in a mixture of flour and water, making use of the microorganisms already present on flour; it does not need any added yeast. A starter may be maintained indefinitely by regular additions of flour and water. Some bakers have starters several generations old, which are said to have a special taste or texture. It is possible to obtain existing starter cultures to begin a new one.
At one time, all yeast-leavened breads were sourdoughs. The leavening process was not understood until the 19th century, when yeast was first identified. Since then, strains of Saccaromyces cerevisiae have been bred for their reliability and speed of leavening and sold as "baker's yeast". Baker's yeast was adopted for the simpicity and flexibility it introduced to bread making, obviating the lengthy cultivation of a sourdough starter. While sourdough breads survived in some parts of Europe, throughout most of the U.S., they were replaced by baker's yeast. Recently there has been a revival of sourdough bread in artisan bakeries.
There are other ways of sourdough baking and culture maintenance. A more traditional one is the process that was followed by peasant families throughout Europe in past centuries. The family (usually the woman was in charge of breadmaking) would bake on a fixed schedule, perhaps once a week. The starter was saved from the previous week's dough. The starter was mixed with the new ingredients, the dough was left to rise, and then a piece of it was saved (to be the starter for next week's bread). The rest was formed into loaves that were marked with the family sign (this is where today's decorative slashing of bread loaves originates from) and taken to the communal oven to bake. These communal ovens with time evolved into the modern bakery.
Steam.
The rapid expansion of steam produced during baking leavens the bread, which is as simple as it is unpredictable. The best known steam-leavened bread is the popover. Steam-leavening is unpredictable since the steam is not produced until the bread is baked.
Steam leavening happens regardless of the rising agents (baking soda, yeast, baking powder, sour dough, beaten egg whites, etc.).
This is the main factor in the rise of bread once it has been put in the oven. CO2 generation, on its own, is too small to account for the rise. Heat kills bacteria or yeast at an early stage, so the CO2 generation is stopped.
Bacteria.
Salt rising bread employs a form of bacterial leavening that does not require yeast. Although the leavening action is not always consistent, and requires close attention to the incubating conditions, this bread is making a comeback due to its unique cheese-like flavor and fine texture.
Aeration.
Aerated bread is leavened by carbon dioxide being forced into dough under pressure. From the mid 19th to 20th centuries bread made this way was somewhat popular in the United Kingdom, made by the Aerated Bread Company and sold in its high-street tearooms. The company was founded in 1862, and ceased independent operations in 1955. While it had some devoted adherents, it never eclipsed the use of baker's yeast worldwide.
The Pressure-Vacuum mixer was later developed by the Flour Milling and Baking Research Association at Chorleywood. With the application of both pressure and vacuum at different points in the mixing process, this mixer not only manipulates the gas bubble size, it may also manipulate the composition of gases in the dough via the gas applied to the headspace.
Fats or shortenings.
Fats, such as butter, vegetable oils, lard, or that contained in eggs, affect the development of gluten in breads by coating and lubricating the individual strands of protein. They also help to hold the structure together. If too much fat is included in a bread dough, the lubrication effect will cause the protein structures to divide. A fat content of approximately 3% by weight is the concentration that will produce the greatest leavening action. In addition to their effects on leavening, fats also serve to tenderize breads and preserve freshness.
Bread improvers.
Bread improvers and dough conditioners are often used in producing commercial breads to reduce the time needed for rising and to improve texture and volume. Chemical substances commonly used as bread improvers include ascorbic acid, hydrochloride, sodium metabisulfate, ammonium chloride, various phosphates, amylase, and protease.
Salt is one of the most common additives used in production. In addition to enhancing flavor and restricting yeast activity, salt affects the crumb and the overall texture by stabilizing and strengthening the gluten. Some artisan bakers are foregoing early addition of salt to the dough, and are waiting until after a 20-minute "rest". This is known as an autolyse and is done with both refined and whole-grain flours.
Properties.
Chemical composition.
In wheat, phenolic compounds are mainly found in hulls in the form of insoluble bound ferulic acid where it is relevant to wheat resistance to fungal diseases.
Rye bread contains phenolic acids and ferulic acid dehydrodimers.
Three natural phenolic glucosides, secoisolariciresinol diglucoside, p-coumaric acid glucoside and ferulic acid glucoside, can be found in commercial breads containing flaxseed.
Serving and consumption.
Bread can be served at many temperatures; once baked, it can subsequently be toasted. It is most commonly eaten with the hands, either by itself or as a carrier for other foods. Bread can be dipped into liquids such as gravy, olive oil, or soup; it can be topped with various sweet and savory spreads, or used to make sandwiches containing myriad varieties of meats, cheeses, vegetables, and condiments.
Bread may also be used as an ingredient in other culinary preparations, such as the use of breadcrumbs to provide crunchy crusts or thicken sauces, sweet or savoury bread puddings, or as a binding agent in sausages and other ground meat products.
Nutritional significance.
Nutritionally, bread is known as an ample source for the grains category of nutrition. Serving size of bread is standard through ounces, counting one slice of bread (white processed bread) as 1 oz. Also, bread is considered a good source of carbohydrates through the whole grains, nutrients such as magnesium, iron, selenium, B vitamins, and dietary fiber. 
As part of the 2010 Dietary Guidelines for Americans, it is recommended to make at least half of the recommended total grain intake as whole grains and to overall increase whole grains intake.
Shelf life.
In 2009, a natural preservative for extending the shelf life of bread for up to two weeks (as opposed to a few days) had been patented and licensed to Puratos, a Belgium-based baking ingredients company that supplies to more than 100 countries. The breakthrough was pioneered by Prof Elke Arendt at the University College Cork (UCC) by incorporating into the bread a lactic acid bacteria strain which also "produces a fine crumb texture" and "improves the flavour, volume and nutritional value of the food as well." Prior to this, "About 20% of all bread is thrown out due to shelf-life issues."
Crust.
The bread crust is formed from surface dough during the cooking process. It is hardened and browned through the Maillard reaction using the sugars and amino acids and the intense heat at the bread surface. The nature of a bread's crust differs depending on the type of bread and the way it is baked. Commercial bread is baked using jets that direct steam toward the bread to help produce a desirable crust.
The crust of most breads is less soft, and more complexly and intensely flavored, than the rest, and judgments vary among individuals and cultures as to whether it is therefore the less palatable or the more flavorful part of a particular style of bread. Some manufacturers, including as of September 2009[ [update]] Sara Lee, market traditional and crustless breads.
The first and last slices of a loaf (or a slice with a high ratio of crust-area to volume compared to others of the same loaf) are sometimes referred to as the heel or the crust of the loaf.
Old wives tales suggest that eating the bread crust makes a person's hair curlier. Additionally, the crust is rumored to be healthier than the rest. Some studies have shown that this is true as the crust has more dietary fiber and antioxidants, notably pronyl-lysine. The pronyl-lysine found in bread crust is being researched for its potential colorectal cancer inhibitory properties.
Cultural significance.
Bread has a significance beyond mere nutrition in many cultures in the West and Near and Middle East because of its history and contemporary importance. Bread is also significant in Christianity as one of the elements (alongside wine) of the Eucharist; see sacramental bread. The word "companion" comes from Latin "com-" "with" + "panis" "bread".
The political significance of bread is considerable. In 19th century Britain, the inflated price of bread due to the Corn Laws caused major political and social divisions, and was central to debates over free trade versus protectionism. The Assize of Bread and Ale in the 13th century demonstrated the importance of bread in medieval times by setting heavy punishments for short-changing bakers, and bread appeared in the "Magna Carta" a half-century earlier.
Like other foods, choosing the "right" kind of bread is used as a type of social signalling, to let others know, for example, that the person buying expensive bread is financially secure, or the person buying whatever type of bread that the current fashions deem most healthful is a health-conscious consumer.
 ... bread has become an article of food of the first necessity; and properly so, for it constitutes of itself a complete life-sustainer, the gluten, starch, and sugar, which it contains, represents azotised and hydro-carbonated nutrients, and combining the sustaining powers of the animal and vegetable kingdoms in one product. Mrs Beeton (1861)
As a simple, cheap, and adaptable type of food, bread is often used as a synecdoche for food in general in some languages and dialects, such as Greek and Punjabi. There are many variations on the basic recipe of bread worldwide, including pizza, chapatis, tortillas, bocadillo, baguettes, brioche, pitas, lavash, biscuits, pretzels, naan, bagels, puris, and many others. There are different types of traditional "cheese breads" in many countries, including Brazil, Colombia, Italy, and Russia.
Europe.
An enormous variety of bread is available across Europe. Germany lays claim to over 1300 basic varieties of breads, rolls, and pastries, as well as having the largest consumption of bread per capita worldwide, followed by Chile. Bread and salt is a welcome greeting ceremony in many central and eastern European cultures. During important occasions when guests arrive, they are offered a loaf of bread with a salt holder to represent hospitality.
There is a wide variety of traditional breads in Great Britain, often baked in a rectangular tin. Round loaves are also produced, such as the North East England speciality called a stottie cake. A cottage loaf is made of two balls of dough, one on top of the other, to form a figure-of-eight shape. A cob is a small round loaf. There are many variations on bread rolls, such as baps, barms, breadcakes and so on. The Chorleywood process for mass-producing bread was developed in England in the 1960s before spreading worldwide. Mass-produced sliced white bread brands such as Wonderloaf and Mother's Pride have been criticized on grounds of poor nutritional value and taste of the loaves produced.
In Spain, bread is called "pan". The traditional Spanish "pan" is a long loaf of bread, similar to the French baguette but wider. One can buy it freshly made every morning in the traditional bakeries, where there is a large assortment of bread. A smaller version is known as bocadillo, an iconic piece of the Hispanic cuisine. In Spain, especially in the Mediterranean area, there have been guilds of bakers for over 750 years. The bakers guild in Barcelona was founded in 1200 AD. There is a region called Tierra del Pan ("Land of the Bread"), located in the province of Zamora, where economy was in the past joined to this activity.
In France, there has been a huge decline in the baguette culture. In the 1970s, French people were consuming an average of one loaf of bread per day. Only a century ago, the French ate approximately 3 loaves of bread per day. Today, French people eat only a half a loaf of bread per day. In response to this decline, bakers have created a national campaign to get people to call at the bakery before and after work just as they used to. The campaign models the American "Got Milk?" campaign, plastering "Hey there, have you picked up the bread?" all over billboards and bread bags.
Latin America.
In Mexico, bread is called "pan". Although corn tortillas are the staple bread in most of Mexico, bread rolls in many varieties are an important daily food for city dwellers. Popular breads in Mexico include the bolillo roll and "pan dulce". "Pan dulce", which is Spanish for "sweet bread", is eaten in the evenings with hot drinks like traditional hot chocolate.
In Peru, "pan" has many variations due to the diversity of Peruvian cuisine. People usually eat "pan de piso" and "pan serrano". There are also some kinds of bread made of potatoes; these are currently popular in the Andes. "Bizcochos" are sweet bread usually eaten with some butter and hot chocolate. A dough made with cooked pumpkin or squash, often shaped and fried into doughnuts and served with a sweet fruity dipping sauce, is a traditional favorite. Bread is an ingredient of sopas de ajo, gazpacho, and salmorejo.
North Africa.
Also consumed is a thick and chewy fried bread that is smothered in oil beforehand. The "rghifa" bread is a staple in the food of Morocco and consists of several layers of lightly cooked bread.
In Ethiopia in east North Africa, a bread called "injera" is made from a grain called "teff". This is a wide, flat, circular bread that is in a similar shape of a tortilla and is also used as a utensil to pick up food. 
Asia.
The traditional bread in China is "mantou". It is made by steaming or deep-frying dough made from wheat flour. In Northern China and northern central China, "mantou" is often eaten as an alternative staple to rice. Steamed "mantou" is similar to Western white bread, but since it is not baked it does not have a brown outer crust. "Mantou" that have a filling such as meat or vegetables (cha siu bao, for example) are called "baozi". The kompyang of Fuzhou is an example of a Chinese bread baked in a clay oven.
In South Asia (including India, Pakistan, and the Middle East), "roti" or "chapati", types of unleavened flatbreads usually made from whole-wheat flour or sometimes refined wheat flour and baked on a hot iron griddle called a "tava", form the mainstay of the people's diet. "Rotis" and "naans" are usually served with curry throughout the region. A variant called "makki di roti" uses maize flour rather than white flour. Another variant is "puri", a thin flat bread that is fried rather than baked and puffs up while cooked. "Paratha" is another variation on "roti". "Naan" (leavened wholewheat bread) is baked in a tandoor or clay oven and is rarely prepared at home. White and brown breads are also very common, but not as common as "roti".
In the Philippines, "pandesal" (or pan de sal, meaning "bread of salt" or "salt bread") is a rounded bread usually eaten by Filipinos during breakfast. The Philippines also produces a cheap generic white bread called "Pinoy Tasty".
The Miracle Chapati as it became known is an unleavened bread with a long tradition. The bread can be spelled Chapati, Chapatti, Chappati, or Chapathi.
North America.
Traditional breads in the United States include cornbreads and various quick breads, such as biscuits. Cornbread is made from cornmeal and can differ significantly in taste and texture from region to region. In general, the South prefers white cornmeal with little to no wheat flour or sweeteners added. It is traditionally baked in a cast-iron skillet and ideally has a crunchy outside and moist inside. The North usually prefers yellow cornmeal with sometimes as much as half wheat flour in its composition, as well as sugar, honey, or maple syrup. This results in a bread that is softer and sweeter than its southern counterpart. Wheat flour was not available to the average North American family until the early 1900s when a new breed of wheat termed Marquis was produced. This was a hybrid of Red Fife and Hard red winter wheat. Marquis grew well and soon average Americans were able to have homemade wheat bread on the table. Homemade wheat breads are made in a rectangular tin similar to those in the United Kingdom.
Rolls, made from wheat flour and yeast, are another popular and traditional bread, eaten with the dinner meal. Sourdough biscuits are traditional "cowboy food" in the West. The San Francisco Bay Area is known for its crusty sourdough. Spoon bread, also called batter bread or egg bread, is made of cornmeal with or without added rice and hominy, and is mixed with milk, eggs, shortening and leavening to such a consistency that it must be served from the baking dish with a spoon. This is popular chiefly in the South.
Up until the 20th century (and even later in certain regions), any flour other than cornmeal was considered a luxury; this would explain the greater variety in cornbread types compared to that of wheat breads. In terms of commercial manufacture, the most popular bread has been a soft-textured type with a thin crust that is usually made with milk and is slightly sweet; this is the type that is generally sold ready-sliced in packages. It is usually eaten with the crust, but some eaters or preparers may remove the crust due to a personal preference or style of serving, as with finger sandwiches served with afternoon tea. Some of the softest bread, including Wonder Bread, is referred to as "balloon bread".
Though white "sandwich bread" is the most popular, Americans are trending toward more artisanal breads. Different regions of the country feature certain ethnic bread varieties including the French baguette, the Ashkenazi Jewish bagel, scali (an Italian-style bread made in New England), Native American frybread (a product of hardship, developed during the Indian resettlements of the 19th century), and Jewish rye, a bread commonly associated with delicatessen cuisine.
Religious significance.
Abrahamic Religions.
During the Jewish festival of Passover, only unleavened bread is eaten, in commemoration of the flight from slavery in Egypt. The Israelites did not have enough time to allow their bread to rise, and so ate only unleavened bread ("matzoh").
In the Christian ritual of the Eucharist, bread symbolically represents the body of Christ, and is eaten as a sacrament. Specific aspects of the ritual itself, including the composition of the bread, vary from denomination to denomination. The differences in the practice of the Eucharist stem from different descriptions and depictions of the Last Supper which provides the scriptural basis for the Eucharist. The Synoptic Gospels present the Last Supper as a Passover meal and suggest that the bread at the Last Supper would be unleavened. However in the chronology in Gospel of John, the Last Supper occurred the day before Passover suggesting that the bread would be leavened. Despite this point of disagreement, the Council of Florence of the Catholic church agreed that “the body of Christ is truly confected in both unleavened and leavened wheat bread, and priests should confect the body of Christ in either”.
Paganism.
Some traditions of Wicca and Neo-Paganism consume bread as part of their religious rituals, attaching varied symbolism to the act.
Anti-bread movements.
Although eaten by nearly all people, some have rejected bread entirely or rejected types of bread that they consider unhealthy. Reasons for doing so have varied through history: whole grain bread has been criticized as being unrefined, and white bread as being unhealthfully processed; homemade bread has been deemed unsanitary, and factory-made bread regarded with suspicion for being adulterated. "Amylophobia", literally "fear of starch", was a movement in the US during the 1920s and 1930s.
In the United States, bread sales fell by 11.3% between 2008 and 2013. This statistic might reflect a change in the types of food from which Americans are getting their carbohydrates, but the trends are unclear because of differences between the markets for different classes of bread products. It is also possible that changing diet fashions affected the decrease in bread sales during that period.
In Medicine.
The ancient Egyptians used moldy bread to treat infections that arose from dirt in burn wounds.

</doc>
<doc id="36979" url="http://en.wikipedia.org/wiki?curid=36979" title="Rice">
Rice

Rice is the seed of the grass species "Oryza sativa" (Asian rice) or "Oryza glaberrima" (African rice). As a cereal grain, it is the most widely consumed staple food for a large part of the world's human population, especially in Asia. It is the agricultural commodity with the third-highest worldwide production, after sugarcane and maize, according to data of FAOSTAT 2012.
Since a large portion of maize crops are grown for purposes other than human consumption, rice is the most important grain with regard to human nutrition and caloric intake, providing more than one fifth of the calories consumed worldwide by humans.
Chinese legends attribute the domestication of rice to Shennong, the legendary Emperor of China and inventor of Chinese agriculture. Genetic evidence has shown that rice originates from a single domestication 8,200–13,500 years ago in the Pearl River valley region of China. Previously, archaeological evidence had suggested that rice was domesticated in the Yangtze River valley region in China. From East Asia, rice was spread to Southeast and South Asia. Rice was introduced to Europe through Western Asia, and to the Americas through European colonization.
There are many varieties of rice and culinary preferences tend to vary regionally. In some areas such as the Far East or Spain, there is a preference for softer and stickier varieties.
Rice, a monocot, is normally grown as an annual plant, although in tropical areas it can survive as a perennial and can produce a ratoon crop for up to 30 years. The rice plant can grow to 1 – tall, occasionally more depending on the variety and soil fertility. It has long, slender leaves 50 – long and 2 – broad. The small wind-pollinated flowers are produced in a branched arching to pendulous inflorescence 30 – long. The edible seed is a grain (caryopsis) 5 – long and 2 – thick.
Rice cultivation is well-suited to countries and regions with low labor costs and high rainfall, as it is labor-intensive to cultivate and requires ample water. However, rice can be grown practically anywhere, even on a steep hill or mountain area with the use of water-controlling terrace systems. Although its parent species are native to Asia and certain parts of Africa, centuries of trade and exportation have made it commonplace in many cultures worldwide.
The traditional method for cultivating rice is flooding the fields while, or after, setting the young seedlings. This simple method requires sound planning and servicing of the water damming and channeling, but reduces the growth of less robust weed and pest plants that have no submerged growth state, and deters vermin. While flooding is not mandatory for the cultivation of rice, all other methods of irrigation require higher effort in weed and pest control during growth periods and a different approach for fertilizing the soil.
The name wild rice is usually used for species of the genera "Zizania" and "Porteresia", both wild and domesticated, although the term may also be used for primitive or uncultivated varieties of "Oryza".
Etymology.
First used in English in the middle of the 13th century, the word "rice" derives from the Old French "ris", which comes from Italian "riso", in turn from the Latin "oriza", which derives from the Greek ὄρυζα ("oruza"). The Greek word is the source of all European words (cf. Welsh "reis", German "Reis", Lithuanian "ryžiai", Serbo-Croatian "riža", Polish "ryż", Dutch "rijst", Hungarian "rizs", Romanian "orez").
The origin of the Greek word is unclear. It is sometimes held to be from the Tamil word அரிசி ("arisi"), or rather Old Tamil "arici". However, Krishnamurti disagrees with the notion that Old Tamil "arici" is the source of the Greek term, and proposes that it was borrowed from descendants of Proto-Dravidian *"wariñci" instead. Mayrhofer suggests that the immediate source of the Greek word is to be sought in Old Iranian words of the types *"vrīz-" or *"vrinj-", but these are ultimately traced back to Indo-Aryan (as in Sanskrit "vrīhí-") and subsequently to Dravidian by Witzel and others.
Cooking.
The varieties of rice are typically classified as long-, medium-, and short-grained. The grains of long-grain rice (high in amylose) tend to remain intact after cooking; medium-grain rice (high in amylopectin) becomes more sticky. Medium-grain rice is used for sweet dishes, for "risotto" in Italy, and many rice dishes, such as "arròs negre", in Spain. Some varieties of long-grain rice that are high in amylopectin, known as Thai Sticky rice, are usually steamed. A stickier medium-grain rice is used for "sushi"; the stickiness allows rice to hold its shape when molded. Short-grain rice is often used for rice pudding.
Instant rice differs from parboiled rice in that it is fully cooked and then dried, though there is a significant degradation in taste and texture. Rice flour and starch often are used in batters and breadings to increase crispiness.
Preparation.
Rice is typically rinsed before cooking to remove excess starch. Rice produced in the US is usually fortified with vitamins and minerals, and rinsing will result in a loss of nutrients. Rice may be rinsed repeatedly until the rinse water is clear to improve the texture and taste.
Rice may be soaked to decrease cooking time, conserve fuel, minimize exposure to high temperature, and reduce stickiness. For some varieties, soaking improves the texture of the cooked rice by increasing expansion of the grains. Rice may be soaked for 30 minutes up to several hours.
Brown rice may be soaked in warm water for 20 hours to stimulate germination. This process, called germinated brown rice (GBR), activates enzymes and enhances amino acids including gamma-aminobutyric acid to improve the nutritional value of brown rice. This method is a result of research carried out for the United Nations International Year of Rice.
Processing.
Rice is cooked by boiling or steaming, and absorbs water during cooking. With the absorption method, rice may be cooked in a volume of water similar to the volume of rice. With the rapid-boil method, rice may be cooked in a large quantity of water which is drained before serving. Rapid-boil preparation is not desirable with enriched rice, as much of the enrichment additives are lost when the water is discarded. Electric rice cookers, popular in Asia and Latin America, simplify the process of cooking rice. Rice (or any other grain) is sometimes quickly fried in oil or fat before boiling (for example saffron rice or risotto); this makes the cooked rice less sticky, and is a cooking style commonly called pilaf in Iran and Afghanistan or biryani (Dam-pukhtak) in India and Pakistan.
Dishes.
In Arab cuisine, rice is an ingredient of many soups and dishes with fish, poultry, and other types of meat. It is also used to stuff vegetables or is wrapped in grape leaves (dolma). When combined with milk, sugar, and honey, it is used to make desserts. In some regions, such as Tabaristan, bread is made using rice flour. Medieval Islamic texts spoke of medical uses for the plant. Rice may also be made into congee (also called rice porridge, fawrclaab, okayu, Xifan, jook, or rice gruel) by adding more water than usual, so that the cooked rice is saturated with water, usually to the point that it disintegrates. Rice porridge is commonly eaten as a breakfast food, and is also a traditional food for the sick.
Nutrition and health.
Nutrients and the nutritional importance of rice.
Rice is the staple food of over half the world's population. It is the predominant dietary energy source for 17 countries in Asia and the Pacific, 9 countries in North and South America and 8 countries in Africa. Rice provides 20% of the world’s dietary energy supply, while wheat supplies 19% and maize (corn) 5%.
A detailed analysis of nutrient content of rice suggests that the nutrition value of rice varies based on a number of factors. It depends on the strain of rice, that is between white, brown, red, and black (or purple) varieties of rice – each prevalent in different parts of the world. It also depends on nutrient quality of the soil rice is grown in, whether and how the rice is polished or processed, the manner it is enriched, and how it is prepared before consumption.
An illustrative comparison between white and brown rice of protein quality, mineral and vitamin quality, carbohydrate and fat quality suggests that neither is a complete nutrition source. Between the two, there is a significant difference in fiber content and minor differences in other nutrients.
Highly colored rice strains, such as black (purple) rice, derive their color from anthocyanins and tocols. Scientific studies suggest that these color pigments have antioxidant properties that may be useful to human health. In purple rice bran, hydrophilic antioxidants are in greater quantity and have higher free radical scavenging activity than lipophilic antioxidants. Anthocyanins and γ-tocols in purple rice are largely located in the inner portion of purple rice bran.
Comparative nutrition studies on red, black and white varieties of rice suggest that pigments in red and black rice varieties may offer nutritional benefits. Red or black rice consumption was found to reduce or retard the progression of atherosclerotic plaque development, induced by dietary cholesterol, in mammals. White rice consumption offered no similar benefits, which the study suggests may be due in part to a lack of antioxidants found in red and black varieties of rice.
Comparison of rice to other major staple foods.
The table below shows the nutrient content of major staple foods in a raw form. Raw grains, however, are not edible and can not be digested. These must be sprouted, or prepared and cooked for human consumption. In sprouted and cooked form, the relative nutritional and anti-nutritional contents of each of these grains is remarkably different from that of raw form of these grains reported in this table.
Arsenic concerns.
Rice and rice products contain arsenic, a known poison and Group 1 carcinogen. There is no safe level of arsenic, but, as of 2012, a limit of 10 parts per billion has been established in the United States for drinking water, twice the level of 5 parts per billion originally proposed by the EPA. Consumption of one serving of some varieties of rice gives more exposure to arsenic than consumption of 1 liter of water that contains 5 parts per billion arsenic; however, the amount of arsenic in rice varies widely with the greatest concentration in brown rice and rice grown on land formerly used to grow cotton; in the United States, Arkansas, Louisiana, Missouri, and Texas. The U.S. Food and Drug Administration (FDA) is studying this issue, but has not established a limit. China has set a limit of 150 ppb for arsenic in rice.
White rice grown in Arkansas, Louisiana, Missouri, and Texas, which account for 76 percent of American-produced rice had higher levels of arsenic than other regions of the world studied, possibly because of past use of arsenic-based pesticides to control cotton weevils. Jasmine rice from Thailand and Basmati rice from Pakistan and India contain the least arsenic among rice varieties in one study.
Bacillus cereus.
Cooked rice can contain "Bacillus cereus" spores, which produce an emetic toxin when left at 4 –. When storing cooked rice for use the next day, rapid cooling is advised to reduce the risk of toxin production. One of the enterotoxins produced by "Bacillus cereus" is heat-resistant; reheating contaminated rice kills the bacteria, but does not destroy the toxin already present.
Rice-growing environments.
Rice can be grown in different environments, depending upon water availability. Generally, rice does not thrive in a waterlogged area, yet it can survive and grow herein and it can also survive flooding.
History of domestication and cultivation.
There have been plenty of debates on the origins of the domesticated rice. Genetic evidence published in the "Proceedings of the National Academy of Sciences of the United States of America" (PNAS) shows that all forms of Asian rice, both "indica" and "japonica", spring from a single domestication that occurred 8,200–13,500 years ago in China of the wild rice "Oryza rufipogon". A 2012 study published in "Nature", through a map of rice genome variation, indicated that the domestication of rice occurred in the Pearl River valley region of China based on the genetic evidence. From East Asia, rice was spread to South and Southeast Asia.
Before this research, the commonly accepted view, based on archaeological evidence, is that rice was first domesticated in the region of the Yangtze River valley in China.
Morphological studies of rice phytoliths from the Diaotonghuan archaeological site clearly show the transition from the collection of wild rice to the cultivation of domesticated rice. The large number of wild rice phytoliths at the Diaotonghuan level dating from 12,000–11,000 BP indicates that wild rice collection was part of the local means of subsistence. Changes in the morphology of Diaotonghuan phytoliths dating from 10,000–8,000 BP show that rice had by this time been domesticated. Soon afterwards the two major varieties of indica and japonica rice were being grown in Central China. In the late 3rd millennium BC, there was a rapid expansion of rice cultivation into mainland Southeast Asia and westwards across India and Nepal.
In 2003, Korean archaeologists claimed to have discovered the world's oldest domesticated rice. Their 15,000-year-old age challenges the accepted view that rice cultivation originated in China about 12,000 years ago. These findings were received by academia with strong skepticism, and the results and their publicizing has been cited as being driven by a combination of nationalist and regional interests. In 2011, a combined effort by the Stanford University, New York University, Washington University in St. Louis, and Purdue University has provided the strongest evidence yet that there is only one single origin of domesticated rice, in the Yangtze Valley of China.
Rice spread to the Middle East where, according to Zohary and Hopf (2000, p. 91), "O. sativa" was recovered from a grave at Susa in Iran (dated to the 1st century AD).
Regional history.
In a recent study, scientist have found a link for differences in human culture based on either wheat or rice cultivating races since ancient times.
Africa.
African rice has been cultivated for 3500 years. Between 1500 and 800 BC, "Oryza glaberrima" propagated from its original centre, the Niger River delta, and extended to Senegal. However, it never developed far from its original region. Its cultivation even declined in favour of the Asian species, which was introduced to East Africa early in the common era and spread westward. African rice helped Africa conquer its famine of 1203.
Asia.
Today, the majority of all rice produced comes from China, India, Indonesia, Bangladesh, Vietnam, Thailand, Myanmar, Pakistan, Philippines, Korea and Japan. Asian farmers still account for 87% of the world's total rice production.
Sri Lanka.
Rice is the staple food amongst all the ethnic groups in Sri Lanka. Agriculture in Sri Lanka mainly depends on the rice cultivation. Rice production is acutely dependent on rainfall and government supply necessity of water through irrigation channels throughout the cultivation seasons. The principal cultivation season, known as "Maha", is from October to March and the subsidiary cultivation season, known as "Yala", is from April to September. During Maha season, there is usually enough water to sustain the cultivation of all rice fields, nevertheless in Yala season there is only enough water for cultivation of half of the land extent.
Traditional rice varieties are now making a comeback with the recent interest in green foods.
Thailand.
Rice is the main export of Thailand, especially the white jasmine rice 105 (Dok Mali 105). Thailand has a large number of rice varieties, 3,500 kinds with different characters, and 5 kinds of wild rice cultivates. In each region of the country there are different rice seed types. Their use depends on weather, atmosphere, and topography.
The northern region has both low lands and high lands. The farmers’ usual crop is non-glutinous rice such as Niew Sun Pah Tong rice seeds. This rice is naturally protected from leaf disease, and the paddy has a brown color. The northeastern region has a large area, where farmers can cultivate about 36 million square meters of rice. Although most of them are plains and dry areas, they can grow the white jasmine rice 105 which is the most famous Thai rice. The white jasmine rice was developed in Chonburi province first and after that it was grown in many areas in the country but the rice from this region has a high quality, because it's softer, whiter and more fragrant. This rice can resist drought, acidic soil, and alkaline soil.
The central region is mostly composed of plains. Most farmers grow Jao rice. For example the Pathum Thani 1 rice which has qualities similar to the white jasmine 105 rice. Their paddy has the color of thatch and their cooked rice has fragrant grains also.
In the southern region, most farmers transplant around boundaries to the flood of plain or plain between mountains. Farming is the region is slower than other regions because the rainy season comes late. The popular rice varieties in this area are the Leb Nok Pattani seeds, a type of Jao rice. Their paddy has the color of thatch and it can be processed to make noodles.
Companion plant.
One of the earliest known examples of companion planting is the growing of rice with Azolla, the mosquito fern, which covers the top of a fresh rice paddy's water, blocking out any competing plants, as well as fixing nitrogen from the atmosphere for the rice to use. The rice is planted when it is tall enough to poke out above the azolla. This method has been used for at least a thousand years.
Middle East.
Rice was grown in some areas of southern Iraq. With the rise of Islam it moved north to Nisibin, the southern shores of the Caspian Sea(Iran) and then beyond the Muslim world into the valley of the Volga. In Egypt, rice is mainly grown in the Nile Delta. In Israel, rice came to be grown in the Jordan Valley. Rice is also grown in Saudi Arabia at Al-hasa Oasis and in Yemen.
Europe.
Rice was known to the Classical world, being imported from Egypt, and perhaps west Asia. It was known to Greece by returning soldiers from Alexander the Great's military expedition to Asia. Large deposits of rice from the first century A.D. have been found in Roman camps in Germany.
The Moors brought Asiatic rice to the Iberian Peninsula in the 10th century. Records indicate it was grown in Valencia and Majorca. In Majorca, rice cultivation seems to have stopped after the Christian conquest, although historians are not certain.
Muslims also brought rice to Sicily, where it was an important crop long before it is noted in the plain of Pisa (1468) or in the Lombard plain (1475), where its cultivation was promoted by Ludovico Sforza, Duke of Milan, and demonstrated in his model farms.
After the 15th century, rice spread throughout Italy and then France, later propagating to all the continents during the age of European exploration.
In European Russia, a short-grain, starchy rice similar to the Italian varieties, has been grown in the Krasnodar Krai, and known in Russia as "Kuban Rice" or "Krasnodar Rice". In the Russian Far East several "japonica" cultivars are grown in Primorye around the Khanka lake. Increasing scale of rice production in the region has recently brought criticism towards growers' alleged bad practices in regards to the environment.
Caribbean and Latin America.
Rice is not native to the Americas but was introduced to Latin America and the Caribbean by European colonizers at an early date with Spanish colonizers introducing Asian rice to Mexico in the 1520s at Veracruz and the Portuguese and their African slaves introducing it at about the same time to colonial Brazil. Recent scholarship suggests that enslaved Africans played an active role in the establishment of rice in the New World and that African rice was an important crop from an early period. Varieties of rice and bean dishes that were a staple dish along the peoples of West Africa remained a staple among their descendants subjected to slavery in the Spanish New World colonies, Brazil and elsewhere in the Americas.
The Native Americans of what is now the Eastern United States may have practiced extensive agriculture with forms of wild rice (Zizania palustris), which looks similar to but it not directly related to rice.
United States.
In 1694, rice arrived in South Carolina, probably originating from Madagascar.
In the United States, colonial South Carolina and Georgia grew and amassed great wealth from the slave labor obtained from the Senegambia area of West Africa and from coastal Sierra Leone. At the port of Charleston, through which 40% of all American slave imports passed, slaves from this region of Africa brought the highest prices due to their prior knowledge of rice culture, which was put to use on the many rice plantations around Georgetown, Charleston, and Savannah.
From the enslaved Africans, plantation owners learned how to dyke the marshes and periodically flood the fields. At first the rice was laboriously milled by hand using large mortars and pestles made of wood, then winnowed in sweetgrass baskets (the making of which was another skill brought by slaves from Africa). The invention of the rice mill increased profitability of the crop, and the addition of water power for the mills in 1787 by millwright Jonathan Lucas was another step forward.
Rice culture in the southeastern U.S. became less profitable with the loss of slave labor after the American Civil War, and it finally died out just after the turn of the 20th century. Today, people can visit the only remaining rice plantation in South Carolina that still has the original winnowing barn and rice mill from the mid-19th century at the historic Mansfield Plantation in Georgetown, South Carolina. The predominant strain of rice in the Carolinas was from Africa and was known as "Carolina Gold." The cultivar has been preserved and there are current attempts to reintroduce it as a commercially grown crop.
In the southern United States, rice has been grown in southern Arkansas, Louisiana, and east Texas since the mid-19th century. Many Cajun farmers grew rice in wet marshes and low lying prairies where they could also farm crayfish when the fields were flooded. In recent years rice production has risen in North America, especially in the Mississippi embayment in the states of Arkansas and Mississippi (see also Arkansas Delta and Mississippi Delta).
Rice cultivation began in California during the California Gold Rush, when an estimated 40,000 Chinese laborers immigrated to the state and grew small amounts of the grain for their own consumption. However, commercial production began only in 1912 in the town of Richvale in Butte County. By 2006, California produced the second largest rice crop in the United States, after Arkansas, with production concentrated in six counties north of Sacramento. Unlike the Arkansas–Mississippi Delta region, California's production is dominated by short- and medium-grain "japonica" varieties, including cultivars developed for the local climate such as Calrose, which makes up as much as 85% of the state's crop.
References to wild rice in the Americas are to the unrelated "Zizania palustris"
More than 100 varieties of rice are commercially produced primarily in six states (Arkansas, Texas, Louisiana, Mississippi, Missouri, and California) in the U.S.
According to estimates for the 2006 crop year, rice production in the U.S. is valued at $1.88 billion, approximately half of which is expected to be exported.
The U.S. provides about 12% of world rice trade.
The majority of domestic utilization of U.S. rice is direct food use (58%), while 16% is used in each of processed foods and beer. 10% is found in pet food.
Australia.
Rice was one of the earliest crops planted in Australia by British settlers, who had experience with rice plantations in the Americas and India.
Although attempts to grow rice in the well-watered north of Australia have been made for many years, they have consistently failed because of inherent iron and manganese toxicities in the soils and destruction by pests.
In the 1920s it was seen as a possible irrigation crop on soils within the Murray-Darling Basin that were too heavy for the cultivation of fruit and too infertile for wheat.
Because irrigation water, despite the extremely low runoff of temperate Australia, was (and remains) very cheap, the growing of rice was taken up by agricultural groups over the following decades. Californian varieties of rice were found suitable for the climate in the Riverina, and the first mill opened at Leeton in 1951.
Even before this Australia's rice production greatly exceeded local needs, and rice exports to Japan have become a major source of foreign currency. Above-average rainfall from the 1950s to the middle 1990s encouraged the expansion of the Riverina rice industry, but its prodigious water use in a practically waterless region began to attract the attention of environmental scientists. These became severely concerned with declining flow in the Snowy River and the lower Murray River.
Although rice growing in Australia is highly profitable due to the cheapness of land, several recent years of severe drought have led many to call for its elimination because of its effects on extremely fragile aquatic ecosystems. The Australian rice industry is somewhat opportunistic, with the area planted varying significantly from season to season depending on water allocations in the Murray and Murrumbidgee irrigation regions.
Production and commerce.
Production.
The world dedicated 162.3 million hectares in 2012 for rice cultivation and the total production was about 738.1 million tonnes.
The average world farm yield for rice was 4.5 tonnes per hectare, in 2012.
Rice farms in Egypt were the most productive in 2012, with a nationwide average of 9.5 tonnes per hectare.
Second place: Australia – 8.9 tonnes per hectare.
Third place: USA – 8.3 tonnes per hectare.
Rice is a major food staple and a mainstay for the rural population and their food security. It is mainly cultivated by small farmers in holdings of less than 1 hectare. Rice is also a wage commodity for workers in the cash crop or non-agricultural sectors. Rice is vital for the nutrition of much of the population in Asia, as well as in Latin America and the Caribbean and in Africa; it is central to the food security of over half the world population. Developing countries account for 95% of the total production, with China and India alone responsible for nearly half of the world output.
World production of rice has risen steadily from about 200 million tonnes of paddy rice in 1960 to over 678 million tonnes in 2009. The three largest producers of rice in 2009 were China (197 million tonnes), India (131 Mt), and Indonesia (64 Mt). Among the six largest rice producers, the most productive farms for rice, in 2009, were in China producing 6.59 tonnes per hectare.
Many rice grain producing countries have significant losses post-harvest at the farm and because of poor roads, inadequate storage technologies, inefficient supply chains and farmer's inability to bring the produce into retail markets dominated by small shopkeepers. A World Bank – FAO study claims 8% to 26% of rice is lost in developing nations, on average, every year, because of post-harvest problems and poor infrastructure. Some sources claim the post-harvest losses to exceed 40%. Not only do these losses reduce food security in the world, the study claims that farmers in developing countries such as China, India and others lose approximately US$89 billion of income in preventable post-harvest farm losses, poor transport, the lack of proper storage and retail. One study claims that if these post-harvest grain losses could be eliminated with better infrastructure and retail network, in India alone enough food would be saved every year to feed 70 to 100 million people over a year. However, other writers have warned against dramatic assessments of post-harvest food losses, arguing that "worst-case scenarios" tend to be used rather than realistic averages and that in many cases the cost of avoiding losses exceeds the value of the food saved.
The seeds of the rice plant are first milled using a rice huller to remove the chaff (the outer husks of the grain). At this point in the process, the product is called brown rice. The milling may be continued, removing the bran, "i.e.", the rest of the husk and the germ, thereby creating white rice. White rice, which keeps longer, lacks some important nutrients; moreover, in a limited diet which does not supplement the rice, brown rice helps to prevent the disease beriberi.
Either by hand or in a rice polisher, white rice may be buffed with glucose or talc powder (often called polished rice, though this term may also refer to white rice in general), parboiled, or processed into flour. White rice may also be enriched by adding nutrients, especially those lost during the milling process. While the cheapest method of enriching involves adding a powdered blend of nutrients that will easily wash off (in the United States, rice which has been so treated requires a label warning against rinsing), more sophisticated methods apply nutrients directly to the grain, coating the grain with a water-insoluble substance which is resistant to washing.
In some countries, a popular form, parboiled rice, is subjected to a steaming or parboiling process while still a brown rice grain. This causes nutrients from the outer husk, especially thiamine, to move into the grain itself. The parboil process causes a gelatinisation of the starch in the grains. The grains become less brittle, and the color of the milled grain changes from white to yellow. The rice is then dried, and can then be milled as usual or used as brown rice. Milled parboiled rice is nutritionally superior to standard milled rice. Parboiled rice has an additional benefit in that it does not stick to the pan during cooking, as happens when cooking regular white rice. This type of rice is eaten in parts of India and countries of West Africa are also accustomed to consuming parboiled rice.
Despite the hypothetical health risks of talc (such as stomach cancer), talc-coated rice remains the norm in some countries due to its attractive shiny appearance, but it has been banned in some, and is no longer widely used in others (such as the United States). Even where talc is not used, glucose, starch, or other coatings may be used to improve the appearance of the grains.
Rice bran, called "nuka" in Japan, is a valuable commodity in Asia and is used for many daily needs. It is a moist, oily inner layer which is heated to produce oil. It is also used as a pickling bed in making rice bran pickles and "takuan".
Raw rice may be ground into flour for many uses, including making many kinds of beverages, such as "amazake, horchata", rice milk, and rice wine. Rice flour does not contain gluten, so is suitable for people on a gluten-free diet. Rice may also be made into various types of noodles. Raw, wild, or brown rice may also be consumed by raw-foodist or fruitarians if soaked and sprouted (usually a week to 30 days – gaba rice).
Processed rice seeds must be boiled or steamed before eating. Boiled rice may be further fried in cooking oil or butter (known as fried rice), or beaten in a tub to make "mochi".
Rice is a good source of protein and a staple food in many parts of the world, but it is not a complete protein: it does not contain all of the essential amino acids in sufficient amounts for good health, and should be combined with other sources of protein, such as nuts, seeds, beans, fish, or meat.
Rice, like other cereal grains, can be puffed (or popped). This process takes advantage of the grains' water content and typically involves heating grains in a special chamber. Further puffing is sometimes accomplished by processing puffed pellets in a low-pressure chamber. The ideal gas law means either lowering the local pressure or raising the water temperature results in an increase in volume prior to water evaporation, resulting in a puffy texture. Bulk raw rice density is about 0.9 g/cm³. It decreases to less than one-tenth that when puffed.
Harvesting, drying and milling.
Unmilled rice, known as paddy (Indonesia and Malaysia: padi; Philippines, palay), is usually harvested when the grains have a moisture content of around 25%. In most Asian countries, where rice is almost entirely the product of smallholder agriculture, harvesting is carried out manually, although there is a growing interest in mechanical harvesting. Harvesting can be carried out by the farmers themselves, but is also frequently done by seasonal labor groups. Harvesting is followed by threshing, either immediately or within a day or two. Again, much threshing is still carried out by hand but there is an increasing use of mechanical threshers. Subsequently, paddy needs to be dried to bring down the moisture content to no more than 20% for milling.
A familiar sight in several Asian countries is paddy laid out to dry along roads. However, in most countries the bulk of drying of marketed paddy takes place in mills, with village-level drying being used for paddy to be consumed by farm families. Mills either sun dry or use mechanical driers or both. Drying has to be carried out quickly to avoid the formation of molds. Mills range from simple hullers, with a throughput of a couple of tonnes a day, that simply remove the outer husk, to enormous operations that can process 4,000 tonnes a day and produce highly polished rice. A good mill can achieve a paddy-to-rice conversion rate of up to 72% but smaller, inefficient mills often struggle to achieve 60%. These smaller mills often do not buy paddy and sell rice but only service farmers who want to mill their paddy for their own consumption.
Distribution.
Because of the importance of rice to human nutrition and food security in Asia, the domestic rice markets tend to be subject to considerable state involvement. While the private sector plays a leading role in most countries, agencies such as BULOG in Indonesia, the NFA in the Philippines, VINAFOOD in Vietnam and the Food Corporation of India are all heavily involved in purchasing of paddy from farmers or rice from mills and in distributing rice to poorer people. BULOG and NFA monopolise rice imports into their countries while VINAFOOD controls all exports from Vietnam.
Trade.
World trade figures are very different from those for production, as less than 8% of rice produced is traded internationally. In economic terms, the global rice trade was a small fraction of 1% of world mercantile trade. Many countries consider rice as a strategic food staple, and various governments subject its trade to a wide range of controls and interventions.
Developing countries are the main players in the world rice trade, accounting for 83% of exports and 85% of imports. While there are numerous importers of rice, the exporters of rice are limited. Just five countries – Thailand, Vietnam, China, the United States and India – in decreasing order of exported quantities, accounted for about three-quarters of world rice exports in 2002. However, this ranking has been rapidly changing in recent years. In 2010, the three largest exporters of rice, in decreasing order of quantity exported were Thailand, Vietnam and India. By 2012, India became the largest exporter of rice with a 100% increase in its exports on year-to-year basis, and Thailand slipped to third position. Together, Thailand, Vietnam and India accounted for nearly 70% of the world rice exports.
The primary variety exported by Thailand and Vietnam were Jasmine rice, while exports from India included aromatic Basmati variety. China, an exporter of rice in early 2000s, was a net importer of rice in 2010 and will become the largest net importer, surpassing Nigeria, in 2013. According to a USDA report, the world's largest exporters of rice in 2012 were India (9.75 million tonnes), Vietnam (7 million tonnes), Thailand (6.5 million tonnes), Pakistan (3.75 million tonnes) and the United States (3.5 million tonnes).
Major importers usually include Nigeria, Indonesia, Bangladesh, Saudi Arabia, Iran, Iraq, Malaysia, the Philippines, Brazil and some African and Persian Gulf countries. In common with other West African countries, Nigeria is actively promoting domestic production. However, its very heavy import duties (110%) open it to smuggling from neighboring countries. Parboiled rice is particularly popular in Nigeria. Although China and India are the two largest producers of rice in the world, both countries consume the majority of the rice produced domestically, leaving little to be traded internationally.
World's most productive rice farms and farmers.
The average world yield for rice was 4.3 tonnes per hectare, in 2010.
Australian rice farms were the most productive in 2010, with a nationwide average of 10.8 tonnes per hectare.
Yuan Longping of China National Hybrid Rice Research and Development Center, China, set a world record for rice yield in 2010 at 19 tonnes per hectare on a demonstration plot. In 2011, this record was surpassed by an Indian farmer, Sumant Kumar, with 22.4 tonnes per hectare in Bihar. Both these farmers claim to have employed newly developed rice breeds and System of Rice Intensification (SRI), a recent innovation in rice farming. SRI is claimed to have set new national records in rice yields, within the last 10 years, in many countries. The claimed Chinese and Indian yields have yet to be demonstrated on seven-hectare lots and to be reproducible over two consecutive years on the same farm.
Price.
In late 2007 to May 2008, the price of grains rose greatly due to droughts in major producing countries (particularly Australia), increased use of grains for animal feed and US subsidies for bio-fuel production. Although there was no shortage of rice on world markets this general upward trend in grain prices led to panic buying by consumers, government rice export bans (in particular, by Vietnam and India) and inflated import orders by the Philippines marketing board, the National Food Authority. This caused significant rises in rice prices. In late April 2008, prices hit 24 US cents a pound, twice the price of seven months earlier. Over the period of 2007 to 2013, the Chinese government has substantially increased the price it pays domestic farmers for their rice, rising to per metric ton by 2013. The 2013 price of rice originating from other southeast Asian countries was a comparably low per metric ton.
On April 30, 2008, Thailand announced plans for the creation of the Organisation of Rice Exporting Countries (OREC) with the intention that this should develop into a price-fixing cartel for rice. However, little progress had been made by mid-2011 to achieve this.
Worldwide consumption.
As of 2009 world food consumption of rice was 531.6 million metric tons of paddy equivalent (354,603 of milled equivalent), while the far largest consumers were China consuming 156.3 million metric tons of paddy equivalent (29.4% of the world consumption) and India consuming 123.5 million metric tons of paddy equivalent (23.3% of the world consumption). Between 1961 and 2002, per capita consumption of rice increased by 40%.
Rice is the most important crop in Asia. In Cambodia, for example, 90% of the total agricultural area is used for rice production.
U.S. rice consumption has risen sharply over the past 25 years, fueled in part by commercial applications such as beer production. Almost one in five adult Americans now report eating at least half a serving of white or brown rice per day.
Environmental impacts.
Rice cultivation on wetland rice fields is thought to be responsible for 1.5% of the anthropogenic methane emissions. Rice requires slightly more water to produce than other grains. Rice production uses almost a third of Earth’s fresh water.
Long-term flooding of rice fields cuts the soil off from atmospheric oxygen and causes anaerobic fermentation of organic matter in the soil. Methane production from rice cultivation contributes ~1.5% of anthropogenic greenhouse gases. Methane is twenty times more potent a greenhouse gas than carbon dioxide.
A 2010 study found that, as a result of rising temperatures and decreasing solar radiation during the later years of the 20th century, the rice yield growth rate has decreased in many parts of Asia, compared to what would have been observed had the temperature and solar radiation trends not occurred. The yield growth rate had fallen 10–20% at some locations. The study was based on records from 227 farms in Thailand, Vietnam, Nepal, India, China, Bangladesh, and Pakistan. The mechanism of this falling yield was not clear, but might involve increased respiration during warm nights, which expends energy without being able to photosynthesize.
Temperature.
Rice requires high temperature above 20 °C but not more than 35 to 40 °C. Optimum temperature is around 30 °C (Tmax) and 20 °C (Tmin).
Solar radiation.
The amount of solar radiation received during 45 days after harvest determines final crop output.
Atmospheric water vapor.
High water vapor content (in humid tropics) subjects unusual stress which favors the spread of fungal and bacterial diseases.
Wind.
Light wind transports CO2 to the leaf canopy but strong wind cause severe damage and may lead to sterility (due to pollen dehydration, spikelet sterility, and abortive endosperms).
Pests and diseases.
Rice pests are any organisms or microbes with the potential to reduce the yield or value of the rice crop (or of rice seeds). Rice pests include weeds, pathogens, insects, nematode, rodents, and birds. A variety of factors can contribute to pest outbreaks, including climatic factors, improper irrigation, the overuse of insecticides and high rates of nitrogen fertilizer application. Weather conditions also contribute to pest outbreaks. For example, rice gall midge and army worm outbreaks tend to follow periods of high rainfall early in the wet season, while thrips outbreaks are associated with drought.
Insects.
Major rice insect pests include: the brown planthopper (BPH), several spp. of stemborers – including those in the genera "Scirpophaga" and "Chilo", the rice gall midge, several spp. of rice bugs – notably in the genus "Leptocorisa", the rice leafroller and rice weevils.
Diseases.
Rice blast, caused by the fungus "Magnaporthe grisea", is the most significant disease affecting rice cultivation. Other major rice diseases include: sheath blight, rice ragged stunt (vector: BPH), and tungro (vector: "Nephotettix" spp). There is also an ascomycete fungus, "Cochliobolus miyabeanus", that causes brown spot disease in rice.
Nematodes.
Several nematode species infect rice crops, causing diseases such as Ufra (Ditylenchus dipsaci), White tip disease (Aphelenchoide bessei), and root knot disease (Meloidogyne graminicola). Some nematode species such as "Pratylenchus" spp. are most dangerous in upland rice of all parts of the world. Rice root nematode ("Hirschmanniella oryzae") is a migratory endoparasite which on higher inoculum levels will lead to complete destruction of a rice crop. Beyond being obligate parasites, they also decrease the vigor of plants and increase the plants' susceptibility to other pests and diseases.
Other Pests.
These include: the apple snail "Pomacea canaliculata", panicle rice mite, rats, and the weed "Echinochloa crusgali".
Integrated Pest Management.
Crop protection scientists are trying to develop rice pest management techniques which are sustainable. In other words, to manage crop pests in such a manner that future crop production is not threatened. Sustainable pest management is based on four principles: biodiversity, host plant resistance (HPR), landscape ecology, and hierarchies in a landscape – from biological to social. At present, rice pest management includes cultural techniques, pest-resistant rice varieties, and pesticides (which include insecticide). Increasingly, there is evidence that farmers' pesticide applications are often unnecessary, and even facilitate pest outbreaks. By reducing the populations of natural enemies of rice pests, misuse of insecticides can actually lead to pest outbreaks. The International Rice Research Institute (IRRI) demonstrated in 1993 that an 87.5% reduction in pesticide use can lead to an overall drop in pest numbers. IRRI also conducted two campaigns in 1994 and 2003, respectively, which discouraged insecticide misuse and smarter pest management in Vietnam.
Rice plants produce their own chemical defenses to protect themselves from pest attacks. Some synthetic chemicals, such as the herbicide 2,4-D, cause the plant to increase the production of certain defensive chemicals and thereby increase the plant’s resistance to some types of pests. Conversely, other chemicals, such as the insecticide imidacloprid, can induce changes in the gene expression of the rice that cause the plant to become more susceptible to attacks by certain types of pests. 5-Alkylresorcinols are chemicals that can also be found in rice.
Botanicals, so-called "natural pesticides", are used by some farmers in an attempt to control rice pests. Botanicals include extracts of leaves, or a mulch of the leaves themselves. Some upland rice farmers in Cambodia spread chopped leaves of the bitter bush ("Chromolaena odorata") over the surface of fields after planting. This practice probably helps the soil retain moisture and thereby facilitates seed germination. Farmers also claim the leaves are a natural fertilizer and helps suppress weed and insect infestations. 
Among rice cultivars, there are differences in the responses to, and recovery from, pest damage. Many rice varieties have been selected for resistance to insect pests. Therefore, particular cultivars are recommended for areas prone to certain pest problems. The genetically based ability of a rice variety to withstand pest attacks is called resistance. Three main types of plant resistance to pests are recognized as nonpreference, antibiosis, and tolerance. Nonpreference (or antixenosis) describes host plants which insects prefer to avoid; antibiosis is where insect survival is reduced after the ingestion of host tissue; and tolerance is the capacity of a plant to produce high yield or retain high quality despite insect infestation.
Over time, the use of pest-resistant rice varieties selects for pests that are able to overcome these mechanisms of resistance. When a rice variety is no longer able to resist pest infestations, resistance is said to have broken down. Rice varieties that can be widely grown for many years in the presence of pests and retain their ability to withstand the pests are said to have durable resistance. Mutants of popular rice varieties are regularly screened by plant breeders to discover new sources of durable resistance.
Parasitic weeds.
Rice is parasitized by the weed eudicot "Striga hermonthica", which is of local importance for this crop.
Ecotypes and cultivars.
While most rice is bred for crop quality and productivity, there are varieties selected for characteristics such as texture, smell, and firmness. There are four major categories of rice worldwide: indica, japonica, aromatic and glutinous. The different varieties of rice are not considered interchangeable, either in food preparation or agriculture, so as a result, each major variety is a completely separate market from other varieties. It is common for one variety of rice to rise in price while another one drops in price.
Rice cultivars also fall into groups according to environmental conditions, season of planting, and season of harvest, called ecotypes. Some major groups are the Japan-type (grown in Japan), "buly" and "tjereh" types (Indonesia); "aman" (main winter crop), "aus" ("aush", summer), and "boro" (spring) (Bengal and Assam). Cultivars exist that are adapted to deep flooding, and these are generally called "floating rice".
The largest collection of rice cultivars is at the International Rice Research Institute in the Philippines, with over 100,000 rice accessions held in the International Rice Genebank. Rice cultivars are often classified by their grain shapes and texture. For example, Thai Jasmine rice is long-grain and relatively less sticky, as some long-grain rice contains less amylopectin than short-grain cultivars. Chinese restaurants often serve long-grain as plain unseasoned steamed rice though short-grain rice is common as well. Japanese mochi rice and Chinese sticky rice are short-grain. Chinese people use sticky rice which is properly known as "glutinous rice" (note: glutinous refer to the glue-like characteristic of rice; does not refer to "gluten") to make zongzi. The Japanese table rice is a sticky, short-grain rice. Japanese sake rice is another kind as well.
Indian rice cultivars include long-grained and aromatic Basmati (ਬਾਸਮਤੀ) (grown in the North), long and medium-grained Patna rice, and in South India (Andhra Pradesh and Karnataka) short-grained Sona Masuri (also called as Bangaru theegalu). In the state of Tamil Nadu, the most prized cultivar is "ponni" which is primarily grown in the delta regions of the Kaveri River. Kaveri is also referred to as ponni in the South and the name reflects the geographic region where it is grown. In the Western Indian state of Maharashtra, a short grain variety called Ambemohar is very popular. This rice has a characteristic fragrance of Mango blossom.
Aromatic rices have definite aromas and flavors; the most noted cultivars are Thai fragrant rice, Basmati, Patna rice, Vietnamese fragrant rice, and a hybrid cultivar from America, sold under the trade name Texmati. Both Basmati and Texmati have a mild popcorn-like aroma and flavor. In Indonesia, there are also "red" and "black" cultivars.
High-yield cultivars of rice suitable for cultivation in Africa and other dry ecosystems, called the new rice for Africa (NERICA) cultivars, have been developed. It is hoped that their cultivation will improve food security in West Africa.
Draft genomes for the two most common rice cultivars, "indica" and "japonica", were published in April 2002. Rice was chosen as a model organism for the biology of grasses because of its relatively small genome (~430 megabase pairs). Rice was the first crop with a complete genome sequence.
On December 16, 2002, the UN General Assembly declared the year 2004 the International Year of Rice. The declaration was sponsored by more than 40 countries.
Biotechnology.
High-yielding varieties.
The high-yielding varieties are a group of crops created intentionally during the Green Revolution to increase global food production. This project enabled labor markets in Asia to shift away from agriculture, and into industrial sectors. The first "Rice Car", IR8 was produced in 1966 at the International Rice Research Institute which is based in the Philippines at the University of the Philippines' Los Baños site. IR8 was created through a cross between an Indonesian variety named "Peta" and a Chinese variety named "Dee Geo Woo Gen."
Scientists have identified and cloned many genes involved in the gibberellin signaling pathway, including GAI1 (Gibberellin Insensitive) and SLR1 (Slender Rice). Disruption of gibberellin signaling can lead to significantly reduced stem growth leading to a dwarf phenotype. Photosynthetic investment in the stem is reduced dramatically as the shorter plants are inherently more stable mechanically. Assimilates become redirected to grain production, amplifying in particular the effect of chemical fertilizers on commercial yield. In the presence of nitrogen fertilizers, and intensive crop management, these varieties increase their yield two to three times.
Future potential.
As the UN Millennium Development project seeks to spread global economic development to Africa, the "Green Revolution" is cited as the model for economic development. With the intent of replicating the successful Asian boom in agronomic productivity, groups like the Earth Institute are doing research on African agricultural systems, hoping to increase productivity. An important way this can happen is the production of "New Rices for Africa" (NERICA). These rices, selected to tolerate the low input and harsh growing conditions of African agriculture, are produced by the African Rice Center, and billed as technology "from Africa, for Africa". The NERICA have appeared in "The New York Times" (October 10, 2007) and "International Herald Tribune" (October 9, 2007), trumpeted as miracle crops that will dramatically increase rice yield in Africa and enable an economic resurgence. Ongoing research in China to develop perennial rice could result in enhanced sustainability and food security.
Golden rice.
Rice kernels do not contain vitamin A, so people who obtain most of their calories from rice are at risk of vitamin A deficiency. German and Swiss researchers have genetically engineered rice to produce beta-carotene, the precursor to vitamin A, in the rice kernel. The beta-carotene turns the processed (white) rice a "gold" color, hence the name "golden rice." The beta-carotene is converted to vitamin A in humans who consume the rice. Although some rice strains produce beta-carotene in the hull, no non-genetically engineered strains have been found that produce beta-carotene in the kernel, despite the testing of thousands of strains. Additional efforts are being made to improve the quantity and quality of other nutrients in golden rice.
The International Rice Research Institute is currently further developing and evaluating Golden Rice as a potential new way to help address vitamin A deficiency.
Expression of human proteins.
Ventria Bioscience has genetically modified rice to express lactoferrin, lysozyme which are proteins usually found in breast milk, and human serum albumin, These proteins have antiviral, antibacterial, and antifungal effects.
Rice containing these added proteins can be used as a component in oral rehydration solutions which are used to treat diarrheal diseases, thereby shortening their duration and reducing recurrence. Such supplements may also help reverse anemia.
Flood tolerant rice.
Flooding is an issue that many rice growers face, especially in South and South East Asia where flooding annually affects 20 million hectares. Standard rice varieties cannot withstand stagnant flooding of more than about a week, mainly as it disallows the plant access to necessary requirements such as sunlight and essential gas exchanges, inevitably leading to plants being unable to recover. 
In the past, this has led to a massive losses in yields, such as in the Philippines, where in 2006, rice crops worth $65 million were lost to flooding.
In response to this hazard, a variety of rice named Swarna Sub1 was developed via marker-assisted selection, with the ability to withstand prolonged periods of around 14 days beneath a flooded plain. The submergence tolerance ability of this variety is conferred by the presence of the Sub1A gene, introgressed from the Indian cultivar FR13A into the flood-vulnerable (but high yielding) cultivar Swarna. 
Swarna Sub1 effectively enters a dormant, energy conserving state upon being submerged in a flooded rice paddy, a process that involves the finely controlled metabolism of enzymes such amylases, starch phosphorylase and alcohol dehydrogenase, allowing the plant to survive with limited oxygen and sunlight unlike its standard variety relatives. 
Given that the presence of the Sub1A gene does not impact upon the quality or quantity of the rice obtained, 
this variety has been very popular, with 1.7 million hectares of land in India having Swarna Sub1 and other flood resistant varieties used instead of conventional rice crops.
Drought tolerant rice.
Drought represents a significant environmental stress for rice production, with 19–23 million hectares of rainfed rice production in South and South East Asia often at risk. Under drought conditions, without sufficient water to afford them the ability to obtain the required levels of nutrients from the soil, conventional commercial rice varieties can be severely impacted – for example yield losses as high as 40% have affected some parts of India, with resulting losses of around US$800 million annually.
The International Rice Research Institute (IRRI) conducts research into developing drought tolerant rice varieties, including the varieties 5411 and Sookha dhan, currently being employed by farmers in the Philippines and Nepal respectively. In addition, in 2013 the Japanese National Institute for Agrobiological Sciences led a team which successfully inserted the DEEPER ROOTING 1 (DRO1), from the Philippine upland rice variety Kinandang Patong, into the popular commercial rice variety IR64, giving rise to a far deeper root system in the resulting plants. 
This facilitates an improved ability for the rice plant to derive its required nutrients in times of drought via accessing deeper layers of soil, a feature demonstrated by trials which saw the IR64 + DRO1 rice yields drop by 10% under moderate drought conditions, compared to 60% for the unmodified IR64 variety.
Salt tolerant rice.
Soil salinity poses a major threat to rice crop productivity, particularly along low-lying coastal areas during the dry season 
– for example, roughly 1 million hectares of the coastal areas of Bangladesh are affected by saline soils. 
These high concentrations of salt can severely impact upon rice plants’ normal physiology, especially during early stages of growth, and as such farmers are often forced to abandon these otherwise potentially usable areas.
Progress has been made, however, in developing rice varieties capable of tolerating such conditions; the hybrid created from the cross between the commercial rice variety IR56 and the wild rice species "Oryza coarctata" is one example. "O. coarctata" is capable of successful growth in soils with double the limit of salinity of normal varieties, but lacks the ability to produce edible rice.
Developed by the International Rice Research Institute, the hybrid variety can utilise specialised leaf glands that allow for the removal of salt into the atmosphere. It was initially produced from one successful embryo out of 34,000 crosses between the two species; this was then backcrossed to IR56 with the aim of preserving the genes responsible for salt tolerance that were inherited from "O. coarctata". 
Furthermore, extensive trials are planned prior to the new variety being available to farmers by approximately 2017–18.
Meiosis and DNA repair.
Rice is used as a model organism for investigating the molecular mechanisms of meiosis and DNA repair in higher plants. Meiosis is a key stage of the sexual cycle in which diploid cells in the ovule (female structure) and the anther (male structure) produce haploid cells that develop further into gametophytes and gametes. So far, 28 meiotic genes of rice have been characterized. Studies of rice gene OsRAD51C showed that this gene is necessary for homologous recombinational repair of DNA, particularly the accurate repair of DNA double-strand breaks during meiosis. Rice gene OsDMC1 was found to be essential for pairing of homologous chromosomes during meiosis, and rice gene OsMRE11 was found to be required for both synapsis of homologous chromosomes and repair of double-strand breaks during meiosis.
Cultural roles of rice.
Rice plays an important role in certain religions and popular beliefs. In many cultures relatives will scatter rice during or towards the end of a wedding ceremony in front of the bride and groom.
The pounded rice ritual is conducted during weddings in Nepal. The bride gives a leafplate full of pounded rice to the groom after he requests it politely from her.
In the Philippines rice wine, popularly known as "tapuy", is used for important occasions such as weddings, rice harvesting ceremonies and other celebrations.
Dewi Sri is the traditional rice goddess of the Javanese, Sundanese, and Balinese people in Indonesia. Most rituals involving Dewi Sri are associated with the mythical origin attributed to the rice plant, the staple food of the region.
In Thailand a similar rice deity is known as "Phosop"; she is a deity more related to ancient local folklore than a goddess of a structured, mainstream religion. The same female rice deity is known as "Po Ino Nogar" in Cambodia and as "Nang Khosop" in Laos. Ritual offerings are made during the different stages of rice production to propitiate the Rice Goddess in the corresponding cultures.

</doc>
<doc id="37073" url="http://en.wikipedia.org/wiki?curid=37073" title="Civil disobedience">
Civil disobedience

Civil disobedience is the active, professed refusal to obey certain laws, demands, or commands of a government, or of an occupying international power. Civil disobedience is sometimes, though not always, defined as being nonviolent resistance.
Overview.
One of its earliest massive implementations was brought about by Egyptians against the British occupation in the 1919 Revolution. Civil disobedience is one of the many ways people have rebelled against what they deem to be unfair laws. It has been used in many nonviolent resistance movements in India (Gandhi's campaigns for independence from the British Empire), in Czechoslovakia's Velvet Revolution and in East Germany to oust their communist governments, In South Africa in the fight against apartheid, in the American Civil Rights Movement, in the Singing Revolution to bring independence to the Baltic countries from the Soviet Union, recently with the 2003 Rose Revolution in Georgia and the 2004 Orange Revolution in Ukraine, among other various movements worldwide.
One of the oldest depictions of civil disobedience is in Sophocles' play "Antigone", in which Antigone, one of the daughters of former King of Thebes, Oedipus, defies Creon, the current King of Thebes, who is trying to stop her from giving her brother Polynices a proper burial. She gives a stirring speech in which she tells him that she must obey her conscience rather than human law. She is not at all afraid of the death he threatens her with (and eventually carries out), but she is afraid of how her conscience will smite her if she does not do this.
Following the Peterloo massacre of 1819, poet Percy Shelley wrote the political poem "The Mask of Anarchy" later that year, that begins with the images of what he thought to be the unjust forms of authority of his time—and then imagines the stirrings of a new form of social action. It is perhaps the first modern statement of the principle of nonviolent protest. A version was taken up by the author Henry David Thoreau in his essay "Civil Disobedience", and later by Gandhi in his doctrine of "Satyagraha". Gandhi's Satyagraha was partially influenced and inspired by Shelley's nonviolence in protest and political action. In particular, it is known that Gandhi would often quote Shelley's "Masque of Anarchy" to vast audiences during the campaign for a free India.
Thoreau's 1848 essay "Civil Disobedience", originally titled "Resistance to Civil Government", has had a wide influence on many later practitioners of civil disobedience. The driving idea behind the essay is that citizens are morally responsible for their support of aggressors, even when such support is required by law. In the essay, Thoreau explained his reasons for having refused to pay taxes as an act of protest against slavery and against the Mexican-American War. He writes, "If I devote myself to other pursuits and contemplations, I must first see, at least, that I do not pursue them sitting upon another man's shoulders. I must get off him first, that he may pursue his contemplations too. See what gross inconsistency is tolerated. I have heard some of my townsmen say, 'I should like to have them order me out to help put down an insurrection of the slaves, or to march to Mexico; — see if I would go'; and yet these very men have each, directly by their allegiance, and so indirectly, at least, by their money, furnished a substitute."
By the 1850s, a range of minority groups in the United States--blacks, Jews, Seventh Day Baptists, Catholics, antiprohibitionists, racial egalitarians, and others--employed civil disobedience to combat a range of legal measures and public practices that to them promoted ethnic, religious, and racial discrimination. Public and typically peaceful resistance to public power would remain an integral tactic in modern American minority-rights politics.
Etymology.
Thoreau's 1849 essay "Resistance to Civil Government" was eventually renamed "Essay on Civil Disobedience." After his landmark lectures were published in 1866, the term began to appear in numerous sermons and lectures relating to slavery and the war in Mexico. Thus, by the time Thoreau's lectures were first published under the title "Civil Disobedience," in 1866, four years after his death, the term had achieved fairly widespread usage.
It has been argued that the term "civil disobedience" has always suffered from ambiguity and in modern times, become utterly debased. Marshall Cohen notes, "It has been used to describe everything from bringing a test-case in the federal courts to taking aim at a federal official. Indeed, for Vice President Agnew it has become a code-word describing the activities of muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins."
LeGrande writes that "the formulation of a single all-encompassing definition of the term is extremely difficult, if not impossible. In reviewing the voluminous literature on the subject, the student of civil disobedience rapidly finds himself surrounded by a maze of semantical problems and grammatical niceties. Like Alice in Wonderland, he often finds that specific terminology has no more (or no less) meaning than the individual orator intends it to have." He encourages a distinction between lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience.
In a letter to P.K.Rao, dated September 10, 1935, Gandhi disputes that his idea of civil disobedience was derived from the writings of Thoreau:
Theories.
In seeking an active form of civil disobedience, one may choose to deliberately break certain laws, such as by forming a peaceful blockade or occupying a facility illegally, though sometimes violence has been known to occur. Often there is an expectation to be attacked or even beaten by the authorities. Protesters often undergo training in advance on how to react to arrest or to attack.
Civil disobedience is usually defined as pertaining to a citizen's relation to the state and its laws, as distinguished from a constitutional impasse in which two public agencies, especially two equally sovereign branches of government, conflict. For instance, if the head of government of a country were to refuse to enforce a decision of that country's highest court, it would not be civil disobedience, since the head of government would be acting in her or his capacity as public official rather than private citizen.
Ronald Dworkin held that there are three types of civil disobedience:
Some theories of civil disobedience hold that civil disobedience is only justified against governmental entities. Brownlee argues that disobedience in opposition to the decisions of non-governmental agencies such as trade unions, banks, and private universities can be justified if it reflects "a larger challenge to the legal system that permits those decisions to be taken". The same principle, she argues, applies to breaches of law in protest against international organizations and foreign governments.
It is usually recognized that lawbreaking, if it is not done publicly, at least must be publicly announced in order to constitute civil disobedience. But Stephen Eilmann argues that if it is necessary to disobey rules that conflict with morality, we might ask why disobedience should take the form of public civil disobedience rather than simply covert lawbreaking. If a lawyer wishes to help a client overcome legal obstacles to securing her or his natural rights, he might, for instance, find that assisting in fabricating evidence or committing perjury is more effective than open disobedience. This assumes that common morality does not have a prohibition on deceit in such situations. The Fully Informed Jury Association's publication "A Primer for Prospective Jurors" notes, "Think of the dilemma faced by German citizens when Hitler's secret police demanded to know if they were hiding a Jew in their house." By this definition, civil disobedience could be traced back to the Book of Exodus, where Shiphrah and Puah refused a direct order of Pharaoh but misrepresented how they did it. (Exodus 1: 15-19)
Violent vs. nonviolent.
There have been debates as to whether civil disobedience must necessarily be non-violent. Black's Law Dictionary includes nonviolence in its definition of civil disobedience. Christian Bay's encyclopedia article states that civil disobedience requires "carefully chosen and legitimate means," but holds that they do not have to be nonviolent. It has been argued that, while both civil disobedience and civil rebellion are justified by appeal to constitutional defects, rebellion is much more destructive; therefore, the defects justifying rebellion must be much more serious than those justifying disobedience, and if one cannot justify civil rebellion, then one cannot justify a civil disobedients' use of force and violence and refusal to submit to arrest. Civil disobedients' refraining from violence is also said to help preserve society's tolerance of civil disobedience. 
Philosopher H.J. McCloskey argues that "if violent, intimidatory, coercive disobedience is more effective, it is, other things being equal, more justified than less effective, nonviolent disobedience." In his best-selling "Disobedience and Democracy: Nine Fallacies on Law and Order", Howard Zinn takes a similar position; Zinn states that while the goals of civil disobedience are generally nonviolent,
Zinn rejects any “easy and righteous dismissal of violence,” noting that Henry Thoreau, the popularizer of the term civil disobedience, approved of the armed insurrection of John Brown. He also notes that some major civil disobedience campaigns which have been classified as nonviolent, such as the Birmingham campaign, have actually included elements of violence. 
Revolutionary vs. non-revolutionary.
Non-revolutionary civil disobedience is a simple disobedience of laws on the grounds that they are judged "wrong" by an individual conscience, or as part of an effort to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue. Revolutionary civil disobedience is more of an active attempt to overthrow a government. Gandhi's acts have been described as revolutionary civil disobedience. It has been claimed that the Hungarians under Ferenc Deák directed revolutionary civil disobedience against the Austrian government. Thoreau also wrote of civil disobedience accomplishing "peaceable revolution." Howard Zinn, Harvey Wheeler, and others have identified the right espoused in "The Declaration of Independence" to "alter or abolish" an unjust government to be a principle of civil disobedience. 
Collective vs. solitary.
The earliest recorded incidents of collective civil disobedience took place during the Roman Empire. Unarmed Jews gathered in the streets to prevent the installation of pagan images in the Temple in Jerusalem. In modern times, some activists who commit civil disobedience as a group collectively refuse to sign bail until certain demands are met, such as favorable bail conditions, or the release of all the activists. This is a form of jail solidarity. There have also been many instances of solitary civil disobedience, such as that committed by Thoreau, but these sometimes go unnoticed. Thoreau, at the time of his arrest, was not yet a well-known author, and his arrest was not covered in any newspapers in the days, weeks and months after it happened. The tax collector who arrested him rose to higher political office, and Thoreau's essay was not published until after the end of the Mexican War.
Techniques.
Choice of specific act.
Civil disobedients have chosen a variety of different illegal acts. Bedau writes, "There is a whole class of acts, undertaken in the name of civil disobedience, which, even if they were widely practiced, would in themselves constitute hardly more than a nuisance (e.g. trespassing at a nuclear-missile installation)...Such acts are often just a harassment and, at least to the bystander, somewhat inane...The remoteness of the connection between the disobedient act and the objectionable law lays such acts open to the charge of ineffectiveness and absurdity." Bedau also notes, though, that the very harmlessness of such entirely symbolic illegal protests toward public policy goals may serve a propaganda purpose. Some civil disobedients, such as the proprietors of illegal medical cannabis dispensaries and Voice in the Wilderness, which brought medicine to Iraq without the permission of the U.S. Government, directly achieve a desired social goal (such as the provision of medication to the sick) while openly breaking the law. Julia Butterfly Hill lived in Luna, a 180 ft-tall, 600-year-old California Redwood tree for 738 days, successfully preventing it from being cut down.
In cases where the criminalized behavior is pure speech, civil disobedience can consist simply of engaging in the forbidden speech. An example would be WBAI's broadcasting the track "Filthy Words" from a George Carlin comedy album, which eventually led to the 1978 Supreme Court case of "FCC v. Pacifica Foundation". Threatening government officials is another classic way of expressing defiance toward the government and unwillingness to stand for its policies. For example, Joseph Haas was arrested for allegedly sending an email to the Lebanon, New Hampshire city councilors stating, "Wise up or die."
More generally, protesters of particular victimless crimes often see fit to openly commit that crime. Laws against public nudity, for instance, have been protested by going naked in public, and laws against cannabis consumption have been protested by openly possessing it and using it at cannabis rallies.
Some forms of civil disobedience, such as illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins, make it more difficult for a system to function. In this way, they might be considered coercive. Brownlee notes that "although civil disobedients are constrained in their use of coercion by their conscientious aim to engage in moral dialogue, nevertheless they may find it necessary to employ limited coercion in order to get their issue onto the table." The Plowshares organization temporarily closed GCSB Waihopai by padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes.
Electronic civil disobedience can include web site defacements, redirects, denial-of-service attacks, information theft and data leaks, illegal web site parodies, virtual sit-ins, and virtual sabotage. It is distinct from other kinds of hacktivism in that the perpetrator openly reveals his identity. Virtual actions rarely succeed in completely shutting down their targets, but they often generate significant media attention.
Dilemma actions are designed to create a "response dilemma" for public authorities "by forcing them to either concede some public space to protesters or make themselves look absurd or heavy-handed by acting against the protest."
Cooperation with authorities.
Some disciplines of civil disobedience hold that the protestor must submit to arrest and cooperate with the authorities. Others advocate falling limp or resisting arrest, especially when it will hinder the police from effectively responding to a mass protest.
Many of the same decisions and principles that apply in other criminal investigations and arrests arise also in civil disobedience cases. For example, the suspect may need to decide whether or not to grant a consent search of his property, and whether or not to talk to police officers. It is generally agreed within the legal community, and is often believed within the activist community, that a suspect's talking to criminal investigators can serve no useful purpose, and may be harmful. However, some civil disobedients have nonetheless found it hard to resist responding to investigators' questions, sometimes due to a lack of understanding of the legal ramifications, or due to a fear of seeming rude. Also, some civil disobedients seek to use the arrest as an opportunity to make an impression on the officers. Thoreau wrote, "My civil neighbor, the tax-gatherer, is the very man I have to deal with--for it is, after all, with men and not with parchment that I quarrel--and he has voluntarily chosen to be an agent of the government. How shall he ever know well that he is and does as an officer of the government, or as a man, until he is obliged to consider whether he will treat me, his neighbor, for whom he has respect, as a neighbor and well-disposed man, or as a maniac and disturber of the peace, and see if he can get over this obstruction to his neighborliness without a ruder and more impetuous thought or speech corresponding with his action."
Some civil disobedients feel it is incumbent upon them to accept punishment because of their belief in the validity of the social contract, which is held to bind all to obey the laws that a government meeting certain standards of legitimacy has established, or else suffer the penalties set out in the law. Other civil disobedients who favor the existence of government still don't believe in the legitimacy of their particular government, or don't believe in the legitimacy of a particular law it has enacted. And still other civil disobedients, being anarchists, don't believe in the legitimacy of any government, and therefore see no need to accept punishment for a violation of criminal law that does not infringe the rights of others.
Choice of plea.
An important decision for civil disobedients is whether or not to plead guilty. There is much debate on this point, as some believe that it is a civil disobedient's duty to submit to the punishment prescribed by law, while others believe that defending oneself in court will increase the possibility of changing the unjust law. It has also been argued that either choice is compatible with the spirit of civil disobedience. ACT-UP's Civil Disobedience Training handbook states that a civil disobedient who pleads guilty is essentially stating, "Yes, I committed the act of which you accuse me. I don't deny it; in fact, I am proud of it. I feel I did the right thing by violating this particular law; I am guilty as charged," but that pleading not guilty sends a message of, "Guilt implies wrong-doing. I feel I have done no wrong. I may have violated some specific laws, but I am guilty of doing no wrong. I therefore plead not guilty." A plea of no contest is sometimes regarded as a compromise between the two. One defendant accused of illegally protesting nuclear power, when asked to enter his plea, stated, "I plead for the beauty that surrounds us"; this is known as a "creative plea," and will usually be interpreted as a plea of not guilty.
When the Committee for Non-Violent Action sponsored a protest in August 1957, at the Camp Mercury nuclear test site near Las Vegas, Nevada, 13 of the protesters attempted to enter the test site knowing that they faced arrest. At a pre-arranged announced time, one at a time they stepped across the "line" and were immediately arrested. They were put on a bus and taken to the Nye County seat of Tonopah, Nevada, and arraigned for trial before the local Justice of the Peace, that afternoon. A well known civil rights attorney, Francis Heisler, had volunteered to defend the arrested persons, advising them to plead "nolo contendere", as an alternative to pleading either guilty or not-guilty. The arrested persons were found "guilty," nevertheless, and given suspended sentences, conditional on their not reentering the test site grounds.
Howard Zinn writes, "There may be many times when protesters "choose" to go to jail, as a way of continuing their protest, as a way of reminding their countrymen of injustice. But that is different than the notion that they "must" go to jail as part of a rule connected with civil disobedience. The key point is that the spirit of protest should be maintained all the way, whether it is done by remaining in jail, or by evading it. To accept jail penitently as an accession to 'the rules' is to switch suddenly to a spirit of subservience, to demean the seriousness of the protest...In particular, the neo-conservative insistence on a guilty plea should be eliminated."
Sometimes the prosecution proposes a plea bargain to civil disobedients, as in the case of the Camden 28, in which the defendants were offered an opportunity to plead guilty to one misdemeanor count and receive no jail time. In some mass arrest situations, the activists decide to use solidarity tactics to secure the same plea bargain for everyone. But some activists have opted to enter a blind plea, pleading guilty without any plea agreement in place. Mohandas Gandhi pleaded guilty and told the court, "I am here to . . . submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen."
Legal implications of civil disobedience.
Barkan writes that if defendants plead not guilty, "they must decide whether their primary goal will be to win an acquittal and avoid imprisonment or a fine, or to use the proceedings as a forum to inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law via civil disobedience." A technical defense may enhance the chances for acquittal but make for more boring proceedings and reduced press coverage. During the Vietnam War era, the Chicago Eight used a political defense, while Benjamin Spock used a technical defense. In countries such as the United States whose laws guarantee the right to a jury trial but do not excuse lawbreaking for political purposes, some civil disobedients seek jury nullification. Over the years, this has been made more difficult by court decisions such as "Sparf v. United States", which held that the judge need not inform jurors of their nullification prerogative, and "United States v. Dougherty", which held that the judge need not allow defendants to openly seek jury nullification.
Governments have generally not recognized the legitimacy of civil disobedience or viewed political objectives as an excuse for breaking the law. Specifically, the law usually distinguishes between criminal motive and criminal intent; the offender's motives or purposes may be admirable and praiseworthy, but his intent may still be criminal. Hence the saying that "if there is any possible justification of civil disobedience it must come from outside the legal system."
One theory is that, while disobedience may be helpful, any great amount of it would undermine the law by encouraging general disobedience which is neither conscientious nor of social benefit. Therefore, conscientious lawbreakers must be punished. Michael Bayles argues that if a person violates a law in order to create a test case as to the constitutionality of a law, and then wins his case, then that act did not constitute civil disobedience. It has also been argued that breaking the law for self-gratification, as in the case of a homosexual or cannabis user who does not direct his act at securing the repeal of amendment of the law, is not civil disobedience. Likewise, a protestor who attempts to escape punishment by committing the crime covertly and avoiding attribution, or by denying having committed the crime, or by fleeing the jurisdiction, is generally viewed as not being a civil disobedient.
Courts have distinguished between two types of civil disobedience: "Indirect civil disobedience involves violating a law which is not, itself, the object of protest, whereas direct civil disobedience involves protesting the existence of a particular law by breaking that law." During the Vietnam War, courts typically refused to excuse the perpetrators of illegal protests from punishment on the basis of their challenging the legality of the Vietnam War; the courts ruled it was a political question. The necessity defense has sometimes been used as a shadow defense by civil disobedients to deny guilt without denouncing their politically motivated acts, and to present their political beliefs in the courtroom. However, court cases such as "U.S. v. Schoon" have greatly curtailed the availability of the political necessity defense. Likewise, when Carter Wentworth was charged for his role in the Clamshell Alliance's 1977 illegal occupation of the Seabrook Station Nuclear Power Plant, the judge instructed the jury to disregard his competing harms defense, and he was found guilty. Fully Informed Jury Association activists have sometimes handed out educational leaflets inside courthouses despite admonitions not to; according to FIJA, many of them have escaped prosecution because "prosecutors have reasoned (correctly) that if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence."
Along with giving the offender his "just deserts", achieving crime control via incapacitation and deterrence is a major goal of criminal punishment. Brownlee argues, "Bringing in deterrence at the level of justification detracts from the law’s engagement in a moral dialogue with the offender as a rational person because it focuses attention on the threat of punishment and not the moral reasons to follow this law." Leonard Hubert Hoffmann writes, "In deciding whether or not to impose punishment, the most important consideration would be whether it would do more harm than good. This means that the objector has no right not to be punished. It is a matter for the state (including the judges) to decide on utilitarian grounds whether to do so or not."
See also.
Ideas
Groups
People
By country
Documents
External links.
</dl>

</doc>
<doc id="37151" url="http://en.wikipedia.org/wiki?curid=37151" title="Association for the Taxation of Financial Transactions and for Citizens' Action">
Association for the Taxation of Financial Transactions and for Citizens' Action

The Association pour la Taxation des Transactions financières et pour l'Action Citoyenne (Association for the Taxation of financial Transactions and Citizen's Action, ATTAC) is an activist organization originally created for promoting the establishment of a tax on foreign exchange transactions.
Background.
Originally called "Action for a Tobin Tax to Assist the Citizen", ATTAC was a single-issue movement demanding the introduction of the so-called Tobin tax on currency speculation. ATTAC now devotes itself to a wide range of issues related to globalisation, monitoring the decisions of the World Trade Organization (WTO), the Organisation for Economic Co-operation and Development (OECD) and the International Monetary Fund (IMF). ATTAC attends the meetings of the G8 with the goal of influencing policymakers' decisions. Attac recently criticised Germany for what it called the criminalisation of anti-G8 groups.
At the founding, ATTAC had specific statutory objectives based on the promotion of the Tobin tax. For example, ATTAC Luxembourg specifies in article 1 of its statutes that it "aims to produce and communicate information, and to promote and carry out activities of all kinds for the recapture, by the citizens, of the power that the financial sector has on all aspects of political, economic, social and cultural life throughout the world. Such means include the taxation of transactions in foreign exchange markets (Tobin tax)."
ATTAC claims not to an anti-globalization movement, but it criticises the neoliberal ideology that it sees as dominating economic globalisation. It supports globalisation policies that they characterise as sustainable and socially just. One of ATTAC's slogans is "The World is not for sale", denouncing the "merchandisation" of society. Another slogan is "Another world is possible" pointing to an alternative globalization where people and not profit is in focus.
James Tobin opposing ATTAC.
Attac was originally founded to promote the Tobin tax by the Keynesian economist James Tobin. Tobin himself has accused Attac for misusing his name and said that he has nothing in common with Attac and is a supporter of free trade — "everything that these movements are attacking. They're misusing my name." 
Organisational history.
In December 1998, Ignacio Ramonet wrote in "Le Monde diplomatique" an editorial in which he advocated the establishment of the Tobin tax and the creation of an organisation to pressure governments around the world to introduce the tax. ATTAC was created on June 3, 1998, during a constitutive assembly in France. While it was founded in France it now exists in over forty countries around the world. In France, politicians from the left are members of the association. In Luxembourg, Francois Bausch of the left Green party is the founding politician in the association's initial member list.
ATTAC functions on a principle of decentralisation: local associations organise meetings, conferences, and compose documents that become counter-arguments to the perceived neoliberal discourse. ATTAC aims to formalise the possibility of an alternative to the neoliberal society that is currently required of globalisation. ATTAC aspires to be a movement of popular education.
Views on Attac and its members in different countries.
Finland.
Communist Juhani Lohikoski, previously a chairman of Communist Youth League and Socialist League, served as the chairman of Finnish Attac for two terms (2002 - 2004). Yrjö Hakanen, pro-Soviet chairman of the Communist Party of Finland, was a member of the board and a member of the founding committee. In March 2002 Aimo Kairamo, the long-time chief editor of the party organ of the Social Democrat Party, resigned from Attac and recommended the same decision for other social democrats because of the left-wing minority communists' leading positions. Soon also the social democrat foreign minister Erkki Tuomioja considered to follow Kairamo's example.
Sweden.
Researcher Malin Gawell covers the birth and development of Attac Sweden in her doctoral thesis on activist entrepreneurship. She suggests that Attac in Sweden was formed by people seeking a new way of organising with flat hierarchy, and with the strongly sensed need of making a change as the driving force.
From another perspective, Sydsvenskan newspaper suggested that the downturn of memberships in Swedish Attac after the hype in the beginning of 2001 may be due to its views on trade policies.
Issues and activities.
The main issues covered by ATTAC today are:
In France, ATTAC associates with many other left-wing causes.
Nestlégate.
In the year 2008 Attac Switzerland was hit by a scandal which was later called Nestlégate by the local media. Between the years 2003 and 2005, the Swiss multinational food and beverage company Nestlé, engaged the external Security company Securitas AG, to spy on the Swiss Attac branch. Nestlé started the monitoring, when Attac Switzerland decided to work on a critical book about Nestlé.
Due to Nestlégate, Attac Switzerland filed a lawsuit against Nestlé which was decided in favour of Attac in January 2013, as the personal rights of the observed were violated. They received a compensation for damages of 3'000 Swiss francs each, which has an equivalent of about 3'230 USD at the date of the proclamation of sentence.

</doc>
<doc id="37160" url="http://en.wikipedia.org/wiki?curid=37160" title="Reinhard">
Reinhard

Reinhard is a surname or given name, and may refer to:
A surname:
A given name:
History:

</doc>
<doc id="37161" url="http://en.wikipedia.org/wiki?curid=37161" title="Fuel injection">
Fuel injection

Fuel injection is a system for admitting fuel into an internal combustion engine. It has become the primary fuel delivery system used in automotive engines, having replaced carburetors during the 1980s and 1990s. A variety of injection systems have existed since the earliest usage of the internal combustion engine.
The primary difference between carburetors and fuel injection is that fuel injection atomizes the fuel through a small nozzle under high pressure, while a carburetor relies on suction created by intake air accelerated through a Venturi tube to draw the fuel into the airstream.
Modern fuel injection systems are designed specifically for the type of fuel being used. Some systems are designed for multiple grades of fuel (using sensors to adapt the tuning for the fuel currently used). Most fuel injection systems are for gasoline or diesel applications. 
Objectives.
The functional objectives for fuel injection systems can vary. All share the central task of supplying fuel to the combustion process, but it is a design decision how a particular system is optimized. There are several competing objectives such as:
The modern digital electronic fuel injection system is more capable at optimizing these competing objectives consistently than earlier fuel delivery systems (such as carburetors). Carburetors have the potential to atomize fuel better (see Pogue and Allen Caggiano patents).
Benefits.
Benefits of fuel injection include smoother and more consistent transient throttle response, such as during quick throttle transitions, easier cold starting, more accurate adjustment to account for extremes of ambient temperatures and changes in air pressure, more stable idling, decreased maintenance needs, and better fuel efficiency.
Fuel injection also dispenses with the need for a separate mechanical choke, which on carburetor-equipped vehicles must be adjusted as the engine warms up to normal temperature. Furthermore on spark ignition engines (direct) fuel injection has the advantage of being able to facilitate stratified combustion which have not been possible with carburetors.
It is only with the advent of multi-point fuel injection certain engine configurations such as inline five cylinder gasoline engines have become more feasible for mass production, as traditional carburetor arrangement with single or twin carburetors could not provide even fuel distribution between cylinders, unless a more complicated individual carburetor per cylinder is used.
Fuel injection systems are also able to operate normally regardless of orientation, whereas carburetors with floats are not able to operate upside down or in zero gravity, such as encountered on airplanes.
Environmental benefits.
Fuel injection generally increases engine fuel efficiency. With the improved cylinder-to-cylinder fuel distribution of multi-point fuel injection, less fuel is needed for the same power output (when cylinder-to-cylinder distribution varies significantly, some cylinders receive excess fuel as a side effect of ensuring that all cylinders receive "sufficient" fuel).
Exhaust emissions are cleaner because the more precise and accurate fuel metering reduces the concentration of toxic combustion byproducts leaving the engine, and because exhaust cleanup devices such as the catalytic converter can be optimized to operate more efficiently since the exhaust is of consistent and predictable composition.
History and development.
Herbert Akroyd Stuart developed the first device with a design similar to modern fuel injection, using a 'jerk pump' to meter out fuel oil at high pressure to an injector. This system was used on the hot bulb engine and was adapted and improved by Bosch and Clessie Cummins for use on diesel engines (Rudolf Diesel's original system employed a cumbersome 'air-blast' system using highly compressed air). Fuel injection was in widespread commercial use in diesel engines by the mid-1920s.
An early use of indirect gasoline injection dates back to 1902, when French aviation engineer Leon Levavasseur installed it on his pioneering Antoinette 8V aircraft powerplant, the first V8 engine of any type ever produced in any quantity.
Another early use of gasoline direct injection was on the Hesselman engine invented by Swedish engineer Jonas Hesselman in 1925. Hesselman engines use the ultra lean burn principle; fuel is injected toward the end of the compression stroke, then ignited with a spark plug. They are often started on gasoline and then switched to diesel or kerosene.
Direct fuel injection was used in notable World War II aero-engines such as the Junkers Jumo 210, the Daimler-Benz DB 601, the BMW 801, the Shvetsov ASh-82FN (M-82FN). German direct injection petrol engines used injection systems developed by Bosch from their diesel injection systems. Later versions of the Rolls-Royce Merlin and Wright R-3350 used single point fuel injection, at the time called "Pressure Carburettor". Due to the wartime relationship between Germany and Japan, Mitsubishi also had two radial aircraft engines utilizing fuel injection, the Mitsubishi Kinsei ("kinsei" means "venus") and the Mitsubishi Kasei ("kasei" means "mars").
Alfa Romeo tested one of the first electronic injection systems (Caproni-Fuscaldo) in Alfa Romeo 6C 2500 with "Ala spessa" body in 1940 Mille Miglia. The engine had six electrically operated injectors and were fed by a semi-high-pressure circulating fuel pump system.
Development in diesel engines.
All diesel engines (with the exception of some tractors and scale model engines) have fuel injected into the combustion chamber. See diesel engines.
Development in gasoline/petrol engines.
Mechanical injection.
The invention of mechanical injection for gasoline-fueled aviation engines was by the French inventor of the V8 engine configuration, Leon Levavasseur in 1902. Levavasseur designed the original Antoinette firm's series of V-form aero engines, starting with the Antoinette 8V to be used by the aircraft the Antoinette firm built that Levavasseur also designed, flown from 1906 to the firm's demise in 1910, with the world's first V16 engine, using Levavasseur's direct injection and producing around 100 hp flying an Antoinette VII monoplane in 1907.
The first post-World War I example of direct gasoline injection was on the Hesselman engine invented by Swedish engineer Jonas Hesselman in 1925. Hesselman engines used the ultra lean burn principle and injected the fuel in the end of the compression stroke and then ignited it with a spark plug, it was often started on gasoline and then switched over to run on diesel or kerosene. The Hesselman engine was a low compression design constructed to run on heavy fuel oils.
Direct gasoline injection was applied during the Second World War to almost all higher-output production aircraft powerplants made in Germany (the widely used BMW 801 radial, and the popular inverted inline V12 Daimler-Benz DB 601, DB 603 and DB 605, along with the similar Junkers Jumo 210G, Jumo 211 and Jumo 213, starting as early as 1937 for both the Jumo 210G and DB 601), the Soviet Union (Shvetsov ASh-82FN radial, 1943, Chemical Automatics Design Bureau - KB Khimavtomatika) and the USA (Wright R-3350 "Duplex Cyclone" radial, 1944).
Immediately following the war, hot rodder Stuart Hilborn started to offer mechanical injection for race cars, salt cars, and midgets, well-known and easily distinguishable because of their prominent velocity stacks projecting upwards from the engines on which they were used.
The first automotive direct injection system used to run on gasoline was developed by Bosch, and was introduced by Goliath for their Goliath GP700 automobile, and Gutbrod in 1952. This was basically a high-pressure diesel direct-injection pump with an intake throttle valve. (Diesels only change the amount of fuel injected to vary output; there is no throttle.) This system used a normal gasoline fuel pump, to provide fuel to a mechanically driven injection pump, which had separate plungers per injector to deliver a very high injection pressure directly into the combustion chamber. The 1954 Mercedes-Benz W196 Formula 1 racing car engine used Bosch direct injection derived from wartime aero engines. Following this racetrack success, the 1955 Mercedes-Benz 300SL, the first production sports car to use fuel injection, used direct injection. The 1955 Mercedes-Benz 300SLR, in which Stirling Moss drove to victory in the 1955 Mille Miglia and Pierre Levegh crashed and died in the 1955 Le Mans disaster, had an engine developed from the W196 engine. The Bosch fuel injectors were placed into the bores on the cylinder wall used by the spark plugs in other Mercedes-Benz six-cylinder engines (the spark plugs were relocated to the cylinder head). Later, more mainstream applications of fuel injection favored the less-expensive indirect injection methods.
Chevrolet introduced a mechanical fuel injection option, made by General Motors' Rochester Products division, for its 283 V8 engine in 1956 (1957 U.S. model year). This system directed the inducted engine air across a "spoon shaped" plunger that moved in proportion to the air volume. The plunger connected to the fuel metering system that mechanically dispensed fuel to the cylinders via distribution tubes. This system was not a "pulse" or intermittent injection, but rather a constant flow system, metering fuel to all cylinders simultaneously from a central "spider" of injection lines. The fuel meter adjusted the amount of flow according to engine speed and load, and included a fuel reservoir, which was similar to a carburetor's float chamber. With its own high-pressure fuel pump driven by a cable from the distributor to the fuel meter, the system supplied the necessary pressure for injection. This was a "port" injection where the injectors are located in the intake manifold, very near the intake valve.
In 1956, Lucas developed its injection system, which was first used for Jaguar racing cars at Le Mans. The system was subsequently adopted very successfully in Formula One racing, securing championships by Cooper, BRM, Lotus, Brabham, Matra and Tyrrell in the years 1959 through 1973. While the racing systems used a simple "fuel cam" for metering, a more sophisticated "Mk 2" vacuum based "shuttle metering" was developed for production cars. This mechanical system was used by some Maserati, Aston Martin, and Triumph models between 1963 and 1975.
During the 1960s, other mechanical injection systems such as Hilborn were occasionally used on modified American V8 engines in various racing applications such as drag racing, oval racing, and road racing. These racing-derived systems were not suitable for everyday street use, having no provisions for low speed metering, or often none even for starting (starting required that fuel be squirted into the injector tubes while cranking the engine). However, they were a favorite in the aforementioned competition trials in which essentially wide-open throttle operation was prevalent. Constant-flow injection systems continue to be used at the highest levels of drag racing, where full-throttle, high-RPM performance is key.
In 1967, one of the first Japanese designed cars to use mechanical fuel injection was the Daihatsu Compagno.
Another mechanical system, made by Bosch called Jetronic, but injecting the fuel into the port above the intake valve, was used by several European car makers, particularly Porsche from 1969 until 1973 in the 911 production range and until 1975 on the Carrera 3.0 in Europe. Porsche continued using this system on its racing cars into the late seventies and early eighties. Porsche racing variants such as the 911 RSR 2.7 & 3.0, 904/6, 906, 907, 908, 910, 917 (in its regular normally aspirated or 5.5 Liter/1500 HP turbocharged form), and 935 all used Bosch or Kugelfischer built variants of injection. The early Bosch Jetronic systems were also used by Audi, Volvo, BMW, Volkswagen, and many others. The Kugelfischer system was also used by the BMW 2000/2002 Tii and some versions of the Peugeot 404/504 and Lancia Flavia. 
A system similar to the Bosch inline mechanical pump was built by SPICA for Alfa Romeo, used on the Alfa Romeo Montreal and on U.S. market 1750 and 2000 models from 1969 to 1981. This was designed to meet the U.S. emission requirements with no loss in performance and it also reduced fuel consumption.
Electronic injection.
The first commercial electronic fuel injection (EFI) system was Electrojector, developed by the Bendix Corporation and was offered by American Motors Corporation (AMC) in 1957. The Rambler Rebel, showcased AMC's new 327 CID engine. The Electrojector was an option and rated at 288 bhp. The EFI produced peak torque 500 rpm lower than the equivalent carburetored engine The Rebel Owners Manual described the design and operation of the new system. (due to cooler, therefore denser, intake air). The cost of the EFI option was US$395 and it was available on 15 June 1957. Electrojector's teething problems meant only pre-production cars were so equipped: thus, very few cars so equipped were ever sold and none were made available to the public. The EFI system in the Rambler ran fine in warm weather, but suffered hard starting in cooler temperatures.
Chrysler offered Electrojector on the 1958 Chrysler 300D, DeSoto Adventurer, Dodge D-500, and Plymouth Fury, arguably the first series-production cars equipped with an EFI system. It was jointly engineered by Chrysler and Bendix. The early electronic components were not equal to the rigors of underhood service, however, and were too slow to keep up with the demands of "on the fly" engine control. Most of the 35 vehicles originally so equipped were field-retrofitted with 4-barrel carburetors. The Electrojector patents were subsequently sold to Bosch.
Bosch developed an electronic fuel injection system, called "D-Jetronic" ("D" for "Druck", German for "pressure"), which was first used on the VW 1600TL/E in 1967. This was a speed/density system, using engine speed and intake manifold air density to calculate "air mass" flow rate and thus fuel requirements. This system was adopted by VW, Mercedes-Benz, Porsche, Citroën, Saab, and Volvo. Lucas licensed the system for production with Jaguar.
Bosch superseded the D-Jetronic system with the "K-Jetronic" and "L-Jetronic" systems for 1974, though some cars (such as the Volvo 164) continued using D-Jetronic for the following several years. In 1970, the Isuzu 117 Coupé was introduced with a Bosch-supplied D-Jetronic fuel injected engine sold only in Japan.
In Japan, the Toyota Celica used electronic, multi-port fuel injection in the optional 18R-E engine in January 1974. Nissan offered electronic, multi-port fuel injection in 1975 with the Bosch L-Jetronic system used in the Nissan L28E engine and installed in the Nissan Fairlady Z, Nissan Cedric, and the Nissan Gloria. Nissan also installed multi-point fuel injection in the Nissan Y44 V8 engine in the Nissan President. Toyota soon followed with the same technology in 1978 on the 4M-E engine installed in the Toyota Crown, the Toyota Supra, and the Toyota Mark II. In the 1980s, the Isuzu Piazza, and the Mitsubishi Starion added fuel injection as standard equipment, developed separately with both companies history of diesel powered engines. 1981 saw Mazda offer fuel injection in the Mazda Luce with the Mazda FE engine, and in 1983, Subaru offered fuel injection in the Subaru EA81 engine installed in the Subaru Leone. Honda followed in 1984 with their own system, called PGM-FI in the Honda Accord, and the Honda Vigor using the Honda ES3 engine.
The limited production Chevrolet Cosworth Vega was introduced in March 1975 using a Bendix EFI system with pulse-time manifold injection, four injector valves, an electronic control unit (ECU), five independent sensors and two fuel pumps. The EFI system was developed to satisfy stringent emission control requirements and market demands for a technologically advanced responsive vehicle. 5000 hand-built Cosworth Vega engines were produced but only 3,508 cars were sold through 1976.
The Cadillac Seville was introduced in 1975 with an EFI system made by Bendix and modelled very closely on Bosch's D-Jetronic. L-Jetronic first appeared on the 1974 Porsche 914, and uses a mechanical airflow meter (L for Luft, German for "air") that produces a signal that is proportional to "air volume". This approach required additional sensors to measure the atmospheric pressure and temperature, to ultimately calculate "air mass". L-Jetronic was widely adopted on European cars of that period, and a few Japanese models a short time later.
In 1980, Motorola (now Freescale) introduced the first electronic engine control unit, the EEC-III. Its integrated control of engine functions (such as fuel injection and spark timing) is now the standard approach for fuel injection systems. The Motorola technology was installed in Ford North American products.
Ellimination of carburetors.
In the 1970s and 1980s in the U.S. and Japan, the respective federal governments imposed increasingly strict exhaust emission regulations. During that time period, the vast majority of gasoline-fueled automobile and light truck engines did not use fuel injection. To comply with the new regulations, automobile manufacturers often made extensive and complex modifications to the engine carburetor(s). While a simple carburetor system is cheaper to manufacture than a fuel injection system, the more complex carburetor systems installed on many engines in the 1970s were much more costly than the earlier simple carburetors. To more easily comply with emissions regulations, automobile manufacturers began installing fuel injection systems in more gasoline engines during the late 1970s.
The open loop fuel injection systems had already improved cylinder-to-cylinder fuel distribution and engine operation over a wide temperature range, but did not offer further scope to sufficient control fuel/air mixtures, in order to further reduce exhaust emissions. Later Closed loop fuel injection systems improved the air/fuel mixture control with an exhaust gas oxygen sensor. Although not part of the injection control, a catalytic converter further reduces exhaust emissions.
Fuel injection was phased in through the latter 1970s and 80s at an accelerating rate, with the German, French, and U.S. markets leading and the UK and Commonwealth markets lagging somewhat. Since the early 1990s, almost all gasoline passenger cars sold in first world markets are equipped with electronic fuel injection (EFI). The carburetor remains in use in developing countries where vehicle emissions are unregulated and diagnostic and repair infrastructure is sparse. Fuel injection is gradually replacing carburetors in these nations too as they adopt emission regulations conceptually similar to those in force in Europe, Japan, Australia, and North America.
Many motorcycles still utilize carburetored engines, though all current high-performance designs have switched to EFI.
NASCAR finally replaced carburetors with fuel-injection, starting at the beginning of the 2012 NASCAR Sprint Cup Series season.
System components.
System overview.
The process of determining the necessary amount of fuel, and its delivery into the engine, are known as fuel metering. Early injection systems used mechanical methods to meter fuel, while nearly all modern systems use electronic metering.
Determining how much fuel to supply.
The primary factor used in determining the amount of fuel required by the engine is the amount (by weight) of air that is being taken in by the engine for use in combustion. Modern systems use a mass airflow sensor to send this information to the engine control unit.
Data representing the amount of power output desired by the driver (sometimes known as "engine load") is also used by the engine control unit in calculating the amount of fuel required. A throttle position sensor (TPS) provides this information. Other engine sensors used in EFI systems include a coolant temperature sensor, a camshaft or crankshaft position sensor (some systems get the position information from the distributor), and an oxygen sensor which is installed in the exhaust system so that it can be used to determine how well the fuel has been combusted, therefore allowing closed loop operation.
Supplying the fuel to the engine.
Fuel is transported from the fuel tank (via fuel lines) and pressurised using fuel pump(s). Maintaining the correct fuel pressure is done by a fuel pressure regulator. Often a fuel rail is used to divide the fuel supply into the required number of cylinders. The fuel injector injects liquid fuel into the intake air (the location of the fuel injector varies between systems). 
Unlike carburettor-based systems, where the float chamber provides a reservoir, fuel injected systems depend on an uninterrupted flow of fuel. To avoid fuel starvation when subject to lateral G-forces, vehicles are often provided by an anti-surge vessel, usually integrated in the fuel tank, but sometimes as a separate, small anti-surge tank.
EFI gasoline engine components.
"Note: These examples specifically apply to a modern EFI gasoline engine. Parallels to fuels other than gasoline can be made, but only conceptually."
Engine control unit.
The engine control unit is central to an EFI system. The ECU interprets data from input sensors to, among other tasks, calculate the appropriate amount of fuel to inject.
Fuel injector.
When signalled by the engine control unit the fuel injector opens and sprays the pressurised fuel into the engine. The duration that the injector is open (called the pulse width) is proportional to the amount of fuel delivered. Depending on the system design, the timing of when injector opens is either relative each individual cylinder (for a sequential fuel injection system), or injectors for multiple cylinders may be signalled to open at the same time (in a batch fire system).
Target air/fuel ratios.
The relative proportions of air and fuel vary according to the type of fuel used and the performance requirements (i.e. power, fuel economy, or exhaust emissions).
See air-fuel ratio, stoichiometry, and combustion.
Various injection schemes.
Single-point injection.
Single-point injection uses a single injector at the throttle body (the same location as was used by carburetors).
It was introduced in the 1940s in large aircraft engines (then called the pressure carburetor) and in the 1980s in the automotive world (called Throttle-body Injection by General Motors, Central Fuel Injection by Ford, PGM-CARB by Honda, and EGI by Mazda). Since the fuel passes through the intake runners (like a carburetor system), it is called a "wet manifold system".
The justification for single-point injection was low cost. Many of the carburetor's supporting components- such as the air cleaner, intake manifold, and fuel line routing- could be reused. This postponed the redesign and tooling costs of these components. Single-point injection was used extensively on American-made passenger cars and light trucks during 1980-1995, and in some European cars in the early and mid-1990s.
Continuous injection.
In a continuous injection system, fuel flows at all times from the fuel injectors, but at a variable flow rate. This is in contrast to most fuel injection systems, which provide fuel during short pulses of varying duration, with a constant rate of flow during each pulse. Continuous injection systems can be multi-point or single-point, but not direct.
The most common automotive continuous injection system is Bosch's K-Jetronic, introduced in 1974. K-Jetronic was used for many years between 1974 and the mid-1990s by BMW, Lamborghini, Ferrari, Mercedes-Benz, Volkswagen, Ford, Porsche, Audi, Saab, DeLorean, and Volvo. Chrysler used a continuous fuel injection system on the 1981-1983 Imperial.
In piston aircraft engines, continuous-flow fuel injection is the most common type. In contrast to automotive fuel injection systems, aircraft continuous flow fuel injection is all mechanical, requiring no electricity to operate. Two common types exist: the Bendix RSA system, and the TCM system. The Bendix system is a direct descendant of the pressure carburetor. However, instead of having a discharge valve in the barrel, it uses a "flow divider" mounted on top of the engine, which controls the discharge rate and evenly distributes the fuel to stainless steel injection lines to the intake ports of each cylinder. The TCM system is even more simple. It has no venturi, no pressure chambers, no diaphragms, and no discharge valve. The control unit is fed by a constant-pressure fuel pump. The control unit simply uses a butterfly valve for the air, which is linked by a mechanical linkage to a rotary valve for the fuel. Inside the control unit is another restriction, which controls the fuel mixture. The pressure drop across the restrictions in the control unit controls the amount of fuel flow, so that fuel flow is directly proportional to the pressure at the flow divider. In fact, most aircraft that use the TCM fuel injection system feature a fuel flow gauge that is actually a pressure gauge calibrated in "gallons per hour" or "pounds per hour" of fuel.
Central port injection.
From 1992 to 1996 General Motors implemented a system called Central Port Injection or Central Port Fuel Injection. The system uses tubes with poppet valves from a central injector to spray fuel at each intake port rather than the central throttle-body. Fuel pressure is similar to a single-point injection system. CPFI (used from 1992 to 1995) is a batch-fire system, while CSFI (from 1996) is a sequential system.
Multiport fuel injection.
Multiport fuel injection injects fuel into the intake ports just upstream of each cylinder's intake valve, rather than at a central point within an intake manifold. MPFI (or just MPI) systems can be sequential, in which injection is timed to coincide with each cylinder's intake stroke; batched, in which fuel is injected to the cylinders in groups, without precise synchronization to any particular cylinder's intake stroke; or simultaneous, in which fuel is injected at the same time to all the cylinders. The intake is only slightly wet, and typical fuel pressure runs between 40-60 psi.
Many modern EFI systems utilize sequential MPFI; however, in newer gasoline engines, direct injection systems are beginning to replace sequential ones.
Direct injection.
In a direct injection engine, fuel is injected into the combustion chamber as opposed to injection before the intake valve (petrol engine) or a separate pre-combustion chamber (diesel engine).
In a common rail system, the fuel from the fuel tank is supplied to the common header (called the accumulator). This fuel is then sent through tubing to the injectors, which inject it into the combustion chamber. The header has a high pressure relief valve to maintain the pressure in the header and return the excess fuel to the fuel tank. The fuel is sprayed with the help of a nozzle that is opened and closed with a needle valve, operated with a solenoid. When the solenoid is not activated, the spring forces the needle valve into the nozzle passage and prevents the injection of fuel into the cylinder. The solenoid lifts the needle valve from the valve seat, and fuel under pressure is sent in the engine cylinder. Third-generation common rail diesels use piezoelectric injectors for increased precision, with fuel pressures up to 1800 bar.
Direct fuel injection costs more than indirect injection systems: the injectors are exposed to more heat and pressure, so more costly materials and higher-precision electronic management systems are required.
Diesel engines.
Most diesel engines (with the exception of some tractors and scale model engines) have fuel injected into the combustion chamber.
Earlier systems, relying on simpler injectors, often injected into a sub-chamber shaped to swirl the compressed air and improve combustion; this was known as indirect injection. However, this was less efficient than the now common direct injection in which initiation of combustion takes place in a depression (often toroidal) in the crown of the piston.
Throughout the early history of diesels, they were always fed by a mechanical pump with a small separate chamber for each cylinder, feeding separate fuel lines and individual injectors. Most such pumps were in-line, though some were rotary.
Most modern diesel engines use common rail or unit injector direct injection systems.
Gasoline engines.
Modern gasoline engines also utilise direct injection, which is referred to as gasoline direct injection. This is the next step in evolution from multi-point fuel injection, and offers another magnitude of emission control by eliminating the "wet" portion of the induction system along the inlet tract.
By virtue of better dispersion and homogeneity of the directly injected fuel, the cylinder and piston are cooled, thereby permitting higher compression ratios and earlier ignition timing, with resultant enhanced power output. More precise management of the fuel injection event also enables better control of emissions. Finally, the homogeneity of the fuel mixture allows for leaner air/fuel ratios, which together with more precise ignition timing can improve fuel efficiency. Along with this, the engine can operate with stratified (lean burn) mixtures, and hence avoid throttling losses at low and part engine load. Some direct-injection systems incorporate piezoelectronic fuel injectors. With their extremely fast response time, multiple injection events can occur during each cycle of each cylinder of the engine.
Swirl injection.
Swirl injectors are used in liquid rocket, gas turbine, and diesel engines to improve atomization and mixing efﬁciency.
The circumferential velocity component is ﬁrst generated as the propellant enters through helical or tangential inlets producing a thin, swirling liquid sheet. A gas-ﬁlled hollow core is then formed along the centerline inside the injector due to centrifugal force of the liquid sheet. Because of the presence of the gas core, the discharge coefﬁcient is generally low. In swirl injector, the spray cone angle is controlled by the ratio of the circumferential velocity to the axial velocity and is generally wide compared with nonswirl injectors.
Maintenance hazards.
Fuel injection introduces potential hazards in engine maintenance due to the high fuel pressures used. Residual pressure can remain in the fuel lines long after an injection-equipped engine has been shut down. This residual pressure must be relieved, and if it is done so by external bleed-off, the fuel must be safely contained. If a high-pressure diesel fuel injector is removed from its seat and operated in open air, there is a risk to the operator of injury by hypodermic jet-injection, even with only 100 psi pressure. The first known such injury occurred in 1937 during a diesel engine maintenance operation.

</doc>
<doc id="37185" url="http://en.wikipedia.org/wiki?curid=37185" title="Pope Leo VIII">
Pope Leo VIII

Pope Leo VIII (died 1 March 965) was Pope from 23 June 964 to his death in 965; before that, he was an antipope from 963 to 964, in opposition to Pope John XII and Pope Benedict V. An appointee of the Holy Roman Emperor, Otto I, his pontificate occurred during the period known as the Saeculum obscurum.
Biography.
Born in Rome in the region around the "Clivus Argentarius", Leo was the son of John who held the office of Protonotary, and a member of an illustrious noble family. Although a layperson, he was the "protoscriniarius" (or superintendent of the Roman public schools for scribes) in the papal court during the pontificate of John XII. In 963 he was included in a party that was sent by John to the Holy Roman Emperor, Otto I, who was besieging the King of Italy, Berengar II at the castle of St. Leo in Umbria. His instructions were to reassure the emperor that the pope was determined to correct the abuses of the papal court, as well as protesting about Otto’s actions in demanding that cities in the Papal States take an oath of fidelity to the emperor instead of the pope.
By the time Otto entered Rome to depose John, Leo had been appointed Protonotary to the Apostolic See. A synod convened by the emperor uncanonically deposed John (who had fled to Tibur) and proceeded to elect Leo, who was the emperor’s nominee, as pope on 4 December 963, although as he was still a layman such an election was also invalid. In the space of a day Leo was ordained Ostiarius, Lector, Acolyte, Subdeacon, Deacon and Priest by Sico, the cardinal-bishop of Ostia, who then proceeded to consecrate him as Bishop of Rome on 6 December 963. The deposed John however still had a large body of sympathisers within Rome; he offered large bribes to the Roman nobility if they would rise up and overthrow Otto and kill Leo, and so in early January 964, the Roman people staged an uprising that was quickly put down by Otto’s troops. Leo, hoping to reach out to the Roman nobility, persuaded Otto to release the hostages he had taken from the leading Roman families in exchange for their continued good behaviour. However, once Otto left Rome in around 12 January 964, the Romans again rebelled, and caused Leo to flee Rome and take refuge with Otto sometime in February 964.
John XII returned and in February convened a synod which in turn deposed Leo on 26 February 964, with John excommunicating Leo in the process. Leo remained with Otto, and with the death of John XII in May 964, the Romans elected Pope Benedict V. Otto proceeded to besiege Rome, taking Leo with him, and when the Romans eventually surrendered to Otto, Leo was reinstalled in the Lateran Palace as pope.
Together with Benedict’s clerical and lay supporters, and clad in his pontifical robes, the former Pope was then brought before Leo, who asked him how Benedict dared to assume the chair of Saint Peter while he was still alive. Benedict responded “If I have sinned, have mercy on me.” Having received a promise from the emperor that his life would be spared if he submitted, Benedict threw himself at Leo’s feet and acknowledged his guilt. Brought before a synod convened by Leo, Benedict’s ordination as Bishop was revoked, his pallium was torn from him, and his pastoral staff was broken over him by Leo. However, through the intercession of Otto, Benedict was allowed to retain the rank of deacon. Then, after having the Roman nobility swear an oath over the Tomb of Saint Peter to obey and be faithful to Leo, Otto departed Rome in late June 964.
Having been cowed by Otto, the remainder of Leo’s pontificate was reasonably trouble free. He issued numerous bulls, many of which detailed the granting of privileges to Otto and his successors. Some of the bulls were alleged to grant the German emperors the right of choosing their successors in the Kingdom of Italy, the right to nominate the Pope, and all popes, archbishops and bishops were to receive investiture from the emperor. In addition, Leo is also claimed to have relinquished to Otto all the territory of the Papal States that had been granted to the Apostolic See by Pepin the Short and Charlemagne. Although it is certain that Leo granted various concessions to his imperial patron, it is now believed that the “investiture” bulls associated with Leo were, if not completely fabricated during the Investiture Controversy, were at the very least so tampered with that it is now largely impossible to reconstruct them in their original form.
Leo VIII died on 1 March 965, and was succeeded by Pope John XIII. According to the Liber Pontificalis he was described as venerable, energetic and honourable. He had a number of streets dedicated to him in and around the "Clivus Argentarius", including the "descensus Leonis Prothi".
Status as pope.
Although Leo was for many years considered an antipope, his current status is still a source of confusion. The Annuario Pontificio makes the following point about the pontificate of Leo VIII:
”At this point, as again in the mid-eleventh century, we come across elections in which problems of harmonizing historical criteria and those of theology and canon law make it impossible to decide clearly which side possessed the legitimacy whose factual existence guarantees the unbroken lawful succession of the Successors of Saint Peter. The uncertainty that in some cases results has made it advisable to abandon the assignation of successive numbers in the list of the Popes.”
Due to Leo’s uncanonical election, it is now accepted that until the deposition of Benedict V, he was almost certainly an antipope. Further, although the deposition of John XII was invalid, the election of Benedict V certainly was canonical. However, if Liutprand of Cremona (who chronicled the events of this period) can be relied upon, if, as he wrote, Benedict did acquiesce to his deposition, and if as seems certain, no further protest was made against Leo's position, it has been the consensus of historians that he may be regarded as a true pope from July 964, to his death in 965. The fact that the next pope to assume the name Leo was consecrated Leo IX also seems to indicate that he is a true pope.

</doc>
<doc id="37207" url="http://en.wikipedia.org/wiki?curid=37207" title="Nuclear engineering">
Nuclear engineering

Nuclear engineering is the branch of engineering concerned with the application of the breakdown (fission) as well as the fusion of atomic nuclei and/or the application of other sub-atomic physics, based on the principles of nuclear physics. In the sub-field of nuclear fission, it particularly includes the interaction and maintenance of systems and components like nuclear reactors, nuclear power plants, and/or nuclear weapons. The field also includes the study of medical and other applications of (generally ionizing) radiation, nuclear safety, heat/thermodynamics transport, nuclear fuel and/or other related technology (e.g., radioactive waste disposal), and the problems of nuclear proliferation.
Professional areas.
The United States generates about 18% of its electricity from nuclear power plants. Nuclear engineers in this field generally work, directly or indirectly, in the nuclear power industry or for national laboratories. Current research in the industry is directed at producing economical, proliferation-resistant reactor designs with passive safety features. Although government labs research the same areas as industry, they also study a myriad of other issues such as nuclear fuels and nuclear fuel cycles, advanced reactor designs, and nuclear weapon design and maintenance. A principal pipeline for trained personnel for US reactor facilities is the Navy Nuclear Power Program. The job outlook for nuclear engineering from the year 2012 to the year 2022 is predicted to grow 9% due to many elder nuclear engineers retiring, safety systems needing to be updated in power plants, and the advancements made in nuclear medicine.
Nuclear medicine and medical physics.
An important field is medical physics, and its subfields nuclear medicine, radiation therapy, health physics, and diagnostic imaging. From x-ray machines to MRI to PET, among many others, medical physics provides most of modern medicine's diagnostic capability along with providing many treatment options. 
Nuclear materials and nuclear fuels.
Nuclear materials research focuses on two main subject areas, nuclear fuels and irradiation-induced modification of materials. Improvement of three nuclear fuels is crucial for obtaining increased efficiency from nuclear reactors. Irradiation effects studies have many purposes, from studying structural changes to reactor components to studying nano-modification of metals using ion-beams or particle accelerators.
Radiation protection and measurement.
Radiation measurement is fundamental to the science and practice of radiation protection, sometimes known as radiological protection, which is the protection of people and the environment from the harmful effects of ionizing radiation
Nuclear engineers and radiological scientists are interested in the development of more advanced systems, and using these to improve imaging technologies. This includes detector design, fabrication and analysis, measurements of fundamental atomic and nuclear parameters, and radiation imaging systems, among other things.

</doc>
<doc id="37223" url="http://en.wikipedia.org/wiki?curid=37223" title="His Dark Materials">
His Dark Materials

His Dark Materials is an epic trilogy of fantasy novels by Philip Pullman consisting of "Northern Lights" (1995, published as "The Golden Compass" in North America), "The Subtle Knife" (1997), and "The Amber Spyglass" (2000). It follows the coming of age of two children, Lyra Belacqua and Will Parry, as they wander through a series of parallel universes. The three novels have won a number of awards, most notably the 2001 Whitbread Book of the Year prize, won by "The Amber Spyglass". "Northern Lights" won the Carnegie Medal for children's fiction in the UK in 1995. The trilogy took third place in the BBC's Big Read poll in 2003.
The fantasy elements include witches and armoured polar bears, but the trilogy also alludes to ideas from physics, philosophy and theology. The trilogy functions in part as a retelling and inversion of John Milton's epic "Paradise Lost", with Pullman commending humanity for what Milton saw as its most tragic failing, original sin. The series has drawn criticism for its negative portrayal of Christianity and religion in general.
Pullman's publishers have primarily marketed the series to young adults, but Pullman also intended to speak to both older children and adults. North American printings of "The Amber Spyglass" have censored passages describing Lyra's incipient sexuality.
Pullman has published two short stories related to "His Dark Materials": "Lyra and the Birds", which appears with accompanying illustrations in the small hardcover book "Lyra's Oxford" (2003), and "Once Upon a Time in the North" (2008). He has been working[ [update]] on another, larger companion book to the series, "The Book of Dust", for several years.
The National Theatre in London staged a major, two-part adaptation of the series in 2003–2004, and New Line Cinema released a film based on "Northern Lights", titled "The Golden Compass", in 2007.
Settings.
The trilogy takes place across a multiverse, moving between many parallel worlds. In "Northern Lights", the story takes place in a world with some similarities to our own; dress-style resembles that of the UK's Victorian era, and technology has not evolved to include automobiles or fixed-wing aircraft, while zeppelins feature as a notable mode of transport.
The dominant religion has parallels with Christianity, and is at certain points in the series "(especially in the later books)" explicitly named so; while Adam and Eve are referenced in the text (particularly in "The Subtle Knife", in which Dust tells Mary Malone that Lyra Belacqua is a new Eve to whom she is to be the serpent), Jesus Christ is not. The Church "(called the "Magisterium", the same name as the Catholic body)" exerts a strong control over society and has some of the appearance and organisation of the Catholic Church, but one in which the centre of power had moved from Rome to Geneva, moved there by Pullman's fictional "Pope John Calvin" (Geneva was the home of the real, historical John Calvin).
In "The Subtle Knife", the story moves between the world of the first novel, our own world, and in another world, a city called Cittàgazze. In "The Amber Spyglass" the story crosses through an array of diverse worlds.
At first glance, the universe of "Northern Lights" appears considerably behind that of our own world (resembling an industrial society between the late 19th century and the outbreak of the First World War), but in many fields it equals or surpasses ours. For instance, it emerges that Lyra's world has the same knowledge of particle physics, referred to as "experimental theology", that we do. In "The Amber Spyglass", discussion takes place about an advanced inter-dimensional weapon which, when aimed using a sample of the target's DNA, can track the target to any universe and disrupt the very fabric of space-time to form a bottomless abyss into nothing, forcing the target to suffer a fate far worse than normal death. Other advanced devices include the Intention Craft, which carries (amongst other things) an extremely potent energy-weapon, though this craft, first seen and used outside Lyra's universe, may originate in the work of engineers from other universes.
Series.
Titles.
The title of the series, "His Dark Materials", comes from 17th century poet John Milton's "Paradise Lost", Book 2:
<poem>
Into this wilde Abyss,
The Womb of nature and perhaps her Grave,
Of neither Sea, nor Shore, nor Air, nor Fire,
But all these in their pregnant causes mixt 
Confus'dly, and which thus must ever fight,
Unless th' Almighty Maker them ordain
His dark materials to create more Worlds,
Into this wilde Abyss the warie fiend
Stood on the brink of Hell and look'd a while,
Pondering his Voyage; for no narrow frith
He had to cross.
</poem>
— Paradise Lost, Book 2, lines 910–920
Pullman earlier proposed to name the series "The Golden Compasses", also a reference to "Paradise Lost", where they denote God's circle-drawing instrument used to establish and set the bounds of all creation:
<poem>
Then staid the fervid wheels, and in his hand
He took the golden compasses, prepared
In God's eternal store, to circumscribe
This universe, and all created things:
One foot he centered, and the other turned
Round through the vast profundity obscure
</poem>
— Paradise Lost, Book 7, lines 224–229
Despite the confusion with the other common meaning of "compass" (the navigational instrument) "The Golden Compass" became the title of the American edition of "Northern Lights" (the book features an 'alethiometer', a device that one might label a "golden compass"). In "The Subtle Knife" Pullman rationalizes the first book's American title, by having Mary twice refer to Lyra's alethiometer as a "compass" or "compass thing."
"Northern Lights" (or "The Golden Compass)".
"Northern Lights" (published in some countries as "The Golden Compass") revolves around Lyra Belacqua, a young girl who lives in a world in which humans are constantly accompanied by dæmons: the animal embodiments of their inner-selves. Dæmons alter their forms frequently when people are young but begin to settle into a fixed, animal form when children reach puberty. Lyra, whose dæmon is named Pantalaimon, is brought up in the cloistered world of Jordan College, Oxford, where she accidentally learns of the existence of Dust — a strange elementary particle being researched by Lord Asriel, whom Lyra has been told is her uncle. The Magisterium is a powerful Church body that represses heresy, and believes Dust to be related to Original Sin. Dust is less attracted to children than to adults, and a desire to learn why and to prevent children from acquiring Dust when they become adults leads to grisly experiments, carried out to separate kidnapped children from their dæmons. The experiments are directed by Mrs. Coulter and conducted in the distant North by experimental "theologists" (scientists) of the Magisterium. The Master of Jordan College, who has been raising Lyra, turns her over to Mrs. Coulter under pressure from the Church. But first he gives Lyra the alethiometer, an instrument that uses Dust to reveal any truth and can answer any question when properly manipulated. Lyra, initially excited at being placed in the care of the elegant and mysterious Mrs. Coulter, discovers to her horror that Coulter heads the secretive General Oblation Board, who are rumoured to be the ones kidnapping children throughout England for experimentation; they are known among children as the "Gobblers" (from the initials of General Oblation Board). Learning of Mrs. Coulter's Gobbler activity, Lyra runs away and the Gyptians, who live on riverboats, rescue her from pursuers. From them she learns that Mrs. Coulter is her mother and Lord Asriel is her father, not her uncle. Taking Lyra along, the Gyptians mount an expedition to rescue the missing children, many of whom are Gyptian children. Lyra hopes to find and save her best friend, Roger Parslow, who she suspects has been taken by the Gobblers. Aided by the exiled armoured bear Iorek Byrnison and a clan of witches, the Gyptians save the kidnapped children, including Roger. Lyra and Iorek, along with the balloonist Lee Scoresby, next continue on to Svalbard, home of the armoured bears. There Lyra helps Iorek regain his kingdom by killing his rival, King Iofur Raknison. Lyra then carries on to find Lord Asriel, exiled to Svalbard at Mrs. Coulter's request. She mistakenly thinks her mission all along has been to bring Asriel her alethiometer, when in fact she was destined to bring him a child, Roger. Lord Asriel has been developing a means of building a bridge to another world that can be seen in the sky through the northern lights. The bridge requires a vast amount of energy to split open the boundary between the two worlds. Asriel acquires the energy by severing Roger from his dæmon, killing Roger in the process. Lyra arrives too late to save Roger. Asriel then travels across the bridge to the new world in order to find the source of Dust. Lyra and Pantalaimon follow Asriel into the new world.
"The Subtle Knife".
In "The Subtle Knife", Lyra journeys through the Aurora to Cittàgazze, an otherworldly city whose denizens have discovered a clean path between worlds at a far earlier point in time than others in the storyline. Cittàgazze's reckless use of the technology has released soul-eating Spectres, to which children are immune, rendering much of the world incapable of transit by adults. Here Lyra meets Will Parry, a twelve-year-old boy from our world. Will, who recently killed a man to protect his ailing mother, has stumbled into Cittàgazze in an effort to locate his long-lost father. Will becomes the bearer of the eponymous Subtle Knife, a tool forged 300 years ago by Cittàgazze's scientists from the same materials used to make Bolvangar's silver guillotine. One edge of the knife can divide even subatomic particles and form subtle divisions in space, creating portals between worlds; the other edge easily cuts through any form of matter. After meeting with witches from Lyra's world, they journey on. Will finds his father, who had gone missing in Lyra's world under the assumed name of Stanislaus Grumman, only to watch him murdered almost immediately by a witch who loved him but was turned down, and Lyra is kidnapped.
"The Amber Spyglass".
"The Amber Spyglass" tells of Lyra's kidnapping by her mother, Mrs. Coulter, an agent of the Magisterium who has learned of the prophecy identifying Lyra as the next Eve. A pair of angels, Balthamos and Baruch, inform Will that he must travel with them to give the Subtle Knife to Lyra's father, Lord Asriel, as a weapon against The Authority. Will ignores the angels; with the help of a local girl named Ama, the Bear King Iorek Byrnison, and Lord Asriel's Gallivespian spies, the Chevalier Tialys and the Lady Salmakia, he rescues Lyra from the cave where her mother has hidden her from the Magisterium, which has become determined to kill her before she yields to temptation and sin like the original Eve.
Will, Lyra, Tialys and Salmakia journey to the Land of the Dead, temporarily parting with their dæmons to release the ghosts from their captivity. Mary Malone, a scientist from Will's world interested in "shadows" (or Dust in Lyra's world), travels to a land populated by strange sentient creatures called Mulefa. There she comes to understand the true nature of Dust, which is both created by and nourishes life which has become self-aware. Lord Asriel and the reformed Mrs. Coulter work to destroy the Authority's Regent Metatron. They succeed, but themselves suffer annihilation in the process by pulling Metatron into the abyss. The Authority himself dies of his own frailty when Will and Lyra free him from the crystal prison wherein Metatron had trapped him, able to do so because an attack by cliff-ghasts kills or drives away the prison's protectors. When Will and Lyra emerge from the land of the dead, they find their dæmons. The book ends with Will and Lyra falling in love but realising they cannot live together in the same world, because all windows — except one from the underworld to the world of the Mulefa — must be closed to prevent the loss of Dust, and because each of them can only live full lives in their native worlds. This is the temptation that Mary was meant to give them; to help them fall in love and then choose whether they should stay together or not. During the return, Mary learns how to see her own dæmon, who takes the form of a black Alpine Chough. Lyra loses her ability to intuitively read the alethiometer and determines to learn how to use her conscious mind to achieve the same effect.
Related works by Philip Pullman.
"Lyra's Oxford".
The first of two short novels, "Lyra's Oxford" takes place two years after the timeline of "The Amber Spyglass". A witch who seeks revenge for her son's death in the war against the Authority draws Lyra, now 15, into a trap. Birds mysteriously rescue her and Pan, and she makes the acquaintance of an alchemist, formerly the witch's lover.
"Once Upon a Time in the North".
This short novel serves as a prequel to "His Dark Materials" and focuses on the 24-year-old Texan aeronaut Lee Scoresby. After winning his hot-air balloon, Scoresby heads to the North, landing on the Arctic island Novy Odense, where he finds himself pulled into a dangerous conflict between the oil-tycoon Larsen Manganese, the corrupt mayoral candidate Ivan Poliakov, and his longtime enemy from the Dakota Country, Pierre McConville. The story tells of Lee and Iorek's first meeting and of how they overcame these enemies.
"The Book of Dust".
The forthcoming companion to the trilogy, "The Book of Dust" will not continue the story, but was originally said to offer several short stories with the same characters, world, etc. Later, however, it was said it would be about Lyra when she is older, about 2 years after Lyra's Oxford, when she will go on a new adventure and learn to read the alethiometer again. The book will touch on research into Dust as well as on the portrayal of religion in "His Dark Materials". Pullman has not yet[ [update]] finished writing this work.
Future books.
Pullman has also told of his hope to publish a small green book about Will:
"Lyra's Oxford" was a dark red book. "Once Upon a Time in the North" will be a dark blue book. There still remains a green book. And that will be Will's book. Eventually...—Philip Pullman
Pullman confirmed this in an interview with two fans in August 2007.
Characters.
All humans in Lyra's world, including witches, have a Dæmon. It is a the physical manifestation of a person's 'inner being', soul or spirit. It takes the form of a creature (moth, bird, dog, monkey, snake, etc.) and is usually the opposite sex to its human counterpart. The dæmons of children have the ability to change form - from one creature to another - but towards the end of a child's puberty, their dæmon "settles" into a permanent form, which reflects the person's personality. When a person dies, the dæmon dies too. Armoured bears, cliff ghasts and other creatures do not have dæmons. An armoured bear's armour is his soul.
Dæmons.
One distinctive aspect of Pullman's story is the presence of "dæmons" (pronounced "demon"). In the birth-universe of the story's protagonist Lyra Belacqua, a human individual's inner-self manifests itself throughout life as an animal-shaped "dæmon", that almost always stays near its human counterpart.
Dæmons usually only talk to their own associated humans, but they can communicate with other humans and with other dæmons autonomously. During the childhood of its associated human, a dæmon can change its shape at will, but with the onset of adolescence it settles into a fixed, final form that reveals the person's true nature and personality. In Lyra's world, it is considered to be "the grossest breach of etiquette imaginable" for one person to touch another's dæmon — this violates the strictest of taboos. "A human being with no dæmon is like someone without a face, or with their ribs laid open and their heart torn out: something unnatural and uncanny that belonged to the world of night-ghasts, not the waking world of sense."
Dæmons and their humans can become separated through intercision, a process involving cutting the link between the dæmon and the human. This process can take place in a medical setting, as with the guillotine used at Bolvangar, or as a form of torture used by the Skraelings. This separation entails a high mortality rate and changes both human and dæmon into a zombie-like state. Severing the link using the silver guillotine method releases tremendous amounts of unnamed energy, convertible to anbaric (electric) power.
Influences.
Pullman has identified three major literary influences on "His Dark Materials": the essay "On the Marionette Theatre" by Heinrich von Kleist, the works of William Blake, and, most important, John Milton's "Paradise Lost", from which the trilogy derives its title. In his introduction, he adapts a famous description of Milton by Blake to quip that he (Pullman) "is of the Devil's party and "does" know it."
Critics have compared the trilogy with "The Chronicles of Narnia", by C. S. Lewis, Pullman however has characterised the "Narnia" series as "blatantly racist", "monumentally disparaging of women", "immoral", and "evil". The trilogy has also been compared with such fantasy books as "Bridge to Terabithia" by Katherine Paterson and "A Wrinkle in Time" by Madeleine L'Engle
Awards and recognition.
"The Amber Spyglass" won the 2001 Whitbread Book of the Year award, a prestigious British literary award. This is the first time that such an award has been bestowed on a book from their "children's literature" category.
The first volume, "Northern Lights", won the Carnegie Medal for children's fiction in the UK in 1995. In 2007, the judges of the CILIP Carnegie Medal for children's literature selected it as one of the ten most important children's novels of the previous 70 years. In June 2007 it was voted, in an online poll, as the best Carnegie Medal winner in the seventy-year history of the award, the Carnegie of Carnegies.
"The Observer" cites "Northern Lights" as one of the 100 best novels.
On 19 May 2005, Pullman attended the British Library in London to receive formal congratulations for his work from culture secretary Tessa Jowell "on behalf of the government".
On 25 May 2005, Pullman received the Swedish government's Astrid Lindgren Memorial Award for children's and youth literature (sharing it with Japanese illustrator Ryōji Arai). Swedes regard this prize as second only to the Nobel Prize in Literature; it has a value of 5 million Swedish Kronor or approximately £385,000.
The trilogy came third in the 2003 BBC's "Big Read", a national poll of viewers' favourite books, after "The Lord of the Rings" and "Pride and Prejudice". At the time, only "His Dark Materials" and "Harry Potter and the Goblet of Fire" amongst the top five works lacked a screen-adaptation (the film version of "Harry Potter and the Goblet of Fire", which came fifth, was released in 2005).
Controversies.
"His Dark Materials" has occasioned controversy, primarily amongst some Christian groups. While sales in the United States equalled those of "The Harry Potter Series", Pullman's series did not receive as much conservative media backlash in the United States as it had received in the United Kingdom.
Pullman has expressed surprise over what he perceives as a low level of criticism for "His Dark Materials" on religious grounds, saying "I've been surprised by how little criticism I've got. Harry Potter's been taking all the flak... Meanwhile, I've been flying under the radar, saying things that are far more subversive than anything poor old Harry has said. My books are about killing God".
Some of the characters criticise institutional religion. Ruta Skadi, a witch and friend of Lyra's calling for war against the Magisterium in Lyra's world, says that ""For all of [the Church's] history... it's tried to suppress and control every natural impulse. And when it can't control them, it cuts them out". Skadi later extends her criticism to all organised religion: "That's what the Church does, and every church is the same: control, destroy, obliterate every good feeling". By this part of the book, the witches have made reference to how they are treated criminally by the church in their worlds. Mary Malone, one of Pullman's main characters, states that "the Christian religion... is a very powerful and convincing mistake, that's all"". Formerly a Catholic nun, she gave up her vows when the experience of falling in love caused her to doubt her faith. Pullman has warned, however, against equating these views with his own, saying of Malone: "Mary is a character in a book. Mary's not me. It's a story, not a treatise, not a sermon or a work of philosophy". In another inversion, the tenet that the Church can absolve a penitent of sin is subverted when the priest selected to assassinate Lyra has built up sufficient penitential credit "before" attempting to carry out this sin for the Church.
Pullman portrays life after death very differently from the Christian concept of heaven: In the third book, the afterlife plays out in a bleak underworld, similar to the Greek vision of the afterlife, wherein harpies torment people until Lyra and Will descend into the land of the dead. At their intercession, the harpies agree to stop tormenting the dead souls, and instead receive the true stories of the dead in exchange for leading them again to the upper world. When the dead souls emerge, they dissolve into atoms and merge with the environment.
Pullman's "Authority", though worshipped on Lyra's earth as God, emerges as the first conscious creature to evolve. Pullman makes it explicit that the Authority did not create worlds, and his trilogy does not speculate on who or what (if anything) might have done so. Members of the Church are typically displayed as zealots.
Cynthia Grenier, in the "Catholic Culture", said: "In the world of Pullman, God Himself (the Authority) is a merciless tyrant". His Church is an instrument of oppression, and true heroism consists of overthrowing both."
William A. Donohue of the Catholic League has described Pullman's trilogy as "atheism for kids". Pullman has said of Donohue's call for a boycott, "Why don't we trust readers? [...] Oh, it causes me to shake my head with sorrow that such nitwits could be loose in the world".
Pullman has, however, found support from some other Christians, most notably from Rowan Williams, the former Archbishop of Canterbury (spiritual head of the Anglican church), who argues that Pullman's attacks focus on the constraints and dangers of dogmatism and the use of religion to oppress, not on Christianity itself.
Williams has also recommended the "His Dark Materials" series of books for inclusion and discussion in Religious Education classes, and stated that "To see large school-parties in the audience of the Pullman plays at the National Theatre is vastly encouraging". Pullman and Williams took part in a National Theatre platform debate a few days later to discuss myth, religious experience and its representation in the arts.
Pullman has singled out certain elements of Christianity for criticism, as in the following: "I suppose technically, you'd have to put me down as an agnostic. But if there is a God, and he is as the Christians describe him, then he deserves to be put down and rebelled against". However, Pullman has also said in interviews and appearances that his argument can extend to all religions.
In a November 2002 interview Pullman was asked to respond to the fact that the Catholic Herald had called his books "the stuff of nightmares" and "worthy of the bonfire". He replied, "My response to that was to ask the publishers to print it in the next book, which they did! I think it's comical, it's just laughable". The original remark in Catholic Herald "(which was "there are numerous candidates that seem to me to be far more worthy of the bonfire than Harry Potter")" was written in the context of parents in South Carolina pressing their Board of Education to ban the Harry Potter books.
Terminology used in the books.
To enhance the feeling of being in parallel universes, Pullman renames various common objects or ideas of our world with archaic terms or new words of his own. The names he chooses often follow plausible alternative etymologies to those which have prevailed in modern English, thus making it possible to guess what everyday object or person he is referring to. Below are some of the significant renamings as well as new words the author has developed.
  List of terms
Pullman underlines the differences between the history of Lyra's world and ours by using archaic or adapted names for otherwise familiar peoples, regions and places.
  List of renamings of peoples and places
Unless stated otherwise, these words are all capitalised.
Pronunciation.
The pronunciations given here are those used in the radio plays and the audio book readings of the trilogy "(narrated by Pullman himself)".
  List of pronunciations
Adaptations.
"His Dark Materials" has been adapted for radio, theatre and film, In addition there have been unabridged audio books of the three main novels in "His Dark Materials" on which Philip Pullman himself is the narrator, the other parts are read by various actors, including Jo Wyatt, Steven Webb, Peter England, Stephen Thorne and Douglas Blackwell.
Radio.
The BBC made "His Dark Materials" into a radio drama on BBC Radio 4 starring Terence Stamp as Lord Asriel and Lulu Popplewell as Lyra. The play was broadcast in 2003 and is now published by the BBC on CD and cassette. In the same year, a radio drama of "Northern Lights" was made by RTÉ (Irish public radio).
The BBC Radio 4 version of "His Dark Materials" was repeated on BBC Radio 7 between 7 December 2008 to 11 January 2009. With 3 episodes in total, each episode was 2.5 hours long.
Theatre.
Nicholas Hytner directed a theatrical version of the books as a two-part, six-hour performance for London's Royal National Theatre in December 2003, running until March 2004. It starred Anna Maxwell-Martin as Lyra, Dominic Cooper as Will, Timothy Dalton as Lord Asriel and Patricia Hodge as Mrs Coulter with dæmon puppets designed by Michael Curry. The play was enormously successful and was revived (with a different cast and a revised script) for a second run between November 2004 and April 2005. It has since been staged by several other theatres in the UK and elsewhere.
A new production was staged at in March and April 2009, directed by Rachel Kavanaugh and Sarah Esdaile and starring Amy McAllister as Lyra. This version toured the UK and included a performance in Philip Pullman's hometown of Oxford. Philip Pullman made a cameo appearance much to the delight of the audience and Oxford media. The production finished up at West Yorkshire Playhouse in June 2009.
Film.
New Line Cinema released a film adaptation, titled "The Golden Compass", on 7 December 2007. Directed by Chris Weitz, the production had a mixed reception, and though worldwide sales were strong, its U.S. earnings were not as high as the studio had hoped.
The filmmakers obscured the explicitly Biblical character of the Authority to avoid offending viewers. Weitz declared that he would not do the same for the planned sequels. "Whereas "The Golden Compass" had to be introduced to the public carefully", he said, "the religious themes in the second and third books can't be minimised without destroying the spirit of these books. ...I will not be involved with any 'watering down' of books two and three, since what I have been working towards the whole time in the first film is to be able to deliver on the second and third". In May 2006, Pullman said of a version of the script that "all the important scenes are there and will have their full value"; in March 2008, he said of the finished film that "a lot of things about it were good... Nothing can bring out all that's in the book. There are always compromises".
"The Golden Compass" film stars Dakota Blue Richards as Lyra, Nicole Kidman as Mrs. Coulter, and Daniel Craig as Lord Asriel. Eva Green plays Serafina Pekkala, Ian McKellen voices Iorek Byrnison, and Freddie Highmore voices Pantalaimon.
No sequels are planned yet. Much publicity was given to "Compass" actor's Sam Elliott blaming Catholic Church opposition for forcing their cancellation, but UK Guardian film critic Stuart Heritage thinks critical "disappointment" with the first film may have been the real reason.

</doc>
<doc id="37366" url="http://en.wikipedia.org/wiki?curid=37366" title="Marcel Marlier">
Marcel Marlier

Marcel Marlier (18 November 1930 – 18 January 2011) was a Belgian artist and illustrator. He was born in Herseaux, Belgium. When he was 16, he enrolled in decorative art at Saint-Luc de Tournai. He finished his studies in 1951 with the greatest distinction. He returned as a teacher two years later.
The Belgian publisher La Procure à Namur organized a drawing contest. They were interested in finding talented artists to illustrate works for school children. Marlier won the competition. Two books resulted from this and these books guided a whole generation of Belgian children through the first few years of school, "I Read with Michel and Nicole" and "I Calculate with Michel and Nicole". Marlier's collaboration with La Procure à Namur lasted more than 25 years.
Pierre Servais at Casterman, a Belgian publishing company, began to notice Marlier's drawings in 1951. He suggested that Marlier should illustrate a series of books for children. The result was editions of Alexandre Dumas' adventure books. This was followed in 1953 by illustrations for a book by Jeanne Cappe about two very similar rabbits. Marlier also took part in the Farandole series, intended for children.
From 1954, Marlier illustrated the children's series "Martine", with the stories written by Gilbert Delahaye. This series spans over 50 issues and has been translated in numerous languages.
In 1969 Marlier also created his own series of children's books, "Jean-Lou and Sophie".
He died in Tournai, 18 January 2011 at 80 years old.

</doc>
<doc id="37374" url="http://en.wikipedia.org/wiki?curid=37374" title="Lockheed Martin RQ-3 DarkStar">
Lockheed Martin RQ-3 DarkStar

The RQ-3 DarkStar (known as Tier III- or "Tier three minus" during development) is an unmanned aerial vehicle (UAV). Its first flight was on March 29, 1996. The Department of Defense terminated DarkStar in January 1999, after determining the UAV was not aerodynamically stable and was not meeting cost and performance objectives.
Design and development.
The RQ-3 DarkStar was designed as a "high-altitude endurance UAV", and incorporated stealth aircraft technology to make it difficult to detect, which allowed it to operate within heavily defended airspace, unlike the RQ-4 Global Hawk, which is unable to operate except under conditions of air supremacy. The DarkStar was fully autonomous: it could take off, fly to its target, operate its sensors, transmit information, return and land without human intervention. Human operators, however, could change the DarkStar's flight plan and sensor orientation through radio or satellite relay. The RQ-3 carried either an optical sensor or radar, and could send digital information to a satellite while still in flight. It used a single airbreathing jet engine of unknown type for propulsion. 
The first prototype made its first flight on March 29, 1996, but its second flight, on April 22, 1996, ended in a crash shortly after takeoff. A modified, more stable design (the RQ-3A) first flew on June 29, 1998, and made a total of five flights before the program was canceled just prior to the sixth and final flight planned for the airworthiness test phase. Two additional RQ-3As were built, but never made any flights before program cancellation.
Although purportedly terminated on January 28, 1999, it was reported in April 2003 that the RQ-3 was still in development as a black project. The size and capabilities were reported to have increased somewhat. It was further alleged that the first such example had been used in the 2003 invasion of Iraq. There has been no independent confirmation. 
The "R" is the Department of Defense designation for reconnaissance; "Q" means unmanned aircraft system. The "3" refers to it being the third of a series of purpose-built unmanned reconnaissance aircraft systems.
Specifications.
General characteristics
Performance

</doc>
<doc id="37377" url="http://en.wikipedia.org/wiki?curid=37377" title="IAI RQ-5 Hunter">
IAI RQ-5 Hunter

The IAI RQ-5 Hunter unmanned aerial vehicle (UAV) was originally intended to serve as the United States Army's Short Range UAV system for division and corps commanders. It took off and landed (using arresting gear) on runways. It used a gimbaled EO/IR sensor to relay its video in real time via a second airborne Hunter over a C-band line-of-sight data link. The RQ-5 is based on the Hunter UAV that was developed by Israel Aircraft Industries.
Operational overview.
System acquisition and training started in 1994 but production was cancelled in 1996 due to concerns over program mismanagement. Seven low rate initial production (LRIP) systems of eight aircraft each were acquired, four of which remained in service: one for training and three for doctrine development, exercise, and contingency support. Hunter was to be replaced by the RQ-7 Shadow, but instead of being replaced, the Army's has kept both systems in operation, because the Hunter has significantly larger payload, range, and time-on-station capabilities than the Shadow.
In 1995, A Company, 15th Military Intelligence Battalion (Aerial Exploitation) out of Fort Hood, TX was the first Army field unit equipped with the Hunter. A Company conducted multiple successful training rotations to the National Training Center. Then in March 1999, they were deployed to the Republic of Macedonia in support of NATO operations in Kosovo. During the 7 month operation, the Hunter was flown over 4000 hours. Significant operational success in Kosovo led to resumption of production and technical improvements. Hunter has been used in Iraq and other military operations since then. The system has also been armed with the Viper Strike munitions.
The Army's Unmanned Aircraft Systems Training Battalion at Fort Huachuca, AZ trains soldiers and civilians in the operation and maintenance of the Hunter UAV.
In 2004, the United States Department of Homeland Security, Bureau of Customs and Border Protection, Office of Air and Marine utilized the Hunter under a trial program for border patrol duties. During this program, the Hunter flew 329 flight hours, resulting in 556 detections.
A version armed with the Northrop Grumman GBU-44/B Viper Strike weapon system is known as the MQ-5A/B.
As of October 2012, the U.S. Army has 20 MQ-5B Hunters in service. The Hunter is being slowly replaced by the MQ-1C Grey Eagle. Retirement of the Hunter was expected to be completed in 2013. However, Northrop was awarded a support contract for the Hunter on January 22, 2013. The completion date for the contract is January 14, 2014, so the Hunter UAV is likely to be flying missions into 2014.
On 7 October 2013, the U.S. Army opened a UAS facility at Vilseck Army Airfield in Germany. A letter of agreement between the U.S. and Germany allows the 7th Army Joint Multinational Training Command to use two ‘air bridges’ in the east of the country to train operators, marking the first time a U.S. UAV will fly beyond the limits of military training areas. Two unarmed MQ-5B Hunters will be used solely for training drone operators.
From 1996 to January 2014, the MQ-5B Hunter unmanned aerial system has flown over 100,000 hours with the U.S. Army.
On 14 March 2014, an RQ-5 was reported downed by a Crimean self-defense unit over Russian occupied Ukrainian territory, although Russia did not substantiate the claim and the Pentagon denies it operated such a vehicle over Crimea.
International use.
In 1998 the Belgian Air Force purchased three B-Hunter UAV-systems, each consisting of six aircraft and two ground control stations.
Specifications.
"Data from" </ul>Armament

</doc>
<doc id="37398" url="http://en.wikipedia.org/wiki?curid=37398" title="The Walt Disney Company">
The Walt Disney Company

The Walt Disney Company, commonly known as Disney, is an American diversified:1 multinational mass media and entertainment conglomerate headquartered at the Walt Disney Studios in Burbank, California. It is the world's second largest broadcasting and cable company in terms of revenue, after Comcast. Disney was founded on October 16, 1923, by Walt Disney and Roy O. Disney as the Disney Brothers Cartoon Studio, and established itself as a leader in the American animation industry before diversifying into live-action film production, television, and theme parks. The company also operated under the names The Walt Disney Studio, then Walt Disney Productions. Taking on its current name in 1986, it expanded its existing operations and also started divisions focused upon theater, radio, music, publishing, and online media.
In addition, Disney has since created corporate divisions in order to market more mature content than is typically associated with its flagship family-oriented brands. The company is best known for the products of its film studio, the Walt Disney Studios, which is today one of the largest and best-known studios in American cinema. Disney also owns and operates the ABC broadcast television network; cable television networks such as Disney Channel, ESPN, A+E Networks, and ABC Family; publishing, merchandising, music, and theatre divisions; and owns and licenses 14 theme parks around the world. The company has been a component of the Dow Jones Industrial Average since May 6, 1991. An early and well-known cartoon creation of the company, Mickey Mouse, is a primary symbol of The Walt Disney Company.
Corporate history.
1923–1928: The silent era.
In early 1923, Kansas City, Missouri, animator Walt Disney created a short film entitled "Alice's Wonderland", which featured child actress Virginia Davis interacting with animated characters. After the bankruptcy in 1923 of his previous firm, Laugh-O-Gram Films, Disney moved to Hollywood to join his brother, Roy O. Disney. Film distributor Margaret J. Winkler of M.J. Winkler Productions contacted Disney with plans to distribute a whole series of "Alice Comedies" purchased for $1,500 per reel with Disney as a production partner. Walt and Roy Disney formed Disney Brothers Cartoon Studio that same year. More animated films followed after Alice. In January 1926, with the completion of the Disney studio on Hyperion Street, the Disney Brothers Studio's name was changed to the Walt Disney Studio.
After the demise of the "Alice" comedies, Disney developed an all-cartoon series starring his first original character, Oswald the Lucky Rabbit, which was distributed by Winkler Pictures through Universal Pictures. The distributor owned Oswald, so Disney only made a few hundred dollars. Disney completed 26 "Oswald" shorts before losing the contract in February 1928, due to a legal loophole, when Winkler's husband Charles Mintz took over their distribution company. After failing to take over the Disney Studio, Mintz hired away four of Disney's primary animators (the exception being Ub Iwerks) to start his own animation studio, Snappy Comedies.
1928–1934: Mickey Mouse and Silly Symphonies.
In 1928, to recover from the loss of Oswald the Lucky Rabbit, Disney came up with the idea of a mouse character named Mortimer while on a train headed to California, drawing up a few simple drawings. The mouse was later renamed Mickey Mouse (Disney's wife, Lillian, disliked the sound of 'Mortimer Mouse') and starred in several Disney produced films. Ub Iwerks refined Disney's initial design of Mickey Mouse. Disney's first sound film "Steamboat Willie", a cartoon starring Mickey, was released on November 18, 1928 through Pat Powers' distribution company. It was the first Mickey Mouse sound cartoon released, but the third to be created, behind "Plane Crazy" and "The Gallopin' Gaucho". "Steamboat Willie" was an immediate smash hit, and its initial success was attributed not just to Mickey's appeal as a character, but to the fact that it was the first cartoon to feature synchronized sound. Disney used Pat Powers' Cinephone system, created by Powers using Lee De Forest's Phonofilm system. "Steamboat Willie" premiered at B. S. Moss's Colony Theater in New York City, now The Broadway Theatre. Disney's "Plane Crazy" and "The Galloping Gaucho" were then retrofitted with synchronized sound tracks and re-released successfully in 1929.
Disney continued to produce cartoons with Mickey Mouse and other characters, and began the Silly Symphonies series with Columbia Pictures signing on as Symphonies distributor in August 1929. In September 1929, theater manager Harry Woodin requested permission to start a Mickey Mouse Club which Walt approved. In November, test comics strips were sent to King Features, who requested additional samples to show to the publisher, William Randolph Hearst. On December 16, the Walt Disney Studios partnership was reorganized as a corporation with the name of Walt Disney Productions, Limited with a merchandising division, Walt Disney Enterprises, and two subsidiaries, Disney Film Recording Company, Limited and Liled Realty and Investment Company for real estate holdings. Walt and his wife held 60% (6,000 shares) and Roy owned 40% of WD Productions. On December 30, King Features signed its first newspaper, New York Mirror, to publish the Mickey Mouse comic strip with Walt's permission.
In 1932, Disney signed an exclusive contract with Technicolor (through the end of 1935) to produce cartoons in color, beginning with "Flowers and Trees" (1932). Disney released cartoons through Powers' Celebrity Pictures (1928–1930), Columbia Pictures (1930–1932), and United Artists (1932–1937). The popularity of the Mickey Mouse series allowed Disney to plan for his first feature-length animation.
The feature film "Walt Before Mickey" based on the book by Diane Disney Miller featured these moments in the studio's history.
1934–1945: "Snow White and the Seven Dwarfs" and World War II.
Deciding to push the boundaries of animation even further, Disney began production of his first feature-length animated film in 1934. Taking three years to complete, "Snow White and the Seven Dwarfs", premiered in December 1937 and became highest-grossing film of that time by 1939. "Snow White" was released through RKO Radio Pictures, which had assumed distribution of Disney's product in July 1937, after United Artists attempted to attain future television rights to the Disney shorts.
Using the profits from "Snow White", Disney financed the construction of a new 51 acre studio complex in Burbank, California. The new Walt Disney Studios, in which the company is headquartered to this day, was completed and open for business by the end of 1939. The following year on April 2, Walt Disney Productions had its initial public offering.
The studio continued releasing animated shorts and features, such as "Pinocchio" (1940), "Fantasia" (1940), "Dumbo" (1941), and "Bambi" (1942). After World War II began, box-office profits declined. When the United States entered the war after the attack on Pearl Harbor, many of Disney's animators were drafted into the armed forces. The U.S. and Canadian governments commissioned the studio to produce training and propaganda films. By 1942 90% of its 550 employees were working on war-related films. Films such as the feature "Victory Through Air Power" and the short "Education for Death" (both 1943) were meant to increase public support for the war effort. Even the studio's characters joined the effort, as Donald Duck appeared in a number of comical propaganda shorts, including the Academy Award-winning "Der Fuehrer's Face" (1943).
1946–1954: Post-war and television.
With limited staff and little operating capital during and after the war, Disney's feature films during much of the 1940s were "package films," or collections of shorts, such as "The Three Caballeros" (1944) and "Melody Time" (1948), which performed poorly at the box-office. At the same time, the studio began producing live-action films and documentaries. "Song of the South" (1946) and "So Dear to My Heart" (1948) featured animated segments, while the "True-Life Adventures" series, which included such films as "Seal Island" (1948) and "The Vanishing Prairie" (1954), were also popular. Eight of the films in the series won Academy Awards.
The release of "Cinderella" in 1950 proved that feature-length animation could still succeed in the marketplace. Other releases of the period included "Alice in Wonderland" (1951) and "Peter Pan" (1953), both in production before the war began, and Disney's first all-live action feature, "Treasure Island" (1950). Other early all-live-action Disney films included "The Story of Robin Hood and His Merrie Men" (1952), "The Sword and the Rose" (1953), and "20,000 Leagues Under the Sea" (1954). Disney ended its distribution contract with RKO in 1953, forming its own distribution arm, Buena Vista Distribution.
In December 1950, Walt Disney Productions and The Coca-Cola Company teamed up for Disney's first venture into television, the NBC television network special "An Hour in Wonderland". In October 1954, the ABC network launched Disney's first regular television series, "Disneyland", which would go on to become one of the longest-running primetime series in history. "Disneyland" allowed Disney a platform to introduce new projects and broadcast older ones, and ABC became Disney's partner in the financing and development of Disney's next venture, located in the middle of an orange grove near Anaheim, California. It was the first phase of a long corporate relationship which, although no one could have anticipated it at the time, would culminate four decades later in the Disney company's acquisition of the ABC network, its owned and operated stations, and its numerous cable and publishing ventures.
1955–1965: Disneyland.
In 1954, Walt Disney used his "Disneyland" series to unveil what would become Disneyland, an idea conceived out of a desire for a place where parents and children could both have fun at the same time. On July 18, 1955, Walt Disney opened Disneyland to the general public. On July 17, 1955, Disneyland was previewed with a live television broadcast hosted by Art Linkletter and Ronald Reagan. After a shaky start, Disneyland continued to grow and attract visitors from across the country and around the world. A major expansion in 1959 included the addition of America's first monorail system.
For the 1964 New York World's Fair, Disney prepared four separate attractions for various sponsors, each of which would find its way to Disneyland in one form or another. During this time, Walt Disney was also secretly scouting out new sites for a second Disney theme park. In November 1965, "Disney World" was announced, with plans for theme parks, hotels, and even a model city on thousands of acres of land purchased outside of Orlando, Florida.
Disney continued to focus its talents on television throughout the 1950s. Its weekday afternoon children's television program "The Mickey Mouse Club", featuring its roster of young "Mouseketeers", premiered in 1955 to great success, as did the "Davy Crockett" miniseries, starring Fess Parker and broadcast on the "Disneyland" anthology show. Two years later, the "Zorro" series would prove just as popular, running for two seasons on ABC. Despite such success, Walt Disney Productions invested little into television ventures in the 1960s, with the exception of the long-running anthology series, later known as "The Wonderful World of Disney".
Disney's film studios stayed busy as well. Averaging five or six releases per year during this period. While the production of shorts slowed significantly during the 1950s and 1960s, the studio released a number of popular animated features, like "Lady and the Tramp" (1955), "Sleeping Beauty" (1959) and "One Hundred and One Dalmatians" (1961), which introduced a new xerography process to transfer the drawings to animation cels. Disney's live-action releases were spread across a number of genres, including historical fiction ("Johnny Tremain", 1957), adaptations of children's books ("Pollyanna", 1960) and modern-day comedies ("The Shaggy Dog", 1959). Disney's most successful film of the 1960s was a live action/animated musical adaptation of "Mary Poppins", which was one of the all-time highest grossing movies and received five Academy Awards, including Best Actress for Julie Andrews and Best Song for Robert B. Sherman & Richard M. Sherman for "Chim Chim Cher-ee" .
The theme park design and architectural group became so integral to the Disney studio's operations that the studio bought it on February 5, 1965, along with the WED Enterprises name.
1966–1971: The deaths of Walt and Roy Disney and the opening of Walt Disney World.
On December 15, 1966, Walt Disney died of complications relating to lung cancer, and Roy Disney took over as chairman, CEO, and president of the company. One of his first acts was to rename Disney World as "Walt Disney World" in honor of his brother and his vision.
In 1967, the last two films Walt actively supervised were released, the animated feature "The Jungle Book" and the musical "The Happiest Millionaire". The studio released a number of comedies in the late 1960s, including "The Love Bug" (1969's highest grossing film) and "The Computer Wore Tennis Shoes" (1969), which starred another young Disney discovery, Kurt Russell. The 1970s opened with the release of Disney's first "post-Walt" animated feature, "The Aristocats", followed by a return to fantasy musicals in 1971's "Bedknobs and Broomsticks". "Blackbeard's Ghost" was another successful film during this period.
On October 1, 1971, Walt Disney World opened to the public, with Roy Disney dedicating the facility in person later that month. On December 20, 1971, Roy Disney died of a stroke. He left the company under control of Donn Tatum, Card Walker, and Walt's son-in-law Ron Miller, each trained by Walt and Roy.
1972–1984: Theatrical malaise and new leadership.
While Walt Disney Productions continued releasing family-friendly films throughout the 1970s, such as "Escape to Witch Mountain" (1975) and "Freaky Friday" (1976), the films did not fare as well at the box office as earlier material. However, the animation studio saw success with "Robin Hood" (1973), "The Rescuers" (1977), and "The Fox and the Hound" (1981).
As head of the studio, Miller attempted to make films to drive the profitable teenage market who generally passed on seeing Disney movies. Inspired by the popularity of ', the Disney studio produced the science-fiction adventure "The Black Hole" in 1979 that cost $20 million to make, but was lost in "Star Wars wake. "The Black Hole" was the first Disney production to carry a PG rating in the United States. Disney dabbled in the horror genre with "The Watcher in the Woods", and financed the boldly innovative "Tron"; both films were released to minimal success.
Disney also hired outside producers for film projects, which had never been done before in the studio's history. In 1979, Disney entered a joint venture with Paramount Pictures on the production of the 1980 film adaptation of "Popeye" and "Dragonslayer" (1981); the first time Disney collaborated with another studio. Paramount distributed Disney films in Canada at the time, and it was hoped that Disney's marketing prestige would help sell the two films.
Finally, in 1982, the Disney family sold the naming rights and rail-based attractions to the Disney film studio for 818,461 shares of Disney stock then worth $42.6 million none of which went to Retlaw. Also, Roy E. Disney objected to the overvalued purchase price of the naming right and voted against the purchase as a Disney board director.
The 1983 release of Mickey's Christmas Carol began a string of successful movies, starting with "Never Cry Wolf" and the Ray Bradbury adaptation "Something Wicked This Way Comes". The Walt Productions film division was incorporated on as Walt Disney Pictures. In 1984, Disney CEO Ron Miller created Touchstone Films as a brand for Disney to release more major release motion pictures. Touchstone's first release was the comedy "Splash" (1984), which was a box office success.
With "The Wonderful World of Disney" remaining a prime-time staple, Disney returned to television in the 1970s with syndicated programing such as the anthology series "The Mouse Factory" and a brief revival of the "Mickey Mouse Club". In 1980, Disney launched Walt Disney Home Video to take advantage of the newly emerging videocassette market. On April 18, 1983, The Disney Channel debuted as a subscription-level channel on cable systems nationwide, featuring its large library of classic films and TV series, along with original programming and family-friendly third-party offerings.
Walt Disney World received much of the company's attention through the 1970s and into the 1980s. In 1978, Disney executives announced plans for the second Walt Disney World theme park, EPCOT Center, which would open in October 1982. Inspired by Walt Disney's dream of a futuristic model city, EPCOT Center was built as a "permanent World's Fair", complete with exhibits sponsored by major American corporations, as well as pavilions based on the cultures of other nations. In Japan, the Oriental Land Company partnered with Walt Disney Productions to build the first Disney theme park outside of the United States, Tokyo Disneyland, which opened in April 1983.
Despite the success of the Disney Channel and its new theme park creations, Walt Disney Productions was financially vulnerable. Its film library was valuable, but offered few current successes, and its leadership team was unable to keep up with other studios, particularly the works of Don Bluth, who defected from Disney in 1979.
By the early 1980s, the parks were generating 70% of Disney's income.
In 1984, financier Saul Steinberg's Reliance Group Holdings launched a hostile takeover bid for Walt Disney Productions, with the intent of selling off some of its operations. Disney bought out Reliance's 11.1% stake in the company. However, another shareholder filed suit claiming the deal devaluated Disney's stock and for Disney management to retain their positions. The shareholder lawsuit was settled in 1989 for a total of $45 million from Disney and Reliance.
1984–2005: The Eisner era and the Save Disney campaign.
With the Sid Bass family purchase of 18.7 percent of Disney, Bass and the board brought in Michael Eisner from Paramount Pictures as CEO and Frank Wells from Warner Bros. as president. Eisner emphasized Touchstone Films with "Down and Out in Beverly Hills" (1985) to start leading to increased output with "Ruthless People" (1986), "Outrageous Fortune" (1987), "Pretty Woman" (1990) and additional hits. Eisner used expanding cable and home video markets to sign deals using Disney shows and films with a long-term deal with Showtime Networks for Disney/Touchstone releases through 1996 and entering television with syndication and distribution for TV series as "The Golden Girls" and "Home Improvement". Disney began limited releases of its previous films on video tapes in the late 1980s. Eisner's Disney purchased KHJ, an independent Los Angeles TV station.
Organized in 1985, Silver Screen Partners II, LP financed films for Disney with $193 million. In January 1987, Silver Screen III began financing movies for Disney with $300 million raised, the largest amount raised for a film financing limited partnership by E.F. Hutton. Silver Screen IV was also set up to finance Disney's studios.
Beginning with "Who Framed Roger Rabbit" in 1988, Disney's flagship animation studio enjoyed a series of commercial and critical successes with such films as "The Little Mermaid" (1989), "Beauty and the Beast" (1991), "Aladdin" (1992) and "The Lion King" (1994). In addition, the company successfully entered the field of television animation with a number of lavishly budgeted and acclaimed series such as "Adventures of the Gummi Bears", "Duck Tales", "Chip 'n Dale Rescue Rangers", "Darkwing Duck" and "Gargoyles". Disney moved to first place in box office receipts by 1988 and had increased revenues by 20% every year.
In 1989, Disney signed an agreement-in-principle to acquire The Jim Henson Company (then known as Henson Associates) from its founder, Muppet creator Jim Henson. The deal included Henson's programming library and Muppet characters (excluding the Muppets created for "Sesame Street"), as well as Jim Henson's personal creative services. However, in May 1990, before the deal was completed, Jim Henson died, and the two companies broke off merger negotiations the following December.
Named the "Disney Decade" by the company, the executive talent attempted to move the company to new heights in the 1990s with huge changes and accomplishments. In September 1990, The Disney Company arranged for financing up to $200 million by a unit of Nomura Securities for Interscope films made for Disney. On October 23, Disney formed Touchwood Pacific Partners I which would supplant the Silver Screen Partnership series as their movie studios' primary source of funding.
In 1991, hotels, home video distribution, and Disney merchandising became 28 percent of total company revenues with international revenues contributed 22 percent of revenues. The company committed its studios in the first quarter of 1991 to produce 25 films in 1992. However, 1991 saw net income drop by 23% and had no growth for the year, but saw the release of Beauty and the Beast, winner of 2 Academy Awards and top grossing film in the genre. Disney next moved into publishing with Hyperion Books and adult music with Hollywood Records while Disney Imagineering was laying off 400 employees.
Disney also broadened its adult offerings in film when then Disney Studio Chairman Jeffrey Katzenberg acquired Miramax Films in 1993. That same year Disney created the NHL team the Mighty Ducks of Anaheim, named after the 1992 hit film of the same name. Disney purchased a minority stake in the Anaheim Angels baseball team around the same time.
Wells died in a helicopter crash in 1994. Shortly thereafter, Katzenberg resigned and formed DreamWorks SKG because Eisner would not appoint Katzenberg to Wells' now-available post (Katzenberg had also sued over the terms of his contract). Instead, Eisner recruited his friend Michael Ovitz, one of the founders of the Creative Artists Agency, to be President, with minimal involvement from Disney's board of directors (which at the time included Oscar-winning actor Sidney Poitier, the CEO of Hilton Hotels Corporation Stephen Bollenbach, former U.S. Senator George Mitchell, Yale dean Robert A. M. Stern, and Eisner's predecessors Raymond Watson and Card Walker). Ovitz lasted only 14 months and left Disney in December 1996 via a "no fault termination" with a severance package of $38 million in cash and 3 million stock options worth roughly $100 million at the time of Ovitz's departure. The Ovitz episode engendered a long running derivative suit, which finally concluded in June 2006, almost 10 years later. Chancellor William B. Chandler, III of the Delaware Court of Chancery, despite describing Eisner's behavior as falling "far short of what shareholders expect and demand from those entrusted with a fiduciary position..." found in favor of Eisner and the rest of the Disney board because they hadn't violated the letter of the law (namely, the duty of care owed by a corporation's officers and board to its shareholders).
Eisner attempted in 1994 to purchase NBC from GE, but the deal failed due to GE wanting to keep 51% ownership of the network. Disney acquired many other media sources during the decade, including a merger with Capital Cities/ABC in 1995 which brought broadcast network ABC and its assets, including the A&E Television Networks and ESPN networks, into the Disney fold. Eisner felt that the purchase of ABC was an important investment to keep Disney surviving and allowing it to compete with international multimedia conglomerates.
Disney lost a $10.4 million lawsuit in September 1997 to Marsu B.V. over Disney's failure to produce as contracted 13 half-hour Marsupilami cartoon shows. Instead Disney felt other internal "hot properties" deserved the company's attention.
Disney took control of the Anaheim Angels in 1996, and purchased a majority stake in the team in 1998. That same year, Disney began a move into the internet field with the purchase of Starwave and 43 percent of Infoseek. In 1999, Disney purchased the remaining shares of Infoseek and launch the Go Network portal in January. Disney also launched its cruise line with the christening of Disney Magic and a sister ship, Disney Wonder.
As the Katzenberg case dragged on as his contract included a portion of the film revenue from ancillary markets forever. Katzenberg had offered $100 to settle the case but Eisner felt the original claim amount of about half a billion too much, but then the ancillary market clause was found. Disney lawyers tried to indicate a decline situation which reveal the some of the problems in the company. ABC had declining rating and increasing costs while the film segment had two film failures. While neither party revealed the settlement amount, it is estimated at $200 million.
Eisner's controlling style inhibited efficiency and progress according to some critics, while other industry experts indicated that "age compression" theory led to a decline in the company's target market due to youth copying teenage behavior earlier.
2000 brought an increase in revenue of 9% and net income of 39% with ABC and ESPN leading the way and Parks and Resorts marking its sixth consecutive year of growth. However the September 11 attacks led to a complete halt of vacation travel and led to a recession. The recession led to a decrease in ABC revenue. Plus, Eisner had the company make an expensive purchase of Fox Family Worldwide. 2001 was a year of cost cutting laying off 4,000 employees, Disney parks operations decreased, slashing annual live-action film investment, and minimizing Internet operations. While 2002 revenue had a small decrease from 2001 with the cost cutting, net income rose to $1.2 billion with two creative film releases. In 2003, the Studio became the first studio to record over $3 billion in worldwide box office receipts.
Eisner did not want the board to renominate Roy E. Disney, the son of Disney co-founder Roy O. Disney, as a board director citing his age of 72 as a required retirement age. Stanley Gold responded by resigning from the board and requesting the other board members oust Eisner. In 2003, Disney resigned from his positions as the company's vice chairman and chairman of Walt Disney Feature Animation, accusing Eisner of micromanagement, flops with the ABC television network, timidity in the theme park business, turning the Walt Disney Company into a "rapacious, soul-less" company, and refusing to establish a clear succession plan, as well as a string of box-office movie flops starting in the year 2000.
On May 15, 2003, Disney sold their stake in the Anaheim Angels baseball team to Arte Moreno. Disney purchased the rights to The Muppets and the "Bear in the Big Blue House" franchises from The Jim Henson Company on February 17, 2004. The two brands were placed under control of the Muppets Holding Company, LLC, a unit of Disney Consumer Products.
In 2004, Pixar Animation Studios began looking for another distributor after its 12-year contract with Disney ended, due to its strained relationship over issues of control and money with Eisner. Also that year, Comcast Corporation made an unsolicited $54 billion bid to acquire Disney. A couple of high budget movies flopped at the box office. With these difficulties and with some board directors dissatisfied, Eisner ceded the board chairmanship.
On March 3, 2004, at Disney's annual shareholders' meeting, a surprising and unprecedented 45% of Disney's shareholders, predominantly rallied by former board members Roy Disney and Stanley Gold, withheld their proxies to re-elect Eisner to the board. Disney's board then gave the chairmanship position to Mitchell. However, the board did not immediately remove Eisner as chief executive.
In 2005, Disney sold the Mighty Ducks of Anaheim hockey team to Henry and Susan Samueli.
On March 13, 2005, Robert Iger was announced as Eisner successor as CEO. On September 30, Eisner resigned both as an executive and as a member of the board of directors.
2005–present: The Iger era.
On July 8, 2005, Walt Disney's nephew, Roy E. Disney returned to The Walt Disney Company as a consultant and with the new title of Non Voting Director, Emeritus. Walt Disney Parks and Resorts celebrated the 50th anniversary of Disneyland Park on July 17, and opened Hong Kong Disneyland on September 12. Walt Disney Feature Animation released "Chicken Little", the company's first film using 3-D animation. On October 1, Bob Iger replaced Michael Eisner as CEO. Miramax co-founders Bob Weinstein and Harvey Weinstein also departed the company to form their own studio. On July 25, 2005, Disney announced that it was closing DisneyToon Studios Australia in October 2006, after 17 years of existence.
In 2006, Disney acquired Oswald the Lucky Rabbit, Disney’s pre-Mickey silent animation star.
Aware that Disney's relationship with Pixar was wearing thin, President and CEO Robert Iger began negotiations with leadership of Pixar Animation Studios, Steve Jobs and Ed Catmull, regarding possible merger. On January 23, 2006, it was announced that Disney would purchase Pixar in an all-stock transaction worth $7.4 billion. The deal was finalized on May 5; and among noteworthy results was the transition of Pixar's CEO and 50.1% shareholder, Steve Jobs, becoming Disney's largest individual shareholder at 7% and a member of Disney's Board of Directors. Ed Catmull took over as President of Pixar Animation Studios. Former Executive Vice-President of Pixar, John Lasseter, became Chief Creative Officer of Walt Disney Animation Studios, its division DisneyToon Studios, and Pixar Animation Studios, as well assuming the role of Principal Creative Advisor at Walt Disney Imagineering.
In April 2007, the Muppets Holding Company, LLC was renamed The Muppets Studio and placed under new leadership in an effort by Iger to re-brand the division. The re-branding was completed in September 2008, when control of The Muppets Studio was transferred from Disney Consumer Products to the Walt Disney Studios.
After a long time working in the company as a senior executive and large shareholder, Director Emeritus Roy E. Disney died from stomach cancer on December 16, 2009. At the time of his death, he owned roughly 1% of all of Disney which amounted to 16 million shares. He is seen to be the last member of the Disney family to be actively involved in the running of the company and working in the company altogether.
On August 31, 2009, Disney announced a deal to acquire Marvel Entertainment, Inc. for $4.24 billion. The deal was finalized on December 31, 2009 in which Disney acquired full ownership on the company. Disney has stated that their acquisition of Marvel Entertainment will not affect Marvel's products, neither will the nature of any Marvel characters be transformed.
In October 2009, Disney Channel president Rich Ross, hired by Iger, replaced Dick Cook as chairman of the company and, in November, began restructuring the company to focus more on family friendly products. Later in January 2010, Disney decided to shut down Miramax after downsizing Touchstone, but one month later, they instead began selling the Miramax brand and its 700-title film library to Filmyard Holdings. On March 12, ImageMovers Digital, Robert Zemeckis's company which Disney had bought in 2007, was shut down. In April 2010, Lyric Street, Disney's country music label in Nashville, was shut down. In May 2010, the company sold the Power Rangers brand, as well as its 700-episode library, back to Haim Saban. In June, the company canceled Jerry Bruckheimer's film project "Killing Rommel". In January 2011, Disney Interactive Studios was downsized. In November, two ABC stations were sold.
With the release of "Tangled" in 2010, Ed Catmull said that the "princess" genre of films was taking a hiatus until "someone has a fresh take on it ... but we don't have any other musicals or fairytales lined up." He explained that they were looking to get away from the princess era due to the changes in audience composition and preference. However, in the Facebook page, Ed Catmull stated that this was just a rumor.
In April 2011, Disney broke ground on Shanghai Disney Resort. Costing $4.4 billion, the resort is slated to open in 2015. Later, in August 2011, Bob Iger stated on a conference call that after the success of the Pixar and Marvel purchases, he and the Walt Disney Company are looking to "buy either new characters or businesses that are capable of creating great characters and great stories." Later, in early February 2012, Disney completed its acquisition of UTV Software Communications, expanding their market further into India and Asia.
On October 30, 2012, Disney announced plans to acquire Lucasfilm, along with plans to produce a in its "Star Wars" franchise for 2015. On December 4, 2012, the Disney-Lucasfilm merger was approved by the Federal Trade Commission, allowing the acquisition to be finalized without dealing with antitrust problems. On December 21, 2012, the deal was completed with the acquisition value amounting to approximately $4.06 billion, and thus Lucasfilm became a wholly owned subsidiary of Disney (which coincidentally reunited Lucasfilm under the same corporate umbrella with its former spin-off and new sibling, Pixar).
On May 29, 2013, Disney set release dates for eight currently untitled animated films through 2018, including four from Disney Animation and four from Pixar Animation.
On March 24, 2014, Disney bought Maker Studios, a YouTube company generating billions of views each year, for over $500 million in order to advertise to viewers in the crucial teenage/young adult demographics.
On May 9, 2014, Disney announced they have reached an agreement with Japan's TV Asahi Corporation to air an English dub of the "Doraemon" anime series on Disney XD.
In July 2014, The Walt Disney Company announced 11 startups that would begin in the company’s accelerator program.
In August 2014, The Walt Disney Company filed three patents for using drones. Patents included using unmanned aerial vehicles (UAV) to lift marionettes in the air, raise mesh screens for floating video projections, and equipping drones with lights to make them part of a new kind of light show.
On February 5, 2015, it was announced that Tom Staggs had been promoted to COO.
Company divisions and subsidiaries.
The Walt Disney Company operates through five primary business units, which it calls "business segments": Studio Entertainment, with the primary business unit The Walt Disney Studios, which includes the company's film, recording label, and theatrical divisions; Parks and Resorts, featuring the company's theme parks, cruise line, and other travel-related assets; Disney Consumer Products, which produces toys, clothing, and other merchandising based upon Disney-owned properties; Media Networks, which includes the company's television properties; and Disney Interactive, which includes Disney's Internet, mobile, social media, virtual worlds, and computer games operations. The first four segments are headed by chairmen, while Disney Interactive is currently headed by a president. Marvel Entertainment is also a direct CEO reporting business, while its financial results are primarily divided between the Studio Entertainment and Consumer Products segments. While Maker Studios is split between Studio Entertainment and Media Networks segments.
Its main entertainment holdings include Walt Disney Studios, Disney Music Group, Disney Theatrical Group, Disney-ABC Television Group, Radio Disney, ESPN Inc., Disney Interactive Media Group, Disney Consumer Products, Disney India Ltd., The Muppets Studio, Pixar Animation Studios, Marvel Entertainment, UTV Software Communications, Lucasfilm and Maker Studios.
Its resorts and diversified related holdings include Walt Disney Parks and Resorts, Disneyland Resort, Walt Disney World Resort, Tokyo Disney Resort, Disneyland Paris, Euro Disney S.C.A., Hong Kong Disneyland Resort, Disney Vacation Club and Disney Cruise Line.
Executive management.
Chairmen of the Board.
Walt Disney dropped his Chairman title in 1960 to focus more on the creative aspects of the company, becoming "executive producer in charge of all production." After a four-year vacancy, Roy O. Disney assumed the chairmanship.
Criticism.
Some of Disney's animated family films have drawn fire for being accused of having sexual references hidden in them, among them "The Little Mermaid" (1989), "Aladdin" (1992), and "The Lion King" (1994). Instances of sexual material hidden in some versions of "The Rescuers" (1977) and "Who Framed Roger Rabbit" (1988) resulted in recalls and modifications of the films to remove such content.
Some religious welfare groups, such as the Catholic League, have opposed films including "Priest" (1994) and "Dogma" (1999). A book called "Growing Up Gay", published by Disney-owned Hyperion and similar publications, as well as the company's extension of benefits to same-sex domestic partners, spurred boycotts of Disney and its advertisers by the Catholic League, the Assemblies of God USA, the American Family Association, and other conservative groups. The boycotts were discontinued by most of these organizations by 2005. In addition to these social controversies, the company has been accused of human rights violations regarding the working conditions in factories that produce their merchandise.

</doc>
<doc id="37409" url="http://en.wikipedia.org/wiki?curid=37409" title="Ravenna">
Ravenna

Ravenna (Romagnol: "Ravêna") is the capital city of the Province of Ravenna, in the Emilia-Romagna region of Italy. It was the capital city of the Western Roman Empire from 402 until that empire collapsed in 476. It then served as the capital of the Kingdom of the Ostrogoths until it was re-conquered in 540 by the Eastern Roman (Byzantine) Empire. Afterwards, the city formed the centre of the Byzantine Exarchate of Ravenna until the invasion of the Franks in 751, after which it became the seat of the Kingdom of the Lombards.
Although an inland city, Ravenna is connected to the Adriatic Sea by the Candiano Canal. It is the location of eight UNESCO World Heritage Sites.
History.
The origin of the name "Ravenna" is unclear, although it is believed the name is Etruscan. Some have speculated that "ravenna" is related to "Rasenna" (later "Rasna"), the term that the Etruscans used for themselves, but there is no agreement on this point.
Ancient era.
The origins of Ravenna are uncertain. The first settlement is variously attributed to (and then has seen the co presence of) the Thessalians, the Etruscans and the Umbrians, afterwards its territory was settled also by the Senones, especially the southern countryside of the city (that wasn't part of the lagoon), the "Ager Decimanus". Ravenna consisted of houses built on piles on a series of small islands in a marshy lagoon – a situation similar to Venice several centuries later. The Romans ignored it during their conquest of the Po River Delta, but later accepted it into the Roman Republic as a federated town in 89 BC. In 49 BC, it was the location where Julius Caesar gathered his forces before crossing the Rubicon. Later, after his battle against Mark Antony in 31 BC, Emperor Augustus founded the military harbor of Classe. This harbor, protected at first by its own walls, was an important station of the Roman Imperial Fleet. Nowadays the city is landlocked, but Ravenna remained an important seaport on the Adriatic until the early Middle Ages. During the German campaigns, Thusnelda, widow of Arminius, and Marbod, King of the Marcomanni, were confined at Ravenna.
Ravenna greatly prospered under Roman rule. Emperor Trajan built a 70 km long aqueduct at the beginning of the 2nd century. During the Marcomannic Wars, Germanic settlers in Ravenna revolted and managed to seize possession of the city. For this reason, Marcus Aurelius decided not only against bringing more barbarians into Italy, but even banished those who had previously been brought there. In AD 402, Emperor Honorius transferred the capital of the Western Roman Empire from Milan to Ravenna. At that time it was home to 50,000 people. The transfer was made partly for defensive purposes: Ravenna was surrounded by swamps and marshes, and was perceived to be easily defensible (although in fact the city fell to opposing forces numerous times in its history); it is also likely that the move to Ravenna was due to the city's port and good sea-borne connections to the Eastern Roman Empire. However, in 409, King Alaric I of the Visigoths simply bypassed Ravenna, and went on to sack Rome in 410 and to take Galla Placidia, daughter of Emperor Theodosius I, hostage. After many vicissitudes, Galla Placidia returned to Ravenna with her son, Emperor Valentinian III and the support of her nephew Theodosius II. Ravenna enjoyed a period of peace, during which time the Christian religion was favoured by the imperial court, and the city gained some of its most famous monuments, including the Orthodox Baptistery, the misnamed Mausoleum of Galla Placidia (she was not really buried there), and San Giovanni Evangelista.
The late 400s saw the dissolution of Roman authority in the west, and the last person to hold the title of emperor in the West was deposed in 476 by the general Odoacer. Odoacer ruled as King of Italy for 13 years, but in 489 the Eastern Emperor Zeno sent the Ostrogoth King Theoderic the Great to re-take the Italian peninsula. After losing the Battle of Verona, Odoacer retreated to Ravenna, where he withstood a siege of three years by Theoderic, until the taking of Rimini deprived Ravenna of supplies. Theoderic took Ravenna in 493, supposedly slew Odoacer with his own hands, and Ravenna became the capital of the Ostrogothic Kingdom of Italy. Theoderic, following his imperial predecessors, also built many splendid buildings in and around Ravenna, including his palace church Sant'Apollinare Nuovo, an Arian cathedral (now Santo Spirito) and Baptistery, and his own Mausoleum just outside the walls.
Theoderic and his followers were Arian Christians, but co-existed peacefully with the Latins, who were largely Orthodox. Ravenna's Orthodox bishops carried out notable building projects, of which the sole surviving one is the Capella Arcivescovile. Theoderic allowed Roman citizens within his kingdom to be subject to Roman law and the Roman judicial system. The Goths, meanwhile, lived under their own laws and customs. In 519, when a mob had burned down the synagogues of Ravenna, Theoderic ordered the town to rebuild them at its own expense.
Theoderic died in 526 and was succeeded by his young grandson Athalaric under the authority of his daughter Amalasunta, but by 535 both were dead and Theoderic's line was represented only by Amalasuntha's daughter Matasuntha. Various Ostrogothic military leaders took the kingship of Italy, but none were as successful as Theoderic had been. Meanwhile, the orthodox Christian Byzantine Emperor Justinian I, opposed both Ostrogoth rule and the Arian variety of Christianity. In 535 his general Belisarius invaded Italy and in 540 conquered Ravenna. After the conquest of Italy was completed in 554, Ravenna became the seat of Byzantine government in Italy.
From 540 to 600, Ravenna's bishops embarked upon a notable building program of churches in Ravenna and in and around the port city of Classe. Surviving monuments include the Basilica of San Vitale and the Basilica of Sant'Apollinare in Classe, as well as the partially surviving San Michele in Africisco.
Exarchate of Ravenna.
Following the conquests of Belisarius for the Emperor Justinian I in the 6th century, Ravenna became the seat of the Byzantine governor of Italy, the Exarch, and was known as the Exarchate of Ravenna. It was at this time that the Ravenna Cosmography was written.
Under Byzantine rule, the archbishop of Ravenna was temporarily granted autocephaly from the Roman Church by the emperor, in 666, but this was soon revoked. Nevertheless, the archbishop of Ravenna held the second place in Italy after the pope, and played an important role in many theological controversies during this period.
Middle Ages and Renaissance.
The Lombards, under King Liutprand, occupied Ravenna in 712, but were forced to return it to the Byzantines. However, in 751 the Lombard king, Aistulf, succeeded in conquering Ravenna, thus ending Byzantine rule in northern Italy.
King Pepin of France attacked the Lombards under orders of Pope Stephen II. Ravenna then gradually came under the direct authority of the popes, although this was contested by the archbishops at various times. Pope Adrian I authorized Charlemagne to take away anything from Ravenna that he liked, and an unknown quantity of Roman columns, mosaics, statues, and other portable items were taken north to enrich his capital of Aachen.
In 1198 Ravenna led a league of Romagna cities against the Emperor, and the Pope was able to subdue it. After the war of 1218 the Traversari family was able to impose its rule in the city, which lasted until 1240. After a short period under an Imperial vicar, Ravenna was returned to the Papal States in 1248 and again to the Traversari until, in 1275, the Da Polenta established their long-lasting seigniory. One of the most illustrious residents of Ravenna at this time was the exiled poet Dante. The last of the Da Polenta, Ostasio III, was ousted by the Republic of Venice in 1440, and the city was annexed to the Venetian territories.
Ravenna was ruled by Venice until 1509, when the area was invaded in the course of the Italian Wars. In 1512, during the Holy League wars, Ravenna was sacked by the French.
After the Venetian withdrawal, Ravenna was again ruled by legates of the Pope as part of the Papal States. The city was damaged in a tremendous flood in May 1636. Over the next 300 years, a network of canals diverted nearby rivers and drained nearby swamps, thus reducing the possibility of flooding and creating a large belt of agricultural land around the city.
Modern age.
Apart from another short occupation by Venice (1527–1529), Ravenna was part of the Papal States until 1796, when it was annexed to the French puppet state of the Cisalpine Republic, (Italian Republic from 1802, and Kingdom of Italy from 1805). It was returned to the Papal States in 1814. Occupied by Piedmontese troops in 1859, Ravenna and the surrounding Romagna area became part of the new unified Kingdom of Italy in 1861. During World War II two troops of the British 27th Lancers entered and occupied Ravenna on 5 December 1944. The town suffered very little damage.
Main sights.
Eight early Christian monuments of Ravenna are inscribed on the World Heritage List. These are
Other attractions include:
Music.
The city annually hosts the Ravenna Festival, one of Italy's prominent classical music gatherings. Opera performances are held at the Teatro Alighieri while concerts take place at the Palazzo Mauro de André as well as in the ancient Basilica of San Vitale and Basilica of Sant'Apollinare in Classe. Chicago Symphony Orchestra music director Riccardo Muti, a longtime resident of the city, regularly participates in the festival, which invites orchestras and other performers from around the world.
Ravenna in Film.
Michelangelo Antonioni filmed his 1964 movie "Red Desert" ("Deserto Rosso") within the industrialised areas of the Pialassa valley within the city limits.
Transport.
Ravenna has an important commercial and tourist port.
Ravenna railway station has direct Trenitalia service to Bologna, Ferrara, Lecce, Milan, Parma, Rimini, Venice and Verona.
Ravenna Airport is located in Ravenna. The nearest commercial airports are those of Forlì, Rimini and Bologna.
Freeways crossing Ravenna include: A14-bis from the hub of Bologna; on the north-south axis of EU routes E45 (from Rome) and E55 (SS-309 "Romea" from Venice); and on the regional Ferrara-Rimini axis of SS-16 (partially called "Adriatica").
Twin towns—Sister cities.
Ravenna is twinned with:
 Chartres, France, since 1957
 Laguna, Brazil
Sports.
The historical Italian football of the city is Ravenna F.C.. Currently it plays in Eccellenza Emilia-Romagna Girone B.
A.P.D. Ribelle 1927 is the Italian football of Castiglione di Ravenna, a fraction of Ravenna and was founded in 1927. Currently it plays in Italy's Serie D after the promotion from Eccellenza Emilia-Romagna Girone B in the 2013-14 season.
The president is Marcello Missiroli and the manager is Enrico Zaccaroni.
Its home ground is "Stadio Massimo Sbrighi" of the fraction with 1,000 seats. The team's colors are white and blue.
The beaches of Ravenna hosted the 2011 FIFA Beach Soccer World Cup, in September 2011.

</doc>
<doc id="37417" url="http://en.wikipedia.org/wiki?curid=37417" title="Mercury (mythology)">
Mercury (mythology)

Mercury (; Latin: "Mercurius"   ) is a major Roman god, being one of the Dii Consentes within the ancient Roman pantheon. He is the patron god of financial gain, commerce, eloquence (and thus poetry), messages/communication (including divination), travelers, boundaries, luck, trickery and thieves; he is also the guide of souls to the underworld.
He was considered the son of Maia and Jupiter in Roman mythology. His name is possibly related to the Latin word "merx" ("merchandise"; compare "merchant", "commerce", etc.), "mercari" ("to trade"), and "merces" ("wages"); another possible connection is the Proto-Indo-European root merĝ- for "boundary, border" (cf. Old English "mearc", Old Norse "mark" and Latin "margō") and Greek οὖρος (by analogy of Arctūrus/Ἀρκτοῦρος), as the "keeper of boundaries," referring to his role as bridge between the upper and lower worlds. In his earliest forms, he appears to have been related to the Etruscan deity Turms, both of which share characteristics with the Greek god Hermes. In Virgil's "Aeneid", Mercury reminds Aeneas of his mission to found the city of Rome. In Ovid's "Fasti", Mercury is assigned to escort the nymph Larunda to the underworld. Mercury, however, fell in love with Larunda and made love to her on the way. Larunda thereby became mother to two children, referred to as the Lares, invisible household gods.
Mercury has influenced the name of many things in a variety of scientific fields, such as the planet Mercury, and the element mercury. The word "mercurial" is commonly used to refer to something or someone erratic, volatile or unstable, derived from Mercury's swift flights from place to place. He is often depicted holding the caduceus in his left hand.
History.
Mercury did not appear among the numinous "di indigetes" of early Roman religion. Rather, he subsumed the earlier Dei Lucrii as Roman religion was syncretized with Greek religion during the time of the Roman Republic, starting around the 4th century BC. From the beginning, Mercury had essentially the same aspects as Hermes, wearing winged shoes (talaria) and a winged hat (petasos), and carrying the caduceus, a herald's staff with two entwined snakes that was Apollo's gift to Hermes. He was often accompanied by a cockerel, herald of the new day, a ram or goat, symbolizing fertility, and a tortoise, referring to Mercury's legendary invention of the lyre from a tortoise shell.
Like Hermes, he was also a god of messages, eloquence and of trade, particularly of the grain trade. Mercury was also considered a god of abundance and commercial success, particularly in Gaul, where he was said to have been particularly revered. He was also, like Hermes, the Romans' psychopomp, leading newly deceased souls to the afterlife. Additionally, Ovid wrote that Mercury carried Morpheus' dreams from the valley of Somnus to sleeping humans.
Archeological evidence from Pompeii suggests that Mercury was among the most popular of Roman gods. The god of commerce was depicted on two early bronze coins of the Roman Republic, the Sextans and the Semuncia.
Syncretism.
When they described the gods of Celtic and Germanic tribes, rather than considering them separate deities, the Romans interpreted them as local manifestations or aspects of their own gods, a cultural trait called the "interpretatio Romana". Mercury in particular was reported as becoming extremely popular among the nations the Roman Empire conquered; Julius Caesar wrote of Mercury being the most popular god in Britain and Gaul, regarded as the inventor of all the arts. This is probably because in the Roman syncretism, Mercury was equated with the Celtic god Lugus, and in this aspect was commonly accompanied by the Celtic goddess Rosmerta. Although Lugus may originally have been a deity of light or the sun (though this is disputed), similar to the Roman Apollo, his importance as a god of trade made him more comparable to Mercury, and Apollo was instead equated with the Celtic deity Belenus.
Romans associated Mercury with the Germanic god Wotan, by "interpretatio Romana"; 1st-century Roman writer Tacitus identifies him as the chief god of the Germanic peoples.
In Celtic areas, Mercury was sometimes portrayed with three heads or faces, and at Tongeren, Belgium, a statuette of Mercury with three phalli was found, with the extra two protruding from his head and replacing his nose; this was probably because the number 3 was considered magical, making such statues good luck and fertility charms. The Romans also made widespread use of small statues of Mercury, probably drawing from the ancient Greek tradition of hermae markers.
Names and epithets.
Mercury is known to the Romans as Mercurius and occasionally in earlier writings as "Merqurius", "Mirqurios" or "Mircurios", had a number of epithets representing different aspects or roles, or representing syncretisms with non-Roman deities. The most common and significant of these epithets included the following:
Mercury's net.
Vulcan created a net out of unbreakable steel so that he could catch Venus, the goddess of love and beauty, and Mars, the god of war, in the act of making love. He was jealous of their relationship, because Venus was his beloved wife. Vulcan managed to catch them but, afterwards, Mercury stole the net from the blacksmith god so that he could catch Chloris, a nymph whom he admired. Chloris was tasked with flying after the sun while it rose and scattering lilies, roses and violets behind it. Mercury lay in wait for at least several days until he caught her wing in the net over an unnamed great river in Ethiopia. Mercury then gave the net to the temple of Anubis at Canopus to protect the sacred spot. In Ludovico Ariosto's "Orlando Furioso", the net is stolen 3,000 years later by Caligorant, who goes on to destroy the temple and the city.
Temple.
Mercury's temple in Rome was situated in the Circus Maximus, between the Aventine and Palatine hills, and was built in 495 BC.
That year saw disturbances at Rome between the patrician senators and the plebeians, which led to a secession of the plebs in the following year. At the completion of its construction, a dispute emerged between the consuls Appius Claudius Sabinus Inregillensis and Publius Servilius Priscus Structus as to which of them should have the honour of dedicating the temple. The senate referred the decision to the popular assembly, and also decreed that whichever was chosen should also exercise additional duties, including presiding over the markets, establish a merchants' guild, and exercising the functions of the pontifex maximus. The people, because of the ongoing public discord, and in order to spite the senate and the consuls, instead awarded the honour of dedicating the temple to the senior military officer of one of the legions named Marcus Laetorius. The senate and the consuls, in particular the conservative Appius, were outraged at this decision, and it inflamed the ongoing situation.
The dedication occurred on 15 May, 495 BC.
The temple was regarded as a fitting place to worship a swift god of trade and travel, since it was a major center of commerce as well as a racetrack. Since it stood between the plebeian stronghold on the Aventine and the patrician center on the Palatine, it also emphasized the role of Mercury as a mediator.
Worship.
Because Mercury was not one of the early deities surviving from the Roman Kingdom, he was not assigned a "flamen" ("priest"), but he did have his own major festival, on May 15, the Mercuralia. During the Mercuralia, merchants sprinkled water from his sacred well near the Porta Capena on their heads.
In Hindu mythology.
Budha (Sanskrit: बुध) or Saumya of Hindu mythology, and not to be confused with the Buddha, is the name for the Hindu god and the planet of Mercury (but not for the chemical element called quicksilver.) Budha is the son of Chandra (the Moon) and his mother is said to be Taraka (or Tara.) Budha is the god of merchandise and protector of merchants; he presides over intersections such as road junctions and the midweek day 'Budhavara' or Wednesday. In modern Hindi, Oriya, Telugu, Bengali, Marathi, Urdu, Kannada and Gujarati, Wednesday is called "Budhavara"; Tamil: "Budhan kizhamai"; Malayalam: "Budhanazhcha"; Thai: "Wan Phut" (วันพุธ).

</doc>
<doc id="37427" url="http://en.wikipedia.org/wiki?curid=37427" title="Le Chatelier's principle">
Le Chatelier's principle

In chemistry, Le Châtelier's principle, also called Chatelier's principle or "The Equilibrium Law", can be used to predict the effect of a change in conditions on a chemical equilibrium. The principle is named after Henry Louis Le Châtelier and sometimes Karl Ferdinand Braun who discovered it independently. It can be stated as:
or whenever a system in equilibrium is disturbed the system will adjust itself in such a way that the effect of the change will be nullified. (in short)
This principle has a variety of names, depending upon the discipline using it (see homeostasis, a term commonly used in biology). It is common to take Le Châtelier's principle to be a more general observation, roughly stated:
In chemistry, the principle is used to manipulate the outcomes of reversible reactions, often to increase the yield of reactions. In pharmacology, the binding of ligands to the receptor may shift the equilibrium according to Le Châtelier's principle, thereby explaining the diverse phenomena of receptor activation and desensitization. In economics, the principle has been generalized to help explain the price equilibrium of efficient economic systems. In simultaneous equilibrium systems, phenomena that are in apparent contradiction to Le Châtelier's principle can occur; these can be resolved by the theory of response reactions.
Status as a Physical Law.
Le Châtelier's principle describes the qualitative behavior of systems where there is an externally induced, instantaneous change in one parameter of a system; it states that a behavioural shift occurs in the system so as to oppose (partially cancel) the parameter change. The duration of adjustment depends on the strength of the negative feedback to the initial shock. Where a shock initially induces positive feedback (such as thermal runaway), the new equilibrium can be far from the old one, and can take a long time to reach. In some dynamic systems, the end-state cannot be determined from the shock. The principle is typically used to describe closed negative-feedback systems, but applies, in general, to thermodynamically closed and isolated systems in nature, since the second law of thermodynamics ensures that the disequilibrium caused by an instantaneous shock must have a finite half-life. The principle has analogs throughout the entire physical world.
Chemistry.
Effect of change in concentration.
Changing the concentration of a chemical will shift the equilibrium to the side that would reduce that change in concentration. The chemical system will attempt to partially oppose the change affected to the original state of equilibrium. In turn, the rate of reaction, extent and yield of products will be altered corresponding to the impact on the system.
This can be illustrated by the equilibrium of carbon monoxide and hydrogen gas, reacting to form methanol.
Suppose we were to increase the concentration of CO in the system. Using Le Châtelier's principle, we can predict that the amount of methanol will increase, decreasing the total change in CO. If we are to add a species to the overall reaction, the reaction will favor the side opposing the addition of the species. Likewise, the subtraction of a species would cause the reaction to "fill the gap" and favor the side where the species was reduced. This observation is supported by the collision theory. As the concentration of CO is increased, the frequency of successful collisions of that reactant would increase also, allowing for an increase in forward reaction, and generation of the product. Even if a desired product is not thermodynamically favored, the end-product can be obtained if it is continuously removed from the solution.
Effect of change in temperature.
The effect of changing the temperature in the equilibrium can be made clear by a) incorporating heat as either a reactant or a product, and b) assuming that an increase in temperature increases the heat content of a system. When the reaction is exothermic (Δ"H" is negative, puts energy out), heat is included as a product, and, when the reaction is endothermic (Δ"H" is positive, takes energy in), heat is included as a reactant. Hence, whether increasing or decreasing the temperature would favor the forward or the reverse reaction can be determined by applying the same principle as with concentration changes.
Take, for example, the reversible reaction of nitrogen gas with hydrogen gas to form ammonia:
Because this reaction is exothermic, it produces heat:
If the temperature was increased, the heat content of the system would increase, so the system would consume some of that heat by shifting the equilibrium to the left, thereby producing less ammonia. More ammonia would be produced if the reaction was run at a lower temperature, but a lower temperature also lowers the rate of the process, so, in practice (the Haber process) the temperature is set at a compromise value that allows ammonia to be made at a reasonable rate with an equilibrium concentration that is not too unfavorable.
In exothermic reactions, increase in temperature decreases the equilibrium constant, "K", whereas, in endothermic reactions, increase in temperature increases the K value.
Le Chatelier's principle applied to changes in concentration or pressure can be understood by having "K" have a constant value. The effect of temperature on equilibria, however, involves a change in the equilibrium constant. The dependence of "K" on temperature is determined by the sign of Δ"H". The theoretical basis of this dependence is given by the Van 't Hoff equation.
Effect of change in pressure.
Changes in pressure are attributable to changes in volume. The equilibrium concentrations of the products and reactants do not directly depend on the pressure subjected to the system. However, a change in pressure due to a change in volume of the system will shift the equilibrium.
Considering the reaction of nitrogen gas with hydrogen gas to form ammonia:
Note the number of moles of gas on the left-hand side and the number of moles of gas on the right-hand side. When the volume of the system is changed, the partial pressures of the gases change. If we were to decrease pressure by increasing volume, the equilibrium of the above reaction will shift to the left, because the reactant side has greater number of moles than does the product side. The system tries to counteract the decrease in partial pressure of gas molecules by shifting to the side that exerts greater pressure. Similarly, if we were to increase pressure by decreasing volume, the equilibrium shifts to the right, counteracting the pressure increase by shifting to the side with fewer moles of gas that exert less pressure. If the volume is increased because there are more moles of gas on the reactant side, this change is more significant in the denominator of the equilibrium constant expression, causing a shift in equilibrium.
Thus, an increase in system pressure due to decreasing volume causes the reaction to shift to the side with the fewer moles of gas. A decrease in pressure due to increasing volume causes the reaction to shift to the side with more moles of gas. There is no effect on a reaction where the number of moles of gas is the same on each side of the chemical equation.
Effect of adding an inert gas.
An inert gas (or noble gas) such as helium is one that does not react with other elements or compounds. Adding an inert gas into a gas-phase equilibrium at constant volume does not result in a shift. This is because the addition of a non-reactive gas does not change the partial pressures of the other gases in the container. While it is true that the total pressure of the system increases, the total pressure does not have any effect on the equilibrium constant; rather, it is a change in partial pressures that will cause a shift in the equilibrium. If, however, the volume is allowed to increase in the process, the partial pressures of all gases would be decreased resulting in a shift towards the side with the greater number of moles of gas.
Effect of a catalyst.
A catalyst has no effect on the position and composition of an equilibrium. It just speeds up both the forward and backward reactions equally, at the same time.
For example, consider the Haber process for the synthesis of ammonia (NH3):
In the above reaction, iron (Fe) and Molybdenum (Mo) will function as catalysts if present. They will accelerate any reactions, but they do not affect the state of the equilibrium.
Applications in economics.
In economics, a similar concept also named after Le Châtelier was introduced by U.S. economist Paul Samuelson in 1947. There the generalized Le Châtelier principle is for a maximum condition of economic equilibrium: Where all unknowns of a function are independently variable, auxiliary constraints—"just-binding" in leaving initial equilibrium unchanged—reduce the response to a parameter change. Thus, factor-demand and commodity-supply elasticities are hypothesized to be lower in the short run than in the long run because of the fixed-cost constraint in the short run.

</doc>
<doc id="37431" url="http://en.wikipedia.org/wiki?curid=37431" title="Solvent">
Solvent

A solvent (from the Latin "solvō", "I loosen, untie, I solve") is a substance that dissolves a solute (a chemically different liquid, solid or gas), resulting in a solution. A solvent is usually a liquid but can also be a solid or a gas. The quantity of solute that can dissolve in a specific volume of solvent varies with temperature. Common uses for organic solvents are in dry cleaning (e.g., tetrachloroethylene), as paint thinners (e.g., toluene, turpentine), as nail polish removers and glue solvents (acetone, methyl acetate, ethyl acetate), in spot removers (e.g., hexane, petrol ether), in detergents (citrus terpenes) and in perfumes (ethanol). Water is a solvent for polar molecules and the most common solvent used by living things; all the ions and proteins in a cell are dissolved in water within a cell. Solvents find various applications in chemical, pharmaceutical, oil and gas industries, including in chemical syntheses and purification processes.
The global solvent market is expected to earn revenues of about US$33 billion in 2019. The dynamic economic development in emerging markets like China, India or Brazil will especially continue to boost demand for solvents. Specialists expect the worldwide solvent consumption to increase at an average annual rate of 2.5% over the subsequent years. Accordingly, the growth rate seen during the past eight years will be surpassed.
Solutions and solvation.
When one substance is dissolved into another, a solution is formed. This is opposed to the situation when the compounds are insoluble like sand in water. In a solution, all of the ingredients are uniformly distributed at a molecular level and no residue remains. A solvent-solute mixture consists of a single phase with all solute molecules occurring as "solvates" (solvent-solute complexes), as opposed to separate continuous phases as in suspensions, emulsions and other types of non-solution mixtures. The ability of one compound to be dissolved in another is known as solubility; if this occurs in all proportions, it is called miscibility.
In addition to mixing, the substances in a solution interact with each other at the molecular level. When something is dissolved, molecules of the solvent arrange around molecules of the solute. Heat transfer is involved and entropy is increased making the solution more thermodynamically stable than the solute and solvent separately. This arrangement is mediated by the respective chemical properties of the solvent and solute, such as hydrogen bonding, dipole moment and polarizability. Solvation does not cause a chemical reaction or chemical configuration changes in the solute. However, solvation resembles a coordination complex formation reaction, often with considerable energetics (heat of solvation and entropy of solvation) and is thus far from a neutral process.
Solvent classifications.
Solvents can be broadly classified into two categories: "polar" and "non-polar". Generally, the dielectric constant of the solvent provides a rough measure of a solvent's polarity. The strong polarity of water is indicated, at 0 °C, by a dielectric constant of 88. Solvents with a dielectric constant of less than 15 are generally considered to be nonpolar. The dielectric constant measures the solvent's tendency to partly cancel the field strength of the electric field of a charged particle immersed in it. This reduction is then compared to the field strength of the charged particle in a vacuum. Heuristically, the dielectric constant of a solvent can be thought of as its ability to reduce the solute's effective internal charge. Generally, the dielectric constant of a solvent is an acceptable predictor of the solvent's ability to dissolve common ionic compounds, such as salts.
Other polarity scales.
Dielectric constants are not the only measure of polarity. Because solvents are used by chemists to carry out chemical reactions or observe chemical and biological phenomena, more specific measures of polarity are required. Most of these measures are sensitive to chemical context.
"The Grunwald Winstein mY scale" measures polarity in terms of solvent influence on buildup of positive charge of a solute during a chemical reaction.
"Kosower's Z scale" measures polarity in terms of the influence of the solvent on UV-absorption maxima of a salt, usually pyridinium iodide or the pyridinium zwitterion.
"Donor number and donor acceptor scale" measures polarity in terms of how a solvent interacts with specific substances, like a strong Lewis acid or a strong Lewis base.
The Hildebrand parameter is the square root of cohesive energy density. It can be used with nonpolar compounds, but cannot accommodate complex chemistry.
Reichardt's dye, a solvatochromic dye that changes color in response to polarity, gives a scale of "ET"(30) values. "ET" is the transition energy between the ground state and the lowest excited state in kcal/mol, and (30) identifies the dye. Another, roughly correlated scale ("ET"(33)) can be defined with Nile red.
The polarity, dipole moment, polarizability and hydrogen bonding of a solvent determines what type of compounds it is able to dissolve and with what other solvents or liquid compounds it is miscible. Generally, polar solvents dissolve polar compounds best and non-polar solvents dissolve non-polar compounds best: "like dissolves like". Strongly polar compounds like sugars (e.g., sucrose) or ionic compounds, like inorganic salts (e.g., table salt) dissolve only in very polar solvents like water, while strongly non-polar compounds like oils or waxes dissolve only in very non-polar organic solvents like hexane. Similarly, water and hexane (or vinegar and vegetable oil) are not miscible with each other and will quickly separate into two layers even after being shaken well.
Polarity can be separated to different contributions. For example, the Kamlet-Taft parameters are dipolarity/polarizability ("π*"), hydrogen-bonding acidity ("α") and hydrogen-bonding basicity ("β"). These can be calculated from the wavelength shifts of 3–6 different solvatochromic dyes in the solvent, usually including Reichardt's dye, nitroaniline and diethylnitroaniline. Another option, Hansen's parameters, separate the cohesive energy density into dispersion, polar and hydrogen bonding contributions.
Polar protic and polar aprotic.
Solvents with a relative static permittivity greater than 15 (i.e. polar or polarizable) can be further divided into protic and aprotic. Protic solvents solvate anions (negatively charged solutes) strongly via hydrogen bonding. Water is a protic solvent. Aprotic solvents such as acetone or dichloromethane tend to have large dipole moments (separation of partial positive and partial negative charges within the same molecule) and solvate positively charged species via their negative dipole. In chemical reactions the use of polar protic solvents favors the SN1 reaction mechanism, while polar aprotic solvents favor the SN2 reaction mechanism.
Physical properties of common solvents.
Properties table of common solvents.
The solvents are grouped into non-polar, polar aprotic, and polar protic solvents and ordered by increasing polarity. The polarity is given as the dielectric constant. The properties of solvents that exceed those of water are bolded.
Hansen solubility parameter values.
The Hansen solubility parameter values are based on dispersion bonds (δD), polar bonds (δP) and hydrogen bonds (δH). These contain information about the inter-molecular interactions with other solvents and also with polymers, pigments, nanoparticles, etc. This allows for rational formulations knowing, for example, that there is a good HSP match between a solvent and a polymer. Rational substitutions can also be made for "good" solvents (effective at dissolving the solute) that are "bad" (expensive or hazardous to health or the environment). The following table shows that the intuitions from "non-polar", "polar aprotic" and "polar protic" are put numerically – the "polar" molecules have higher levels of δP and the protic solvents have higher levels of δH. Because numerical values are used, comparisons can be made rationally by comparing numbers. For example, acetonitrile is much more polar than acetone but exhibits slightly less hydrogen bonding.
If, for environmental or other reasons, a solvent or solvent blend is required to replace another of equivalent solvency, the substitution can be made on the basis of the Hansen solubility parameters of each. The values for mixtures are taken as the weighted averages of the values for the neat solvents. This can be calculated by trial-and-error, a spreadsheet of values, or HSP software. A 1:1 mixture of toluene and 1,4 dioxane has δD, δP and δH values of 17.8, 1.6 and 5.5, comparable to those of chloroform at 17.8, 3.1 and 5.7 respectively. Because of the health hazards associated with toluene itself, other mixtures of solvents may be found using a full HSP dataset.
Boiling point.
An important property of solvents is the boiling point. This also determines the speed of evaporation. Small amounts of low-boiling-point solvents like diethyl ether, dichloromethane, or acetone will evaporate in seconds at room temperature, while high-boiling-point solvents like water or dimethyl sulfoxide need higher temperatures, an air flow, or the application of vacuum for fast evaporation.
Density.
Most organic solvents have a lower density than water, which means they are lighter and will form a separate layer on top of water. An important exception: most of the halogenated solvents like dichloromethane or chloroform will sink to the bottom of a container, leaving water as the top layer. This is important to remember when partitioning compounds between solvents and water in a separatory funnel during chemical syntheses.
Often, specific gravity is cited in place of density. Specific gravity is defined as the density of the solvent divided by the density of water at the same temperature. As such, specific gravity is a unitless value. It readily communicates whether a water-insoluble solvent will float (SG < 1.0) or sink (SG > 1.0) when mixed with water.
Health and safety.
Fire.
Most organic solvents are flammable or highly flammable, depending on their volatility. Exceptions are some chlorinated solvents like dichloromethane and chloroform. Mixtures of solvent vapors and air can explode. Solvent vapors are heavier than air; they will sink to the bottom and can travel large distances nearly undiluted. Solvent vapors can also be found in supposedly empty drums and cans, posing a flash fire hazard; hence empty containers of volatile solvents should be stored open and upside down.
Both diethyl ether and carbon disulfide have exceptionally low autoignition temperatures which increase greatly the fire risk associated with these solvents. The autoignition temperature of carbon disulfide is below 100 °C (212 °F), so objects such as steam pipes, light bulbs, hotplates and recently extinguished bunsen burners are able to ignite its vapours.
Explosive peroxide formation.
Ethers like diethyl ether and tetrahydrofuran (THF) can form highly explosive organic peroxides upon exposure to oxygen and light, THF is normally more able to form such peroxides than diethyl ether. One of the most susceptible solvents is diisopropyl ether.
The heteroatom (oxygen) stabilizes the formation of a free radical which is formed by the abstraction of a hydrogen atom by another free radical. The carbon centred free radical thus formed is able to react with an oxygen molecule to form a peroxide compound. A range of tests can be used to detect the presence of a peroxide in an ether; one is to use a combination of iron sulfate and potassium thiocyanate. The peroxide is able to oxidize the Fe2+ ion to an Fe3+ ion which then form a deep red coordination complex with the thiocyanate. In extreme cases the peroxides can form crystalline solids within the vessel of the ether.
Unless the desiccant used can destroy the peroxides, they will concentrate during distillation due to their higher boiling point. When sufficient peroxides have formed, they can form a crystalline and shock sensitive solid precipitate. When this solid is formed at the mouth of the bottle, turning the cap may provide sufficient energy for the peroxide to detonate. Peroxide formation is not a significant problem when solvents are used up quickly; they are more of a problem for laboratories which take years to finish a single bottle. Ethers have to be stored in the dark in closed canisters in the presence of stabilizers like butylated hydroxytoluene (BHT) or over sodium hydroxide.
Peroxides may be removed by washing with acidic iron(II) sulfate, filtering through alumina, or distilling from sodium/benzophenone. Alumina does not destroy the peroxides; it merely traps them. The advantage of using sodium/benzophenone is that moisture and oxygen are removed as well.
Health effects.
General health hazards associated with solvent exposure include toxicity to the nervous system, reproductive damage, liver and kidney damage, respiratory impairment, cancer, and dermatitis.
Many solvents can lead to a sudden loss of consciousness if inhaled in large amounts. Solvents like diethyl ether and chloroform have been used in medicine as anesthetics, sedatives, and hypnotics for a long time. Ethanol (grain alcohol) is a widely used and abused psychoactive drug. Diethyl ether, chloroform, and many other solvents (e.g., from gasoline or glues) are used recreationally in glue sniffing, often with harmful long term health effects like neurotoxicity or cancer. Fraudulent substitution of 1,5-pentanediol for the psychoactive 1,4-butanediol by a subcontractor caused the Bindeez product recall.
If ingested, alcohols (other than ethanol) such as methanol, propanol and ethylene glycol metabolize into toxic aldehydes and acids, which cause potentially fatal metabolic acidosis. Thus, the commonly available alcohol solvent methanol can cause permanent blindness and death if ingested, and is also dangerous because it burns with an invisible flame. The solvent 2-butoxyethanol, used in fracking fluids, can cause hypotension and metabolic acidosis.
Some solvents including chloroform and benzene (an ingredient of gasoline) are carcinogenic. Many others can damage internal organs like the liver, the kidneys, or the brain.
Chronic exposure to organic solvents in the work environment can produce a range of adverse neuropsychiatric effects. For example, occupational exposure to organic solvents has been associated with higher numbers of painters suffering from alcoholism. Ethanol has a synergistic effect when taken in combination with many solvents; for instance, a combination of toluene/benzene and ethanol causes greater nausea/vomiting than either substance alone.
Many solvents are known or suspected to be cataractogenic, greatly increasing the risk of developing cataracts of the lens of the eye. Solvent exposure has also been associated with neurotoxic damage to color vision.
Environmental contamination.
A major pathway to induce health effects arises from spills or leaks of solvents that reach the underlying soil. Since solvents readily migrate substantial distances, the creation of widespread soil contamination is not uncommon; there may be about 5000 sites worldwide that have major subsurface solvent contamination; this is particularly a health risk if aquifers are affected.

</doc>
<doc id="37436" url="http://en.wikipedia.org/wiki?curid=37436" title="Emergence">
Emergence

In philosophy, systems theory, science, and art, emergence is a process whereby larger entities, patterns, and regularities arise through interactions among smaller or simpler entities that themselves do not exhibit such properties.
Emergence is central in theories of integrative levels and of complex systems. For instance, the phenomenon "life" as studied in biology is commonly perceived as an emergent property of interacting molecules as studied in chemistry, whose phenomena reflect interactions among elementary particles, modeled in particle physics, that at such higher mass—via substantial conglomeration—exhibit motion as modeled in gravitational physics. Neurobiological phenomena are often presumed to suffice as the underlying basis of psychological phenomena, whereby economic phenomena are in turn presumed to principally emerge.
In philosophy, emergence typically refers to emergentism. Almost all accounts of emergentism include a form of epistemic or ontological irreducibility to the lower levels.
In philosophy.
In philosophy, emergence is often understood to be a claim about the etiology of a system's properties. An emergent property of a system, in this context, is one that is not a property of any component of that system, but is still a feature of the system as a whole. Nicolai Hartmann, one of the first modern philosophers to write on emergence, termed this "categorial novum" (new category).
Definitions.
This idea of emergence has been around since at least the time of Aristotle.  John Stuart Mill and Julian Huxley are two of many scientists and philosophers who have written on the concept.
The term "emergent" was coined by philosopher G. H. Lewes, who wrote:
"Every resultant is either a sum or a difference of the co-operant forces; their sum, when their directions are the same -- their difference, when their directions are contrary. Further, every resultant is clearly traceable in its components, because these are homogeneous and commensurable. It is otherwise with emergents, when, instead of adding measurable motion to measurable motion, or things of one kind to other individuals of their kind, there is a co-operation of things of unlike kinds. The emergent is unlike its components insofar as these are incommensurable, and it cannot be reduced to their sum or their difference."
Economist Jeffrey Goldstein provided a current definition of emergence in the journal "Emergence". Goldstein initially defined emergence as: "the arising of novel and coherent structures, patterns and properties during the process of self-organization in complex systems".
Goldstein's definition can be further elaborated to describe the qualities of this definition in more detail:
"The common characteristics are: (1) radical novelty (features not previously observed in systems); (2) coherence or correlation (meaning integrated wholes that maintain themselves over some period of time); (3) A global or macro "level" (i.e. there is some property of "wholeness"); (4) it is the product of a dynamical process (it evolves); and (5) it is "ostensive" (it can be perceived). For good measure, Goldstein throws in supervenience."
Systems scientist Peter Corning also says that living systems cannot be reduced to underlying laws of physics:
Rules, or laws, have no causal efficacy; they do not in fact “generate” anything. They serve merely to describe regularities and consistent relationships in nature. These patterns may be very illuminating and important, but the underlying causal agencies must be separately specified (though often they are not). But that aside, the game of chess illustrates ... why any laws or rules of emergence and evolution are insufficient. Even in a chess game, you cannot use the rules to predict “history” — i.e., the course of any given game. Indeed, you cannot even reliably predict the next move in a chess game. Why? Because the “system” involves more than the rules of the game. It also includes the players and their unfolding, moment-by-moment decisions among a very large number of available options at each choice point. The game of chess is inescapably historical, even though it is also constrained and shaped by a set of rules, not to mention the laws of physics. Moreover, and this is a key point, the game of chess is also shaped by teleonomic, cybernetic, feedback-driven influences. It is not simply a self-ordered process; it involves an organized, “purposeful” activity.
Strong and weak emergence.
Usage of the notion "emergence" may generally be subdivided into two perspectives, that of "weak emergence" and "strong emergence". In terms of physical systems, weak emergence is a type of emergence in which the emergent property is amenable to computer simulation. This is opposed to the older notion of strong emergence, in which the emergent property cannot be simulated by a computer.
Some common points between the two notions are that emergence concerns new properties produced as the system grows, which is to say ones which are not shared with its components or prior states. Also, it is assumed that the properties are supervenient rather than metaphysically primitive .
Weak emergence describes new properties arising in systems as a result of the interactions at an elemental level. However, it is stipulated that the properties can be determined by observing or simulating the system, and not by any process of a priori analysis.
Bedau notes that weak emergence is not a universal metaphysical solvent, as weak emergence leads to the conclusion that matter itself contains elements of awareness to it. However, Bedau concludes that adopting this view would provide a precise notion that emergence is involved in consciousness, and second, the notion of weak emergence is metaphysically benign .
Strong emergence describes the direct causal action of a high-level system upon its components; qualities produced this way are irreducible to the system's constituent parts . The whole is greater than the sum of its parts. It follows that no simulation of the system can exist, for such a simulation would itself constitute a reduction of the system to its constituent parts .
However, "the debate about whether or not the whole can be predicted from the properties of the parts misses the point. Wholes produce unique combined effects, but many of these effects may be co-determined by the context and the interactions between the whole and its environment(s)" . In accordance with his Synergism Hypothesis , Corning also stated, "It is the synergistic effects produced by wholes that are the very cause of the evolution of complexity in nature." Novelist Arthur Koestler used the metaphor of Janus (a symbol of the unity underlying complements like open/shut, peace/war) to illustrate how the two perspectives (strong vs. weak or holistic vs. reductionistic) should be treated as non-exclusive, and should work together to address the issues of emergence . Further,
The ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexity. At each level of complexity entirely new properties appear. Psychology is not applied biology, nor is biology applied chemistry. We can now see that the whole becomes not merely more, but very different from the sum of its parts. 
The plausibility of strong emergence is questioned by some as contravening our usual understanding of physics. Mark A. Bedau observes:
Although strong emergence is logically possible, it is uncomfortably like magic. How does an irreducible but supervenient downward causal power arise, since by definition it cannot be due to the aggregation of the micro-level potentialities? Such causal powers would be quite unlike anything within our scientific ken. This not only indicates how they will discomfort reasonable forms of materialism. Their mysteriousness will only heighten the traditional worry that emergence entails illegitimately getting something from nothing.
Meanwhile, others have worked towards developing analytical evidence of strong emergence. In 2009, Gu et al. presented a class of physical systems that exhibits non-computable macroscopic properties. More precisely, if one could compute certain macroscopic properties of these systems from the microscopic description of these systems, they one would be able to solve computational problems known to be undecidable in computer science. They concluded that
Although macroscopic concepts are essential for understanding our world, much of fundamental physics has been devoted to the search for a `theory of everything', a set of equations that perfectly describe the behavior of all fundamental particles. The view that this is the goal of science rests in part on the rationale that such a theory would allow us to derive the behavior of all macroscopic concepts, at least in principle. The evidence we have presented suggests that this view may be overly optimistic. A `theory of everything' is one of many components necessary for complete understanding of the universe, but is not necessarily the only one. The development of macroscopic laws from first principles may involve more than just systematic logic, and could require conjectures suggested by experiments, simulations or insight.
Emergent structures are patterns that emerge via collective actions of many individual entities. To explain such patterns, one might conclude, per Aristotle, that emergent structures are more than the sum of their parts on the assumption that the emergent order will not arise if the various parts simply interact independently of one another. However, there are those who disagree. According to this argument, the interaction of each part with its immediate surroundings causes a complex chain of processes that can lead to order in some form. In fact, some systems in nature are observed to exhibit emergence based upon the interactions of autonomous parts, and some others exhibit emergence that at least at present cannot be reduced in this way.
Objective or subjective quality.
The properties of complexity and organization of any system are considered by Crutchfield to be subjective qualities determined by the observer.
"Defining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analysed in terms of how model-building observers infer from measurements the computational capabilities embedded in non-linear processes. An observer’s notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtly, though, on how those resources are organized. The descriptive power of the observer’s chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data."
On the other hand, Peter Corning argues "Must the synergies be perceived/observed in order to qualify as emergent effects, as some theorists claim? Most emphatically not. The synergies associated with emergence are real and measurable, even if nobody is there to observe them." 
In religion, art and humanities.
In religion, emergence grounds expressions of religious naturalism in which a sense of the sacred is perceived in the workings of entirely naturalistic processes by which more complex forms arise or evolve from simpler forms. Examples are detailed in a 2006 essay titled 'The Sacred Emergence of Nature' by Ursula Goodenough and Terrence Deacon and a 2006 essay titled by Stuart Kauffman.
An early argument (1904-5) for the emergence of social formations, in part stemming from religion, can be found in Max Weber's most famous work, "The Protestant Ethic and the Spirit of Capitalism" 
In art, emergence is used to explore the origins of novelty, creativity, and authorship. Some art/literary theorists (Wheeler, 2006; Alexander, 2011 have proposed alternatives to postmodern understandings of "authorship" using the complexity sciences and emergence theory. They contend that artistic selfhood and meaning are emergent, relatively objective phenomena. Michael J. Pearce has used emergence to describe the experience of works of art in relation to contemporary neuroscience.) The concept of emergence has also been applied to the theory of literature and art, history, linguistics, cognitive sciences, etc. by the teachings of Jean-Marie Grassin at the (v. esp.: J. Fontanille, B. Westphal, J. Vion-Dury, éds. L'Émergence—Poétique de l'Émergence, en réponse aux travaux de Jean-Marie Grassin, Bern, Berlin, etc., 2011; and: the article " in the ". 
In international development, concepts of emergence have been used within a theory of social change termed SEED-SCALE to show how standard principles interact to bring forward socio-economic development fitted to cultural values, community economics, and natural environment (local solutions emerging from the larger socio-econo-biosphere). These principles can be implemented utilizing a sequence of standardized tasks that self-assemble in individually specific ways utilizing recursive evaluative criteria.
In postcolonial studies, the term "Emerging Literature" refers to a contemporary body of texts that is gaining momentum in the global literary landscape (v. esp.: J.M. Grassin, ed. "Emerging Literatures", Bern, Berlin, etc. : Peter Lang, 1996). By opposition, "emergent literature" is rather a concept used in the theory of literature.
Emergent properties and processes.
An emergent behavior or emergent property can appear when a number of simple entities (agents) operate in an environment, forming more complex behaviors as a collective. If emergence happens over disparate size scales, then the reason is usually a causal relation across different scales. In other words there is often a form of top-down feedback in systems with emergent properties. The processes from which emergent properties result may occur in either the observed or observing system, and can commonly be identified by their patterns of accumulating change, most generally called 'growth'. Emergent behaviours can occur because of intricate causal relations across different scales and feedback, known as interconnectivity. The emergent property itself may be either very predictable or unpredictable and unprecedented, and represent a new level of the system's evolution. The complex behaviour or properties are not a property of any single such entity, nor can they easily be predicted or deduced from behaviour in the lower-level entities, and might in fact be irreducible to such behavior. The shape and behaviour of a flock of birds or school of fish are good examples of emergent properties.
One reason why emergent behaviour is hard to predict is that the number of interactions between components of a system increases exponentially with the number of components, thus potentially allowing for many new and subtle types of behaviour to emerge. Emergence is often a product of particular patterns of interaction. Negative feedback introduces constraints that serve to fix structures or behaviours. In contrast, positive feedback promotes change, allowing local variations to grow into global patterns. Another way in which interactions leads to emergent properties is dual-phase evolution. This occurs where interactions are applied intermittently, leading to two phases: one in which patterns form or grow, the other in which they are refined or removed.
On the other hand, merely having a large number of interactions is not enough by itself to guarantee emergent behaviour; many of the interactions may be negligible or irrelevant, or may cancel each other out. In some cases, a large number of interactions can in fact work against the emergence of interesting behaviour, by creating a lot of "noise" to drown out any emerging "signal"; the emergent behaviour may need to be temporarily isolated from other interactions before it reaches enough critical mass to be self-supporting. Thus it is not just the sheer number of connections between components which encourages emergence; it is also how these connections are organised. A hierarchical organisation is one example that can generate emergent behaviour (a bureaucracy may behave in a way quite different from that of the individual humans in that bureaucracy); but perhaps more interestingly, emergent behaviour can also arise from more decentralized organisational structures, such as a marketplace. In some cases, the system has to reach a combined threshold of diversity, organisation, and connectivity before emergent behaviour appears.
Unintended consequences and side effects are closely related to emergent properties. Luc Steels writes: "A component has a particular functionality but this is not recognizable as a subfunction of the global functionality. Instead a component implements a behaviour whose side effect contributes to the global functionality [...] Each behaviour has a side effect and the sum of the side effects gives the desired functionality" . In other words, the global or macroscopic functionality of a system with "emergent functionality" is the sum of all "side effects", of all emergent properties and functionalities.
Systems with emergent properties or emergent structures may appear to defy entropic principles and the second law of thermodynamics, because they form and increase order despite the lack of command and central control. This is possible because open systems can extract information and order out of the environment.
Emergence helps to explain why the fallacy of division is a fallacy.
Emergent structures in nature.
Emergent structures can be found in many natural phenomena, from the physical to the biological domain. For example, the shape of weather phenomena such as hurricanes are emergent structures. The development and growth of complex, orderly crystals, as driven by the random motion of water molecules within a conducive natural environment, is another example of an emergent process, where randomness can give rise to complex and deeply attractive, orderly structures.
 However, crystalline structure and hurricanes are said to have a self-organizing phase.
It is useful to distinguish three forms of emergent structures. A "first-order" emergent structure occurs as a result of shape interactions (for example, hydrogen bonds in water molecules lead to surface tension). A "second-order" emergent structure involves shape interactions played out sequentially over time (for example, changing atmospheric conditions as a snowflake falls to the ground build upon and alter its form). Finally, a "third-order" emergent structure is a consequence of shape, time, and heritable instructions. For example, an organism's genetic code sets boundary conditions on the interaction of biological systems in space and time.
Non-living, physical systems.
In physics, emergence is used to describe a property, law, or phenomenon which occurs at macroscopic scales (in space or time) but not at microscopic scales, despite the fact that a macroscopic system can be viewed as a very large ensemble of microscopic systems.
An emergent property need not be more complicated than the underlying non-emergent properties which generate it. For instance, the laws of thermodynamics are remarkably simple, even if the laws which govern the interactions between component particles are complex. The term emergence in physics is thus used not to signify complexity, but rather to distinguish which laws and concepts apply to macroscopic scales, and which ones apply to microscopic scales.
Some examples include:
Temperature is sometimes used as an example of an emergent macroscopic behaviour. In classical dynamics, a "snapshot" of the instantaneous momenta of a large number of particles at equilibrium is sufficient to find the average kinetic energy per degree of freedom which is proportional to the temperature. For a small number of particles the instantaneous momenta at a given time are not statistically sufficient to determine the temperature of the system. However, using the ergodic hypothesis, the temperature can still be obtained to arbitrary precision by further averaging the momenta over a long enough time.
Convection in a liquid or gas is another example of emergent macroscopic behaviour that makes sense only when considering differentials of temperature. Convection cells, particularly Bénard cells, are an example of a self-organizing system (more specifically, a dissipative system) whose structure is determined both by the constraints of the system and by random perturbations: the possible realizations of the shape and size of the cells depends on the temperature gradient as well as the nature of the fluid and shape of the container, but which configurations are actually realized is due to random perturbations (thus these systems exhibit a form of symmetry breaking).
In some theories of particle physics, even such basic structures as mass, space, and time are viewed as emergent phenomena, arising from more fundamental concepts such as the Higgs boson or strings. In some interpretations of quantum mechanics, the perception of a deterministic reality, in which all objects have a definite position, momentum, and so forth, is actually an emergent phenomenon, with the true state of matter being described instead by a wavefunction which need not have a single position or momentum.
Most of the laws of physics themselves as we experience them today appear to have emerged during the course of time making emergence the most fundamental principle in the universe and raising the question of what might be the most fundamental law of physics from which all others emerged. Chemistry can in turn be viewed as an emergent property of the laws of physics. Biology (including biological evolution) can be viewed as an emergent property of the laws of chemistry. Similarly, psychology could be understood as an emergent property of neurobiological laws. Finally, free-market theories understand economy as an emergent feature of psychology.
In Laughlin's book, he explains that for many particle systems, nothing can be calculated exactly from the microscopic equations, and that macroscopic systems are characterised by broken symmetry: the symmetry present in the microscopic equations is not present in the macroscopic system, due to phase transitions. As a result, these macroscopic systems are described in their own terminology, and have properties that do not depend on many microscopic details. This does not mean that the microscopic interactions are irrelevant, but simply that you do not see them anymore — you only see a renormalized effect of them. Laughlin is a pragmatic theoretical physicist: if you cannot, possibly ever, calculate the broken symmetry macroscopic properties from the microscopic equations, then what is the point of talking about reducibility?
Living, biological systems.
Emergence and evolution.
Life is a major source of complexity, and evolution is the major process behind the varying forms of life. In this view, evolution is the process describing the growth of complexity in the natural world and in speaking of the emergence of complex living beings and life-forms, this view refers therefore to processes of sudden changes in evolution.
Life is thought to have emerged in the early RNA world when RNA chains began to express the basic conditions necessary for natural selection to operate as conceived by Darwin: heritability, variation of type, and competition for limited resources. Fitness of an RNA replicator (its per capita rate of increase) would likely be a function of adaptive capacities that were intrinsic (in the sense that they were determined by the nucleotide sequence) and the availability of resources. The three primary adaptive capacities may have been (1) the capacity to replicate with moderate fidelity (giving rise to both heritability and variation of type); (2) the capacity to avoid decay; and (3) the capacity to acquire and process resources. These capacities would have been determined initially by the folded configurations of the RNA replicators (see “Ribozyme”) that, in turn, would be encoded in their individual nucleotide sequences. Competitive success among different replicators would have depended on the relative values of these adaptive capacities.
Regarding causality in evolution Peter Corning observes:
"Synergistic effects of various kinds have played a major causal role in the evolutionary process generally and in the evolution of cooperation and complexity in particular... Natural selection is often portrayed as a “mechanism”, or is personified as a causal agency... In reality, the differential “selection” of a trait, or an adaptation, is a consequence of the functional effects it produces in relation to the survival and reproductive success of a given organism in a given environment. It is these functional effects that are ultimately responsible for the trans-generational continuities and changes in nature." 
Per his definition of emergence, Corning also addresses emergence and evolution:
"[In] evolutionary processes, causation is iterative; effects are also causes. And this is equally true of the synergistic effects produced by emergent systems. In other words, emergence itself... has been the underlying cause of the evolution of emergent phenomena in biological evolution; it is the synergies produced by organized systems that are the key." 
Swarming is a well-known behaviour in many animal species from marching locusts to schooling fish to flocking birds. Emergent structures are a common strategy found in many animal groups: colonies of ants, mounds built by termites, swarms of bees, shoals/schools of fish, flocks of birds, and herds/packs of mammals.
An example to consider in detail is an ant colony. The queen does not give direct orders and does not tell the ants what to do. Instead, each ant reacts to stimuli in the form of chemical scent from larvae, other ants, intruders, food and buildup of waste, and leaves behind a chemical trail, which, in turn, provides a stimulus to other ants. Here each ant is an autonomous unit that reacts depending only on its local environment and the genetically encoded rules for its variety of ant. Despite the lack of centralized decision making, ant colonies exhibit complex behavior and have even been able to demonstrate the ability to solve geometric problems. For example, colonies routinely find the maximum distance from all colony entrances to dispose of dead bodies.
Organization of life.
A broader example of emergent properties in biology is viewed in the biological organisation of life, ranging from the subatomic level to the entire biosphere. For example, individual atoms can be combined to form molecules such as polypeptide chains, which in turn fold and refold to form proteins, which in turn create even more complex structures. These proteins, assuming their functional status from their spatial conformation, interact together and with other molecules to achieve higher biological functions and eventually create an organism. Another example is how cascade phenotype reactions, as detailed in chaos theory, arise from individual genes mutating respective positioning. At the highest level, all the biological communities in the world form the biosphere, where its human participants form societies, and the complex interactions of meta-social systems such as the stock market.
In humanity.
Spontaneous order.
Groups of human beings, left free to each regulate themselves, tend to produce spontaneous order, rather than the meaningless chaos often feared. This has been observed in society at least since Chuang Tzu in ancient China. A classic traffic roundabout is a good example, with cars moving in and out with such effective organization that some modern cities have begun replacing stoplights at problem intersections with traffic circles , and getting better results. Open-source software and Wiki projects form an even more compelling illustration.
Emergent processes or behaviours can be seen in many other places, such as cities, cabal and market-dominant minority phenomena in economics, organizational phenomena in computer simulations and cellular automata. Whenever you have a multitude of individuals interacting with one another, there often comes a moment when disorder gives way to order and something new emerges: a pattern, a decision, a structure, or a change in direction (Miller 2010, 29).
Economics.
The stock market (or any market for that matter) is an example of emergence on a grand scale. As a whole it precisely regulates the relative security prices of companies across the world, yet it has no leader; when no central planning is in place, there is no one entity which controls the workings of the entire market. Agents, or investors, have knowledge of only a limited number of companies within their portfolio, and must follow the regulatory rules of the market and analyse the transactions individually or in large groupings. Trends and patterns emerge which are studied intensively by technical analysts.
World Wide Web and the Internet.
The World Wide Web is a popular example of a decentralized system exhibiting emergent properties. There is no central organization rationing the number of links, yet the number of links pointing to each page follows a power law in which a few pages are linked to many times and most pages are seldom linked to. A related property of the network of links in the World Wide Web is that almost any pair of pages can be connected to each other through a relatively short chain of links. Although relatively well known now, this property was initially unexpected in an unregulated network. It is shared with many other types of networks called small-world networks .
Internet traffic can also exhibit some seemingly emergent properties. In the congestion control mechanism, TCP flows can become globally synchronized at bottlenecks, simultaneously increasing and then decreasing throughput in coordination. Congestion, widely regarded as a nuisance, is possibly an emergent property of the spreading of bottlenecks across a network in high traffic flows which can be considered as a phase transition [see review of related research in ].
Another important example of emergence in web-based systems is social bookmarking (also called collaborative tagging). In social bookmarking systems, users assign tags to resources shared with other users, which gives rise to a type of information organisation that emerges from this crowdsourcing process. Recent research which analyzes empirically the complex dynamics of such systems has shown that consensus on stable distributions and a simple form of shared vocabularies does indeed emerge, even in the absence of a central controlled vocabulary. Some believe that this could be because users who contribute tags all use the same language, and they share similar semantic structures underlying the choice of words. The convergence in social tags may therefore be interpreted as the emergence of structures as people who have similar semantic interpretation collaboratively index online information, a process called semantic imitation.
Open-source software, or Wiki projects such as Wikipedia and Wikivoyage are other impressive examples of emergence. The "zeroeth law of Wikipedia" is often cited by its editors to highlight its apparently surprising and unpredictable quality: "The problem with Wikipedia is that it only works in practice. In theory, it can never work."
Architecture and cities.
Emergent structures appear at many different levels of organization or as spontaneous order. Emergent self-organization appears frequently in cities where no planning or zoning entity predetermines the layout of the city. The interdisciplinary study of emergent behaviors is not generally considered a homogeneous field, but divided across its application or problem domains.
Architects and Landscape Architects may not design all the pathways of a complex of buildings. Instead they might let usage patterns emerge and then place pavement where pathways have become worn in.
The on-course action and vehicle progression of the 2007 Urban Challenge could possibly be regarded as an example of cybernetic emergence. Patterns of road use, indeterministic obstacle clearance times, etc. will work together to form a complex emergent pattern that can not be deterministically planned in advance.
The architectural school of Christopher Alexander takes a deeper approach to emergence attempting to rewrite the process of urban growth itself in order to affect form, establishing a new methodology of planning and design tied to traditional practices, an . Urban emergence has also been linked to theories of urban complexity and urban evolution .
Building ecology is a conceptual framework for understanding architecture and the built environment as the interface between the dynamically interdependent elements of buildings, their occupants, and the larger environment. Rather than viewing buildings as inanimate or static objects, building ecologist Hal Levin views them as interfaces or intersecting domains of living and non-living systems. The microbial ecology of the indoor environment is strongly dependent on the building materials, occupants, contents, environmental context and the indoor and outdoor climate. The strong relationship between atmospheric chemistry and indoor air quality and the chemical reactions occurring indoors. The chemicals may be nutrients, neutral or biocides for the microbial organisms. The microbes produce chemicals that affect the building materials and occupant health and well being. Humans manipulate the ventilation, temperature and humidity to achieve comfort with the concomitant effects on the microbes that populate and evolve.
Eric Bonabeau's attempt to define emergent phenomena is through traffic: "traffic jams are actually very complicated and mysterious. On an individual level, each driver is trying to get somewhere and is following (or breaking) certain rules, some legal (the speed limit) and others societal or personal (slow down to let another driver change into your lane). But a traffic jam is a separate and distinct entity that emerges from those individual behaviors. Gridlock on a highway, for example, can travel backward for no apparent reason, even as the cars are moving forward." He has also likened emergent phenomena to the analysis of market trends and employee behavior.
Computational emergent phenomena have also been utilized in architectural design processes, for example for formal explorations and experiments in digital materiality.
Computer AI.
Some artificially intelligent computer applications utilize emergent behavior for animation. One example is Boids, which mimics the swarming behavior of birds.
Language.
It has been argued that the structure and regularity of language--grammar, or at least language change, is an emergence phenomenon . While each speaker merely tries to reach his or her own communicative goals, he or she uses language in a particular way. If enough speakers behave in that way, language is changed . In a wider sense, the norms of a language, i.e. the linguistic conventions of its speech society, can be seen as a system emerging from long-time participation in communicative problem-solving in various social circumstances. 
Emergent change processes.
Within the field of group facilitation and organization development, there have been a number of new group processes that are designed to maximize emergence and self-organization, by offering a minimal set of effective initial conditions. Examples of these processes include SEED-SCALE, Appreciative Inquiry, Future Search, the World Cafe or Knowledge Cafe, Open Space Technology, and others. (Holman, 2010)

</doc>
<doc id="37476" url="http://en.wikipedia.org/wiki?curid=37476" title="Civil liberties">
Civil liberties

Civil liberties are personal guarantees and freedoms that the government cannot abridge, either by law or by judicial interpretation. Though the scope of the term differs amongst various countries, some examples of civil liberties include the freedom from torture, freedom from forced disappearance, freedom of conscience, freedom of press, freedom of religion, freedom of expression, freedom of assembly, the right to security and liberty, freedom of speech, the right to privacy, the right to equal treatment under the law and due process, the right to a fair trial, and the right to life. Other civil liberties include the right to own property, the right to defend oneself, and the right to bodily integrity. Within the distinctions between civil liberties and other types of liberty, distinctions exist between positive liberty/positive rights and negative liberty/negative rights.
Overview.
Many contemporary states have a constitution, a bill of rights, or similar constitutional documents that enumerate and seek to guarantee civil liberties. Other states have enacted similar laws through a variety of legal means, including signing and ratifying or otherwise giving effect to key conventions such as the European Convention on Human Rights and the International Covenant on Civil and Political Rights. The existence of some claimed civil liberties is a matter of dispute, as are the extent of most civil rights. Controversial examples include property rights, reproductive rights, and civil marriage. Whether the existence of victimless crimes infringes upon civil liberties is a matter of dispute. Another matter of debate is the suspension or alteration of certain civil liberties in times of war or state of emergency, including whether and to what extent this should occur.
The formal concept of civil liberties dates back to the English legal charter Magna Carta, which in turn was based on pre-existing documents namely the Charter of Liberties. Sealed in 1215, Magna Carta is a landmark document in English legal history and constitutional history.
Asia.
China.
The Constitution of People's Republic of China (which applies only to mainland China, not to Hong Kong, Macau and Taiwan), especially its Fundamental Rights and Duties of Citizens, claims to protect many civil liberties.
India.
The Fundamental Rights — embodied in Part III of the constitution — guarantee liberties such that all Indians can lead their lives in peace as citizens of India. The six fundamental rights are right to equality, right to freedom, right against exploitation, right to freedom of religion, cultural and educational rights and right to constitutional remedies.
These include individual rights common to most liberal democracies, incorporated in the fundamental law of the land and are enforceable in a court of law. Violations of these rights result in punishments as prescribed in the Indian Penal Code, subject to discretion of the judiciary. These rights are neither absolute nor immune from constitutional amendments. They have been aimed at overturning the inequalities of pre-independence social practices. Specifically, they resulted in abolishment of un-touchability and prohibit discrimination on the grounds of religion, race, caste, sex, or place of birth. They forbid human trafficking and unfree labour. They protect cultural and educational rights of ethnic and religious minorities by allowing them to preserve their languages and administer their own educational institutions.
All people, irrespective of race, religion, caste or sex, have the right to approach the High Courts or the Supreme Court for the enforcement of their fundamental rights. It is not necessary that the aggrieved party has to be the one to do so. In public interest, anyone can initiate litigation in the court on their behalf. This is known as "Public interest litigation". High Court and Supreme Court judges can also act on their own on the basis of media reports.
The Fundamental Rights emphasize equality by guaranteeing to all citizens the access and use of public institutions and protections, irrespective of their background. The rights to life and personal liberty apply for persons of any nationality, while others, such as the freedom of speech and expression are applicable only to the citizens of India (including non-resident Indian citizens). The right to equality in matters of public employment cannot be conferred to overseas citizens of India.
Fundamental Rights primarily protect individuals from any arbitrary State actions, but some rights are enforceable against private individuals too. For instance, the constitution abolishes untouchability and prohibits "begar". These provisions act as a check both on State action and actions of private individuals. Fundamental Rights are not absolute and are subject to reasonable restrictions as necessary for the protection of national interest. In the "Kesavananda Bharati vs. state of Kerala" case, the Supreme Court ruled that all provisions of the constitution, including Fundamental Rights can be amended. However, the Parliament cannot alter the basic structure of the constitution like secularism, democracy, federalism, separation of powers. Often called the "Basic structure doctrine", this decision is widely regarded as an important part of Indian history. In the 1978 "Maneka Gandhi v. Union of India" case, the Supreme Court extended the doctrine's importance as superior to any parliamentary legislation.According to the verdict, no act of parliament can be considered a law if it violated the basic structure of the constitution. This landmark guarantee of Fundamental Rights was regarded as a unique example of judicial independence in preserving the sanctity of Fundamental Rights.
The Fundamental Rights can only be altered by a constitutional amendment, hence their inclusion is a check not only on the executive branch, but also on the Parliament and state legislatures. The imposition of a state of emergency may lead to a temporary suspension of the rights conferred by Article 19 (including freedoms of speech, assembly and movement, etc.) to preserve national security and public order. The President can, by order, suspend the constitutional written remedies as well.
Russia.
The Constitution of Russian Federation guarantees in theory many of the same rights and civil liberties as the U.S. except to bear arms, i.e.: freedom of speech, freedom of religion, freedom of association and assembly, freedom to choose language, to due process, to a fair trial, privacy, freedom to vote, right for education, etc. However, human rights groups like Amnesty International have warned that Vladimir Putin has seriously curtailed freedom of expression, freedom of assembly and freedom of association amidst growing authoritarianism.
Australia.
Although Australia does not have an enshrined Bill of Rights or similar binding legal document, civil liberties are assumed as protected through a series of rules and conventions. Australia was a key player and signatory to the UN Universal Declaration on Human Rights (1948)
The Constitution of Australia (1900) does offer very limited protection of rights:
Certain High Court interpretations of the Constitution have allowed for implied rights such as freedom of speech and the right to vote to be established, however others such as freedom of assembly and freedom of association are yet to be identified.
Refugee Issues
Within the past decade Australia has experienced increasing contention regarding its treatment of those seeking asylum. Although Australia is a signatory to the UN Refugee Convention (1951), successive governments have demonstrated an increasing tightening of borders; particularly against those who seek passage via small water vessels.
The Abbott Government (2013) like its predecessors (the Gillard and Howard Governments) has encountered particular difficulty curbing asylum seekers via sea, increasingly identified as "illegal immigration". The recent involvement of the Australian Navy in refugee rescue operations has many human rights groups such as Amnesty International concerned over the "militarisation" of treatment of refugees. The current "turn-back" policy is particularly divisive, as it involves placing refugees in government lifeboats and turning them towards Indonesia. Despite opposition however, the Abbott government's response has so far seen a reduction in the amount of potential refugees undertaking the hazardous cross to Australia, which is argued by the government as an indicator for its policy success.
Europe.
European Convention on Human Rights.
The European Convention on Human Rights, to which almost all European countries belong (apart from Belarus), enumerates a number of civil liberties and is of varying constitutional force in different European states.
Czech Republic.
Following the Velvet Revolution, a constitutional overhaul took place in Czechoslovakia. In 1991, the Charter of Fundamental Rights and Basic Freedoms was adopted, having the same legal standing as the Constitution. The Czech Republic has kept the Charter in its entirety following the dissolution of Czechoslovakia as Act No. 2/1993 Coll. (Constitution being No. 1).
France.
France's 1789 Declaration of the Rights of Man and of the Citizen listed many civil liberties and is of constitutional force.
Germany.
The German constitution, the "Grundgesetz" (lit. "Basic Law"), starts with an elaborate listing of civil liberties and states in sec. 1 "The dignity of man is inviolable. To respect and protect it shall be the duty of all public authority." Following the "Austrian System", the people have the right to appeal to the Federal Constitutional Court of Germany ("Bundesverfassungsgericht") if they feel their civil rights are being violated. This procedure has shaped German law considerably over the years.
United Kingdom.
Civil liberties in the United Kingdom date back to Magna Carta in 1215 and 17th century common law and statute law, such as the 1628 Petition of Right and the Bill of Rights 1689. Parts of these laws remain in statute today and are supplemented by other pieces of legislation and conventions that collectively form the uncodified Constitution of the United Kingdom. In addition, the United Kingdom is a signatory to the European Convention on Human Rights which covers both human rights and civil liberties. The Human Rights Act 1998 incorporates the great majority of Convention rights directly into UK law.
In June 2008 the then Shadow Home Secretary David Davis resigned his parliamentary seat over what he described as the "erosion of civil liberties" by the then Labour government, and was re-elected on a civil liberties platform (although he was not opposed by candidates of other major parties). This was in reference to anti-terrorism laws and in particular the extension to pre-trial detention, that is perceived by many to be an infringement of habeas corpus established in Magna Carta.
North America.
Canada.
The Constitution of Canada includes the Canadian Charter of Rights and Freedoms which guarantees many of the same rights as the U.S. constitution, with the notable exceptions of protection against establishment of religion. However, the Charter does protect freedom of religion. The Charter also omits any mention of, or protection for, property.
United States.
The United States Constitution, especially its Bill of Rights, protects civil liberties. The passage of the Fourteenth Amendment further protected civil liberties by introducing the Privileges or Immunities Clause, Due Process Clause, and Equal Protection Clause. Human rights within the United States are often called civil rights, which are those rights, privileges and immunities held by all people, in distinction to "political" rights, which are the rights that inhere to those who are entitled to participate in elections, as candidates or voters. Before universal suffrage, this distinction was important, since many people were ineligible to vote but still were considered to have the fundamental freedoms derived from the rights to life, liberty and the pursuit of happiness. This distinction is less important now that Americans enjoy near universal suffrage, and civil liberties are now taken to include the political rights to vote and participate in elections. Because Indian tribal governments retain sovereignty over tribal members, the U.S. Congress in 1968 enacted a law that essentially applies most of the protections of the Bill of Rights to tribal members, to be enforced mainly by tribal courts.
Civil liberties on the internet.
Civil liberties have assumed a new role and shape due to technological changes, e-surveillance and conflict of laws in cyberspace.<ref name=" http://perry4law.org/clic/?p=83"></ref> A new category of civil liberties has emerged that is known as civil liberties protection in cyberspace.<ref name=" http://ptlb.in/clpic/"></ref> Further, securing cyberspace while protecting privacy and civil liberties has also become a challenge for various countries.<ref name=" https://www.dhs.gov/blog/2013/04/02/securing-cyberspace-while-protecting-privacy-and-civil-liberties"></ref><ref name=" http://www.hoover.org/sites/default/files/uploads/documents/0817999825_183.pdf"></ref>
The United States Supreme Court has also held that generally the cell phone of an arrested person cannot be searched without a warrant.<ref name=http://www.electroniccourts.in/privacylawsindia/?p=103"></ref> The European Union Court of Justice has also held that individuals have a Right to be forgotten in cyberspace.<ref name=http://www.electroniccourts.in/privacylawsindia/?p=79"></ref> Even United Nations Third Committee has approved the text titled Right to Privacy in the Digital Age.<ref name=http://ptlb.in/clpic/?p=244"></ref>
Further reading.
</dl>

</doc>
<doc id="37483" url="http://en.wikipedia.org/wiki?curid=37483" title="Womyn">
Womyn

"Womyn" is one of several alternative spellings of the English word "women" used by some feminists. There are many alternative spellings, including "womban" and "womon" (singular), and "wimmin" (plural). Some writers who use alternative spellings may see them as an expression of female independence and a repudiation of traditions that define females by reference to a male norm.
Background.
In Old English sources, the word "man" was gender-neutral, with a meaning similar to the modern English usage of "one" as an indefinite pronoun. The words "wer" and "wyf" were used to specify a man or woman where necessary, respectively. Combining them into "wer-man" or "wyf-man" expressed the concept of "any man" or "any woman". Feminist writers have suggested that the less prejudicial usage of the Old English sources reflects more egalitarian notions of gender at the time. 
Reasoning.
Some feminists object to the fact that "woman" and "women" are just "man" and "men" with a "wo-" prepended.
Variants.
Womon/womyn.
"Womyn" appeared as an Older Scots spelling of "woman" in the Scots poetry of James Hogg. Its usage as a feminist spelling of "women" (with "womon" as the singular form) first appeared in print in 1976 referring to the first Michigan Womyn's Music Festival, an annual art festival that admits only womyn-born womyn.
Womon/wimmin.
"Wimmin" appeared in 19th century renderings of Black American English, without any feminist significance. Z. Budapest promoted the use of "wimmin" (singular "womon") in the 1970s as part of her Dianic Wicca movement, which claims that present-day patriarchy represents a fall from a matriarchal golden age.

</doc>
<doc id="37500" url="http://en.wikipedia.org/wiki?curid=37500" title="Rutger Hauer">
Rutger Hauer

Rutger Oelsen Hauer (]; born 23 January 1944) is a Dutch actor, writer, and environmentalist. He has acted in both Dutch and English-language TV series and films.
His career began in 1969 with the title role in the Dutch television series "Floris". His film credits include "Flesh+Blood", "Blind Fury", "Blade Runner", "The Hitcher", "Escape from Sobibor" (for which he won a Golden Globe Award for Best Supporting Actor), "Nighthawks", "Wedlock", "Sin City", "Confessions of a Dangerous Mind", "Ladyhawke", "Buffy the Vampire Slayer", "The Osterman Weekend", "The Blood of Heroes", "Batman Begins", "Hobo with a Shotgun", and "The Rite".
Hauer founded the Rutger Hauer Starfish Association, an AIDS awareness organization.
Early life.
Rutger Oelsen Hauer was born on 23 January 1944 in Breukelen in the Netherlands. He is the son of drama teachers Teunke (neé Mellema) and Arend Hauer. He has three sisters, one older and two younger.
Hauer grew up in Amsterdam. Since his parents were very occupied with their careers, he and his sisters were brought up mostly by nannies. He went to a Waldorf school.
At the age of 15, Hauer ran off to sea and spent a year scrubbing decks aboard a freighter. Returning home, he worked as an electrician and a joiner for three years while attending acting classes at night school. Hauer served in the Royal Netherlands Navy.
Career.
Hauer joined an experimental troupe, with which he remained for five years before Paul Verhoeven cast him in the lead role of the successful 1969 television series "Floris", a Dutch medieval action drama. The role made him famous in his native country, and Hauer reprised his role for the 1975 German remake "Floris von Rosemund". Hauer's career changed course when Verhoeven cast him in "Turkish Delight" (1973). The movie found box-office favour abroad as well as at home, and within two years, Hauer was invited to make his English-language debut in the British film "The Wilby Conspiracy" (1975). Set in South Africa, the film was an action-drama with a focus on apartheid. Hauer's supporting role, however, was barely noticed in Hollywood, and he returned to Dutch films for several years. During this period, he made "Katie Tippel" (1975) and worked again with Verhoeven on "Soldier of Orange" (1977), and "Spetters" (1980). These two films paired Hauer with fellow Dutch actor Jeroen Krabbé.
Hauer made his American debut in the Sylvester Stallone film "Nighthawks" (1981) as a psychopathic and cold blooded terrorist named Wulfgar. The following year, he appeared in arguably his most famous and acclaimed role as the eccentric and violent but sympathetic anti-hero Roy Batty in Ridley Scott's 1982 science fiction thriller "Blade Runner", in which he improvised the famous tears in rain monologue. Hauer went on to play the adventurer courting Theresa Russell in "Eureka" (1983), the investigative reporter opposite John Hurt in "The Osterman Weekend" (1983), the hardened Landsknecht mercenary Martin in "Flesh & Blood" (1985), and the knight paired with Michelle Pfeiffer in "Ladyhawke" (1985).
He continued to make an impression on audiences in "The Hitcher" (1986), in which he played a mysterious hitchhiker intent on murdering a lone motorist and anyone else in his way. At the height of Hauer's fame, he was set to be cast as RoboCop though the role went to Peter Weller. That same year, Hauer starred as Nick Randall in "" as the descendant of the character played by Steve McQueen in the television series of the same name. In "The Legend of the Holy Drinker" (1989), Hauer showed a more soulful side. Phillip Noyce also attempted to capitalize, with far less success, on Hauer's spiritual qualities in the martial arts action adventure "Blind Fury" (1989). Hauer returned to science fiction with "The Blood of Heroes" (1990), in which he played a former champion in a post-apocalyptic world.
By the 1990s, Hauer was well known for his humorous Guinness commercials as well as his screen roles, which had increasingly involved low-budget films such as "Split Second", "Omega Doom", and "New World Disorder". He also appeared in the Kylie Minogue music video "On a Night Like This". In the late 1980s and 1990s, as well as in 2000, Hauer acted in several British and American television productions, including "Inside the Third Reich", "Escape from Sobibor" (for which he received a Golden Globe Award for Best Supporting Actor), "Fatherland", "" as Amelia Earhart's navigator Noonan, "Hostile Waters", "Merlin", "The 10th Kingdom", "Smallville", "Alias", and "Salem's Lot". In 1999, Hauer was awarded the Dutch "Best Actor of the Century Rembrandt Award".
Hauer played an assassin in "Confessions of a Dangerous Mind" (2003), a villainous cardinal with influential power in "Sin City" (2005) and a devious corporate executive running Wayne Enterprises in "Batman Begins" (2005). He also hosted the British reality television documentary "Shock Treatment" in 2005. He starred in "" as Real Madrid coach Rudi Van Der Merwe. In 2007 he recorded the voice-overs for the UK advertising campaign for Lurpak butter. In 2009, his role in avant-garde filmmaker Cyrus Frisch's "Dazzle", received positive reviews. The film was praised in Dutch press as "the most relevant Dutch film of the year". The same year, Hauer starred in the title role of "Barbarossa", an Italian film directed by Renzo Martinelli. In April 2010, he was cast in the live action adaptation of the short and fictitious "Grindhouse" trailer "Hobo With a Shotgun" (2011). In March 2011, it was announced that Hauer would play vampire hunter Van Helsing in legendary horror director Dario Argento's "Dracula 3D". He also starred as "Niall Brigant" in season 6 of HBO's "True Blood".
Hauer is a knight in the Order of the Netherlands Lion since 2013.
In 2015 he was the voice in a Lurpak commercials.
Personal life.
Hauer is an environmentalist. He advocated for the release of Sea Shepherd Conservation Society leader, Paul Watson, who was convicted in 1994 for sinking a Norwegian whaling vessel. Hauer has also established an AIDS awareness organization called the Rutger Hauer Starfish Association. Hauer married his second wife, Ineke ten Cate, in 1985 (they had been together since 1968); and he has one child born in 1966, actress Aysha Hauer who made him a grandfather in 1987. In April 2007, he published his autobiography "All Those Moments: Stories of Heroes, Villains, Replicants, and Blade Runners" (co-written with Patrick Quinlan), where he discusses many of his movie roles. Proceeds of the book go to Hauer's Starfish Association.
Filmography.
The filmography of Rutger Hauer gives an overview of all his performances as an actor in films, television films, and television series from 1969 to date, and also in upcoming films.

</doc>
<doc id="37521" url="http://en.wikipedia.org/wiki?curid=37521" title="Hoysala architecture">
Hoysala architecture

Hoysala architecture (Kannada: ಹೊಯ್ಸಳ ವಾಸ್ತುಶಿಲ್ಪ) is the building style developed under the rule of the Hoysala Empire between the 11th and 14th centuries, in the region known today as Karnataka, a state of India. Hoysala influence was at its peak in the 13th century, when it dominated the Southern Deccan Plateau region. Large and small temples built during this era remain as examples of the Hoysala architectural style, including the Chennakesava Temple at Belur, the Hoysaleswara Temple at Halebidu, and the Kesava Temple at Somanathapura. Other examples of fine Hoysala craftsmanship are the temples at Belavadi, Amruthapura, Hosaholalu, Mosale, Arasikere, Basaralu, Kikkeri and Nuggehalli. Study of the Hoysala architectural style has revealed a negligible Indo-Aryan influence while the impact of Southern Indian style is more distinct.
The vigorous temple building activity of the Hoysala Empire was due to the social, cultural and political events of the period. The stylistic transformation of the "Karnata" temple building tradition reflected religious trends popularized by the Vaishnava and Virashaiva philosophers as well as the growing military prowess of the Hoysala kings who desired to surpass their Western Chalukya overlords in artistic achievement. Temples built prior to Hoysala independence in the mid-12th century reflect significant Western Chalukya influences, while later temples retain some features salient to Chalukyan art but have additional inventive decoration and ornamentation, features unique to Hoysala artisans. About one hundred temples have survived in present-day Karnataka state, mostly in the Malnad (hill) districts, the native home of the Hoysala kings.
As popular tourist destinations in Karnataka, Hoysala temples offer an opportunity for pilgrims and students of architecture to examine medieval Hindu architecture in the "Karnata Dravida" tradition. This tradition began in the 7th century under the patronage of the Chalukya dynasty of Badami, developed further under the Western Chalukyas of Basavakalyan in the 11th century and finally transformed into an independent style by the 12th century during the reign of the Hoysalas. Medieval Kannada language inscriptions displayed prominently at temple locations give details of the temples and offer information about the history of the Hoysala dynasty.
Temple deities.
Hinduism is a combination of secular and sacred beliefs, rituals, daily practices and traditions that has evolved over the course of over two thousand years and embodies complex symbolism combining the natural world with philosophy.
Hindu temples began as simple shrines housing a deity and by the time of the Hoysalas had evolved into well articulated edifices in which worshippers sought transcendence of the daily world. Hoysala temples were not limited to any specific organised tradition of Hinduism and encouraged pilgrims of different Hindu devotional movements. The Hoysalas usually dedicated their temples to Lord Shiva or to Lord Vishnu (two of the major Hindu gods), but they occasionally chose a different deity. Worshippers of Shiva are called Shaivas or Lingayats and worshippers of Vishnu are called Vaishnavas. While King Vishnuvardhana and his descendants were Vaishnava by faith, records show that the Hoysalas maintained religious harmony by building as many temples dedicated to Shiva as they did to Vishnu. Most of these temples have secular features with broad themes depicted in their sculptures. This can be seen in the famous Chennakesava Temple at Belur dedicated to Vishnu and in the Hoysaleswara temple at Halebidu dedicated to Shiva. The Kesava temple at Somanathapura is different in that its ornamentation is strictly Vaishnavan. Generally Vaishnava temples are dedicated to Keshava (or to Chennakeshava, meaning "Beautiful Vishnu") while a small number are dedicated to Lakshminarayana and Lakshminarasimha (Narayana and Narasimha both being Avatars, or physical manifestations, of Vishnu) with Lakshmi, consort of Vishnu, seated at his feet. Temples dedicated to Vishnu are always named after the deity.
The Shaiva temples have a Shiva linga, symbol of fertility and the universal symbol of Shiva, in the shrine. The names of Shiva temples can end with the suffix "eshwara" meaning "Lord of". The name "Hoysaleswara", for instance, means "Lord of Hoysala". The temple can also be named after the devotee who commissioned the construction of the temple, an example being the Bucesvara temple at Koravangala, named after the devotee Buci. The most striking sculptural decorations are the horizontal rows of exquisitely detailed, intricately carved images of gods, goddesses and their attendants on the outer temple wall panels.
The Doddagaddavalli Lakshmi Devi ("Goddess of Wealth") Temple is an exception as it is dedicated to neither Vishnu nor Shiva. The defeat of the Jain Western Ganga Dynasty (of present-day south Karnataka) by the Cholas in the early 11th century and the rising numbers of followers of Vaishnava Hinduism and Virashaivism in the 12th century was mirrored by a decreased interest in Jainism. However, two notable locations of Jain worship in the Hoysala territory were Shravanabelagola and Kambadahalli. The Hoysalas built Jain temples to satisfy the needs of its Jain population, a few of which have survived in Halebidu containing icons of Jain tirthankaras. They constructed stepped wells called "Pushkarni" or "Kalyani", the ornate tank at Hulikere being an example. The tank has twelve minor shrines containing Hindu deities.
The two main deities found in Hoysala temple sculpture are Lord Shiva and Lord Vishnu in their various forms and avatars (incarnations). Shiva is usually shown with four arms holding a trident and a small drum among other emblems that symbolise objects worshiped independently of the divine image with which they are associated. Any male icon portrayed in this way is Shiva although a female icon may sometimes be portrayed with these attributes as Shiva's consort, Parvati. Various depictions of Lord Shiva show him in action, such as slaying a demon or dancing on the head of an elephant. He is often accompanied by his consort Parvati or shown with Nandi the bull. He may be represented as Bhairava, another of Shiva's many manifestations.
A male figure depicted holding certain objects such as a conch (symbol of eternal, heavenly space) and a wheel (eternal time and destructive power) is Vishnu. If a female figure is depicted holding these objects, she is seen as his consort, Lakshmi. In all the depictions Vishnu is holding four objects: a conch, a wheel, a lotus and a mace. These can be held in any of the icon's hands, making possible twenty-four different forms of Vishnu, each with a unique name. Apart from these, Vishnu is depicted in any of his "ten avataras", which include Vishnu sitting on Anantha (the celestial snake and keeper of life energy), Vishnu with Lakshmi seated on his lap (Lakshminarayana), with the head of a lion disemboweling a demon on his lap (Lakshminarasimha), with head of a boar walking over a demon (Varaha), in the Krishna avatar (as Venugopala or the cow herder playing the Venu (flute), dancing on the head of the snake Kaliya, lifting a hill such as Govardhana), with his feet over head of a small figure ("Vamana"), with Lakshmi seated on Garuda, and the eagle (stealing the parijata tree).
Temple complex.
A Hindu temple is a place of contact between the gods or deities and man. The focus of a temple is the centre or sanctum sanctorum (garbhagriha) where the image of the deity resides, so temple architecture is designed to move the devotee from outside to the garbhagriha through ambulatory passageways for circumambulation and halls or chambers ("mantapas") that become increasingly sacred as the deity is approached.
Hoysala temples have distinct parts that are merged to form a unified organic whole, in contrast to the temples of Tamil country where different parts of a temple stand independently. Although superficially unique, Hoysala temples resemble each other structurally. They are characterised by a complex profusion of sculpture decorating all the temple parts chiseled of soft soapstone (chloritic schist), a good material for intricate carving, executed mostly by local craftsmen, and exhibit architectural features that distinguish them from other temple architectures of South India.
Most Hoysala temples have a plain covered entrance porch supported by lathe turned (circular or bell-shaped) pillars which were sometimes further carved with deep fluting and moulded with decorative motifs. The temples may be built upon a platform raised by about a metre called a "jagati". The "jagati", apart from giving a raised look to the temple, serves as a "pradakshinapatha" or "circumambulation path" for circumambulation around the temple, as the "garbagriha" (inner sanctum) provides no such feature. Such temples will have an additional set of steps leading to an open "mantapa" (open hall) with parapet walls. A good example of this style is the Kesava Temple at Somanathapura. The "jagati" which is in unity with the rest of the temple follows a star-shaped design and the walls of the temple follow a zig-zag pattern, a Hoysala innovation. A pair of small shrines, each with a deity and a miniature tower directly facing the entrance, could adorn either side of steps of the "jagati". This would be repeated for all entrances leading to the "jagati".
Devotees can first complete a ritual circumambulation on the "jagati" starting from the main entrance by walking in a clockwise direction (towards the left) before entering the "mantapa", following the sculptural clockwise-sequenced reliefs on the outer temple walls depicting a sequence of epic scenes from the Hindu epics. Temples that are not built on a "jagati" can have steps flanked by elephant balustrades (parapets) that lead to the "mantapa" from ground level. An example of a temple that does not exhibit the raised platform is the "Bucesvara" temple in Korvangla, Hassan District. In temples with two shrines ("dvikuta"), the "vimanas" (the shrines or cellae) may be placed either next to each other or on opposite sides. The Lakshmidevi temple at Doddagaddavalli has a minor shrine at each of the four corners of the walled temple complex in addition to five major shrines.
Architectural elements.
Mantapa.
The "mantapa" is the hall where groups of people gather during prayers. The entrance to the "mantapa" normally has a highly ornate overhead lintel called a "makaratorana" ("makara" is an imaginary beast and "torana" is an overhead decoration). The open "mantapa" which serves the purpose of an outer hall (outer "mantapa") is a regular feature in larger Hoysala temples leading to an inner small closed "mantapa" and the shrine(s). The open "mantapas" have seating areas made of stone with the "mantapa's" parapet wall acting as a back rest. The seats may follow the same staggered square shape of the parapet wall. The open "mantapa" is the largest part of the temple and is the place supporting larger congregations of people. The ceiling here is supported by numerous pillars that create many bays. The shape of the open "mantapa" is best described as staggered-square and is the style used in most Hoysala temples. Even the smallest open "mantapa" has 13 bays. The walls have parapets that have half pillars supporting the outer ends of the roof which allow plenty of light making all the sculptural details visible. The "mantapa" ceiling is generally ornate with sculptures, both mythological and floral. The ceiling consists of deep and domical surfaces and contains sculptural depictions of banana bud motifs and other such decorations. The Amruteswara temple in Chikmagalur district has forty-eight domes in the "mahamantapa" ("great open hall").
If the temple is small it will consist of only a closed "mantapa" (enclosed with walls extending all the way to the ceiling) and the shrine. The closed "mantapa", well decorated inside and out, is larger than the vestibule connecting the shrine and the "mantapa" and has four lathe-turned pillars to support the ceiling, which may be deeply domed. The four pillars divide the hall into nine bays. The nine bays result in nine finely decorated ceilings. Pierced stone latticework screens placed between pillars to filter the light is a characteristic Hoysala stylistic element.
A porch adorns the entrance to a closed "mantapa", consisting of an awning supported by two half-pillars (engaged columns) and two parapets, all richly decorated. The closed "mantapa" is connected to the shrine(s) by a vestibule, a square area that also connects the shrines. Its outer walls are finely decorated, but as the size the vestibule is not large, this may not be a conspicuous part of the temple. The vestibule also has a short tower called the "sukanasi" or "nose" upon which is mounted the Hoysala emblem. In Belur and Halebidu, these sculptures are quite large and are placed at all doorways.
The outer and inner "mantapa" (open and closed) have circular lathe-turned pillars having four brackets at the top. Over each bracket stands sculptured figure(s) called "salabhanjika" or "madanika". The pillars may also exhibit fine ornamental carvings on the surface and no two pillars are alike. This is how Hoysala art differs from the work of their early overlords, the Western Chalukyas, who added sculptural details to the circular pillar base and left the top plain. The lathe-turned pillars are 16, 32, or 64-pointed; some are bell-shaped and have properties that reflect light. The Parsvanatha Basadi at Halebidu is a good example. The shaft of the pillar is a monolith with the base left as a square and with well-sculpted figures adorning the top.
Vimana.
The "vimana", also called the cella, contains the most sacred shrine wherein resides the image of the presiding deity. The "vimana" is often topped by a tower which is quite different on the outside than on the inside. Inside, the vimana is plain and square, whereas outside it is profusely decorated and can be either stellate ("star-shaped") or shaped as a staggered square, or feature a combination of these designs, giving it many projections and recesses that seem to multiply as the light falls on it. Each projection and recess has a complete decorative articulation that is rhythmic and repetitive and composed of blocks and mouldings, obscuring the tower profile. Depending on the number of shrines (and hence on the number of towers), the temples are classified as "ekakuta" (one), "dvikuta" (two), "trikuta" (three), "chatushkuta" (four) and "panchakuta" (five). Most Hoysala temples are "ekakuta", "dvikuta" or "trikuta". In temples with multiple shrines, all essential parts are duplicated for symmetry and balance. A temple's minor shrine usually has its own tower. There are cases where a temple is "trikuta" but has only one tower over the main shrine (in the middle). So the terminology "trikuta" may not be literally accurate. Smaller shrines attached to the outer walls and facing outward from a larger "vimana" are a common feature.
The highest point of the temple ("kalasa") has the shape of a beautiful water pot and stands on top of the tower. This portion of the "vimana" is often lost due to age and has been replaced with a metallic pinnacle. Below the "kalasa" is a large, highly- sculptured structure resembling a dome which is made from large stones and looks like a helmet. It may be 2 m by 2 m in size and follows the shape of the shrine. Below this structure are domed roofs in a square plan, all of them much smaller and crowned with small "kalasas". They are mixed with other small roofs of different shapes and are ornately decorated. The tower of the shrine usually has three or four tiers of rows of decorative roofs while the tower on top of the "sukanasi" has one less tier, making the tower look like an extension of the main tower (the "nose"). One decorated roof tier runs on top of the wall of a closed "mantapa" above the heavy eaves of an open "mantapa" and above the porches.
Below the superstructure of the "vimana" are temple "eaves" projecting half a meter from the wall. Below the eaves two different decorative schemes may be found, depending on whether a temple was built in the early or the later period of the empire. In the early temples built prior to the 13th century, there is one eave and below this are decorative miniature towers. A panel of Hindu deities and their attendants are below these towers, followed by a set of five different mouldings forming the base of the wall. In the later temples there is a second eave running about a metre below the upper eaves with decorative miniature towers placed between them. The wall images of gods are below the lower eaves, followed by six different mouldings of equal size. This is broadly termed "horizontal treatment". The six mouldings at the base are divided in two sections. Going from the very base of the wall, the first horizontal layer contains a procession of elephants, above which are horsemen and then a band of foliage. The second horizontal section has depictions of the Hindu epics and "Puranic" scenes executed with detail. Above this are two friezes of "yalli"s or "makara"s (imaginary beasts) and "hamsas" (swans). The "vimana" (tower) is divided into three horizontal sections and is even more ornate than the walls.
Sculpture.
Hoysala artists are famous for their sculptural detail, be it in the depiction of the Hindu epics, "yallis", deities, "Kirtimukha" (gargoyles), eroticism or aspects of daily life. Their medium, the soft chlorite schist, enabled a virtuoso carving style. Their workmanship shows an attention paid to precise detail. Every aspect down to a fingernail or toenail is perfected.
"Salabhanjika", a common form of Hoysala sculpture, is an old Indian tradition going back to Buddhist sculpture. "Sala" is the sala tree and "bhanjika" is the chaste maiden. In the Hoysala idiom, "madanika" figures are decorative objects put at an angle on the outer walls of the temple near the roof so that worshipers circumambulating the temple can view them. They served the function of bracket figures to pillars inside the "mantapa". These "madanika" were sculpted as seemingly engaged in artistic activities such as music (holding musical instruments) and dance. "Kirtimukhas" (demon faces) adorn the towers of "vimana"s in some temples. Sometimes the artists left behind their signatures on the sculptures they created.
The "sthamba buttalikas" are pillar images that show traces of Chola art in the Chalukyan touches. Some of the artists working for the Hoysalas may have been from Chola country, a result of the expansion of the empire into Tamil-speaking regions of Southern India. The image of "mohini" on one of the pillars in the "mantapa" (closed hall) of the Chennakeshava temple is a fine example of Chola art.
General life themes are portrayed on wall panels such as the way horses were reined, the type of stirrup used, the depiction of dancers, musicians, instrumentalists, and rows of animals such as lions and elephants (where no two animals are identical). Perhaps no other temple in the country depicts the Ramayana and Mahabharata epics more effectively than the Hoysaleshwara temple at Halebidu.
Erotica was a subject the Hoysala artist handled with discretion. There is no exhibitionism in this, and erotic themes were carved into recesses and niches, generally miniature in form, making them inconspicuous. These erotic representations are associated with the "Shakta" practice. The temple doorway is heavily engraved with ornamentation called "Makaratorana" ("makara" being an imaginary beast) and each side of the doorway exhibits sculptured "Salabhanjika" (maidens).
Apart from these sculptures, entire sequences from the Hindu epics (commonly the Ramayana and the Mahabharata) have been sculpted in a clockwise direction starting at the main entrance. The right to left sequence is the same direction taken by the devotees in their ritual circumambulation as they wind inward toward the inner sanctum. Depictions from mythology such as the epic hero Arjuna shooting fish, the elephant-headed god Ganesha, the Sun god Surya, the weather and war god Indra, and Brahma with Sarasvati are common. Also frequently seen in these temples is Durga, with several arms holding weapons given to her by other gods, in the act of killing a buffalo (a demon in a buffalo's form) and Harihara (a fusion of Shiva and Vishnu) holding a conch, wheel and trident. Many of these friezes were signed by the artisans, the first known instance of signed artwork in India.
Research.
Surveys in modern times have indicated that 1000–1500 structures were built by the Hoysalas, of which about a hundred temples have survived to date. The Hoysala style is an offshoot of the Western Chalukya style, which was popular in the 10th and 11th centuries. It is distinctively Dravidian, and owing to its unique features, Hoysala architecture qualifies as an independent style. While the Hoysalas introduced innovative features into their architecture, they also borrowed features from the earlier great builders of "Karnata" like the Kadambas, Western Chalukyas. These features were the use of chloritic schist or soapstone as a basic building material, pierced stone window screens which were very popular in Hoysala temples, and the "vimana" which follows a stellate pattern. All these features were popular with their early overlords, the Western Chalukyas. Other features were the stepped style of "vimana" tower called the "Kadamba shikhara", which was inherited from the Kadambas. Engrained in the craftsmanship of Hoysala sculptors was their knowledge of the effect of light and shade on carved walls, which they used to maximum effect in their sculptures in the numerous projections and recesses. The Hoysala sculpture in all its richness is said to be a challenge to photography. The artistry of the Hoysalas in stone has been compared to the finesse of an ivory worker or a goldsmith. The abundance of jewelry worn by the sculpted figures and the variety of hairstyles and headdresses depicted give a fair idea of the lifestyles of the Hoysala times.
Notable craftsmen.
While the Hoysalas had the services of great architects and sculptors, some names stand out in their history. While medieval Indian artisans preferred to remain anonymous, Hoysala artisans signed their works, which has given researchers details about their lives, families, guilds, etc. Apart from the architects and sculptors, people of other guilds such as goldsmiths, ivory carvers, carpenters, and silversmiths also contributed to the completion of temples. The artisans were from diverse geographical backgrounds and included famous locals. Prolific architects included Amarashilpi Jakanachari, a native of Kaidala in Tumkur district, who also built temples for the Western Chalukyas. Ruvari Malithamma built the Kesava Temple at Somanathapura and worked on forty other monuments, including the Amruteshwara temple at Amruthapura. Malithamma specialised in ornamentation, and his works span six decades. His sculptures were typically signed in shorthand as "Malli" or simply "Ma". Dasoja and his son Chavana from Balligavi were the architects of Chennakesava Temple at Belur; Kedaroja was the chief architect of the Hoysaleswara Temple at Halebidu. Their influence is seen in other temples built by the Hoysalas as well. Names of other locals found in inscriptions are Maridamma, Baicoja, Caudaya, Nanjaya and Bama, Malloja, Nadoja, Siddoja, Masanithamma, Chameya and Rameya. Artists from Tamil country included Pallavachari and Cholavachari.
Notable temples.
Selected famous temples built by the Hoysalas are shown in the table.

</doc>
<doc id="37530" url="http://en.wikipedia.org/wiki?curid=37530" title="Tornado">
Tornado

A tornado is a violently rotating column of air that is in contact with both the surface of the earth and a cumulonimbus cloud or, in rare cases, the base of a cumulus cloud. They are often referred to as twisters or cyclones, although the word cyclone is used in meteorology, in a wider sense, to name any closed low pressure circulation. Tornadoes come in many shapes and sizes, but they are typically in the form of a visible condensation funnel, whose narrow end touches the earth and is often encircled by a cloud of debris and dust. Most tornadoes have wind speeds less than 110 mph, are about 250 ft across, and travel a few miles (several kilometers) before dissipating. The most extreme tornadoes can attain wind speeds of more than 300 mph, stretch more than 2 mi across, and stay on the ground for dozens of miles (more than 100 km).
Various types of tornadoes include the landspout, multiple vortex tornado, and waterspout. Waterspouts are characterized by a spiraling funnel-shaped wind current, connecting to a large cumulus or cumulonimbus cloud. They are generally classified as non-supercellular tornadoes that develop over bodies of water, but there is disagreement over whether to classify them as true tornadoes. These spiraling columns of air frequently develop in tropical areas close to the equator, and are less common at high latitudes. Other tornado-like phenomena that exist in nature include the gustnado, dust devil, fire whirls, and steam devil; downbursts are frequently confused with tornadoes, though their action is dissimilar.
Tornadoes have been observed on every continent except Antarctica. However, the vast majority of tornadoes occur in the Tornado Alley region of the United States, although they can occur nearly anywhere in North America. They also occasionally occur in south-central and eastern Asia, northern and east-central South America, Southern Africa, northwestern and southeast Europe, western and southeastern Australia, and New Zealand. Tornadoes can be detected before or as they occur through the use of Pulse-Doppler radar by recognizing patterns in velocity and reflectivity data, such as hook echoes or debris balls, as well as through the efforts of storm spotters.
There are several scales for rating the strength of tornadoes. The Fujita scale rates tornadoes by damage caused and has been replaced in some countries by the updated Enhanced Fujita Scale. An F0 or EF0 tornado, the weakest category, damages trees, but not substantial structures. An F5 or EF5 tornado, the strongest category, rips buildings off their foundations and can deform large skyscrapers. The similar TORRO scale ranges from a T0 for extremely weak tornadoes to T11 for the most powerful known tornadoes. Doppler radar data, photogrammetry, and ground swirl patterns (cycloidal marks) may also be analyzed to determine intensity and assign a rating.
Etymology.
The word "tornado" is an altered form of the Spanish word "tronada", which means "thunderstorm". This in turn was taken from the Latin "tonare", meaning "to thunder". It most likely reached its present form through a combination of the Spanish "tronada" and "tornar" ("to turn"); however, this may be a folk etymology. A tornado is also commonly referred to as a "twister", and is also sometimes referred to by the old-fashioned colloquial term "cyclone". The term "cyclone" is used as a synonym for "tornado" in the often-aired 1939 film "The Wizard of Oz". The term "twister" is also used in that film, along with being the title of the 1996 tornado-related film "Twister".
Definitions.
A tornado is "a violently rotating column of air, in contact with the ground, either pendant from a cumuliform cloud or underneath a cumuliform cloud, and often (but not always) visible as a funnel cloud". For a vortex to be classified as a tornado, it must be in contact with both the ground and the cloud base. Scientists have not yet created a complete definition of the word; for example, there is disagreement as to whether separate touchdowns of the same funnel constitute separate tornadoes. "Tornado" refers to the vortex of wind, not the condensation cloud.
Funnel cloud.
A tornado is not necessarily visible; however, the intense low pressure caused by the high wind speeds (as described by Bernoulli's principle) and rapid rotation (due to cyclostrophic balance) usually causes water vapor in the air to condense into cloud droplets due to adiabatic cooling. This results in the formation of a visible funnel cloud or condensation funnel.
There is some disagreement over the definition of funnel cloud and condensation funnel. According to the "Glossary of Meteorology", a funnel cloud is any rotating cloud pendant from a cumulus or cumulonimbus, and thus most tornadoes are included under this definition. Among many meteorologists, the funnel cloud term is strictly defined as a rotating cloud which is not associated with strong winds at the surface, and condensation funnel is a broad term for any rotating cloud below a cumuliform cloud.
Tornadoes often begin as funnel clouds with no associated strong winds at the surface, and not all funnel clouds evolve into tornadoes. Most tornadoes produce strong winds at the surface while the visible funnel is still above the ground, so it is difficult to discern the difference between a funnel cloud and a tornado from a distance.
Outbreaks and families.
Occasionally, a single storm will produce more than one tornado, either simultaneously or in succession. Multiple tornadoes produced by the same storm cell are referred to as a "tornado family". Several tornadoes are sometimes spawned from the same large-scale storm system. If there is no break in activity, this is considered a tornado outbreak (although the term "tornado outbreak" has various definitions). A period of several successive days with tornado outbreaks in the same general area (spawned by multiple weather systems) is a tornado outbreak sequence, occasionally called an extended tornado outbreak.
Characteristics.
Size and shape.
Most tornadoes take on the appearance of a narrow funnel, a few hundred yards (meters) across, with a small cloud of debris near the ground. Tornadoes may be obscured completely by rain or dust. These tornadoes are especially dangerous, as even experienced meteorologists might not see them. Tornadoes can appear in many shapes and sizes.
Small, relatively weak landspouts may be visible only as a small swirl of dust on the ground. Although the condensation funnel may not extend all the way to the ground, if associated surface winds are greater than 40 mph (64 km/h), the circulation is considered a tornado. A tornado with a nearly cylindrical profile and relative low height is sometimes referred to as a "stovepipe" tornado. Large single-vortex tornadoes can look like large wedges stuck into the ground, and so are known as "wedge tornadoes" or "wedges". The "stovepipe" classification is also used for this type of tornado, if it otherwise fits that profile. A wedge can be so wide that it appears to be a block of dark clouds, wider than the distance from the cloud base to the ground. Even experienced storm observers may not be able to tell the difference between a low-hanging cloud and a wedge tornado from a distance. Many, but not all major tornadoes are wedges.
Tornadoes in the dissipating stage can resemble narrow tubes or ropes, and often curl or twist into complex shapes. These tornadoes are said to be "roping out", or becoming a "rope tornado". When they rope out, the length of their funnel increases, which forces the winds within the funnel to weaken due to conservation of angular momentum. Multiple-vortex tornadoes can appear as a family of swirls circling a common center, or they may be completely obscured by condensation, dust, and debris, appearing to be a single funnel.
In the United States, tornadoes are around 500 feet (150 m) across on average and travel on the ground for 5 mi. However, there is a wide range of tornado sizes. Weak tornadoes, or strong yet dissipating tornadoes, can be exceedingly narrow, sometimes only a few feet or couple meters across. One tornado was reported to have a damage path only 7 feet (2 m) long. On the other end of the spectrum, wedge tornadoes can have a damage path a mile (1.6 km) wide or more. A tornado that affected Hallam, Nebraska on May 22, 2004, was up to 2.5 mi wide at the ground, and a tornado in El Reno, Oklahoma on May 31, 2013 was approximately 2.6 mi wide, the widest on record.
In terms of path length, the Tri-State Tornado, which affected parts of Missouri, Illinois, and Indiana on March 18, 1925, was on the ground continuously for 219 mi. Many tornadoes which appear to have path lengths of 100 mi or longer are composed of a family of tornadoes which have formed in quick succession; however, there is no substantial evidence that this occurred in the case of the Tri-State Tornado. In fact, modern reanalysis of the path suggests that the tornado may have begun 15 mi further west than previously thought.
Appearance.
Tornadoes can have a wide range of colors, depending on the environment in which they form. Those that form in dry environments can be nearly invisible, marked only by swirling debris at the base of the funnel. Condensation funnels that pick up little or no debris can be gray to white. While traveling over a body of water (as a waterspout), tornadoes can turn very white or even blue. Slow-moving funnels, which ingest a considerable amount of debris and dirt, are usually darker, taking on the color of debris. Tornadoes in the Great Plains can turn red because of the reddish tint of the soil, and tornadoes in mountainous areas can travel over snow-covered ground, turning white.
Lighting conditions are a major factor in the appearance of a tornado. A tornado which is "back-lit" (viewed with the sun behind it) appears very dark. The same tornado, viewed with the sun at the observer's back, may appear gray or brilliant white. Tornadoes which occur near the time of sunset can be many different colors, appearing in hues of yellow, orange, and pink.
Dust kicked up by the winds of the parent thunderstorm, heavy rain and hail, and the darkness of night are all factors which can reduce the visibility of tornadoes. Tornadoes occurring in these conditions are especially dangerous, since only weather radar observations, or possibly the sound of an approaching tornado, serve as any warning to those in the storm's path. Most significant tornadoes form under the storm's "updraft base", which is rain-free, making them visible. Also, most tornadoes occur in the late afternoon, when the bright sun can penetrate even the thickest clouds. Night-time tornadoes are often illuminated by frequent lightning.
There is mounting evidence, including Doppler On Wheels mobile radar images and eyewitness accounts, that most tornadoes have a clear, calm center with extremely low pressure, akin to the eye of tropical cyclones. Lightning is said to be the source of illumination for those who claim to have seen the interior of a tornado.
Rotation.
Tornadoes normally rotate cyclonically (when viewed from above, this is counterclockwise in the northern hemisphere and clockwise in the southern). While large-scale storms always rotate cyclonically due to the Coriolis effect, thunderstorms and tornadoes are so small that the direct influence of the Coriolis effect is unimportant, as indicated by their large Rossby numbers. Supercells and tornadoes rotate cyclonically in numerical simulations even when the Coriolis effect is neglected. Low-level mesocyclones and tornadoes owe their rotation to complex processes within the supercell and ambient environment.
Approximately 1 percent of tornadoes rotate in an anticyclonic direction in the northern hemisphere. Typically, systems as weak as landspouts and gustnadoes can rotate anticyclonically, and usually only those which form on the anticyclonic shear side of the descending rear flank downdraft (RFD) in a cyclonic supercell. On rare occasions, anticyclonic tornadoes form in association with the mesoanticyclone of an anticyclonic supercell, in the same manner as the typical cyclonic tornado, or as a companion tornado either as a satellite tornado or associated with anticyclonic eddies within a supercell.
Sound and seismology.
Tornadoes emit widely on the acoustics spectrum and the sounds are caused by multiple mechanisms. Various sounds of tornadoes have been reported, mostly related to familiar sounds for the witness and generally some variation of a whooshing roar. Popularly reported sounds include a freight train, rushing rapids or waterfall, a nearby jet engine, or combinations of these. Many tornadoes are not audible from much distance; the nature and propagation distance of the audible sound depends on atmospheric conditions and topography.
The winds of the tornado vortex and of constituent turbulent eddies, as well as airflow interaction with the surface and debris, contribute to the sounds. Funnel clouds also produce sounds. Funnel clouds and small tornadoes are reported as whistling, whining, humming, or the buzzing of innumerable bees or electricity, or more or less harmonic, whereas many tornadoes are reported as a continuous, deep rumbling, or an irregular sound of "noise".
Since many tornadoes are audible only when very near, sound is not reliable warning of a tornado. Tornadoes are also not the only source of such sounds in severe thunderstorms; any strong, damaging wind, a severe hail volley, or continuous thunder in a thunderstorm may produce a roaring sound.
Tornadoes also produce identifiable inaudible infrasonic signatures.
Unlike audible signatures, tornadic signatures have been isolated; due to the long distance propagation of low-frequency sound, efforts are ongoing to develop tornado prediction and detection devices with additional value in understanding tornado morphology, dynamics, and creation. Tornadoes also produce a detectable seismic signature, and research continues on isolating it and understanding the process.
Electromagnetic, lightning, and other effects.
Tornadoes emit on the electromagnetic spectrum, with sferics and E-field effects detected. There are observed correlations between tornadoes and patterns of lightning. Tornadic storms do not contain more lightning than other storms and some tornadic cells never produce lightning. More often than not, overall cloud-to-ground (CG) lightning activity decreases as a tornado reaches the surface and returns to the baseline level when the tornado lifts. In many cases, intense tornadoes and thunderstorms exhibit an increased and anomalous dominance of positive polarity CG discharges. Electromagnetics and lightning have little or nothing to do directly with what drives tornadoes (tornadoes are basically a thermodynamic phenomenon), although there are likely connections with the storm and environment affecting both phenomena.
Luminosity has been reported in the past and is probably due to misidentification of external light sources such as lightning, city lights, and power flashes from broken lines, as internal sources are now uncommonly reported and are not known to ever have been recorded. In addition to winds, tornadoes also exhibit changes in atmospheric variables such as temperature, moisture, and pressure. For example, on June 24, 2003 near Manchester, South Dakota, a probe measured a 100 mbar (hPa) (2.95 inHg) pressure decrease. The pressure dropped gradually as the vortex approached then dropped extremely rapidly to 850 mbar (hPa) (25.10 inHg) in the core of the violent tornado before rising rapidly as the vortex moved away, resulting in a V-shape pressure trace. Temperature tends to decrease and moisture content to increase in the immediate vicinity of a tornado.
Life cycle.
Supercell relationship.
Tornadoes often develop from a class of thunderstorms known as supercells. Supercells contain mesocyclones, an area of organized rotation a few miles up in the atmosphere, usually 1–6 miles (2–10 km) across. Most intense tornadoes (EF3 to EF5 on the Enhanced Fujita Scale) develop from supercells. In addition to tornadoes, very heavy rain, frequent lightning, strong wind gusts, and hail are common in such storms.
Most tornadoes from supercells follow a recognizable life cycle. That begins when increasing rainfall drags with it an area of quickly descending air known as the rear flank downdraft (RFD). This downdraft accelerates as it approaches the ground, and drags the supercell's rotating mesocyclone towards the ground with it.
Formation.
As the mesocyclone lowers below the cloud base, it begins to take in cool, moist air from the downdraft region of the storm. This convergence of warm air in the updraft, and this cool air, causes a rotating wall cloud to form. The RFD also focuses the mesocyclone's base, causing it to siphon air from a smaller and smaller area on the ground. As the updraft intensifies, it creates an area of low pressure at the surface. This pulls the focused mesocyclone down, in the form of a visible condensation funnel. As the funnel descends, the RFD also reaches the ground, creating a gust front that can cause severe damage a good distance from the tornado. Usually, the funnel cloud begins causing damage on the ground (becoming a tornado) within a few minutes of the RFD reaching the ground.
Maturity.
Initially, the tornado has a good source of warm, moist inflow to power it, so it grows until it reaches the "mature stage". This can last anywhere from a few minutes to more than an hour, and during that time a tornado often causes the most damage, and in rare cases can be more than one mile (1.6 km) across. Meanwhile, the RFD, now an area of cool surface winds, begins to wrap around the tornado, cutting off the inflow of warm air which feeds the tornado.
Dissipation.
As the RFD completely wraps around and chokes off the tornado's air supply, the vortex begins to weaken, and become thin and rope-like. This is the "dissipating stage", often lasting no more than a few minutes, after which the tornado fizzles. During this stage the shape of the tornado becomes highly influenced by the winds of the parent storm, and can be blown into fantastic patterns. Even though the tornado is dissipating, it is still capable of causing damage. The storm is contracting into a rope-like tube and, like the ice skater who pulls her arms in to spin faster, winds can increase at this point.
As the tornado enters the dissipating stage, its associated mesocyclone often weakens as well, as the rear flank downdraft cuts off the inflow powering it. Sometimes, in intense supercells, tornadoes can develop cyclically. As the first mesocyclone and associated tornado dissipate, the storm's inflow may be concentrated into a new area closer to the center of the storm. If a new mesocyclone develops, the cycle may start again, producing one or more new tornadoes. Occasionally, the old (occluded) mesocyclone and the new mesocyclone produce a tornado at the same time.
Although this is a widely accepted theory for how most tornadoes form, live, and die, it does not explain the formation of smaller tornadoes, such as landspouts, long-lived tornadoes, or tornadoes with multiple vortices. These each have different mechanisms which influence their development—however, most tornadoes follow a pattern similar to this one.
Types.
Multiple vortex.
A "multiple-vortex tornado" is a type of tornado in which two or more columns of spinning air rotate around a common center. A multi-vortex structure can occur in almost any circulation, but is very often observed in intense tornadoes. These vortices often create small areas of heavier damage along the main tornado path. This is a distinct phenomenon from a satellite tornado, which is a smaller tornado which forms very near a large, strong tornado contained within the same mesocyclone. The satellite tornado may appear to "orbit" the larger tornado (hence the name), giving the appearance of one, large multi-vortex tornado. However, a satellite tornado is a distinct circulation, and is much smaller than the main funnel.
Waterspout.
A "waterspout" is defined by the National Weather Service as a tornado over water. However, researchers typically distinguish "fair weather" waterspouts from tornadic waterspouts. Fair weather waterspouts are less severe but far more common, and are similar to dust devils and landspouts. They form at the bases of cumulus congestus clouds over tropical and subtropical waters. They have relatively weak winds, smooth laminar walls, and typically travel very slowly. They occur most commonly in the Florida Keys and in the northern Adriatic Sea. In contrast, tornadic waterspouts are stronger tornadoes over water. They form over water similarly to mesocyclonic tornadoes, or are stronger tornadoes which cross over water. Since they form from severe thunderstorms and can be far more intense, faster, and longer-lived than fair weather waterspouts, they are more dangerous. In official tornado statistics, waterspouts are generally not counted unless they affect land, though some European weather agencies count waterspouts and tornadoes together.
Landspout.
A "landspout", or "dust-tube tornado", is a tornado not associated with a mesocyclone. The name stems from their characterization as a "fair weather waterspout on land". Waterspouts and landspouts share many defining characteristics, including relative weakness, short lifespan, and a small, smooth condensation funnel which often does not reach the surface. Landspouts also create a distinctively laminar cloud of dust when they make contact with the ground, due to their differing mechanics from true mesoform tornadoes. Though usually weaker than classic tornadoes, they can produce strong winds which could cause serious damage.
Similar circulations.
Gustnado.
A "gustnado", or "gust front tornado", is a small, vertical swirl associated with a gust front or downburst. Because they are not connected with a cloud base, there is some debate as to whether or not gustnadoes are tornadoes. They are formed when fast moving cold, dry outflow air from a thunderstorm is blown through a mass of stationary, warm, moist air near the outflow boundary, resulting in a "rolling" effect (often exemplified through a roll cloud). If low level wind shear is strong enough, the rotation can be turned vertically or diagonally and make contact with the ground. The result is a gustnado. They usually cause small areas of heavier rotational wind damage among areas of straight-line wind damage.
Dust devil.
A "dust devil" resembles a tornado in that it is a vertical swirling column of air. However, they form under clear skies and are no stronger than the weakest tornadoes. They form when a strong convective updraft is formed near the ground on a hot day. If there is enough low level wind shear, the column of hot, rising air can develop a small cyclonic motion that can be seen near the ground. They are not considered tornadoes because they form during fair weather and are not associated with any clouds. However, they can, on occasion, result in major damage in arid areas.
Fire whirls.
Small-scale, tornado-like circulations can occur near any intense surface heat source. Those that occur near intense wildfires are called "fire whirls". They are not considered tornadoes, except in the rare case where they connect to a pyrocumulus or other cumuliform cloud above. Fire whirls usually are not as strong as tornadoes associated with thunderstorms. They can, however, produce significant damage.
Steam devils.
A "steam devil" is a rotating updraft that involves steam or smoke. Steam devils are very rare. They most often form from smoke issuing from a power plant's smokestack. Hot springs and deserts may also be suitable locations for a steam devil to form. The phenomenon can occur over water, when cold arctic air passes over relatively warm water.
Intensity and damage.
The Fujita scale and the Enhanced Fujita Scale rate tornadoes by damage caused. The Enhanced Fujita (EF) Scale was an update to the older Fujita scale, by expert elicitation, using engineered wind estimates and better damage descriptions. The EF Scale was designed so that a tornado rated on the Fujita scale would receive the same numerical rating, and was implemented starting in the United States in 2007. An EF0 tornado will probably damage trees but not substantial structures, whereas an EF5 tornado can rip buildings off their foundations leaving them bare and even deform large skyscrapers. The similar TORRO scale ranges from a T0 for extremely weak tornadoes to T11 for the most powerful known tornadoes. Doppler weather radar data, photogrammetry, and ground swirl patterns (cycloidal marks) may also be analyzed to determine intensity and award a rating.
Tornadoes vary in intensity regardless of shape, size, and location, though strong tornadoes are typically larger than weak tornadoes. The association with track length and duration also varies, although longer track tornadoes tend to be stronger.<ref name="width/length intensity relationship"></ref> In the case of violent tornadoes, only a small portion of the path is of violent intensity, most of the higher intensity from subvortices.
In the United States, 80% of tornadoes are EF0 and EF1 (T0 through T3) tornadoes. The rate of occurrence drops off quickly with increasing strength—less than 1% are violent tornadoes (EF4, T8 or stronger). Outside Tornado Alley, and North America in general, violent tornadoes are extremely rare. This is apparently mostly due to the lesser number of tornadoes overall, as research shows that tornado intensity distributions are fairly similar worldwide. A few significant tornadoes occur annually in Europe, Asia, southern Africa, and southeastern South America, respectively.
Climatology.
The United States has the most tornadoes of any country, nearly four times more than estimated in all of Europe, excluding waterspouts. This is mostly due to the unique geography of the continent. North America is a large continent that extends from the tropics north into arctic areas, and has no major east-west mountain range to block air flow between these two areas. In the middle latitudes, where most tornadoes of the world occur, the Rocky Mountains block moisture and buckle the atmospheric flow, forcing drier air at mid-levels of the troposphere due to downsloped winds, and causing the formation of a low pressure area downwind to the east of the mountains. Increased westerly flow off the Rockies force the formation of a dry line when the flow aloft is strong, while the Gulf of Mexico fuels abundant low-level moisture in the southerly flow to its east. This unique topography allows for frequent collisions of warm and cold air, the conditions that breed strong, long-lived storms throughout the year. A large portion of these tornadoes form in an area of the central United States known as Tornado Alley. This area extends into Canada, particularly Ontario and the Prairie Provinces, although southeast Quebec, the interior of British Columbia, and western New Brunswick are also tornado-prone. Tornadoes also occur across northeastern Mexico.
The United States averages about 1,200 tornadoes per year. The Netherlands has the highest average number of recorded tornadoes per area of any country (more than 20, or 0.0013 per sq mi (0.00048 per km2), annually), followed by the UK (around 33, or 0.00035 per sq mi (0.00013 per km2), per year), but most are small and cause minor damage. In absolute number of events, ignoring area, the UK experiences more tornadoes than any other European country, excluding waterspouts.
Tornadoes kill an average of 179 people per year in Bangladesh, the most in the world. This is due to high population density, poor quality of construction and lack of tornado safety knowledge, as well as other factors. Other areas of the world that have frequent tornadoes include South Africa, the La Plata Basin area, portions of Europe, Australia and New Zealand, and far eastern Asia.
Tornadoes are most common in spring and least common in winter, but tornadoes can occur any time of year that favorable conditions occur. Spring and fall experience peaks of activity as those are the seasons when stronger winds, wind shear, and atmospheric instability are present. Tornadoes are focused in the right front quadrant of landfalling tropical cyclones, which tend to occur in the late summer and autumn. Tornadoes can also be spawned as a result of eyewall mesovortices, which persist until landfall.
Tornado occurrence is highly dependent on the time of day, because of solar heating. Worldwide, most tornadoes occur in the late afternoon, between 3 pm and 7 pm local time, with a peak near 5 pm. Destructive tornadoes can occur at any time of day. The Gainesville Tornado of 1936, one of the deadliest tornadoes in history, occurred at 8:30 am local time.
Associations with climate and climate change.
Associations with various climate and environmental trends exist. For example, an increase in the sea surface temperature of a source region (e.g. Gulf of Mexico and Mediterranean Sea) increases atmospheric moisture content. Increased moisture can fuel an increase in severe weather and tornado activity, particularly in the cool season.
Some evidence does suggest that the Southern Oscillation is weakly correlated with changes in tornado activity, which vary by season and region, as well as whether the ENSO phase is that of El Niño or La Niña.
Climatic shifts may affect tornadoes via teleconnections in shifting the jet stream and the larger weather patterns. The climate-tornado link is confounded by the forces affecting larger patterns and by the local, nuanced nature of tornadoes. Although it is reasonable to suspect that global warming may affect trends in tornado activity, any such effect is not yet identifiable due to the complexity, local nature of the storms, and database quality issues. Any effect would vary by region.
Detection.
Rigorous attempts to warn of tornadoes began in the United States in the mid-20th century. Before the 1950s, the only method of detecting a tornado was by someone seeing it on the ground. Often, news of a tornado would reach a local weather office after the storm. However, with the advent of weather radar, areas near a local office could get advance warning of severe weather. The first public tornado warnings were issued in 1950 and the first tornado watches and convective outlooks in 1952. In 1953 it was confirmed that hook echoes are associated with tornadoes. By recognizing these radar signatures, meteorologists could detect thunderstorms probably producing tornadoes from dozens of miles away.
Radar.
Today, most developed countries have a network of weather radars, which remains the main method of detecting signatures probably associated with tornadoes. In the United States and a few other countries, Doppler weather radar stations are used. These devices measure the velocity and radial direction (towards or away from the radar) of the winds in a storm, and so can spot evidence of rotation in storms from more than a hundred miles (160 km) away. When storms are distant from a radar, only areas high within the storm are observed and the important areas below are not sampled. Data resolution also decreases with distance from the radar. Some meteorological situations leading to tornadogenesis are not readily detectable by radar and on occasion tornado development may occur more quickly than radar can complete a scan and send the batch of data. Also, most populated areas on Earth are now visible from the Geostationary Operational Environmental Satellites (GOES), which aid in the nowcasting of tornadic storms.
Storm spotting.
In the mid-1970s, the U.S. National Weather Service (NWS) increased its efforts to train storm spotters to spot key features of storms which indicate severe hail, damaging winds, and tornadoes, as well as damage itself and flash flooding. The program was called Skywarn, and the spotters were local sheriff's deputies, state troopers, firefighters, ambulance drivers, amateur radio operators, civil defense (now emergency management) spotters, storm chasers, and ordinary citizens. When severe weather is anticipated, local weather service offices request that these spotters look out for severe weather, and report any tornadoes immediately, so that the office can warn of the hazard.
Usually spotters are trained by the NWS on behalf of their respective organizations, and report to them. The organizations activate public warning systems such as sirens and the Emergency Alert System (EAS), and forward the report to the NWS.
There are more than 230,000 trained Skywarn weather spotters across the United States.
In Canada, a similar network of volunteer weather watchers, called Canwarn, helps spot severe weather, with more than 1,000 volunteers. In Europe, several nations are organizing spotter networks under the auspices of Skywarn Europe and the Tornado and Storm Research Organisation (TORRO) has maintained a network of spotters in the United Kingdom since 1974.
Storm spotters are needed because radar systems such as NEXRAD do not detect a tornado; merely signatures which hint at the presence of tornadoes. Radar may give a warning before there is any visual evidence of a tornado or imminent tornado, but ground truth from an observer can either verify the threat or determine that a tornado is not imminent. The spotter's ability to see what radar cannot is especially important as distance from the radar site increases, because the radar beam becomes progressively higher in altitude further away from the radar, chiefly due to curvature of Earth, and the beam also spreads out.
Visual evidence.
Storm spotters are trained to discern whether a storm seen from a distance is a supercell. They typically look to its rear, the main region of updraft and inflow. Under the updraft is a rain-free base, and the next step of tornadogenesis is the formation of a rotating wall cloud. The vast majority of intense tornadoes occur with a wall cloud on the backside of a supercell.
Evidence of a supercell comes from the storm's shape and structure, and cloud tower features such as a hard and vigorous updraft tower, a persistent, large overshooting top, a hard anvil (especially when backsheared against strong upper level winds), and a corkscrew look or striations. Under the storm and closer to where most tornadoes are found, evidence of a supercell and likelihood of a tornado includes inflow bands (particularly when curved) such as a "beaver tail", and other clues such as strength of inflow, warmth and moistness of inflow air, how outflow- or inflow-dominant a storm appears, and how far is the front flank precipitation core from the wall cloud. Tornadogenesis is most likely at the interface of the updraft and rear flank downdraft, and requires a balance between the outflow and inflow.
Only wall clouds that rotate spawn tornadoes, and usually precede the tornado by five to thirty minutes. Rotating wall clouds may be a visual manifestation of a low-level mesocyclone. Barring a low-level boundary, tornadogenesis is highly unlikely unless a rear flank downdraft occurs, which is usually visibly evidenced by evaporation of cloud adjacent to a corner of a wall cloud. A tornado often occurs as this happens or shortly after; first, a funnel cloud dips and in nearly all cases by the time it reaches halfway down, a surface swirl has already developed, signifying a tornado is on the ground before condensation connects the surface circulation to the storm. Tornadoes may also occur without wall clouds, under flanking lines, and on the leading edge. Spotters watch all areas of a storm, and the cloud base and surface.
Extremes.
The most record-breaking tornado in recorded history was the Tri-State Tornado, which roared through parts of Missouri, Illinois, and Indiana on March 18, 1925. It was likely an F5, though tornadoes were not ranked on any scale in that era. It holds records for longest path length (219 miles, 352 km), longest duration (about 3.5 hours), and fastest forward speed for a significant tornado (73 mph, 117 km/h) anywhere on Earth. In addition, it is the deadliest single tornado in United States history (695 dead). The tornado was also the costliest tornado in history at the time (unadjusted for inflation), but in the years since has been surpassed by several others if population changes over time are not considered. When costs are normalized for wealth and inflation, it ranks third today.
The deadliest tornado in world history was the Daultipur-Salturia Tornado in Bangladesh on April 26, 1989, which killed approximately 1,300 people. Bangladesh has had at least 19 tornadoes in its history kill more than 100 people, almost half of the total in the rest of the world.
The most extensive tornado outbreak on record was the April 25–28, 2011 tornado outbreak, which spawned 355 confirmed tornadoes over the southeastern United States - 211 of them within a single 24-hour period. The previous record was the Super Outbreak of 1974 which spawned nearly 148 tornadoes.
While direct measurement of the most violent tornado wind speeds is nearly impossible, since conventional anemometers would be destroyed by the intense winds and flying debris, some tornadoes have been scanned by mobile Doppler radar units, which can provide a good estimate of the tornado's winds. The highest wind speed ever measured in a tornado, which is also the highest wind speed ever recorded on the planet, is 301 ± 20 mph (484 ± 32 km/h) in the F5 Bridge Creek-Moore, Oklahoma, tornado which killed 36 people. Though the reading was taken about 100 feet (30 m) above the ground, this is a testament to the power of the strongest tornadoes.
Storms that produce tornadoes can feature intense updrafts, sometimes exceeding 150 mi/h. Debris from a tornado can be lofted into the parent storm and carried a very long distance. A tornado which affected Great Bend, Kansas, in November 1915, was an extreme case, where a "rain of debris" occurred 80 mi from the town, a sack of flour was found 110 mi away, and a cancelled check from the Great Bend bank was found in a field outside of Palmyra, Nebraska, 305 mi to the northeast. Waterspouts and tornadoes have been advanced as an explanation for instances of raining fish and other animals.
Safety.
Though tornadoes can strike in an instant, there are precautions and preventative measures that people can take to increase the chances of surviving a tornado. Authorities such as the Storm Prediction Center advise having a pre-determined plan should a tornado warning be issued. When a warning is issued, going to a basement or an interior first-floor room of a sturdy building greatly increases chances of survival. In tornado-prone areas, many buildings have storm cellars on the property. These underground refuges have saved thousands of lives.
Some countries have meteorological agencies which distribute tornado forecasts and increase levels of alert of a possible tornado (such as tornado watches and warnings in the United States and Canada). Weather radios provide an alarm when a severe weather advisory is issued for the local area, though these are mainly available only in the United States. Unless the tornado is far away and highly visible, meteorologists advise that drivers park their vehicles far to the side of the road (so as not to block emergency traffic), and find a sturdy shelter. If no sturdy shelter is nearby, getting low in a ditch is the next best option. Highway overpasses are one of the worst places to take shelter during tornadoes, as the constricted space can be subject to increased wind speed and funneling of debris underneath the overpass.
Myths and misconceptions.
Folklore often identifies a green sky with tornadoes, and though the phenomenon may be associated with severe weather, there is no evidence linking it specifically with tornadoes. It is often thought that opening windows will lessen the damage caused by the tornado. While there is a large drop in atmospheric pressure inside a strong tornado, it is unlikely that the pressure drop would be enough to cause the house to explode. Opening windows may actually increase the severity of the tornado's damage. A violent tornado can destroy a house whether its windows are open or closed.
Another commonly held misconception is that highway overpasses provide adequate shelter from tornadoes. This belief is partly inspired by widely circulated video captured during the 1991 tornado outbreak near Andover, Kansas, where a news crew and several other people take shelter under an overpass on the Kansas Turnpike and safely ride out a tornado as it passes by. However, a highway overpass is a dangerous place during a tornado: the subjects of the video remained safe due to an unlikely combination of events: the storm in question was a weak tornado, did not directly strike the overpass, and the overpass itself was of a unique design. Due to the Venturi effect, tornadic winds are accelerated in the confined space of an overpass. Indeed, in the 1999 Oklahoma tornado outbreak of May 3, 1999, three highway overpasses were directly struck by tornadoes, and at all three locations there was a fatality, along with many life-threatening injuries. By comparison, during the same tornado outbreak, more than 2000 homes were completely destroyed, with another 7000 damaged, and yet only a few dozen people died in their homes.
An old belief is that the southwest corner of a basement provides the most protection during a tornado. The safest place is the side or corner of an underground room opposite the tornado's direction of approach (usually the northeast corner), or the central-most room on the lowest floor. Taking shelter in a basement, under a staircase, or under a sturdy piece of furniture such as a workbench further increases chances of survival.
Finally, there are areas which people believe to be protected from tornadoes, whether by being in a city, near a major river, hill, or mountain, or even protected by supernatural forces. Tornadoes have been known to cross major rivers, climb mountains, affect valleys, and have damaged several city centers. As a general rule, no area is safe from tornadoes, though some areas are more susceptible than others.
Ongoing research.
Meteorology is a relatively young science and the study of tornadoes is newer still. Although researched for about 140 years and intensively for around 60 years, there are still aspects of tornadoes which remain a mystery. Scientists have a fairly good understanding of the development of thunderstorms and mesocyclones, and the meteorological conditions conducive to their formation. However, the step from supercell (or other respective formative processes) to tornadogenesis and predicting tornadic vs. non-tornadic mesocyclones is not yet well known and is the focus of much research.
Also under study are the low-level mesocyclone and the stretching of low-level vorticity which tightens into a tornado, namely, what are the processes and what is the relationship of the environment and the convective storm. Intense tornadoes have been observed forming simultaneously with a mesocyclone aloft (rather than succeeding mesocyclogenesis) and some intense tornadoes have occurred without a mid-level mesocyclone.
In particular, the role of downdrafts, particularly the rear-flank downdraft, and the role of baroclinic boundaries, are intense areas of study.
Reliably predicting tornado intensity and longevity remains a problem, as do details affecting characteristics of a tornado during its life cycle and tornadolysis. Other rich areas of research are tornadoes associated with mesovortices within linear thunderstorm structures and within tropical cyclones.
Scientists still do not know the exact mechanisms by which most tornadoes form, and occasional tornadoes still strike without a tornado warning being issued. Analysis of observations including both stationary and mobile (surface and aerial) in-situ and remote sensing (passive and active) instruments generates new ideas and refines existing notions. Numerical modeling also provides new insights as observations and new discoveries are integrated into our physical understanding and then tested in computer simulations which validate new notions as well as produce entirely new theoretical findings, many of which are otherwise unattainable. Importantly, development of new observation technologies and installation of finer spatial and temporal resolution observation networks have aided increased understanding and better predictions.
Research programs, including field projects such as the VORTEX projects (Verification of the Origins of Rotation in Tornadoes Experiment), deployment of TOTO (the TOtable Tornado Observatory), Doppler On Wheels (DOW), and dozens of other programs, hope to solve many questions that still plague meteorologists. Universities, government agencies such as the National Severe Storms Laboratory, private-sector meteorologists, and the National Center for Atmospheric Research are some of the organizations very active in research; with various sources of funding, both private and public, a chief entity being the National Science Foundation. The pace of research is partly constrained by the number of observations that can be taken; gaps in information about the wind, pressure, and moisture content throughout the local atmosphere; and the computing power available for simulation.
Solar storms similar to tornadoes have been recorded, but it is unknown how closely related they are to their terrestrial counterparts.

</doc>
<doc id="37542" url="http://en.wikipedia.org/wiki?curid=37542" title="DDR">
DDR

DDR is the German acronym for "Deutsche Demokratische Republik" (German) or "German Democratic Republic" (English), a former state in Europe, 1949–1990. It is also referred to as East Germany.
DDR may also refer to:
In politics:
In science:
In technology:
In entertainment and music:
In sport:
In transport:
Other

</doc>
<doc id="37545" url="http://en.wikipedia.org/wiki?curid=37545" title="Palm OS">
Palm OS

Palm OS (also known as Garnet OS) is a mobile operating system initially developed by Palm, Inc., for personal digital assistants (PDAs) in 1996. Palm OS was designed for ease of use with a touchscreen-based graphical user interface. It is provided with a suite of basic applications for personal information management. Later versions of the OS have been extended to support smartphones. Several other licensees have manufactured devices powered by Palm OS.
Following Palm's purchase of the Palm trademark, the currently licensed version from ACCESS was renamed "Garnet OS". In 2007, ACCESS introduced the successor to Garnet OS, called Access Linux Platform and in 2009, the main licensee of Palm OS, Palm, Inc., switched from Palm OS to webOS for their forthcoming devices.
Creator and ownership.
Palm OS was originally developed under the direction of Jeff Hawkins at Palm Computing, Inc. Palm was later acquired by U.S. Robotics Corp., which in turn was later bought by 3Com, which made the Palm subsidiary an independent publicly traded company on March 2, 2000.
In January 2002, Palm set up a wholly owned subsidiary to develop and license Palm OS, which was named PalmSource. PalmSource was then spun off from Palm as an independent company on October 28, 2003. Palm (then called palmOne) became a regular licensee of Palm OS, no longer in control of the operating system.
In September 2005, PalmSource announced that it was being acquired by ACCESS.
In December 2006, Palm gained perpetual rights to the Palm OS source code from ACCESS. With this Palm can modify the licensed operating system as needed without paying further royalties to ACCESS. Together with the May 2005 acquisition of full rights to the "Palm" brand name, only Palm can publish releases of the operating system under the name 'Palm OS'.
As a consequence, on January 25, 2007, ACCESS announced a name change to their current Palm OS operating system, now titled "Garnet OS".
OS overview.
Palm OS is a proprietary mobile operating system. Designed in 1996 for Palm Computing, Inc.'s new Pilot PDA, it has been implemented on a wide array of mobile devices, including smartphones, wrist watches, handheld gaming consoles, barcode readers and GPS devices.
Palm OS versions earlier than 5.0 run on Motorola/Freescale DragonBall processors. From version 5.0 onwards, Palm OS runs on ARM architecture-based processors.
The key features of the current Palm OS Garnet are:
Included with the OS is also a set of standard applications, with the most relevant ones for the four mentioned PIM operations.
Version history and technical background.
Manufacturers are free to implement different features of the OS in their devices or even add new features. This version history describes the officially licensed version from Palm/PalmSource/ACCESS.
All versions prior to Palm OS 5 are based on top of the AMX 68000 kernel licensed from . While this kernel is technically capable of multitasking, the "terms and conditions of that license specifically state that Palm may not expose the API for creating/manipulating tasks within the OS."
Palm OS 1.0.
Palm OS 1.0 is the original version present on the Pilot 1000 and 5000.
Version 1.0 features the classic PIM applications "Address", "Date Book", "Memo Pad", and "To Do List". Also included is a calculator and the Security tool to hide records for private use.
Palm OS 1.0 does not differentiate between RAM and file system storage. Applications are installed directly into RAM and executed in place. As no dedicated file system is supported, the operating system depends on constant RAM refresh cycles to keep its memory. The OS supports 160x160 monochrome output displays. User input is generated through the Graffiti handwriting recognition system or optionally through a virtual keyboard. The system supports data synchronization to another PC via its HotSync technology over a serial interface. The latest bugfix release is version 1.0.7.
Palm OS 2.0.
Palm OS 2.0 was introduced on March 10, 1997 with the PalmPilot Personal and Professional. This version adds TCP/IP network, network HotSync, and display backlight support. The last bugfix release is version 2.0.5.
Two new applications, "Mail" and "Expense" are added, and the standard PIM applications have been enhanced.
Palm OS 3.0.
Palm OS 3.0 was introduced on March 9, 1998 with the launch of the Palm III series. This version adds IrDA infrared and enhanced font support. This version also features updated PIM applications and an update to the application launcher.
Palm OS 3.1 adds only minor new features, like network HotSync support. It was introduced with the Palm IIIx and Palm V.
Palm OS 3.2 adds Web Clipping support, which is an early Palm-specific solution to bring web-content to a small PDA screen. It was introduced with the Palm VII organizer.
Palm OS 3.3 adds faster HotSync speeds and the ability to do infrared hotsyncing. It was introduced with the Palm Vx organizer.
Palm OS 3.5 is the first version to include native 8-bit color support. It also adds major convenience features that simplify operation, like a context-sensitive icon-bar or simpler menu activation. The datebook application is extended with an additional agenda view. This version was first introduced with the Palm IIIc device. The latest bugfix release is version 3.5.3.
As a companion, Palm later offered a "Mobile Internet Kit" software upgrade for Palm OS 3.5. This included Palm's Web Clipping software, MultiMail (which was later renamed to VersaMail) Version 2.26 e-mail software, handPHONE Version 1.3 SMS software, and Neomar Version 1.5 WAP browser.
Palm OS 4.0.
Palm OS 4.0 was released with the new Palm m500 series on March 19, 2001. This version adds a standard interface for external file system access (such as SD cards). External file systems are a radical change to the operating system's previous in-place execution. Now, application code and data need to be loaded into the device's RAM, similar to desktop operating system behavior. A new Universal Connector with USB support is introduced. The previous optional Mobile Internet Kit is now part of the operating system. Version 4.0 adds an attention manager to coordinate information from different applications, with several possibilities to get the user's attention, including sound, LED blinking or vibration. 16-bit color screens and different time zones are supported. This version also has security and UI enhancements.
Palm OS 4.1 is a bugfix release. It was introduced with the launch of the Palm i705. The later minor OS update to version 4.1.2 includes a backport of Graffiti 2 from Palm OS 5.2.
Palm OS 4.2 Simplified Chinese Edition is targeted especially for the Chinese market with fully Simplified Chinese support, co-released with Palm OS 5.3. No device has been manufactured with this version up to now.
Palm OS 5 (Garnet).
Palm OS 5 (not called 5.0) was unveiled by the Palm subsidiary PalmSource in June 2002 and first implemented on the Palm Tungsten T. It is the first version to support ARM devices and replaced the Kadak AMX68000 kernel with the custom MCK kernel, named for its developer, that was written in-house by Palm. Applications written for the prior OS versions use the older DragonBall 68K instruction set and are supported via the "Palm Application Compatibility Environment" (PACE) emulator in Garnet. Even with the additional overhead of PACE, Palm applications usually run faster on ARM devices than on previous generation hardware. New software can take advantage of the ARM processors with small units of ARM code, referred to as "ARMlets".
With a more powerful hardware basis, Palm OS 5 adds substantial enhancements for multimedia capabilities. High density 320x320 screens are supported together with a full digital sound playback and record API. Palm's separate Bluetooth stack is added together with an IEEE 802.11b Wi-Fi stack. Secure network connections over SSL are supported. The OS can be customized with different color schemes.
For Palm OS 5, PalmSource developed and licensed a web browser called "PalmSource Web Browser" based on ACCESS' NetFront 3.0 browser.
Palm OS 5.2 is mainly a bugfix release, first implemented in the Samsung SGH-i500 in March 2003. It added support for 480x320 resolutions and introduced the new handwriting input system called Graffiti 2; the new input system was prompted by Xerox' lawsuit win against Palm. Graffiti 2 is based on "Jot" from . The last bugfix release is version 5.2.8.
Palm OS 5.3 Simplified Chinese Edition released in September 2003, added full Simplified Chinese support, further support for QVGA resolutions, and a standard API for virtual Graffiti called "Dynamic Input Area". This version first shipped on Lenovo's P100 and P300 handhelds.
Palm OS Garnet (5.4) added updated Bluetooth libraries and support for multiple screen resolutions ranging from 160x160 up to 480x320. It first shipped on the Treo 650 in November 2004. This version also introduced the "Garnet" moniker to distinguish it from Palm OS Cobalt 6.0. The last bugfix release is version 5.4.9.
Garnet OS 5.5 dropped the "Palm" moniker and, , is the current version developed by ACCESS. This version is dedicated for use inside of the "Garnet VM" virtual machine.
Garnet VM was announced and released by ACCESS in November 2007 as a core part of the Access Linux Platform and as an emulator allowing Nokia Internet Tablets to run applications written for the Garnet OS. In June 2010, ACCESS release Garnet VM version 6 (aka Garnet VM Beta 6 1.05b).
Palm OS Cobalt.
Palm OS Cobalt (6.0) was the designated successor for Palm OS 5. It was introduced on February 10, 2004, but is no longer offered by ACCESS (see next section). Palm OS 6.0 was renamed to Palm OS Cobalt to make clear that this version was initially not designated to replace Palm OS 5, which adopted the name Palm OS Garnet at the same time.
Palm OS Cobalt introduced modern operating system features to an embedded operating system based on a new kernel with multitasking and memory protection, a modern multimedia and graphic framework (derived from Palm's acquired BeOS), new security features, and adjustments of the PIM file formats to better cooperate with Microsoft Outlook.
Palm OS Cobalt 6.1 presented standard communication libraries for telecommunication, Wi-Fi, and Bluetooth connectivity. Despite other additions, it failed to interest potential licensees to Palm OS Cobalt.
Third-party OS enhancements.
Several licensees have made custom modifications to the operating system. These are not part of the official licensed version.
Modernization.
For several years, PalmSource had been attempting to create a modern successor for Palm OS 5 and have licensees implement it. Although PalmSource shipped Palm OS Cobalt 6.0 to licensees in January 2004, none adopted it for release devices. PalmSource made major improvements to Palm OS Cobalt with the release of Palm OS Cobalt 6.1 in September 2004 to please licensees, but even the new version did not lead to production devices.
In December 2004, PalmSource announced a new OS strategy. With the acquisition of the mobile phone software company China Mobilesoft, PalmSource planned to port Palm OS on top of a Linux kernel, while still offering both Palm OS Garnet and Palm OS Cobalt. This strategy was revised in June 2005, when still no device with Palm OS Cobalt was announced. PalmSource announced it was halting all development efforts on any product not directly related to its future Linux based platform.
With the acquisition of PalmSource by ACCESS, Palm OS for Linux was changed to become the Access Linux Platform which was first announced in February 2006. The initial versions of the platform and software development kits for the Access Linux Platform were officially released in February 2007. As of January 2011, the Access Linux Platform has yet to ship on devices, however development kits exists and public demonstrations have been showcased.
Palm, Inc. the main licensee of Palm OS Garnet did not license Access Linux Platform for their own devices. Instead, Palm developed another Linux-based operating system called Palm webOS. On February 11, 2009, Palm CEO Ed Colligan said there would be no additional Palm OS devices (excepting the Centro being released to other carriers). Palm is focusing on Palm webOS and Windows Mobile devices. On April 1, 2009, Palm announced the availability of a Palm OS emulator for its webOS.
Built-in applications.
Palm OS licensees decide which applications are included on their Palm OS devices. Licensees can also customize the applications.
Standard Palm OS applications.
Note : On the newer models, the standard PIM apps "Address", "Date Book", "Memo Pad" and "ToDos" have been replaced by their improved counterparts "Contacts", "Calendar" "Memos" and "Tasks".
The Palm's Address program stores contact information, keyed by any of several user-definable categories. Entries are displayed and sorted in last name, first name order (this can be changed only to Company, Last Name order). There are five slots for phone or e-mail, each of which may be designated Work, Home, Fax, Other, E-mail, Main, Pager or Mobile (the slot designations cannot be changed).
The newer Contacts app adds the following features : several addresses, 9 new fields : Website, Birthday, More phone numbers, Instant Messaging with quick connect.
Calc turns the Palm into a standard 4-function pocket calculator with three shades of purple and blue buttons contrasting with the two red clear buttons. It supports square root and percent keys and has one memory.
It also has an option to display a running history of the calculations, much like the paper-tape calculators that were once common.
Date Book shows a daily or weekly schedule, or a simple monthly view. The daily schedule has one line per hour, between user-selected begin and end times. Clicking on an empty line creates a new event. Empty lines are crowded out by actual events, whose start and stop times are shown by default bracketed in the left margin. The newer Calendar app adds the following features : New Day view, use of categories for events, event location, event can span midnight, event details, birthdays as timeless events. It supports timezone designation for events, a feature lacking in some more recent competitors.
An event, or appointment, can be heralded by an alarm, any number of minutes, hours or days before it begins. These alarms sound even when the unit is switched off.
Appointments can recur in a specified number of days, weeks, months or years and can contain notes.
Expense tracks common business expenses. No totals are calculated on the Palm. The user must sync with a host computer and view the expense data in a worksheet (templates for Microsoft Excel are supplied).
HotSync integrates with the user's PC. Usually activated by a press of the physical HotSync button on the Palm's cradle (a dock station), this application communicates with various conduits on the desktop PC to install software, backup databases, or merge changes made on the PC or the handheld to both devices. It can communicate with the PC through a physical connection (USB on newer models; although drivers for Windows x64 based platforms are still unavailable, 32 bit editions work well), Bluetooth or IrDA wireless connections, and direct network connections on devices with networking capability.
In addition to the conduits provided by the licensee, developers can create their own conduits for integration with other Palm OS applications and desktop products. For example, a time tracking package could provide a conduit to communicate information between Palm OS and Windows executables.
A Backup conduit included with the HotSync software backs up (and restores, if necessary) most of the data on a Palm OS device. This allows users to hard reset their Palm—thus clearing all of the data—with few noticeable consequences. This also allows users to migrate to new Palm devices of the same Palm OS version, a feature that is helpful to those who lose or damage their device.
Some models of Palm keep their data storage in volatile memory and require constant power to maintain their memory. Although these handhelds attempt to save the contents of memory in low battery situations by not "turning on," leaving a "dead" handheld for an extended period of time can cause this reserve power to be used up and the contents of storage memory to be lost. Some later Palms use NVRAM or microdrive for storage.
Memo Pad can hold notes of up to 4,000 characters each; the newer Memos app increases field size from 3 to 30 kB. Memos are ordered in two ways: alphabetically, and manually (which allows the user to choose the order of the memos), and memos can be grouped in user-configurable categories. Memo Pad is for text only, not for drawings, and text can be entered using the Graffiti alphabet, using hardware or software keyboards, or using the 'paste' function. When Palm devices first became available, some Palm users started to create and exchange Memo Pad documents containing generally useful information, which came to be known as Memoware.
To do list creates personal reminders and prioritizes the things the user has to do.
Each To Do List item may also have: a priority, categories (to organize and view items in logical groups), attached Note (to add more description and clarification of the task). <br>To Do List item can be sorted by: due date, priority or category.
The newer Tasks app features the following improvements : new interface, repeating tasks, alarms, etc.
Preferences (also referred to as Prefs) shows program files with a special preference panel type which are not shown by the normal launcher. Programs can be changed by switching the 'appl' type to 'panl' and vice versa. Palm OS contains approximately 15 preference panels by default and new preference panels can be added just like any other application.
Preference panels allow users to manage a number of settings, including Graffiti settings, sound settings, text shortcuts, network settings and the system time.
Security (which is a panel on newer Palm OS devices) is used to configure Palm OS's security settings. These include the password needed to display hidden records and unlock the device when locked, as well as set up an automatic lockdown time or inactivity threshold. On the PC, only Palm Desktop honors this password but other PC programs can view everything—in other words, all the data protected by this password can be seen by anyone opening the .dat files using a text editor or word processor.
Common third-party core OS applications.
Starting with Palm OS version 5.2, Palm created customized versions of the common PIM application. Some new features have been added, e.g. support for Address categories, Ringtone associations to users, longer memo texts, etc.. They were also renamed to reflect designations from Microsoft Outlook, thus Address became "Contacts", Datebook became "Calendar", Memo Pad became "Memos" and To do list became "Tasks".
Blazer is a web browser for Palm handhelds. The versions 1.0 and 2.0 run on Palm OS 3.1 or higher handhelds, but they needed a proxy server which has been shut down, so they can no longer be used. Version 3.0 is used on the Treo 600 smartphone. The current version of Blazer is Blazer 4.5, which is compliant with most major standards. It is generally bundled with newer smartphones and newer Palm devices capable of accessing the Internet.
Palm's Note Pad can be used for quick drawings. With neat handwriting, 20-30 words will fit on one page; for more text, Memo Pad is the better choice. There are three sizes of pen width, plus an eraser and a background color change feature in some models. It is possible to draw a very simple map. The more "advanced" desktop version saves the Memo pad drawings to the desktop.
As of 2006, most new Palm handhelds include Photos, which creates a digital photo album used to view pictures on a Palm OS device. As with all the other photo programs, photos can be beamed to other mobile devices. Each photo can be labeled and organized into separate photo albums. A slideshow can also be shown for a specific album, and each photo in the album will be shown full screen.
Photos can be edited with the Palm Photos PC software (Windows only), and when the photos are transferred to the handheld they will contain all changes made to the photo.
The Palm Photos software is available in the Zire 71, Tungsten C, Tungsten E, Tungsten T2, Tungsten T3 and several others.
With the support for Video, Palm Photos was later renamed to "Media" and even later to "Pics& Videos".
Some models feature the ability to make voice recordings which are synced using the Voice conduit and can be viewed on a desktop with the Voice Memo application which is part of the Palm Desktop Suite.
Third-party applications.
There are many successful applications that can be installed on a Palm OS device. As of 2008, there are more than 50,000 third-party applications available for the Palm OS platform, which have various licensing types, including open-source, and various closed licensing schemes such as freeware, shareware, and traditional pay-up-front purchase.
Application development.
Palm OS Garnet applications are primarily coded in C/C++. Two officially supported compilers exist: a commercial product, CodeWarrior Development Studio for Palm OS, and an open source tool chain called prc-tools, based on an old version of gcc. CodeWarrior is criticized for being expensive and is no longer being developed, whereas PRC-Tools lacks several of CodeWarrior's features. A version of PRC-Tools is included in a free Palm OS Developer Suite (PODS).
OnBoardC is a C compiler, assembler, linker and programming editor that runs on the Palm itself.
Palm OS Cobalt applications are also coded in a variation of gcc, but the Cobalt compilers have fewer limitations.
There are development tools available for Palm programming that do not require low-level programming in C/C++, such as PocketC/PocketC Architect, CASL, AppForge Crossfire (which uses Visual Basic, Visual Basic.NET, or C#), Handheld Basic, Pendragon Forms, and NSBasic/Palm (Visual Basic like languages). A Java Virtual Machine was previously available for the Palm OS platform, however on 2008-01-12, Palm, Inc. announced that it would no longer be available. Palm, Inc. further said "There is no alternate Java Virtual Machine that we are aware of for Palm OS." Waba and a derivative of it, SuperWaba, provide a Java-like virtual machine and programming language. A version of the Lua language, called Plua, is also available for Palm; however, due to the fact that it requires an additional runtime to be installed along with the application, it is only used for mainstream applications by a minority of software companies. is an ISO/ANSI Standard Forth compiler that runs on the Palm itself. It also has an interactive console for dynamic development and debugging.
Three environments allow programming in Pascal for Palm OS. The free PP Compiler runs directly on the handheld computer, while PocketStudio is a Delphi-like IDE for Windows Computers that has a visual form designer and generates PRC files for being transferred to handhelds via HotSync. The third option was , developed by Danish developer , based on his experience with the compiler for various 16-bit computer systems, including the Commodore Amiga. The in now available for a donation.
As Palm has no connection drivers that enable the transfer of data with a server DBMS (Oracle, mySQL, MS SQL Server), the programmer can use Middleware software that enables this connectivity.
A roughly R4RS-compatible implementation of Scheme, LispMe, provides the Palm platform with a GPL-licensed onboard Lisp REPL with some Palm OS-specific adaptations, but although it is functionally a compiler it does not produce code that operates outside the development environment, so its use is restricted to prototyping.
A free development tool, LaFac, works directly on the Palm device, using the Memo Pad for source code editing, and provides support for a limited subset of C, Pascal, and Basic.
Legal issues.
Palm OS has been involved in various lawsuits over the years.

</doc>
<doc id="37556" url="http://en.wikipedia.org/wiki?curid=37556" title="Asperger syndrome">
Asperger syndrome

 
Asperger syndrome (AS), also known as Asperger's syndrome, Asperger disorder (AD) or simply Asperger's, is an autism spectrum disorder (ASD) that is characterized by significant difficulties in social interaction and nonverbal communication, alongside restricted and repetitive patterns of behavior and interests. It differs from other autism spectrum disorders by its relative preservation of linguistic and cognitive development. Although not required for diagnosis, physical clumsiness and atypical (peculiar or odd) use of language are frequently reported. The diagnosis of Asperger's was eliminated in the 2013 fifth edition of the "Diagnostic and Statistical Manual of Mental Disorders" (DSM-5) and replaced by a diagnosis of autism spectrum disorder on a severity scale.
The syndrome is named after the Austrian pediatrician Hans Asperger who, in 1944, studied and described children in his practice who lacked nonverbal communication skills, demonstrated limited empathy with their peers, and were physically clumsy. The modern conception of Asperger syndrome came into existence in 1981 and went through a period of popularization, becoming standardized as a diagnosis in the early 1990s. Many questions and controversies remain about aspects of the disorder. There is doubt about whether it is distinct from high-functioning autism (HFA); partly because of this, its prevalence is not firmly established.
The exact cause of Asperger's is unknown. Although research suggests the likelihood of a genetic basis, there is no known genetic cause and brain imaging techniques have not identified a clear common pathology. There is no single treatment, and the effectiveness of particular interventions is supported by only limited data. Intervention is aimed at improving symptoms and function. The mainstay of management is behavioral therapy, focusing on specific deficits to address poor communication skills, obsessive or repetitive routines, and physical clumsiness. Most children improve as they mature to adulthood, but social and communication difficulties may persist. Some researchers and people with Asperger's have advocated a shift in attitudes toward the view that it is a difference, rather than a disability that must be treated or cured.
Classification.
The extent of the overlap between AS and high-functioning autism (HFA—autism unaccompanied by intellectual disability) is unclear. The ASD classification is to some extent an artifact of how autism was discovered, and may not reflect the true nature of the spectrum; methodological problems have beset Asperger syndrome as a valid diagnosis from the outset. In the fifth edition of the "Diagnostic and Statistical Manual of Mental Disorders" (DSM-5), published in May 2013, AS, as a separate diagnosis, was eliminated and folded into autism spectrum disorder. Like the diagnosis of Asperger syndrome, the change was controversial and AS was not removed from the WHO's ICD-10.
The World Health Organization (WHO) defines Asperger syndrome (AS) as one of the autism spectrum disorders (ASD) or pervasive developmental disorders (PDD), which are a spectrum of psychological conditions that are characterized by abnormalities of social interaction and communication that pervade the individual's functioning, and by restricted and repetitive interests and behavior. Like other psychological development disorders, ASD begins in infancy or childhood, has a steady course without remission or relapse, and has impairments that result from maturation-related changes in various systems of the brain. ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as social deficits. Of the other four ASD forms, autism is the most similar to AS in signs and likely causes, but its diagnosis requires impaired communication and allows delay in cognitive development; Rett syndrome and childhood disintegrative disorder share several signs with autism but may have unrelated causes; and pervasive developmental disorder not otherwise specified (PDD-NOS) is diagnosed when the criteria for a more specific disorder are unmet.
Characteristics.
As a pervasive developmental disorder, Asperger syndrome is distinguished by a pattern of symptoms rather than a single symptom. It is characterized by qualitative impairment in social interaction, by stereotyped and restricted patterns of behavior, activities and interests, and by no clinically significant delay in cognitive development or general delay in language. Intense preoccupation with a narrow subject, one-sided verbosity, restricted prosody, and physical clumsiness are typical of the condition, but are not required for diagnosis.
Social interaction.
A lack of demonstrated empathy has a significant impact on aspects of communal living for persons with Asperger syndrome. Individuals with AS experience difficulties in basic elements of social interaction, which may include a failure to develop friendships or to seek shared enjoyments or achievements with others (for example, showing others objects of interest), a lack of social or emotional reciprocity (social "games" give-and-take mechanic), and impaired nonverbal behaviors in areas such as eye contact, facial expression, posture, and gesture. 
People with AS may not be as withdrawn around others, compared to those with other, more debilitating forms of autism; they approach others, even if awkwardly. For example, a person with AS may engage in a one-sided, long-winded speech about a favorite topic, while misunderstanding or not recognizing the listener's feelings or reactions, such as a wish to change the topic of talk or end the interaction. This social awkwardness has been called "active but odd". This failure to react appropriately to social interaction may appear as disregard for other people's feelings, and may come across as insensitive. However, not all individuals with AS will approach others. Some of them may even display selective mutism, speaking not at all to most people and excessively to specific people. Some may choose only to talk to people they like.
The cognitive ability of children with AS often allows them to articulate social norms in a laboratory context, where they may be able to show a theoretical understanding of other people's emotions; however, they typically have difficulty acting on this knowledge in fluid, real-life situations. People with AS may analyze and distill their observations of social interaction into rigid behavioral guidelines, and apply these rules in awkward ways, such as forced eye contact, resulting in a demeanor that appears rigid or socially naive. Childhood desire for companionship can become numbed through a history of failed social encounters.
The hypothesis that individuals with AS are predisposed to violent or criminal behavior has been investigated, but is not supported by data. More evidence suggests children with AS are victims rather than victimizers. A 2008 review found that an overwhelming number of reported violent criminals with AS had coexisting psychiatric disorders such as schizoaffective disorder.
Restricted and repetitive interests and behavior.
People with Asperger syndrome display behavior, interests, and activities that are restricted and repetitive and are sometimes abnormally intense or focused. They may stick to inflexible routines, move in stereotyped and repetitive ways, or preoccupy themselves with parts of objects.
Pursuit of specific and narrow areas of interest is one of the most striking features of AS. Individuals with AS may collect volumes of detailed information on a relatively narrow topic such as weather data or star names, without necessarily having a genuine understanding of the broader topic. For example, a child might memorize camera model numbers while caring little about photography. This behavior is usually apparent by age 5 or 6. Although these special interests may change from time to time, they typically become more unusual and narrowly focused, and often dominate social interaction so much that the entire family may become immersed. Because narrow topics often capture the interest of children, this symptom may go unrecognized.
Stereotyped and repetitive motor behaviors are a core part of the diagnosis of AS and other ASDs. They include hand movements such as flapping or twisting, and complex whole-body movements. These are typically repeated in longer bursts and look more voluntary or ritualistic than tics, which are usually faster, less rhythmical and less often symmetrical.
According to the Adult Asperger Assessment (AAA) diagnostic test, a lack of interest in fiction and a positive preference towards non-fiction is common among adults with AS.
Speech and language.
Although individuals with Asperger syndrome acquire language skills without significant general delay and their speech typically lacks significant abnormalities, language acquisition and use is often atypical. Abnormalities include verbosity, abrupt transitions, literal interpretations and miscomprehension of nuance, use of metaphor meaningful only to the speaker, auditory perception deficits, unusually pedantic, formal or idiosyncratic speech, and oddities in loudness, pitch, intonation, prosody, and rhythm. Echolalia has also been observed in individuals with AS.
Three aspects of communication patterns are of clinical interest: poor prosody, tangential and circumstantial speech, and marked verbosity. Although inflection and intonation may be less rigid or monotonic than in classic autism, people with AS often have a limited range of intonation: speech may be unusually fast, jerky or loud. Speech may convey a sense of incoherence; the conversational style often includes monologues about topics that bore the listener, fails to provide context for comments, or fails to suppress internal thoughts. Individuals with AS may fail to detect whether the listener is interested or engaged in the conversation. The speaker's conclusion or point may never be made, and attempts by the listener to elaborate on the speech's content or logic, or to shift to related topics, are often unsuccessful.
Children with AS may have an unusually sophisticated vocabulary at a young age and have been colloquially called "little professors", but have difficulty understanding figurative language and tend to use language literally. Children with AS appear to have particular weaknesses in areas of nonliteral language that include humor, irony, teasing, and sarcasm. Although individuals with AS usually understand the cognitive basis of humor, they seem to lack understanding of the intent of humor to share enjoyment with others. Despite strong evidence of impaired humor appreciation, anecdotal reports of humor in individuals with AS seem to challenge some psychological theories of AS and autism.
Motor and sensory perception.
Individuals with Asperger syndrome may have signs or symptoms that are independent of the diagnosis, but can affect the individual or the family. These include differences in perception and problems with motor skills, sleep, and emotions.
Individuals with AS often have excellent auditory and visual perception. Children with ASD often demonstrate enhanced perception of small changes in patterns such as arrangements of objects or well-known images; typically this is domain-specific and involves processing of fine-grained features. Conversely, compared to individuals with high-functioning autism, individuals with AS have deficits in some tasks involving visual-spatial perception, auditory perception, or visual memory. Many accounts of individuals with AS and ASD report other unusual sensory and perceptual skills and experiences. They may be unusually sensitive or insensitive to sound, light, and other stimuli; these sensory responses are found in other developmental disorders and are not specific to AS or to ASD. There is little support for increased fight-or-flight response or failure of habituation in autism; there is more evidence of decreased responsiveness to sensory stimuli, although several studies show no differences.
Hans Asperger's initial accounts and other diagnostic schemes include descriptions of physical clumsiness. Children with AS may be delayed in acquiring skills requiring motor dexterity, such as riding a bicycle or opening a jar, and may seem to move awkwardly or feel "uncomfortable in their own skin". They may be poorly coordinated, or have an odd or bouncy gait or posture, poor handwriting, or problems with visual-motor integration. They may show problems with proprioception (sensation of body position) on measures of developmental coordination disorder (motor planning disorder), balance, tandem gait, and finger-thumb apposition. There is no evidence that these motor skills problems differentiate AS from other high-functioning ASDs.
Children with AS are more likely to have sleep problems, including difficulty in falling asleep, frequent nocturnal awakenings, and early morning awakenings. AS is also associated with high levels of alexithymia, which is difficulty in identifying and describing one's emotions. Although AS, lower sleep quality, and alexithymia are associated, their causal relationship is unclear.
Causes.
Hans Asperger described common symptoms among his patients' family members, especially fathers, and research supports this observation and suggests a genetic contribution to Asperger syndrome. Although no specific gene has yet been identified, multiple factors are believed to play a role in the expression of autism, given the phenotypic variability seen in children with AS. Evidence for a genetic link is the tendency for AS to run in families and an observed higher incidence of family members who have behavioral symptoms similar to AS but in a more limited form (for example, slight difficulties with social interaction, language, or reading). Most research suggests that all autism spectrum disorders have shared genetic mechanisms, but AS may have a stronger genetic component than autism. There is probably a common group of genes where particular alleles render an individual vulnerable to developing AS; if this is the case, the particular combination of alleles would determine the severity and symptoms for each individual with AS.
A few ASD cases have been linked to exposure to teratogens (agents that cause birth defects) during the first eight weeks from conception. Although this does not exclude the possibility that ASD can be initiated or affected later, it is strong evidence that it arises very early in development. Many environmental factors have been hypothesized to act after birth, but none has been confirmed by scientific investigation.
Mechanism.
Asperger syndrome appears to result from developmental factors that affect many or all functional brain systems, as opposed to localized effects. Although the specific underpinnings of AS or factors that distinguish it from other ASDs are unknown, and no clear pathology common to individuals with AS has emerged, it is still possible that AS's mechanism is separate from other ASDs. Neuroanatomical studies and the associations with teratogens strongly suggest that the mechanism includes alteration of brain development soon after conception. Abnormal migration of embryonic cells during fetal development may affect the final structure and connectivity of the brain, resulting in alterations in the neural circuits that control thought and behavior. Several theories of mechanism are available; none are likely to provide a complete explanation.
The underconnectivity theory hypothesizes underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes. It maps well to general-processing theories such as weak central coherence theory, which hypothesizes that a limited ability to see the big picture underlies the central disturbance in ASD. A related theory—enhanced perceptual functioning—focuses more on the superiority of locally oriented and perceptual operations in autistic individuals.
The mirror neuron system (MNS) theory hypothesizes that alterations to the development of the MNS interfere with imitation and lead to Asperger's core feature of social impairment. For example, one study found that activation is delayed in the core circuit for imitation in individuals with AS. This theory maps well to social cognition theories like the theory of mind, which hypothesizes that autistic behavior arises from impairments in ascribing mental states to oneself and others, or hyper-systemizing, which hypothesizes that autistic individuals can systematize internal operation to handle internal events but are less effective at empathizing by handling events generated by other agents.
Diagnosis.
Standard diagnostic criteria require impairment in social interaction and repetitive and stereotyped patterns of behavior, activities and interests, without significant delay in language or cognitive development. Unlike the international standard, the DSM-IV-TR criteria also required significant impairment in day-to-day functioning; DSM-5 eliminated AS as a separate diagnosis in 2013, and folded it into the umbrella of autism spectrum disorders. Other sets of diagnostic criteria have been proposed by Szatmari "et al." and by Gillberg and Gillberg.
Diagnosis is most commonly made between the ages of four and eleven. A comprehensive assessment involves a multidisciplinary team that observes across multiple settings, and includes neurological and genetic assessment as well as tests for cognition, psychomotor function, verbal and nonverbal strengths and weaknesses, style of learning, and skills for independent living. The "gold standard" in diagnosing ASDs combines clinical judgment with the Autism Diagnostic Interview-Revised (ADI-R)—a semistructured parent interview—and the Autism Diagnostic Observation Schedule (ADOS)—a conversation and play-based interview with the child. Delayed or mistaken diagnosis can be traumatic for individuals and families; for example, misdiagnosis can lead to medications that worsen behavior. Many children with AS are initially misdiagnosed with attention deficit hyperactivity disorder (ADHD). Diagnosing adults is more challenging, as standard diagnostic criteria are designed for children and the expression of AS changes with age; adult diagnosis requires painstaking clinical examination and thorough medical history gained from both the individual and other people who know the person, focusing on childhood behavior. Conditions that must be considered in a differential diagnosis include other ASDs, the schizophrenia spectrum, ADHD, obsessive–compulsive disorder, major depressive disorder, semantic pragmatic disorder, nonverbal learning disorder, Tourette syndrome, stereotypic movement disorder, bipolar disorder, and social-cognitive deficits due to brain damage from alcohol abuse.
Underdiagnosis and overdiagnosis are problems in marginal cases. The cost and difficulty of screening and assessment can delay diagnosis. Conversely, the increasing popularity of drug treatment options and the expansion of benefits has motivated providers to overdiagnose ASD. There are indications AS has been diagnosed more frequently in recent years, partly as a residual diagnosis for children of normal intelligence who are not autistic but have social difficulties.
There are questions about the external validity of the AS diagnosis. That is, it is unclear whether there is a practical benefit in distinguishing AS from HFA and from PDD-NOS; the same child can receive different diagnoses depending on the screening tool. The debate about distinguishing AS from HFA is partly due to a tautological dilemma where disorders are defined based on severity of impairment, so that studies that appear to confirm differences based on severity are to be expected.
Screening.
Parents of children with Asperger syndrome can typically trace differences in their children's development to as early as 30 months of age. Developmental screening during a routine check-up by a general practitioner or pediatrician may identify signs that warrant further investigation. The diagnosis of AS is complicated by the use of several different screening instruments, including the Asperger Syndrome Diagnostic Scale (ASDS), Autism Spectrum Screening Questionnaire (ASSQ), Childhood Autism Spectrum Test (CAST) (previously called the Childhood Asperger Syndrome Test), Gilliam Asperger's disorder scale (GADS), Krug Asperger's Disorder Index (KADI), and the Autism-spectrum quotient (AQ; with versions for children, adolescents and adults). None have been shown to reliably differentiate between AS and other ASDs.
Management.
Asperger syndrome treatment attempts to manage distressing symptoms and to teach age-appropriate social, communication and vocational skills that are not naturally acquired during development, with intervention tailored to the needs of the individual based on multidisciplinary assessment. Although progress has been made, data supporting the efficacy of particular interventions are limited.
Therapies.
The ideal treatment for AS coordinates therapies that address core symptoms of the disorder, including poor communication skills and obsessive or repetitive routines. While most professionals agree that the earlier the intervention, the better, there is no single best treatment package. AS treatment resembles that of other high-functioning ASDs, except that it takes into account the linguistic capabilities, verbal strengths, and nonverbal vulnerabilities of individuals with AS. A typical program generally includes:
Of the many studies on behavior-based early intervention programs, most are case reports of up to five participants and typically examine a few problem behaviors such as self-injury, aggression, noncompliance, stereotypies, or spontaneous language; unintended side effects are largely ignored. Despite the popularity of social skills training, its effectiveness is not firmly established. A randomized controlled study of a model for training parents in problem behaviors in their children with AS showed that parents attending a one-day workshop or six individual lessons reported fewer behavioral problems, while parents receiving the individual lessons reported less intense behavioral problems in their AS children. Vocational training is important to teach job interview etiquette and workplace behavior to older children and adults with AS, and organization software and personal data assistants can improve the work and life management of people with AS.
Medications.
No medications directly treat the core symptoms of AS. Although research into the efficacy of pharmaceutical intervention for AS is limited, it is essential to diagnose and treat comorbid conditions. Deficits in self-identifying emotions or in observing effects of one's behavior on others can make it difficult for individuals with AS to see why medication may be appropriate. Medication can be effective in combination with behavioral interventions and environmental accommodations in treating comorbid symptoms such as anxiety disorder, major depressive disorder, inattention and aggression. The atypical antipsychotic medications risperidone and olanzapine have been shown to reduce the associated symptoms of AS; risperidone can reduce repetitive and self-injurious behaviors, aggressive outbursts and impulsivity, and improve stereotypical patterns of behavior and social relatedness. The selective serotonin reuptake inhibitors (SSRIs) fluoxetine, fluvoxamine, and sertraline have been effective in treating restricted and repetitive interests and behaviors.
Care must be taken with medications, as side effects may be more common and harder to evaluate in individuals with AS, and tests of drugs' effectiveness against comorbid conditions routinely exclude individuals from the autism spectrum. Abnormalities in metabolism, cardiac conduction times, and an increased risk of type 2 diabetes have been raised as concerns with these medications, along with serious long-term neurological side effects. SSRIs can lead to manifestations of behavioral activation such as increased impulsivity, aggression, and sleep disturbance. Weight gain and fatigue are commonly reported side effects of risperidone, which may also lead to increased risk for extrapyramidal symptoms such as restlessness and dystonia and increased serum prolactin levels. Sedation and weight gain are more common with olanzapine, which has also been linked with diabetes. Sedative side-effects in school-age children have ramifications for classroom learning. Individuals with AS may be unable to identify and communicate their internal moods and emotions or to tolerate side effects that for most people would not be problematic.
Prognosis.
There is some evidence that children with AS may see a lessening of symptoms; up to 20% of children may no longer meet the diagnostic criteria as adults, although social and communication difficulties may persist. As of 2006, no studies addressing the long-term outcome of individuals with Asperger syndrome are available and there are no systematic long-term follow-up studies of children with AS. Individuals with AS appear to have normal life expectancy, but have an increased prevalence of comorbid psychiatric conditions, such as major depressive disorder and anxiety disorder that may significantly affect prognosis. Although social impairment may be lifelong, the outcome is generally more positive than with individuals with lower functioning autism spectrum disorders; for example, ASD symptoms are more likely to diminish with time in children with AS or HFA. Most students with AS/HFA have average mathematical ability and test slightly worse in mathematics than in general intelligence, but some are gifted in mathematics and AS has not prevented some adults from major accomplishments, such as Vernon L. Smith winning the Nobel Memorial Prize in Economic Sciences.
Although many attend regular education classes, some children with AS may utilize special education services because of their social and behavioral difficulties. Adolescents with AS may exhibit ongoing difficulty with self care or organization, and disturbances in social and romantic relationships. Despite high cognitive potential, most young adults with AS remain at home, yet some do marry and work independently. The "different-ness" adolescents experience can be traumatic. Anxiety may stem from preoccupation over possible violations of routines and rituals, from being placed in a situation without a clear schedule or expectations, or from concern with failing in social encounters; the resulting stress may manifest as inattention, withdrawal, reliance on obsessions, hyperactivity, or aggressive or oppositional behavior. Depression is often the result of chronic frustration from repeated failure to engage others socially, and mood disorders requiring treatment may develop. Clinical experience suggests the rate of suicide may be higher among those with AS, but this has not been confirmed by systematic empirical studies.
Education of families is critical in developing strategies for understanding strengths and weaknesses; helping the family to cope improves outcomes in children. Prognosis may be improved by diagnosis at a younger age that allows for early interventions, while interventions in adulthood are valuable but less beneficial. There are legal implications for individuals with AS as they run the risk of exploitation by others and may be unable to comprehend the societal implications of their actions.
Epidemiology.
Prevalence estimates vary enormously. A 2003 review of epidemiological studies of children found autism prevalence rates ranging from 0.03 to 4.84 per 1,000, with the ratio of autism to Asperger syndrome ranging from 1.5:1 to 16:1; combining the geometric mean ratio of 5:1 with a conservative prevalence estimate for autism of 1.3 per 1,000 suggests indirectly that the prevalence of AS might be around 0.26 per 1,000. Part of the variance in estimates arises from differences in diagnostic criteria. For example, a relatively small 2007 study of 5,484 eight-year-old children in Finland found 2.9 children per 1,000 met the ICD-10 criteria for an AS diagnosis, 2.7 per 1,000 for Gillberg and Gillberg criteria, 2.5 for DSM-IV, 1.6 for Szatmari "et al.", and 4.3 per 1,000 for the union of the four criteria. Boys seem to be more likely to have AS than girls; estimates of the sex ratio range from 1.6:1 to 4:1, using the Gillberg and Gillberg criteria.
Anxiety disorder and major depressive disorder are the most common conditions seen at the same time; comorbidity of these in persons with AS is estimated at 65%. Reports have associated AS with medical conditions such as aminoaciduria and ligamentous laxity, but these have been case reports or small studies and no factors have been associated with AS across studies. One study of males with AS found an increased rate of epilepsy and a high rate (51%) of nonverbal learning disorder. AS is associated with tics, Tourette syndrome, and bipolar disorder, and the repetitive behaviors of AS have many similarities with the symptoms of obsessive–compulsive disorder and obsessive–compulsive personality disorder. However many of these studies are based on clinical samples or lack standardized measures; nonetheless, comorbid conditions are relatively common.
History.
Named after the Austrian pediatrician Hans Asperger (1906–1980), Asperger syndrome is a relatively new diagnosis in the field of autism.
As a child, Asperger appears to have exhibited some features of the very condition named after him, such as remoteness and talent in language. In 1944, Asperger described four children in his practice who had difficulty in integrating themselves socially. The children lacked nonverbal communication skills, failed to demonstrate empathy with their peers, and were physically clumsy. Asperger called the condition "autistic psychopathy" and described it as primarily marked by social isolation. Fifty years later, several standardizations of AS as a diagnosis were tentatively proposed, many of which diverge significantly from Asperger's original work.
Unlike today's AS, autistic psychopathy could be found in people of all levels of intelligence, including those with intellectual disability. In the context of the Nazi eugenics policy of sterilizing and killing social deviants and the mentally handicapped, Asperger passionately defended the value of autistic individuals, writing "We are convinced, then, that autistic people have their place in the organism of the social community. They fulfill their role well, perhaps better than anyone else could, and we are talking of people who as children had the greatest difficulties and caused untold worries to their care-givers." Asperger also called his young patients "little professors", and believed some would be capable of exceptional achievement and original thought later in life. His paper was published during wartime and in German, so it was not widely read elsewhere.
Lorna Wing popularized the term "Asperger syndrome" in the English-speaking medical community in her 1981 publication of a series of case studies of children showing similar symptoms, and Uta Frith translated Asperger's paper to English in 1991. Sets of diagnostic criteria were outlined by Gillberg and Gillberg in 1989 and by Szatmari "et al." in the same year. AS became a standard diagnosis in 1992, when it was included in the tenth edition of the World Health Organization's diagnostic manual, "International Classification of Diseases" (ICD-10); in 1994, it was added to the fourth edition of the American Psychiatric Association's diagnostic reference, "Diagnostic and Statistical Manual of Mental Disorders" (DSM-IV).
Hundreds of books, articles and websites now describe AS, and prevalence estimates have increased dramatically for ASD, with AS recognized as an important subgroup. Whether it should be seen as distinct from high-functioning autism is a fundamental issue requiring further study, and there are questions about the empirical validation of the DSM-IV and ICD-10 criteria. In 2013, DSM-5 eliminated AS as a separate diagnosis, folding it into the autism spectrum on a severity scale.
Society and culture.
People identifying with Asperger syndrome may refer to themselves in casual conversation as "aspies" (a term first used in print by Liane Holliday Willey in 1999). The word "neurotypical" (abbreviated "NT") describes a person whose neurological development and state are typical, and is often used to refer to non-autistic people. The Internet has allowed individuals with AS to communicate and celebrate diversity with each other in a way that was not previously possible because of their rarity and geographic dispersal. A subculture of aspies has formed. Internet sites like Wrong Planet have made it easier for individuals to connect.
Autistic people have advocated a shift in perception of autism spectrum disorders as complex syndromes rather than diseases that must be cured. Proponents of this view reject the notion that there is an "ideal" brain configuration and that any deviation from the norm is pathological; they promote tolerance for what they call neurodiversity. These views are the basis for the autistic rights and autistic pride movements. There is a contrast between the attitude of adults with self-identified AS, who typically do not want to be cured and are proud of their identity, and parents of children with AS, who typically seek assistance and a cure for their children.
Some researchers have argued that AS can be viewed as a different cognitive style, not a disorder or a disability, and that it should be removed from the standard "Diagnostic and Statistical Manual", much as homosexuality was removed. In a 2002 paper, Simon Baron-Cohen wrote of those with AS, "In the social world, there is no great benefit to a precise eye for detail, but in the worlds of maths, computing, cataloging, music, linguistics, engineering, and science, such an eye for detail can lead to success rather than failure." Baron-Cohen cited two reasons why it might still be useful to consider AS to be a disability: to ensure provision for legally required special support, and to recognize emotional difficulties from reduced empathy. Baron-Cohen argues that the genes for Asperger's combination of abilities have operated throughout recent human evolution and have made remarkable contributions to human history.

</doc>
<doc id="37575" url="http://en.wikipedia.org/wiki?curid=37575" title="Airport">
Airport

An airport is an aerodrome with facilities for commercial aviation flights to take off and land. Airports often have facilities to store and maintain aircraft, and a control tower. An airport consists of a landing area, which comprises an aerially accessible open space including at least one operationally active surface such as a runway for a plane to take off or a helipad, and often includes adjacent utility buildings such as control towers, hangars and terminals. Larger airports may have fixed base operator services, airport aprons, air traffic control centres, passenger facilities such as restaurants and lounges, and emergency services.
An airport with a helipad for rotorcraft but no runway is called a heliport. An airport for use by seaplanes and amphibious aircraft is called a Seaplane base. Such a base typically includes a stretch of open water for takeoffs and landings, and seaplane docks for tying-up.
An international airport has additional facilities for customs and immigration.
In warfare, airports can become the focus of intense fighting, for example the Battle of Tripoli Airport or the Battle for Donetsk Airport, both taking place in 2014. An airport primarily for military use is called an airbase or air station.
Most of the world's airports are owned by local, regional, or national government bodies.
Landside and airside areas.
Airports are divided into landside and airside areas. Landside areas include parking lots, public transport railway stations and access roads. Airside areas include all areas accessible to aircraft, including runways, taxiways and ramps. Access from landside areas to airside areas is tightly controlled at most airports.
Most major airports provide commercial outlets for products and services. Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters. In addition to people, airports move cargo around the clock. Many large airports are located near railway trunk routes.
Air traffic control presence.
The majority of the world's airports are non-towered, with no air traffic control presence. Busy airports have air traffic control (ATC) system. All airports use a traffic pattern to assure smooth traffic flow between departing and arriving aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. Many airports have lighting that help guide planes using the runways and taxiways at night or in rain, snow, or fog. In the US and Canada, the vast majority of airports, large and small, will either have some form of automated airport weather station, a human observer or a combination of the two. Air safety is an important concern in the operation of an airport, and airports often have their own safety services.
Terminology.
The terms "aerodrome", "airfield", and airstrip may also be used to refer to airports, and the terms "heliport", "seaplane base", and "STOLport" refer to airports dedicated exclusively to helicopters, seaplanes, or short take-off and landing aircraft.
In colloquial use, the terms "airport" and "aerodrome" are often interchanged. However, in general, the term "airport" may imply or confer a certain stature upon the aviation facility that an aerodrome may not have achieved. In some jurisdictions, "airport" is a legal term of art reserved exclusively for those aerodromes certified or licensed as airports by the relevant national aviation authority after meeting specified certification criteria or regulatory requirements.
That is to say, all airports are aerodromes, but not all aerodromes are airports. In jurisdictions where there is no legal distinction between "aerodrome" and "airport", which term to use in the name of an aerodrome may be a commercial decision.
Infrastructure.
Smaller or less-developed airports, which represent the vast majority, often have a single runway shorter than 1000 m. Larger airports for airline flights generally have paved runways 2000 m or longer. Many small airports have dirt, grass, or gravel runways, rather than asphalt or concrete.
In the United States, the minimum dimensions for dry, hard landing fields are defined by the FAR Landing And Takeoff Field Lengths. These include considerations for safety margins during landing and takeoff. Heavier aircraft require longer runways.
The longest public-use runway in the world is at Qamdo Bangda Airport in China. It has a length of 5500 m. The world's widest paved runway is at Ulyanovsk Vostochny Airport in Russia and is 105 m wide.
s of 2009[ [update]], the CIA stated that there were approximately 44,000 "... airports or airfields recognizable from the air" around the world, including 15,095 in the US, the US having the most in the world.
Airport ownership and operation.
Most of the world's airports are owned by local, regional, or national government bodies who then lease the airport to private corporations who oversee the airport's operation. For example, in the United Kingdom the state-owned British Airports Authority originally operated eight of the nation's major commercial airports - it was subsequently privatized in the late 1980s, and following its takeover by the Spanish Ferrovial consortium in 2006, has been further divested and downsized to operating just five. Germany's Frankfurt Airport is managed by the quasi-private firm Fraport. While in India GMR Group operates, through joint ventures, Indira Gandhi International Airport and Rajiv Gandhi International Airport. Bengaluru International Airport and Chhatrapati Shivaji International Airport are controlled by GVK Group. The rest of India's airports are managed by the Airports Authority of India.
In the United States commercial airports are generally operated directly by government entities or government-created airport authorities (also known as port authorities), such as the Los Angeles World Airports authority that oversees several airports in the Greater Los Angeles area, including Los Angeles International Airport.
In Canada, the federal authority, Transport Canada, divested itself of all but the remotest airports in 1999/2000. Now most airports in Canada are owned and operated by individual legal authorities or are municipally owned.
Many US airports still lease part or all of their facilities to outside firms, who operate functions such as retail management and parking. In the US, all commercial airport runways are certified by the FAA under the Code of Federal Regulations Title 14 Part 139, "Certification of Commercial Service Airports" but maintained by the local airport under the regulatory authority of the FAA.
Despite the reluctance to privatize airports in the US (despite the FAA sponsoring a privatization program since 1996), the government-owned, contractor-operated (GOCO) arrangement is the standard for the operation of commercial airports in the rest of the world.
Airport structures.
Airports are divided into landside and airside areas. Landside areas include parking lots, public transportation train stations and access roads. Airside areas include all areas accessible to aircraft, including runways, taxiways and ramps. Access from landside areas to airside areas is tightly controlled at most airports. Passengers on commercial flights access airside areas through terminals, where they can purchase tickets, clear security check, or claim luggage and board aircraft through gates. The waiting areas which provide passenger access to aircraft are typically called concourses, although this term is often used interchangeably with terminal.
The area where aircraft park next to a terminal to load passengers and baggage is known as a "ramp" (or "the tarmac"). Parking areas for aircraft away from terminals are called aprons.
Airports can be towered or non-towered, depending on air traffic density and available funds. Due to their high capacity and busy airspace, many international airports have air traffic control located on site.
Airports with international flights have customs and immigration facilities. However, as some countries have agreements that allow travel between them without customs and immigrations, such facilities are not a definitive need for an international airport. International flights often require a higher level of physical security, although in recent years, many countries have adopted the same level of security for international and domestic travel.
Some airport structures include on-site hotels built within or attached to a terminal building. Airport hotels have grown popular due to their convenience for transient passengers and easy accessibility to the airport terminal. Many airport hotels also have agreements with airlines to provide overnight lodging for displaced passengers.
"Floating airports" are being designed which could be located out at sea and which would use designs such as pneumatic stabilized platform technology.
Products and services.
Most major airports provide commercial outlets for products and services. Most of these companies, many of which are internationally known brands, are located within the departure areas. These include clothing boutiques and restaurants. Prices charged for items sold at these outlets are generally higher than those outside the airport. However, some airports now regulate costs to keep them comparable to "street prices". This term is misleading as prices often match the manufacturers' suggested retail price (MSRP) but are almost never discounted.
Apart from major fast food chains, some airport restaurants offer regional cuisine specialties for those in transit so that they may sample local food or culture without leaving the airport.
Major airports in such countries as Russia and Japan offer miniature sleeping units within the airport that are available for rent by the hour. The smallest type is the capsule hotel popular in Japan. A slightly larger variety is known as a sleep box. An even larger type is provided by the company YOTEL.
Premium and VIP services.
Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters.
These services are usually reserved for First and Business class passengers, premium frequent flyers, and members of the airline's clubs. Premium services may sometimes be open to passengers who are members of a different airline's frequent flyer program. This can sometimes be part of a reciprocal deal, as when multiple airlines are part of the same alliance, or as a ploy to attract premium customers away from rival airlines.
Sometimes these premium services will be offered to a non-premium passenger if the airline has made a mistake in handling of the passenger, such as unreasonable delays or mishandling of checked baggage.
Airline lounges frequently offer free or reduced cost food, as well as alcoholic and non-alcoholic beverages. Lounges themselves typically have seating, showers, quiet areas, televisions, computer, Wi-Fi and Internet access, and power outlets that passengers may use for their electronic equipment. Some airline lounges employ baristas, bartenders and gourmet chefs.
Airlines sometimes operate multiple lounges within the one airport terminal allowing ultra-premium customers, such as first class customers, additional services, which are not available to other premium customers. Multiple lounges may also prevent overcrowding of the lounge facilities.
Cargo and freight services.
In addition to people, airports move cargo around the clock. Cargo airlines often have their own on-site and adjacent infrastructure to transfer parcels between ground and air.
Cargo Terminal Facilities are areas where international airports export cargo has to be stored after customs clearance and prior to loading on the aircraft. Similarly import cargo that is offloaded needs to be in bond before the consignee decides to take delivery. Areas have to be kept aside for examination of export and import cargo by the airport authorities. Designated areas or sheds may be given to airlines or freight forward ring agencies.
Every cargo terminal has a landside and an airside. The landside is where the exporters and importers through either their agents or by themselves deliver or collect shipments while the airside is where loads are moved to or from the aircraft. In addition cargo terminals are divided into distinct areas – export, import and interline or transhipment
Support services.
Aircraft and Passenger Boarding Bridges Maintenance, Pilot Operations, Commissioning, Training Services, aircraft rental, and hangar rental are most often performed by a fixed base operator (FBO). At major airports, particularly those used as hubs, airlines may operate their own support facilities.
Some airports, typically military airbases, have long runways used as emergency landing sites. Many airbases have arresting equipment for fast aircraft, known as arresting gear – a strong cable suspended just above the runway and attached to a hydraulic reduction gear mechanism. Together with the landing aircraft's arresting hook, it is used in situations where the aircraft's brakes would be insufficient by themselves.
In the United States, many larger civilian airports also host an Air National Guard base.
Airport access.
Many large airports are located near railway trunk routes for seamless connection of multimodal transport, for instance Frankfurt Airport, Amsterdam Airport Schiphol, London Heathrow Airport, London Gatwick Airport and London Stansted Airport. It is also common to connect an airport and a city with rapid transit, light rail lines or other non-road public transport systems. Some examples of this would include the AirTrain JFK at John F. Kennedy International Airport in New York, Link Light Rail that runs from the heart of downtown Seattle to Seattle–Tacoma International Airport, and the Silver Line T at Boston's Logan International Airport by the Massachusetts Bay Transportation Authority (MBTA). Such a connection lowers risk of missed flights due to traffic congestion. Large airports usually have access also through controlled-access highways ('freeways' or 'motorways') from which motor vehicles enter either the departure loop or the arrival loop.
Internal transport.
The distances passengers need to move within a large airport can be substantial. It is common for airports to provide moving walkways and buses. The Hartsfield–Jackson Atlanta International Airport has a tram that takes people through the concourses and baggage claim. Major airports with more than one terminal offer inter-terminal transportation, such as Mexico City International Airport, where the domestic building of Terminal 1 is connected by Aerotrén to Terminal 2, on the other side of the airport.
History and development.
The earliest aircraft takeoff and landing sites were grassy fields. The plane could approach at any angle that provided a favorable wind direction. A slight improvement was the dirt-only field, which eliminated the drag from grass. However, these only functioned well in dry conditions. Later, concrete surfaces would allow landings, rain or shine, day or night.
The title of "world's oldest airport" is disputed, but College Park Airport in Maryland, US, established in 1909 by Wilbur Wright, is generally agreed to be the world's oldest continually operating airfield, although it serves only general aviation traffic. Bisbee-Douglas International Airport in Arizona was declared "the first international airport of the Americas" by US president Franklin D. Roosevelt in 1943.
Pearson Field Airport in Vancouver, Washington had a dirigible land in 1905 and planes in 1911 and is still in use. Bremen Airport opened in 1913 and remains in use, although it served as an American military field between 1945 and 1949. Amsterdam Airport Schiphol opened on September 16, 1916 as a military airfield, but only accepted civil aircraft from December 17, 1920, allowing Sydney Airport in Sydney, Australia—which started operations in January 1920—to claim to be one of the world's oldest continually operating commercial airports. Minneapolis-Saint Paul International Airport in Minneapolis-Saint Paul, Minnesota, opened in 1920 and has been in continuous commercial service since. It serves about 35,000,000 passengers each year and continues to expand, recently opening a new 11,000 foot (3,355 meter) runway. Of the airports constructed during this early period in aviation, it is one of the largest and busiest that is still currently operating. Rome Ciampino Airport, opened 1916, is also a contender, as well as the Don Mueang International Airport near Bangkok,Thailand, which opened in 1914.
Increased aircraft traffic during World War I led to the construction of landing fields. Aircraft had to approach these from certain directions and this led to the development of aids for directing the approach and landing slope.
Following the war, some of these military airfields added civil facilities for handling passenger traffic. One of the earliest such fields was Paris – Le Bourget Airport at Le Bourget, near Paris. The first airport to operate scheduled international commercial services was Hounslow Heath Aerodrome in August 1919, but it was closed and supplanted by Croydon Airport in March 1920. In 1922, the first permanent airport and commercial terminal solely for commercial aviation was opened at Flughafen Devau near what was then Königsberg, East Prussia. The airports of this era used a paved "apron", which permitted night flying as well as landing heavier aircraft.
The first lighting used on an airport was during the latter part of the 1920s; in the 1930s approach lighting came into use. These indicated the proper direction and angle of descent. The colours and flash intervals of these lights became standardized under the International Civil Aviation Organization (ICAO). In the 1940s, the slope-line approach system was introduced. This consisted of two rows of lights that formed a funnel indicating an aircraft's position on the glideslope. Additional lights indicated incorrect altitude and direction.
After World War II, airport design became more sophisticated. Passenger buildings were being grouped together in an island, with runways arranged in groups about the terminal. This arrangement permitted expansion of the facilities. But it also meant that passengers had to travel further to reach their plane.
An improvement in the landing field was the introduction of grooves in the concrete surface. These run perpendicular to the direction of the landing aircraft and serve to draw off excess water in rainy conditions that could build up in front of the plane's wheels.
Airport construction boomed during the 1960s with the increase in jet aircraft traffic. Runways were extended out to 3000 m. The fields were constructed out of reinforced concrete using a slip-form machine that produces a continual slab with no disruptions along the length. The early 1960s also saw the introduction of jet bridge systems to modern airport terminals, an innovation which eliminated outdoor passenger boarding. These systems became commonplace in the United States by the 1970s.
Airport designation and naming.
Airports are uniquely represented by their IATA airport code and ICAO airport code.
Most airport names include the location. Many airport names honour a public figure, commonly a politician (e.g. Paris-Charles de Gaulle Airport) or a prominent figure in aviation history of the region (e.g. Will Rogers World Airport).
Some airports have unofficial names, possibly so widely circulated that its official name is little used or even known.
Some airport names include the word "International" to indicate their ability to handle international air traffic. This includes some airports that do not have scheduled airline services (e.g. Texel International Airport).
Airport security.
Airport security normally requires baggage checks, metal screenings of individual persons, and rules against any object that could be used as a weapon. Since the September 11, 2001 attacks, airport security has dramatically increased.
Airport operations.
Air traffic control.
The majority of the world's airports are non-towered, with no air traffic control presence. However, at particularly busy airports, or airports with other special requirements, there is an air traffic control (ATC) system whereby controllers (usually ground-based) direct aircraft movements via radio or other communications links. This coordinated oversight facilitates safety and speed in complex operations where traffic moves in all three dimensions. Air traffic control responsibilities at airports are usually divided into at least two main areas: "ground" and "tower", though a single controller may work both stations. The busiest airports also have "clearance delivery", "apron control", and other specialized ATC stations.
Ground Control is responsible for directing all ground traffic in designated "movement areas", except the traffic on runways. This includes planes, baggage trains, snowplows, grass cutters, fuel trucks, stair trucks, airline food trucks, conveyor belt vehicles and other vehicles. Ground Control will instruct these vehicles on which taxiways to use, which runway they will use (in the case of planes), where they will park, and when it is safe to cross runways. When a plane is ready to takeoff it will stop short of the runway, at which point it will be turned over to Tower Control. After a plane has landed, it will depart the runway and be returned to Ground Control.
Tower Control controls aircraft on the runway and in the controlled airspace immediately surrounding the airport. Tower controllers may use radar to locate an aircraft's position in three-dimensional space, or they may rely on pilot position reports and visual observation. They coordinate the sequencing of aircraft in the traffic pattern and direct aircraft on how to safely join and leave the circuit. Aircraft which are only passing through the airspace must also contact Tower Control in order to be sure that they remain clear of other traffic.
Traffic pattern.
At all airports the use of an traffic pattern (often called a "traffic circuit" outside the U.S.) is possible. They may help to assure smooth traffic flow between departing and arriving aircraft. There is no technical need whithin modern aviation for performing this pattern, "provided there is no queue". And due to the so-called SLOT-times, the overall traffic planning tend to assure landing queues are avoided. If for instance an aircraft approaches runway 17 (which has a heading of approx. 170 degrees) from the north (coming from 360/0 degrees heading towards 180 degrees), the aircraft will land as fast as possible by just turning 10 degrees and follow the glidepath, without orbit the runway for visual reasons, whenever this is possible. For smaller piston engined airplanes at smaller airfields without ILS equipment, things are very differently though.
Generally, this pattern is a circuit consisting of five "legs" that form a rectangle (two legs and the runway form one side, with the remaining legs forming three more sides). Each leg is named (see diagram), and ATC directs pilots on how to join and leave the circuit. Traffic patterns are flown at one specific altitude, usually 800 or above ground level (AGL). Standard traffic patterns are "left-handed", meaning all turns are made to the left. One of the main reason for this is that pilots sit on the left side of the airplane, and a Left-hand patterns improves their visibility of the airport and pattern. Right-handed patterns do exist, usually because of obstacles such as a mountain, or to reduce noise for local residents. The predetermined circuit helps traffic flow smoothly because all pilots know what to expect, and helps reduce the chance of a mid-air collision.
At extremely large airports, a circuit is in place but not usually used. Rather, aircraft (usually only commercial with long routes) request approach clearance while they are still hours away from the airport, often before they even takeoff from their departure point. Large airports have a frequency called "Clearance Delivery" which is used by departing aircraft specifically for this purpose. This then allows aircraft to take the most direct approach path to the runway and land without worrying about interference from other aircraft. While this system keeps the airspace free and is simpler for pilots, it requires detailed knowledge of how aircraft are planning to use the airport ahead of time and is therefore only possible with large commercial airliners on pre-scheduled flights. The system has recently become so advanced that controllers can predict whether an aircraft will be delayed on landing before it even takes off; that aircraft can then be delayed on the ground, rather than wasting expensive fuel waiting in the air.
Navigational aids.
There are a number of aids available to pilots, though not all airports are equipped with them. A Visual Approach Slope Indicator (VASI) helps pilots fly the approach for landing. Some airports are equipped with a VHF omnidirectional range (VOR) to help pilots find the direction to the airport. VORs are often accompanied by a distance measuring equipment (DME) to determine the distance to the VOR. VORs are also located off airports, where they serve to provide airways for aircraft to navigate upon. In poor weather, pilots will use an instrument landing system (ILS) to find the runway and fly the correct approach, even if they cannot see the ground. The number of instrument approaches based on the use of the Global Positioning System (GPS) is rapidly increasing and may eventually be the primary means for instrument landings.
Larger airports sometimes offer precision approach radar (PAR), but these systems are more common at military air bases than civilian airports. The aircraft's horizontal and vertical movement is tracked via radar, and the controller tells the pilot his position relative to the approach slope. Once the pilots can see the runway lights, they may continue with a visual landing.
Taxiway signs.
Airport guidance signs provide direction and information to taxiing aircraft and airport vehicles. Smaller aerodromes may have few or no signs, relying instead on diagrams and charts.
Lighting.
Many airports have lighting that help guide planes using the runways and taxiways at night or in rain or fog.
On runways, green lights indicate the beginning of the runway for landing, while red lights indicate the end of the runway. Runway edge lighting consists of white lights spaced out on both sides of the runway, indicating the edge. Some airports have more complicated lighting on the runways including lights that run down the centerline of the runway and lights that help indicate the approach (an approach lighting system, or ALS). Low-traffic airports may use pilot controlled lighting to save electricity and staffing costs.
Along taxiways, blue lights indicate the taxiway's edge, and some airports have embedded green lights that indicate the centerline.
Weather observations.
Weather observations at the airport are crucial to safe takeoffs and landings. In the US and Canada, the vast majority of airports, large and small, will either have some form of automated airport weather station, whether an AWOS, ASOS, or AWSS, a human observer or a combination of the two. These weather observations, predominantly in the METAR format, are available over the radio, through Automatic Terminal Information Service (ATIS), via the ATC or the Flight Service Station.
Planes take-off and land "into" the wind in order to achieve maximum performance. Because pilots need instantaneous information during landing, a windsock is also kept in view of the runway.
Safety management.
Air safety is an important concern in the operation of an airport, and almost every airfield includes equipment and procedures for handling emergency situations. Airport crash tender crews are equipped for dealing with airfield accidents, crew and passenger extractions, and the hazards of highly flammable aviation fuel. The crews are also trained to deal with situations such as bomb threats, hijacking, and terrorist activities.
Hazards to aircraft include debris, nesting birds, and reduced friction levels due to environmental conditions such as ice, snow, or rain. Part of runway maintenance is airfield rubber removal which helps maintain friction levels. The fields must be kept clear of debris using cleaning equipment so that loose material does not become a projectile and enter an engine duct (see foreign object damage). In adverse weather conditions, ice and snow clearing equipment can be used to improve traction on the landing strip. For waiting aircraft, equipment is used to spray special deicing fluids on the wings.
Many airports are built near open fields or wetlands. These tend to attract bird populations, which can pose a hazard to aircraft in the form of bird strikes. Airport crews often need to discourage birds from taking up residence.
Some airports are located next to parks, golf courses, or other low-density uses of land. Other airports are located near densely populated urban or suburban areas.
An airport can have areas where collisions between aircraft on the ground tend to occur. Records are kept of any incursions where aircraft or vehicles are in an inappropriate location, allowing these "hot spots" to be identified. These locations then undergo special attention by transportation authorities (such as the FAA in the US) and airport administrators.
During the 1980s, a phenomenon known as microburst became a growing concern due to aircraft accidents caused by microburst wind shear, such as Delta Air Lines Flight 191. Microburst radar was developed as an aid to safety during landing, giving two to five minutes warning to aircraft in the vicinity of the field of a microburst event.
Some airfields now have a special surface known as soft concrete at the end of the runway (stopway or blastpad) that behaves somewhat like styrofoam, bringing the plane to a relatively rapid halt as the material disintegrates. These surfaces are useful when the runway is located next to a body of water or other hazard, and prevent the planes from overrunning the end of the field.
Airport ground crew.
Most airports have groundcrew handling the loading and unloading of passengers, crew, baggage and other services. Some groundcrew are linked to specific airlines operating at the airport.
Many ground crew at the airport work at the aircraft. A tow tractor pulls the aircraft to one of the airbridges, The ground power unit is plugged in. It keeps the electricity running in the plane when it stands at the terminal. The engines are not working, therefore they do not generate the electricity, as they do in flight. The passengers disembark using the airbridge. Mobile stairs can give the ground crew more access to the aircraft's cabin. There is a cleaning service to clean the aircraft after the aircraft lands. Flight catering provides the food and drinks on flights. A toilet waste truck removes the human waste from the tank which holds the waste from the toilets in the aircraft. A water truck fills the water tanks of the aircraft. A fuel transfer vehicle transfers aviation fuel from fuel tanks underground, to the aircraft tanks. A tractor and its dollies bring in luggage from the terminal to the aircraft. They also carry luggage to the terminal if the aircraft has landed, and is being unloaded. Hi-loaders lift the heavy luggage containers to the gate of the cargo hold. The ground crew push the luggage containers into the hold. If it has landed, they rise, the ground crew push the luggage container on the hi-loader, which carries it down. The luggage container is then pushed on one of the tractors dollies. The conveyor, which is a conveyor belt on a truck, brings in the awkwardly shaped, or late luggage. The airbridge is used again by the new passengers to embark the aircraft. The tow tractor pushes the aircraft away from the terminal to a taxi area. The aircraft should be off of the airport and in the air in 90 minutes. The airport charges the airline for the time the aircraft spends at the airport.
Environmental concerns.
Aircraft noise is a major cause of noise disturbance to residents living near airports. Sleep can be affected if the airports operate night and early morning flights. Aircraft noise not only occurs from take-off and landings, but also ground operations including maintenance and testing of aircraft. Noise can have other noise health effects. Other noise and environmental concerns are vehicle traffic causing noise and pollution on roads leading the airport.
The construction of new airports or addition of runways to existing airports, is often resisted by local residents because of the effect on countryside, historical sites, local flora and fauna. Due to the risk of collision between birds and aircraft, large airports undertake population control programs where they frighten or shoot birds.
The construction of airports has been known to change local weather patterns. For example, because they often flatten out large areas, they can be susceptible to fog in areas where fog rarely forms. In addition, they generally replace trees and grass with pavement, they often change drainage patterns in agricultural areas, leading to more flooding, run-off and erosion in the surrounding land.
Some of the airport administrations prepare and publish annual environmental reports in order to show how they consider these environmental concerns in airport management issues and how they protect environment from airport operations. These reports contain all environmental protection measures performed by airport administration in terms of water, air, soil and noise pollution, resource conservation and protection of natural life around the airport.
Military airbase.
An airbase, sometimes referred to as an "air station" or "airfield", provides basing and support of military aircraft. Some airbases, known as "military airports", provide facilities similar to their civilian counterparts. For example, RAF Brize Norton in the UK has a terminal which caters to passengers for the Royal Air Force's scheduled TriStar flights to the Falkland Islands. Some airbases are co-located with civilian airports, sharing the same ATC facilities, runways, taxiways and emergency services, but with separate terminals, parking areas and hangars. Bardufoss Airport , Bardufoss Air Station in Norway and Pune Airport in India are examples of this.
An aircraft carrier is a warship that functions as a mobile airbase. Aircraft carriers allow a naval force to project air power without having to depend on local bases for land-based aircraft. After their development in World War I, aircraft carriers replaced the battleship as the centrepiece of a modern fleet during World War II.
Airports in entertainment.
Airports have played major roles in films and television programs due to their very nature as a transport and international hub, and sometimes because of distinctive architectural features of particular airports. One such example of this is "The Terminal", a film about a man who becomes permanently grounded in an airport terminal and must survive only on the food and shelter provided by the airport. They are also one of the major elements in movies such as "The V.I.P.s", "Airplane!", "Airport" (1970), "Die Hard 2", "Soul Plane", "Jackie Brown", "Get Shorty", "Home Alone", "Liar Liar", "Passenger 57", "Final Destination" (2000), "Unaccompanied Minors", "Catch Me If You Can", "Rendition" and "The Langoliers". They have also played important parts in television series like "Lost", "The Amazing Race", "America's Next Top Model, Cycle 10" which have significant parts of their story set within airports. In other programmes and films, airports are merely indicative of journeys, e.g. "Good Will Hunting".
Several computer simulation games put the player in charge of an airport. These include the Airport Tycoon series.
Airport directories.
Each national aviation authority has a source of information about airports in their country. This will contain information on airport elevation, airport lighting, runway information, communications facilities and frequencies, hours of operation, nearby NAVAIDs and contact information where prior arrangement for landing is necessary.
See also.
Lists:
References.
</dl>

</doc>
<doc id="37579" url="http://en.wikipedia.org/wiki?curid=37579" title="1996 European Men's Handball Championship">
1996 European Men's Handball Championship

The 1996 EHF European Men's Handball Championship (2nd tournament) was held in Spain from 24 May–2 June, in the cities of Ciudad Real and Seville. Russia won the tournament with Spain second and Yugoslavia the third.

</doc>
<doc id="37584" url="http://en.wikipedia.org/wiki?curid=37584" title="List of wine-producing regions">
List of wine-producing regions

This list of wine-producing regions catalogues significant growing regions where vineyards are planted. Wine grapes mostly grow between the 30th and the 50th degree of latitude, in both the Northern and Southern hemispheres. Grapes will sometimes grow beyond this range and minor amounts of wine are made in some very unexpected places.
In 2009, the five largest producers of wine in the world were, in order, Italy, France, Spain, the United States and China (see list of wine-producing countries for a complete rank).
Europe.
Romania.
Moldavia wine regions
Muntenia wine regions
Oltenia wine regions
Transylvania wine regions
Crişana wine regions
Banat wine regions
Dobrogea wine regions
Slovakia.
Whole of southern Slovakia
Ukraine.
In Ukraine, at the present time there are seven administrative regions (provinces) in which the wine industry developed at a very good level. 
Given the favorable climatic location, under the law of Ukraine allocated 15 wine-growing areas (macrozones), which are the basis for growing certain varieties of grapes and 58 natural wine regions (microzones). Especially in:
United Kingdom.
In the UK, area under vines is small, and whilst viticulture isn't a major part of the rural economy, significant planting of new vines has been made in the early 21st century. The greatest concentration of vineyards is found in the south east of England, in the counties of Hampshire, Kent, Surrey, and Sussex.
Asia.
China.
Regions producing native wines have been present since the Qin Dynasty, with wines being brought to China from Persia. Some of the more famous wine-producing regions are:
With the import of Western wine-making technologies, especially French technology, production of wines similar to modern French wine has begun in many parts of China with the direction of experienced French wine-makers; China is now the sixth largest producer of wine in the world. The following regions produce significant quality of wine:
Indonesia.
Indonesia has been producing wine for over 18 years, with the North of Bali's vineyards producing 3 main grape varieties, the Belgia, the Alphonse Lavallee and the Probolinggo Biru grapes. One main producer has revolutionalize the world of winemaking with 8 wines produced out of these 3 varieties, Hatten Wines.
Iran.
Prior to the Iranian Islamic Revolution of 1979, Iran was a producer of wine. While production has stopped, the vineyards continue to exist and their product diverted to non alcoholic purposes.
Israel.
"Also includes wine regions in Israeli-occupied territories."*
Oceania.
Australia.
Geographic indications for Australian wine are governed by law. The geographic indication must indicate where the "grapes" are grown, irrespective of where the wine itself is made. A geographic indication may be "Australia", "South Eastern Australia", a state name, zone, region or subregion if defined.
The zones, regions and subregions in each state are listed below:
"Adelaide Super Zone includes Mount Lofty Ranges, Fleurieu and Barossa wine zones"

</doc>
<doc id="37595" url="http://en.wikipedia.org/wiki?curid=37595" title="Weasel">
Weasel

Weasels are mammals forming the genus Mustela of the Mustelidae family. The genus includes the least weasels, European polecats, stoats, ferrets and European minks. Members of this genus are small, active predators, with long and slender bodies and short legs. The Mustelidae family (which also includes badgers, otters, and wolverines) is often referred to as the weasel family. In the UK, the term "weasel" usually refers to the smallest species "Mustela nivalis" (also known as the Least Weasel).
Weasels vary in length from 173 to, females being smaller than the males, and usually have red or brown upper coats and white bellies; some populations of some species moult to a wholly white coat in winter. They have long, slender bodies, which enable them to follow their prey into burrows. Their tails may be from 34 to long.
Weasels feed on small mammals, and have from time to time been considered vermin, since some species took poultry from farms, or rabbits from commercial warrens. They can be found all across the world except for Antarctica, Australia, and neighbouring islands.
Terminology.
The English word "weasel" was originally applied to one species of the genus, the European form of the least weasel ("Mustela nivalis"). This usage is retained in British English, where the name is also extended to cover several other small species of the genus. However, in technical discourse and in American usage, the term "weasel" can refer to any member of the genus, or to the genus as a whole. Of the 17 extant species currently classified in the genus "Mustela", ten have "weasel" in their common names. Among those that do not are the stoat, the polecats, the ferret, and the European mink. (The superficially similar American mink is now regarded as belonging in another genus, "Neovison".)
Diet.
The diet of Irish stoats includes eggs. Prey included: rabbits, pygmy shrews, field mice, brown rats, house mice, bank voles.
Species.
The following information is according to the Integrated Taxonomic Information System.
1 Europe and northern Asia division excludes China.
The extinct "sea mink" was commonly included in this genus as "Mustela macrodon", but in 1999 was moved to the genus "Neovison".
Cultural meanings.
Weasels have been assigned a variety of different cultural meanings. 
In Greek culture, a weasel near the house is a sign of bad luck, even evil, "especially if there is in the household a girl about to be married", since the animal (based on its Greek etymology) was thought to be an unhappy bride who was transformed into a weasel and consequently delights in destroying wedding dresses. In neighboring Macedonia, however, weasels are generally seen as an omen of good fortune.
In early modern Mecklenburg, Germany, amulets from weasels were deemed to have strong magic; the period between August 15 and September 8 was specifically designated for the killing of weasels. 
In Montagne Noire (France), Ruthenia (Eastern Europe), and the early medieval culture of the Wends, weasels were not meant to be killed.
In North America, native Americans deemed the weasel to be a bad sign; crossing its path meant a "speedy death". According to Daniel Defoe also, meeting a weasel is a bad omen. In English-speaking areas, weasel can be a disparaging term, noun or verb, for someone regarded as sneaky, conniving or untrustworthy. Similarly, weasel words is a critical term for words or phrasing that are vague, misleading or equivocal. 
Japanese folklore.
In Japan, weasels (鼬、鼬鼠, itachi) were seen as yōkai from time immemorial, and they cause various strange occurrences. According to the encyclopedia Wakan Sansai Zue from the Edo period, a nate of weasels would cause conflagrations, and the cry of a weasel was considered a harbinger of misfortune. In the Niigata Prefecture, the sound of a nate of weasels making a rustle resembled 6 people hulling rice, and therefore was called the "the weasel's six-person mortar", and it was an omen for one's home to decline or flourish. It is said that when people chase after this sound, the sound stops.
They are also said to shapeshift like the fox (kitsune) or tanuki, and the nyūdō-bōzu told about in legends in the Tōhoku region and the Chūbu region are considered weasels in disguise, and they are also said to shapeshift into ōnyūdō and little monks.
In the collection of depictions, the Gazu Hyakki Yagyō by Sekien Toriyama, they were depicted under the title 鼬, but they were read not as "itachi" but rather as "ten", and "ten" were considered to be weasels that have reached one hundred years of age and became yōkai that possessed supernatural powers. Another theory is that when weasels reach several hundred years of age, they become mujina.
In Japanese weasels are called "iizuna" or "izuna" (飯綱) and in the Tōhoku Region and Shinshū, it was believed that there were families that were able to use a certain practice to freely use kudagitsune as "iizuna-tsukai" or "kitsune-mochi". It is said that Mount Iizuna, from the Nagano Prefecture, got its name due to how the gods gave people mastery of this technique from there.
According to the folkloristician Mutō Tetsujō, "They are called 'izuna' in the Senboku District, Akita Prefecture, and there are also the ichiko (itako) that use them." Also, in the Kitaakita District, they are called mōsuke (猛助), and they are feared as yōkai even more than foxes (kitsune).
In the Ainu language, ermines are called "upas-čironnup" or "sáčiri", but since least weasels are also called "sáčiri", Mashio Chiri surmised that the honorary title "poy-sáčiri-kamuy" (where "poy" means "small") refers to least weasels.
Kamaitachi.
Kamaitachi are a phenomenon wherein one who is idle is suddenly injured, as if his or her skin were cut by a scythe. In the past, this was thought to be "the deed of an invisible yōkai weasel". However, this has been established as a physiological phenomenon that dried skin that receives a shock would tear off. An alternate theory, asserts that "kamaitachi" are derived from "kamae tachi (構え太刀, "stance sword")", and therefore were not originally related to weasels at all.
In popular culture.
Literature.
In Phil Drabble's superb tale "A weasel in my meat safe" (1979), the book alleged a weasel can make the most endearing pet.

</doc>
<doc id="37673" url="http://en.wikipedia.org/wiki?curid=37673" title="Symbol">
Symbol

A symbol is an object that represents, stands for, or suggests an idea, visual image, belief, action, or material entity. Symbols take the form of words, sounds, gestures, or visual images and are used to convey ideas and beliefs. For example, a red octagon may be a symbol for "STOP". On a map, a picture of a tent might represent a campsite. Numerals are symbols for numbers. Alphabetic letters are symbols for sounds. Personal names are symbols representing individuals. A red rose symbolizes love and compassion. 
In cartography, an organized collection of symbols forms a legend for a map.
Etymology.
The word derives from the Greek "symbolon" meaning token or watchword. It is an amalgam of syn- "together" + bole "a throwing, a casting, the stroke of a missile, bolt, beam." The sense evolution in Greek is from "throwing things together" to "contrasting" to "comparing" to "token used in comparisons to determine if something is genuine." Hence, "outward sign" of something. The meaning "something which stands for something else" was first recorded in 1590, in Edmund Spenser's "Faerie Queene".
Definitions.
In considering the effect of a symbol on the psyche, in his seminal essay "The Symbol without Meaning" Joseph Campbell proposes the following definition:
"A symbol is an energy evoking, and directing, agent".
Later, expanding on what he means by this definition Campbell says:
Heinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.
In the book "Signs and Symbols, "it is stated that "A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth."
Symbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.
Human cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.
Symbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.
Symbols and semiotics.
Semiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.
Psychoanalysis, rhetoric, and archetypes.
Swiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term "sign". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with "symbol", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called "self". For example, written languages are composed of a variety of different symbols that create words. Through these written words humans communicate with each other. Kenneth Burke described "Homo sapiens" as a "symbol-using, symbol making, and symbol misusing animal" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of "blubber" representing something inedible in his mind. In addition, the symbol of "blubber" was created by the man through various kinds of learning.
Burke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to "normal symbol systems". He says they are related through "substitution", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.
Jean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls "core images" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.
William Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfill psychological functions and hence are why archetypes such as "the hero," "the princess" and "the witch" have remained popular for centuries.
Paul Tillich.
Paul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always "points beyond itself" to something that is unquantifiable and mysterious: the symbol's "depth dimension". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol. The Greek Gods might be an example of symbols that were once living for the ancient Greeks but whose meaning and power are now gone.
When a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the "symbol is taken for reality." The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.
Role of context in symbolism.
A symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.
Historical meaning.
This history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.
Context.
The context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.
Symbolic action.
A symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.
Symbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.
In response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.
Symbolic actions are sometimes derided as slacktivism.

</doc>
<doc id="37684" url="http://en.wikipedia.org/wiki?curid=37684" title="Libertarian Movement (Costa Rica)">
Libertarian Movement (Costa Rica)

The Libertarian Movement Party (Spanish: "Partido Movimiento Libertario"; PML) is a political party based on classical liberalism in Costa Rica.
It was founded in May 1994 and has since enjoyed a number of victories. It succeeded in getting attorney Otto Guevara elected to the Legislative Assembly in its first campaign in 1998. In 2002, Guevara ran for president (unsuccessfully, 1.7% of the vote), and the party at the legislative elections won 9.3% of the popular vote and 6 out of 57 seats. A few weeks after taking office, one Congressman left the party and became independent, leaving PML with five seats. In 2006, Guevara again ran for president (unsuccessfully, 8.4% of the vote), and the party at the legislative elections won 9.1% of the popular vote and 6 out of 57 seats. In the 2010 general election Guevara was again the PML's presidential candidate and received 20% of the popular vote.
In its 2014 electoral campaign, the party has taken a more socially conservative position, totally opposing the legalisation of abortion and rejecting homosexual couples' right to a marriage license.
Purpose.
The party claims to represent hundreds of thousands of Costa Rican citizens from all walks of life, tired of politics, parties, traditional politicians, and the country's deteriorating situation.

</doc>
<doc id="37701" url="http://en.wikipedia.org/wiki?curid=37701" title="Outline of South Asian history">
Outline of South Asian history

The term South Asia refers to the contemporary political entities of the Indian subcontinent and associated islands. These are the states of India, Pakistan, Bangladesh, Nepal, Afghanistan, Bhutan, and the island nations of Sri Lanka and the Maldives.
The following is a list of articles on the history of the various regions of South Asia. See History of India for a general history of the entire subcontinent.
Sources.
</dl>

</doc>
<doc id="37712" url="http://en.wikipedia.org/wiki?curid=37712" title="Chemical patent">
Chemical patent

A chemical patent, pharmaceutical patent or drug patent is a patent for an invention in the chemical or pharmaceuticals industry. Strictly speaking, in most jurisdictions, there are essentially no differences between the legal requirements to obtain a patent for an invention in the chemical or pharmaceutical fields, in comparison to obtaining a patent in the other fields, such as in the mechanical field. A chemical patent or a pharmaceutical patent is therefore "not" a "sui generis" right, i.e. a special legal type of patent.
In the pharmaceutical industry, the patent protection of drugs and medicines is accorded a particular importance, because drugs and medicines can easily be copied or imitated (by analyzing a pharmaceutical substance) and because of the significant research and development spending and the high risks associated with the development of a new drug.
Chemical patents are different from other sources of technical information because of the generic, Markush structures contained within them, named after the inventor Eugene Markush who won a claim in the US in 1925 to allow such structures to be used in patent claims. These generic structures are used to make the patent claim as broad as possible.

</doc>
<doc id="37713" url="http://en.wikipedia.org/wiki?curid=37713" title="The Merchant of Venice">
The Merchant of Venice

The Merchant of Venice is a play by William Shakespeare in which a merchant in 16th century Venice must default on a large loan provided by an abused Jewish moneylender. It is believed to have been written between 1596 and 1598. Though classified as a comedy in the First Folio and sharing certain aspects with Shakespeare's other romantic comedies, the play is perhaps most remembered for its dramatic scenes, and is best known for Shylock and the famous "Hath not a Jew eyes?" speech. Also notable is Portia's speech about "the quality of mercy".
Summary.
Bassanio, a young Venetian of noble rank, wishes to woo the beautiful and wealthy heiress Portia of Belmont. Having squandered his estate, he needs 3,000 ducats to subsidise his expenditures as a suitor. Bassanio approaches his friend Antonio, a wealthy merchant of Venice who has previously and repeatedly bailed him out. Antonio agrees, but since he is cash-poor – his ships and merchandise are busy at sea – he promises to cover a bond if Bassanio can find a lender, so Bassanio turns to the Jewish moneylender Shylock and names Antonio as the loan's guarantor.
Antonio has already antagonized Shylock through his outspoken antisemitism, and because Antonio's habit of lending money without interest forces Shylock to charge lower rates. Shylock is at first reluctant to grant the loan, citing abuse he has suffered at Antonio's hand. He finally agrees to lend the sum to Antonio without interest upon one condition: if Antonio is unable to repay it at the specified date, Shylock may take a pound of Antonio's flesh. Bassanio does not want Antonio to accept such a risky condition; Antonio is surprised by what he sees as the moneylender's generosity (no "usance" – interest – is asked for), and he signs the contract. With money at hand, Bassanio leaves for Belmont with his friend Gratiano, who has asked to accompany him. Gratiano is a likeable young man, but is often flippant, overly talkative, and tactless. Bassanio warns his companion to exercise self-control, and the two leave for Belmont and Portia.
Meanwhile in Belmont, Portia is awash with suitors. Her father left a will stipulating each of her suitors must choose correctly from one of three caskets – one each of gold, silver and lead. If he picks the right casket, he gets Portia. The first suitor, the Prince of Morocco, chooses the gold casket, interpreting its slogan, "Who chooseth me shall gain what many men desire," as referring to Portia. The second suitor, the conceited Prince of Arragon, chooses the silver casket, which proclaims, "Who chooseth me shall get as much as he deserves", as he believes he is full of merit. Both suitors leave empty-handed, having rejected the lead casket because of the baseness of its material and the uninviting nature of its slogan, "Who chooseth me must give and hazard all he hath." The last suitor is Bassanio, whom Portia wishes to succeed, having met him before. As Bassanio ponders his choice, members of Portia's household sing a song which says that "fancy" (not true love) is "engend'red in the eyes, With gazing fed.", Bassanio chooses the lead casket and wins Portia's hand.
At Venice, Antonio's ships are reported lost at sea so the merchant cannot repay the bond. Shylock has become more determined to exact revenge from Christians because his daughter Jessica eloped with the Christian Lorenzo and converted. She took a substantial amount of Shylock's wealth with her, as well as a turquoise ring which Shylock had been given by his late wife, Leah. Shylock has Antonio brought before court.
At Belmont, Bassanio receives a letter telling him that Antonio has been unable to repay the loan from Shylock. Portia and Bassanio marry, as do Gratiano and Portia's handmaid Nerissa. Bassanio and Gratiano leave for Venice, with money from Portia, to save Antonio's life by offering the money to Shylock. Unknown to Bassanio and Gratiano, Portia sent her servant, Balthazar, to seek the counsel of Portia's cousin, Bellario, a lawyer, at Padua.
The climax of the play takes place in the court of the Duke of Venice. Shylock refuses Bassanio's offer of 6,000 ducats, twice the amount of the loan. He demands his pound of flesh from Antonio. The Duke, wishing to save Antonio but unable to nullify a contract, refers the case to a visitor. He identifies himself as Balthazar, a young male "doctor of the law", bearing a letter of recommendation to the Duke from the learned lawyer Bellario. The doctor is Portia in disguise, and the law clerk who accompanies her is Nerissa, also disguised as a man. As Balthazar, Portia repeatedly asks Shylock to show mercy in a famous speech, advising him that mercy "is twice blest: It blesseth him that gives and him that takes" (IV, i, 185). However, Shylock adamantly refuses any compensations and insists on the pound of flesh.
As the court grants Shylock his bond and Antonio prepares for Shylock's knife, Portia deftly appropriates Shylock's argument for 'specific performance.' She says that the contract allows Shylock only to remove the "flesh", not the "blood", of Antonio (see quibble). Thus, if Shylock were to shed any drop of Antonio's blood, his "lands and goods" would be forfeited under Venetian laws. She tells him that he must cut precisely one pound of flesh, no more, no less; she advises him that "if the scale do turn, But in the estimation of a hair, Thou diest and all thy goods are confiscate."
Defeated, Shylock concedes to accepting Bassanio's offer of money for the defaulted bond, first his offer to pay "the bond thrice", which Portia rebuffs, telling him to take his bond, and then merely the principal, which Portia also prevents him from doing on the ground that he has already refused it "in the open court." She cites a law under which Shylock, as a Jew and therefore an "alien", having attempted to take the life of a citizen, has forfeited his property, half to the government and half to Antonio, leaving his life at the mercy of the Duke. The Duke pardons Shylock's life. Antonio asks for his share "in use" until Shylock's death, when the principal will be given to Lorenzo and Jessica. At Antonio's request, the Duke grants remission of the state's half of forfeiture, but on the condition that Shylock convert to Christianity and bequeath his entire estate to Lorenzo and Jessica (IV,i).
Bassanio does not recognise his disguised wife, but offers to give a present to the supposed lawyer. First she declines, but after he insists, Portia requests his ring and Antonio's gloves. Antonio parts with his gloves without a second thought, but Bassanio gives the ring only after much persuasion from Antonio, as earlier in the play he promised his wife never to lose, sell or give it. Nerissa, as the lawyer's clerk, succeeds in likewise retrieving her ring from Gratiano, who does not see through her disguise.
At Belmont, Portia and Nerissa taunt and pretend to accuse their husbands before revealing they were really the lawyer and his clerk in disguise (V). After all the other characters make amends, Antonio learns from Portia that three of his ships were not stranded and have returned safely after all.
Sources.
The forfeit of a merchant's deadly bond after standing surety for a friend's loan was a common tale in England in the late 16th century. In addition, the test of the suitors at Belmont, the merchant's rescue from the "pound of flesh" penalty by his friend's new wife disguised as a lawyer, and her demand for the betrothal ring in payment are all elements present in the 14th-century tale "" by Giovanni Fiorentino, which was published in Milan in 1558. Elements of the trial scene are also found in "The Orator" by Alexandre Sylvane, published in translation in 1596. The story of the three caskets can be found in "Gesta Romanorum", a collection of tales probably compiled at the end of the 13th century.
Date and text.
The date of composition for "The Merchant of Venice" is believed to be between 1596 and 1598. The play was mentioned by Francis Meres in 1598, so it must have been familiar on the stage by that date. The title page of the first edition in 1600 states that it had been performed "divers times" by that date. Salerino's reference to his ship the "Andrew" (I,i,27) is thought to be an allusion to the Spanish ship "St. Andrew," captured by the English at Cádiz in 1596. A date of 1596–97 is considered consistent with the play's style.
The play was entered in the Register of the Stationers Company, the method at that time of obtaining copyright for a new play, by James Roberts on 22 July 1598 under the title "The Merchant of Venice", otherwise called "The Jew of Venice". On 28 October 1600 Roberts transferred his right to the play to the stationer Thomas Heyes; Heyes published the first quarto before the end of the year. It was printed again in a pirated edition in 1619, as part of William Jaggard's so-called False Folio. (Afterward, Thomas Heyes' son and heir Laurence Heyes asked for and was granted a confirmation of his right to the play, on 8 July 1619.) The 1600 edition is generally regarded as being accurate and reliable. It is the basis of the text published in the 1623 First Folio, which adds a number of stage directions, mainly musical cues.
Themes.
Shylock and the antisemitism debate.
The play is frequently staged today, but is potentially troubling to modern audiences due to its central themes, which can easily appear antisemitic. Critics today still continue to argue over the play's stance on antisemitism.
Shylock as a villain.
English society in the Elizabethan era has been described as "judeophobic". English Jews had been expelled under Edward I in 1290 and were not permitted to return until 1656 under the rule of Oliver Cromwell. In Venice and in some other places, Jews were required to wear a red hat at all times in public to make sure that they were easily identified, and had to live in a ghetto protected by Christian guards. On the Elizabethan stage, Jews were often presented in an Orientalist caricature, with hooked noses and bright red wigs, and were usually depicted as avaricious usurers; an example is Christopher Marlowe's play "The Jew of Malta", which features a comically wicked Jewish villain called Barabas. They were usually characterised as evil, deceitful and greedy.
Shakespeare's play may be seen as a continuation of this tradition. The title page of the Quarto indicates that the play was sometimes known as "The Jew of Venice" in its day, which suggests that it was seen as similar to Marlowe's "The Jew of Malta". One interpretation of the play's structure is that Shakespeare meant to contrast the mercy of the main Christian characters with the vengefulness of a Jew, who lacks the religious grace to comprehend mercy. Similarly, it is possible that Shakespeare meant Shylock's forced conversion to Christianity to be a "happy ending" for the character, as, to a Christian audience, it saves his soul and allows him to enter Heaven.
Regardless of what Shakespeare's authorial intent may have been, the play has been made use of by antisemites throughout the play's history. One must note that the end of the title in the 1619 edition "With the Extreme Cruelty of Shylock the Jew..." must aptly describe how Shylock was viewed by the English public. The Nazis used the usurious Shylock for their propaganda. Shortly after Kristallnacht in 1938, "The Merchant of Venice" was broadcast for propagandistic ends over the German airwaves. Productions of the play followed in Lübeck (1938), Berlin (1940), and elsewhere within the Nazi Territory.
In a series of articles called "Observer", first published in 1785, British playwright Richard Cumberland created a character named Abraham Abrahams who is quoted as saying, "I verily believe the odious character of Shylock has brought little less persecution upon us, poor scattered sons of Abraham, than the Inquisition itself." Cumberland later wrote a successful play, "The Jew" (1794), in which his title character, Sheva, is portrayed sympathetically, as both a kindhearted and generous man. This was the first known attempt by a dramatist to reverse the negative stereotype that Shylock personified.
The depiction of Jews in literature throughout the centuries bears the close imprint of Shylock. With slight variations much of English literature up until the 20th century depicts the Jew as "a monied, cruel, lecherous, avaricious outsider tolerated only because of his golden hoard".
Shylock as a sympathetic character.
Many modern readers and theatregoers have read the play as a plea for tolerance, noting that Shylock is a sympathetic character. They cite as evidence that Shylock's 'trial' at the end of the play is a mockery of justice, with Portia acting as a judge when she has no right to do so. The characters who berated Shylock for dishonesty resort to trickery in order to win. In addition, Shakespeare gives Shylock one of his most eloquent speeches:
<poem>
To bait fish withal; If it will feed nothing else, it will feed my revenge.
He hath disgraced me and hindered me half a million
Laughed at my losses, mocked at my gains,
Scorned my nation, Thwarted my bargains,
And what's his reason? I am a Jew!
Hath not a Jew eyes? Hath not a Jew hands, organs,
dimensions, senses, affections, passions; fed with
the same food, hurt with the same weapons, subject
to the same diseases, healed by the same means,
warmed and cooled by the same winter and summer
as a Christian is? If you prick us, do we not bleed?
If you tickle us, do we not laugh? If you poison us,
do we not die? And if you wrong us, shall we not revenge?
If we are like you in the rest, we will resemble you in that.
If a Jew wrong a Christian, what is his humility?
Revenge. If a Christian wrong a Jew, what should his
sufferance be by Christian example? Why, revenge.
The villainy you teach me, I will execute,
and it shall go hard but I will better the instruction.
(Act III, scene I)</poem>
It is difficult to know whether the sympathetic reading of Shylock is entirely due to changing sensibilities among readers, or whether Shakespeare, a writer who created complex, multi-faceted characters, deliberately intended this reading.
One of the reasons for this interpretation is that Shylock's painful status in Venetian society is emphasised. To some critics, Shylock's celebrated "Hath Not a Jew eyes?" speech (see above) redeems him and even makes him into something of a tragic figure; in the speech, Shylock argues that he is no different from the Christian characters. Detractors note that Shylock ends the speech with a tone of revenge: "if you wrong us, shall we not revenge?" Those who see the speech as sympathetic point out that Shylock says he learned the desire for revenge from the Christian characters: "If a Christian wrong a Jew, what should his sufferance be by Christian example? Why, revenge. The villainy you teach me, I will execute, and it shall go hard but I will better the instruction."
Even if Shakespeare did not intend the play to be read this way, the fact that it retains its power on stage for audiences who may perceive its central conflicts in radically different terms is an illustration of the subtlety of Shakespeare's characterisations. In the trial Shylock represents what Elizabethan Christians believed to be the Jewish desire for "justice", contrasted with their obviously superior Christian value of mercy. The Christians in the courtroom urge Shylock to love his enemies, although they themselves have failed in the past. Harold Bloom explains that, although the play gives merit to both cases, the portraits are not even-handed: "Shylock’s shrewd indictment of Christian hypocrisy [delights us, but]…Shakespeare’s intimations do not alleviate the savagery of his portrait of the Jew…" However, it can rightly be said that
<poem>
Is Shylock content truly?
Even after losing his religion eternally,
As someone bids farewell,
To a departing soul bound for heaven or hell!
It indeed is arguable,
Yet what my heart longs to tell,
Is that Shylock does deserve penalty,
But his religion certainly is not guilty!</poem>
Antonio, Bassanio.
Antonio's unexplained depression — "In sooth I know not why I am so sad" — and utter devotion to Bassanio has led some critics to theorise that he is suffering from unrequited love for Bassanio and is depressed because Bassanio is coming to an age where he will marry a woman. In his plays and poetry Shakespeare often depicted strong male bonds of varying homosociality, which has led some critics to infer that Bassanio returns Antonio's affections despite his obligation to marry:
<poem>ANTONIO: Commend me to your honourable wife:
Tell her the process of Antonio's end,
Say how I lov'd you, speak me fair in death;
And, when the tale is told, bid her be judge
Whether Bassanio had not once a love.
BASSANIO: But life itself, my wife, and all the world
Are not with me esteemed above thy life;
I would lose all, ay, sacrifice them all
Here to this devil, to deliver you. (IV,i)</poem>
In his essay "Brothers and Others", published in "The Dyer's Hand," W. H. Auden describes Antonio as "a man whose emotional life, though his conduct may be chaste, is concentrated upon a member of his own sex." Antonio's feelings for Bassanio are likened to a couplet from Shakespeare's Sonnets: "But since she pricked thee out for women's pleasure,/ Mine be thy love, and my love's use their treasure." Antonio, says Auden, embodies the words on Portia's leaden casket: "Who chooseth me, must give and hazard all he hath." Antonio has taken this potentially fatal turn because he despairs, not only over the loss of Bassanio in marriage, but also because Bassanio cannot requite what Antonio feels for him. Antonio's frustrated devotion is a form of idolatry: the right to live is yielded for the sake of the loved one. There is one other such idolator in the play: Shylock himself. "Shylock, however unintentionally, did, in fact, hazard all for the sake of destroying the enemy he hated; and Antonio, however unthinkingly he signed the bond, hazarded all to secure the happiness of the man he loved." Both Antonio and Shylock, agreeing to put Antonio's life at a forfeit, stand outside the normal bounds of society. There was, states Auden, a traditional "association of sodomy with usury", reaching back at least as far as Dante, with which Shakespeare was likely familiar. (Auden sees the theme of usury in the play as a comment on human relations in a mercantile society.)
Other interpreters of the play regard Auden's conception of Antonio's sexual desire for Bassanio as questionable. Michael Radford, director of the 2004 film version starring Al Pacino, explained that although the film contains a scene where Antonio and Bassanio actually kiss, the friendship between the two is platonic, in line with the prevailing view of male friendship at the time. Jeremy Irons, in an interview, concurs with the director's view and states that he did not "play Antonio as gay". Joseph Fiennes, however, who plays Bassanio, encouraged a homoerotic interpretation and, in fact, surprised Irons with the kiss on set, which was filmed in one take. Fiennes defended his choice, saying "I would never invent something before doing my detective work in the text. If you look at the choice of language ... you'll read very sensuous language. That's the key for me in the relationship. The great thing about Shakespeare and why he's so difficult to pin down is his ambiguity. He's not saying they're gay or they're straight, he's leaving it up to his actors. I feel there has to be a great love between the two characters ... there's great attraction. I don't think they have slept together but that's for the audience to decide."
Performance history.
The earliest performance of which a record has survived was held at the court of King James in the spring of 1605, followed by a second performance a few days later, but there is no record of any further performances in the 17th century. In 1701, George Granville staged a successful adaptation, titled "The Jew of Venice", with Thomas Betterton as Bassanio. This version (which featured a masque) was popular, and was acted for the next forty years. Granville cut the clownish Gobbos in line with neoclassical decorum; he added a jail scene between Shylock and Antonio, and a more extended scene of toasting at a banquet scene. Thomas Doggett was Shylock, playing the role comically, perhaps even farcically. Rowe expressed doubts about this interpretation as early as 1709; Doggett's success in the role meant that later productions would feature the troupe clown as Shylock.
In 1741, Charles Macklin returned to the original text in a very successful production at Drury Lane, paving the way for Edmund Kean seventy years later (see below).
Arthur Sullivan wrote incidental music for the play in 1871.
Shylock on stage.
Jewish actor Jacob Adler and others report that the tradition of playing Shylock sympathetically began in the first half of the 19th century with Edmund Kean, and that previously the role had been played "by a comedian as a repulsive clown or, alternatively, as a monster of unrelieved evil." Kean's Shylock established his reputation as an actor.
From Kean's time forward, all of the actors who have famously played the role, with the exception of Edwin Booth, who played Shylock as a simple villain, have chosen a sympathetic approach to the character; even Booth's father, Junius Brutus Booth, played the role sympathetically. Henry Irving's portrayal of an aristocratic, proud Shylock (first seen at the Lyceum in 1879, with Portia played by Ellen Terry) has been called "the summit of his career". Jacob Adler was the most notable of the early 20th century: Adler played the role in Yiddish-language translation, first in Manhattan's Yiddish Theater District in the Lower East Side, and later on Broadway, where, to great acclaim, he performed the role in Yiddish in an otherwise English-language production.
Kean and Irving presented a Shylock justified in wanting his revenge; Adler's Shylock evolved over the years he played the role, first as a stock Shakespearean villain, then as a man whose better nature was overcome by a desire for revenge, and finally as a man who operated not from revenge but from pride. In a 1902 interview with "Theater" magazine, Adler pointed out that Shylock is a wealthy man, "rich enough to forgo the interest on three thousand ducats" and that Antonio is "far from the chivalrous gentleman he is made to appear. He has insulted the Jew and spat on him, yet he comes with hypocritical politeness to borrow money of him." Shylock's fatal flaw is to depend on the law, but "would he not walk out of that courtroom head erect, the very apotheosis of defiant hatred and scorn?"
Some modern productions take further pains to show the sources of Shylock's thirst for vengeance. For instance, in the 2004 film adaptation directed by Michael Radford and starring Al Pacino as Shylock, the film begins with text and a montage of how Venetian Jews are cruelly abused by bigoted Christians. One of the last shots of the film also brings attention to the fact that, as a convert, Shylock would have been cast out of the Jewish community in Venice, no longer allowed to live in the ghetto. Another interpretation of Shylock and a vision of how "must he be acted" appears at the conclusion of the autobiography of Alexander Granach, a noted Jewish stage and film actor in Weimar Germany (and later in Hollywood and on Broadway).
Adaptations and cultural references.
Film and TV versions.
The Shakespeare play has inspired several films.
Cultural references.
Arnold Wesker's play "The Merchant" tells the same story from Shylock's point of view. In this retelling, Shylock and Antonio are fast friends bound by a mutual love of books and culture and a disdain for the crass anti-Semitism of the Christian community's laws. They make the bond in defiant mockery of the Christian establishment, never anticipating that the bond might become forfeit. When it does, the play argues, Shylock must carry through on the letter of the law or jeopardise the scant legal security of the entire Jewish community. He is, therefore, quite as grateful as Antonio when Portia, as in Shakespeare's play, shows the legal way out. The play received its American premiere on 16 November 1977 at New York's Plymouth Theatre, with Joseph Leon as Shylock and Marian Seldes as Shylock's sister Rivka. This production had a challenging history in previews on the road, culminating (after the first night out of town in Philadelphia on 8 September 1977) with the death of the larger-than-life Broadway star Zero Mostel, who was initially cast as Shylock. The play's author, Arnold Wesker, wrote a book chronicling the out-of-town tribulations that beset the play and Zero's death called "The Birth of Shylock and the Death of Zero Mostel".
David Henry Wilson's play "Shylock's Revenge", which was first performed by The University Players at the Audimax, Hamburg, on 9 June 1989, can be seen as a full-length sequel to Shakespeare's drama.
The title of the film "Seven Pounds" is a reference to the "pound of flesh" from the play.
Edmond Haraucourt, the French playwright and poet, was commissioned in the 1880s by the actor and theatrical director Paul Porel to make a French-verse adaptation of "The Merchant of Venice". His play "Shylock", first performed at the Théâtre de l'Odéon in December 1889, had incidental music by the French composer Gabriel Fauré, later incorporated into an orchestral suite of the same name.
One of the four short stories comprising Alan Isler's "Op Non Cit" is also told from Shylock's point of view. In this story, Antonio was a boy of Jewish origin kidnapped at an early age by priests.
Ralph Vaughan Williams' choral work "Serenade to Music" draws its text from the discussion about music and the music of the spheres in Act V, scene 1.
In both versions of the comic film "To Be or Not to Be" the character "Greenberg", specified as a Jew only in the later version, gives a recitation of the "Hath Not a Jew eyes?" speech to Nazi soldiers.
In "The Pianist", Henryk Szpilman quotes a passage from Shylock's "Hath Not a Jew eyes?" speech to his brother Władysław Szpilman in a Jewish ghetto in Warsaw, Poland, during the Nazi occupation in World War II. Given the questioning of Antisemitism in the speech and also the Nazi use of the play for antisemitic propaganda purposes, the quote is seen as particularly poignant and symbolic.
Steven Spielberg's "Schindler's List" depicts SS Lieutenant Amon Göth quoting Shylock's "Hath Not a Jew eyes?" speech when deciding whether or not to rape his Jewish maid.
The rock musical "Fire Angel" was based on the story of the play, with the scene changed to the Little Italy district of New York. It was performed in Edinburgh in 1974 and in a revised form at Her Majesty's Theatre, London, in 1977.
Christopher Moore combines "The Merchant of Venice" and "Othello" in his 2014 comic novel "The Serpent of Venice", in which he makes Portia (from "The Merchant of Venice") and Desdemona (from "Othello") sisters. All of the characters come from those two plays with the exception of Pocket, the Fool, who comes from Moore's earlier novel based on "King Lear".
Jane Lindskold's book "Changer" contains a scene in which the protagonists consider "using Portia's gambit from "The Merchant of Venice"" to escape from a situation and binding contract analogous to Antonio's.
The online satirical news site "The Onion" satirized the play in its article "'Unconventional Director Sets Shakespeare Play In Time, Place Shakespeare Intended".
The play has been quoted and paraphrased several times in the "Star Trek" Universe:
The David Seltzer screenplay for the 1971 film "Willy Wonka and the Chocolate Factory" contains this line, near the end of the film: spoken by Willy Wonka, as if to himself, when Charlie returns the Everlasting Gobstopper: "So shines a good deed in a weary world"- derived perhaps from Portia's lines in Act V, Scene 1: "That light we see is burning in my hall. How far that little candle throws his beams! So shines a good deed in a naughty world."

</doc>
<doc id="37727" url="http://en.wikipedia.org/wiki?curid=37727" title="Willow Rosenberg">
Willow Rosenberg

Willow Rosenberg is a fictional character created for the fantasy television series "Buffy the Vampire Slayer" (1997–2003). She was developed by Joss Whedon and portrayed throughout the TV series by Alyson Hannigan.
Willow plays an integral role within the inner circle of friends—called the Scooby Gang—who support Buffy Summers, a teenager gifted with superhuman powers to defeat vampires, demons, and other evil in the fictional town of Sunnydale. The series begins as Buffy, Willow, and their friend Xander are in 10th grade and Willow is a shy and nerdy girl with little confidence. She has inherent magical abilities and begins to study witchcraft; as the series progresses, Willow becomes more sure of herself and her magical powers become significant if inconsistent. Her dependence on magic becomes so consuming that it develops into a dark force that takes her on a redemptive journey in a major story arc when she becomes the sixth season's main villain, threatening to destroy the world in a fit of grief and rage.
The "Buffy" series became extremely popular and earned a devoted fanbase; Willow's intelligence, shy nature, and vulnerability often resounded strongly with viewers in early seasons. Of the core characters, Willow changes the most, becoming a complex portrayal of a woman whose powers force her to seek balance between what is best for the people she loves and what she is capable of doing. Her character stood out as a positive portrayal of a Jewish woman and at the height of her popularity, she fell in love with another woman, a witch named Tara Maclay. They became one of the first lesbian couples on U.S. television and one of the most positive relationships of the series. Willow appears in every "Buffy" episode (making her the only character besides Buffy herself to do so), is featured in three episodes of the spinoff "Angel", an animated series and video game—both of which use Hannigan's voice, and the comic "Buffy the Vampire Slayer Season Eight" (2007–2011), which uses Hannigan's likeness and continues Willow's storyline following the television series.
Character history.
Pilot and casting.
"Buffy the Vampire Slayer" (often simplified as "Buffy") was originally conceived by Joss Whedon for a 1992 feature film. However, in its development Whedon felt it lost some of the quirkiness he considered was the heart of the project, and it was not received as well as he liked. He began to develop for television the concept of a fashion-conscious girl named Buffy, who is imbued with superhuman abilities and attends a high school situated on a portal to hell. Whedon created a group of friends for the main character, including Willow Rosenberg and Xander Harris. A half-hour pilot was filmed starring Riff Regan as Willow, but it was eventually left unaired and network executives requested that Regan be replaced. Willow's character demanded that she be shy and unsure of herself, and the casting department encountered some difficulty finding actors who could portray this effectively and still be likable. After seven auditions, 23-year-old Alyson Hannigan was hired for the role. She was chosen for being able to spin the character's lines with a self-effacing optimism. She later stated in an interview, "I didn't want to do Willow as someone who's feeling sorry for herself. Especially in the first season, she couldn't talk to guys, and nobody liked her. I was like, 'I don't want to play somebody who's down on herself."
In the beginning of the series, Hannigan used her own experiences in high school—which she called "overwhelmingly depressing"—to guide her portrayal of Willow. "My theory on high school was, get in, get out and hopefully I won't get hurt. Basically it was a miserable experience, because you're a walking hormone in this place that is just so cruel. There were times that were OK, but it's not the little myth that high school is the best years of your life. No way." Whedon intended Willow to be realistically introverted, saying "I wanted Willow to have that kind of insanely colorful interior life that truly shy people have. And Alyson has that. She definitely has a loopiness I found creeping into the way Willow talked, which was great. To an extent, all the actors conform to the way I write the character, but it really stands out in Willow's case."
Television series (1997–2003).
Seasons 1–3.
The "Buffy" television series first aired mid-season in March 1997, almost immediately earning positive critical reviews. Willow is presented as a bookish nerd with considerable computer skills, dowdily dressed and easily intimidated by more popular girls in school. She grows faint at the sight of monsters, but quickly forms a friendship with Buffy Summers (Sarah Michelle Gellar) and is revealed to have grown up as friends with Xander (Nicholas Brendon). They are mentored by the school librarian who is also Buffy's Watcher, Rupert Giles (Anthony Stewart Head), who often works closely with Willow in researching the various monsters the group encounters. Joss Whedon found that Hannigan was especially gifted reacting with fear (calling her the "king of pain") and viewers responded strongly when she was placed in danger, needing to be rescued by Buffy. Willow in various predicaments became common in early episodes. However, Willow establishes herself as integral to the group's effectiveness, often willing to break rules by hacking into highly secure computer systems.
In the second season when the characters are in 11th grade, Willow becomes more sure of herself, standing up to the conceited Cordelia Chase (Charisma Carpenter), and approaching Xander, on whom she has had a crush for years, although it is unrequited as Xander is in love with Buffy. Seth Green joined the cast during the second season as Oz, a high school senior who becomes a werewolf, and Willow's primary romantic interest. The show's popularity by early 1998 was evident to the cast members, and Hannigan remarked on her surprise specifically. Willow was noted to be the spirit of the Scooby Gang, and Hannigan attributed Willow's popularity with viewers (she had by May 1998 seven websites devoted to her) to being an underdog who develops confidence and is accepted by Buffy, a strong, popular person in school. Hannigan described her appeal: "Willow is the only reality-based character. She really is what a lot of high-schoolers are like, with that awkwardness and shyness, and all those adolescent feelings."
At the end of the second season, Willow begins to study magic following the murder of the computer teacher and spell caster Jenny Calendar (Robia LaMorte). Willow is able to perform a complicated spell to restore the soul of Angel, a vampire who is also Calendar's murderer and Buffy's boyfriend. During the third season three episodes explore Willow's backstory and foreshadow her development. In "Gingerbread", her home life is made clearer: Sunnydale falls under the spell of a demon who throws the town's adults into a moral panic, and Willow's mother is portrayed as a career-obsessed academic who is unable to communicate with her daughter, eventually trying to burn Willow at the stake for being involved in witchcraft; her father is never featured. In "The Wish" a vengeance demon named Anya (Emma Caulfield) grants Cordelia's wish that Buffy never came to Sunnydale, showing what would happen if it were overrun with vampires. In this alternate reality, Willow is an aggressively bisexual vampire. In a related episode, "Doppelgangland", Willow meets "Vamp Willow", who dresses provocatively and flirts with her.
Seasons 4–6.
Willow chooses to attend college with Buffy in Sunnydale although she is accepted to prestigious schools elsewhere. Her relationships with Buffy and Xander become strained as they try to find their place following high school. Willow becomes much more confident in college, finally finding a place that respects her intellect, while Buffy has difficulty in classes and Xander does not attend school. Willow's relationship with Oz continues until a female werewolf appears on the scene, aggressively pursuing him, and he leaves town to learn how to control the wolf within. She becomes depressed and explores magic more deeply, often with powerful but inconsistent results. She joins the campus Wicca group, meeting Tara Maclay, eventually falling in love with and choosing to be with her even when Oz returns to Sunnydale after apparently getting his lycanthropic tendencies under control.
Each season the Scoobies face a villain they call the Big Bad. In the fifth season, this is a goddess named Glory (Clare Kramer) that Buffy is unable to fight by herself. 
The writers of the series often use elements of fantasy and horror as metaphors for real-life conflicts. The series' use of magic, as noted by religion professor Gregory Stevenson, neither promotes nor denigrates Wiccan ideals and Willow rejects Wiccan colleagues for not practicing the magic she favors. Throughout the series, magic is employed to represent different ideas -— relationships, sexuality, ostracism, power, and particularly for Willow, addiction -— that change between episodes and seasons. The ethical judgment of magic, therefore, lies in the results: performing magic to meet selfish needs or neglecting to appreciate its power often ends disastrously. Using it wisely for altruistic reasons is considered a positive act on the series. 
Through witchcraft, Willow becomes the only member of the group to cause damage to Glory. She reveals that the spells she casts are physically demanding, giving her headaches and nosebleeds. When Glory assaults Tara, making her insane, Willow, in a magical rage that causes her eyes to turn black, finds Glory and battles her. She does not come from the battle unscathed and must be assisted by Buffy, but her power is evident and surprising to her friends. The final episode of the fifth season sees Willow restoring Tara's sanity and crucially weakening Glory in the process. It also features Buffy's death, sacrificing herself to save the world.
Willow and Tara move into the Summers house and raise Buffy's younger sister Dawn (Michelle Trachtenberg). Fearing that Buffy is in hell, Willow suggests at the beginning of the sixth season that she be raised from the dead. In a dark ceremony in which she expels a snake from her mouth, Willow performs the magic necessary to bring Buffy back. She is successful, but Buffy keeps it secret that she believes she was in heaven. 
Willow's powers grow stronger; she uses telepathy which her friends find intrusive, and she begins to cast spells to manipulate Tara. After Wilow fails Tara's challenge to go for one week without performing magic, Tara leaves her, and for two episodes Willow descends into addiction that almost gets Dawn killed. Willow goes for months without any magic, helping Buffy track three geeks called The Trio who grandiosely aspire to be supervillains. 
Immediately following a reconciliation with Tara, Warren (Adam Busch), one of the Trio, shoots Buffy and a stray shot kills Tara right in front of Willow. In an explosion of rage and grief, Willow soaks up all the dark magic text she can, turning her hair and eyes black. In the final episodes of the season Willow becomes exceedingly strong, surviving unharmed when Warren hits her in the back with an axe. She hunts Warren, tortures him by slowly pushing a bullet into his body, then ends by killing him by instantly magically flaying him. Unsatisfied, she attempts to kill the other two members of the Trio and unsuccessful at this, tries to destroy the world, only to be stopped by Xander.
Season 7.
The seventh season starts with Willow in England, unnerved by her power, studying with a coven near Giles' home to harness it. She fears returning to Sunnydale and what she is capable of doing if she loses control again, a fear that dogs her the whole season.
Buffy and the Scoobies face the First Evil, bent on ending the Slayer line and destroying the world. Potential Slayers from around the globe congregate at Buffy's home and she trains them to battle the First Evil. Willow continues to face her grief over Tara's death and, reluctantly, becomes involved with one of the Potentials, Kennedy (Iyari Limon). 
In the final episode of the series, "Chosen", Buffy calls upon Willow to perform the most powerful spell she has ever attempted. With Kennedy nearby, cautioned to kill her if she becomes out of control, Willow infuses every Potential Slayer in the world with the same powers Buffy and Faith have. The spell momentarily turns her hair white and makes her shine -- Kennedy calls her "a goddess" -- and it ensures that Buffy and the Potentials defeat the First Evil. Willow is able to escape with Buffy, Xander, Giles, and Kennedy as Sunnydale is destroyed.
Through the gamut of changes Willow endures in the series, "Buffy" studies scholar Ian Shuttleworth states that Alyson Hannigan's performances are the reason for Willow's popularity: "Hannigan can play on audience heartstrings like a concert harpist... As an actress she is a perfect interpreter in particular of the bare emotional directness which is the specialty of [series writer Marti] Noxon on form."
Comic series (since 2007).
Subsequent to "Buffy"‍ '​s television finale, Dark Horse Comics collaborated with Joss Whedon to produce a canonical comic book continuation of the series, "Buffy the Vampire Slayer Season Eight" (2007–11), written by Whedon and many other writers from the television series. Unfettered by the practical limitations of casting or a television special effects budget, "Season Eight" explores more fantastic storylines, characters, and abilities for Willow. Willow's cover art is done by Jo Chen, and Georges Jeanty and Karl Moline produce character artwork and provide alternative covers. It was followed by two closely interlinked sequels, "Buffy the Vampire Slayer Season Nine" and "Angel & Faith" (both 2012–). Willow features at different times in both series, as well as in her own spin-off miniseries. Jeanty continues to provide Willow's likeness in "Season Nine", while Rebekah Isaacs and Brian Ching are the primary pencillers of "Angel & Faith" and "Willow: Wonderland" respectively. While "Season Nine" and "Angel & Faith" are substantially less fantastical in tone than "Season Eight", Willow's spin-off is high fantasy and focuses on her journey through magical alternate worlds.
Willow appears to Buffy and Xander, who are in charge of thousands of Slayers, a year after the destruction of Sunnydale. Willow reveals a host of new abilities including being able to fly and absorbing others' magic to deconstruct it. The Big Bad of "Season Eight" is a being named Twilight who is bent on destroying magic in the world. A one-shot comic dedicated to Willow's story was released in 2009 titled "Willow: Goddesses and Monsters". It explores the time she took away to discover more about her magical powers, under the tutelage a half-woman half-snake demon named Aluwyn. Willow is still involved with Kennedy through "Season Eight", but becomes intimate with Aluwyn while they are together. She also continues to deal with grief from Tara's death, and struggles with the dark forces of magic that put her in opposition to Buffy. At the conclusion of the season, Buffy destroys an object, a seed, that is the source of the magic in the world, leaving Willow powerless. Whedon divulged that recovering her magical abilities will become Willow's "personal obsession" in a miniseries where she will be the central character.
Identities.
From the inception of Willow's character in the first season, she is presented with contradictions. Bookish, rational, naive, and sometimes absent-minded, she is also shown being open to magic, aggressively boyish, and intensely focused. Willow is malleable, in continuous transition more so than any other "Buffy" character. She is, however, consistently labeled as dependable and reliable by the other characters and thus to the audience, making her appear to be stable. She is unsure of who she is; despite all the tasks she takes on and excels at, for much of the series she has no identity. This is specifically exhibited in the fourth season finale "Restless", an enigmatic pastiche of characters' dream sequences. In Willow's dream, she moves from an intimate moment painting a love poem by Sappho on Tara's bare back, to attending the first day of drama class to learn that she is to be in a play performed immediately for which she does not know the lines or understand. The dream presents poignant anxieties about how she appears to others, not belonging, and the consequences of people finding out her true self. As Willow gives a book report in front of her high school class, she discovers herself wearing the same mousy outfit she wore in the first episode of the show ("Welcome to the Hellmouth") as her friends and classmates shout derisively at her, and Oz and Tara whisper intimately to each other in the audience. She is attacked and strangled by the First Slayer as the class ignores her cries for help.
Long a level-headed character who sacrifices her own desires for those of her friends, she gradually abandons these priorities to be more independent and please herself. She is often shown making choices that allow her to acquire power or knowledge and avoid emotional conflict. The story arc of Willow's growing dependence on magic was noted by Marti Noxon as the representation of "adult crossroads" and Willow's inability and unwillingness to be accountable for her own life. Willow enjoys power she is unable to control. She steals to accomplish her vocational goals and rationalizes her amoral behavior. This also manifests itself in a competitive streak and she accuses others who share their concerns that she uses magic for selfish purposes of being jealous. No longer the conscience of the Scooby Gang, Willow cedes this role to Tara then revels in breaking more rules. After Tara leaves Willow, Willow divulges to Buffy that she does not know who she is and doubts her worth and appeal—specifically to Tara—without magic. Contradicting the characterization of Willow's issues with magic as addiction, "Buffy" essayist Jacqueline Lichtenberg writes "Willow is not addicted to magic. Willow is addicted to the surging hope that this deed or the next or the next will finally assuage her inner pain."
Vamp Willow.
Vamp Willow appears in the third season episodes "The Wish" and "Doppelgangland". She is and aggressive, the opposite of Willow's usual nature; her bad behavior so exaggerated that it does not instill fear into the viewer like other female vampires in the series, but indicates more about Willow's personality. Shocked upon seeing her alter ego in "Doppelgangland", Willow states "That's me as a vampire? I'm so evil and . And I think I'm kinda gay!" Angel is stopped by Buffy in telling the Scoobies that the vampire self carries many of the same attributes as the human self, at which Willow says that is nothing like her. Many Buffy fans saw this as a funny Easter egg when Willow revealed herself to actually be a lesbian in later seasons. As surprised as Willow is with Vamp Willow, she feels bound to her, and does not have the heart to allow Buffy to kill her. Both Willows make the observation that "this world's no fun", before they send Vamp Willow back into the alternate dimension from which she came, whereupon she is staked and dies immediately.
Dark Willow.
A shadow of Dark Willow appears to fight Glory in the fifth season episode "Tough Love", but she does not come into full force until the sixth season in "Villains", "Two to Go", and "Grave". The transition from Willow into Dark Willow, precipitated by Tara's immediate death when she is shot through the heart, was ambiguously received by audiences, many of whom never foresaw Willow's psychic break. It was simultaneously lauded for being an overwhelming depiction of a powerful woman, and derided as representative of a worn cliché that lesbians are amoral and murderous. Dark Willow proved to be exceptionally more powerful than Buffy. She changes visually when she walks into the Magic Box, a store owned by Anya and Giles, telekinetically retrieves dozens of dark magic books from the shelves, and leeches the words from the pages with her fingertips. As the words crawl up her arms and soak into her skin, her eyes and hair become black and her posture "aggressively aware and confident".
Susan Driver writes that it is "crucial to recognize that never before in a teen series has raw fury been so vividly explored through a young queer girl responding to the sudden death of her lover". Dark Willow is preternaturally focused on revenge, relentless and unstoppable. Lights explode when she walks past. She forcefully takes advantage of any opportunity to further her goals. She saves Buffy by removing the bullet from her chest, but later commandeers a tractor trailer, making it slam into Xander's car while he and Buffy are inside protecting Jonathan and Andrew, the other two members of the Trio. She floats, teleports herself at will, and dismantles the local jail where Jonathan and Andrew are held. She is cruelly honest to Dawn and Buffy, and overpowers everyone with whom she comes in contact. When she takes Giles' magic from him, she gains the ability to feel the world's pain, becoming determined to put the world out of its misery. She does not acknowledge her grief, and only Xander can force her to face it when he tells her that he loves her no matter what or who she is, and if she is determined to end the world she must start by killing him. Only then does Willow return, sobbing.
At Salon.com, Stephanie Zacharek writes that Dark Willow is "far from being a cut-out angry lesbian, is more fleshed out, and more terrifyingly alive, than she has ever been before. More than any other character, she has driven the momentum of the past few episodes; she very nearly drove it off a cliff." Several writers state that Willow's transition into Dark Willow is inevitable, grounded in Willow's self-hatred that had been festering from the first season. Both Dark Willow and even Willow herself state that Willow's sacrifices for her friends and lack of assertiveness are her undoing. In "Doppelgangland", Willow (posing as Vampire Willow) says "It's pathetic. She lets everyone walk all over her and gets cranky at her friends for no reason." In "Two to Go", Dark Willow remarks "Let me tell you something about Willow. She's a loser. And she always has been. People picked on Willow... and now Willow's a junkie." Vamp Willow served as an indicator of what Willow is capable of; immediately before she flays Warren in one violent magical flash, she uses the same line Vamp Willow used in the third season: "Bored now."
Following the sixth season, Willow struggles to allow herself to perform magic without the darkness within her taking her over. She is no longer able to abstain from magic as it is such an integral part of her that doing so will kill her. In the instances when she is highly emotional the darkness comes out. Willow must control that part of her and is occasionally unable to do so, giving her a trait similar to Angel, a cursed vampire who fears losing his soul will turn him evil. In a redemptive turn, when Willow turns all the Potentials into Slayers, she glows and her hair turns white, astonishing Kennedy and prompting her to call Willow a goddess.
Relationships.
Willow's earliest and most consistent relationships are with Buffy and Xander, both of whom she refers to as her best friends although they have their conflicts, and Giles as a father figure. Willow takes on the leadership role when Buffy is unavailable, and her growing powers sometimes make her resent being positioned as Buffy's sidekick. Some scholars see Willow as Buffy's sister-figure or the anti-Buffy, similar to Faith, another Slayer whose morals are less strict. In early seasons, Willow's unrequited crush on Xander creates some storylines involving the relationships between Xander, Cordelia, and Oz. Willow is part of a powerful quartet: she represents the spirit, Giles intelligence, Xander heart, and Buffy strength of the Scoobies. Although they often drift apart, they are forced to come together and work in these roles to defeat forces they are unable to fight individually.
Oz.
Willow meets stoic Oz in the second season. Their courtship is slow and patient. Oz is bitten by a werewolf, and just as Willow begins to confront him about why he does not spend time with her, he transforms and attacks her. She must shoot him with a tranquilizer gun several times while he is wild, but her assertiveness in doing so makes her more confident in their relationship. Oz's trials in dealing with a power he cannot control is, according to authors J. Michael Richardson and J. Douglas Rabb, a model for Willow to reference when she encounters her own attraction to evil. When Willow and Oz decide to commit to each other, Willow is enthusiastic that she has a boyfriend, and, as a guitarist in a band, one so cool. Her relationship with Oz endures the high school storylines of exploring her attraction to Xander, which briefly separates them. She worries that she is not as close to Oz as she could be. They stay together through graduation into college, but Oz is drawn to Veruca, another werewolf. He admits an animal attraction to Veruca, which he does not share with Willow. He sleeps with Veruca and leaves shortly after to explore the werewolf part of himself. Willow becomes very depressed and doubts herself. She drinks, her magical abilities are compromised, her spells come out wrong, and she lashes out at her friends when they suggest she get over it ("Something Blue").
Joss Whedon did not intend to write Oz out of the series. Seth Green came to Whedon early in the fourth season to announce that he wished to work on his film career. Whedon admitted he was upset by Green's announcement and that if he had wanted to continue, Oz would have been a part of the story. However, to resolve the relationship between Oz and Willow Whedon says, "we had to scramble. And out of the heavens came Amber Benson."
Tara Maclay.
"Buffy" earned international attention for its unflinching focus on the relationship between Willow and Tara Maclay. Whedon and the writing staff had been considering developing a story arc in which a character explores his or her sexuality as the Scoobies left high school, but no particular effort was made to assign this arc to Willow. In 1999, at the end of the third season, the "Boston Herald" called "Buffy" "the most gay show on network TV this year" despite having no overtly gay characters among the core cast. It simply presented storylines that resembled coming out stories. In the fourth season episode "Hush", Willow meets Tara, and to avoid being killed by a group of ghouls, they join hands to move a large vending machine telekinetically to barricade a door. The scene was, upon completion, noticeably sensual to Whedon, the producers, and network executives, who encouraged Whedon to develop a romantic storyline between Willow and Tara, but at the same time placed barriers on how far it could go and what could be shown. Two episodes later, Hannigan and Amber Benson were informed that their characters would become romantically involved. The actors were not told the end result of the Willow–Oz–Tara storyline, not sure what the eventual trajectory of the relationship would be, until Hannigan said, "Then finally it was, 'Great! It's official. We're in luurrvvve.'"
Whedon made a conscious effort to focus on Willow and Tara's relationship instead of either's identity as a lesbian or the coming out process. When Willow discloses to Buffy what she feels for Tara, she indicates that she has fallen in love with Tara, not that she is a lesbian, and avoids categorizing herself. Some critics regard this as a failure on Willow's part to be strong; Em McAvan interprets this to mean that Willow may be bisexual. Scholar Farah Mendlesohn asserts that Willow's realization that she is in love with Tara allows viewers to re-interpret Willow's relationship with Buffy; in the first three seasons, Willow is often disappointed that she is not a higher priority to Buffy, and even after Willow enters a relationship with Tara, still desires to feel integral to Buffy's cause and the Scooby Gang.
Willow's progression has been noted to be unique in television. Her relationship with Tara coincides with the development of her magical abilities becoming much more profound. By the seventh season, she is the most powerful person in Buffy's circle. Jessica Ford at PopMatters asserts that Willow's sexuality and her magical abilities are connected and represented by her relationships. In her unrequited attraction to Xander, she has no power. With Oz, she has some that gives her the confidence she sorely lacks, but his departure leaves her unsure of herself. Only when she meets Tara do her magical abilities flourish; to Ford, sexuality and magic are both empowering agents in Willow's story arc. David Bianculli in the "New York Daily News" writes that Willow's progression is "unlike anything else I can recall on regular prime-time television: a character evolving naturally over four seasons of stories and arriving at a place of sexual rediscovery".
Not all viewers considered Willow and Tara's relationship a positive development. Some fans loyal to Willow reacted angrily as she chose to be with Tara when Oz made himself available, and they lashed out at Tara and Amber Benson on the fansite message boards. Whedon replied sardonically, "we're going to shift away from this whole lifestyle choice that Willow has made. Just wipe the slate. From now on, Willow will no longer be a Jew. And I think we can all breathe easier." However, he seriously explained his motivation, writing "My show is about emotion. Love is the most powerful, messy, delightful and dangerous emotion... Willow's in love. I think it's cool." Hannigan was also positive about the way the character and her relationship with Tara was written: "It is not about being controversial or making a statement. I think the show is handling it really nicely. It's about two people who care about each other."
Contrasting with some of the more sexual relationships of the other characters, Willow and Tara demonstrate a sentimental, soft, and consistent affection for each other. Some of this was pragmatic: the show was restricted in what it could present to viewers. Willow and Tara did not kiss until the fifth season in an episode that diverted the focus away from the display of affection when Buffy's mother dies in "The Body". Before this, much of their sexuality is represented by allusions to witchcraft; spells doubled for physical affection such as an erotic ritual in "Who Are You?" where Willow and Tara chant and perspire in a circle of light until Willow falls back on a pillow gasping and moaning. Within the "Buffy" universe, magic is portrayed in a mostly female realm. As opposed to it being evil, it is an earth-bound force that is most proficiently harvested by women. The treatment of the lesbian relationship as integral to magic, representative of each other (love is magic, magic is love), earned the series some critical commentary from conservative Christians. To avoid large-scale criticism, scenes had to be shot several different ways because censors would not allow some types of action on screen. In the fourth and fifth seasons, the characters could be shown on a bed, but not under the covers. Hannigan noted the inconsistent standards with the other relationships on the show: "you've got Spike and Harmony just going at it like rabbits, so it's very hypocritical". As a couple, Willow and Tara are treated by the rest of the Scoobies with acceptance and little fanfare. Susan Driver writes that younger viewers especially appreciate that Willow and Tara are able to be affectionate without becoming overly sexual, thus making them objects of fantasy for male enjoyment. Willow and Tara's influence on specifically younger female viewers is, according to Driver, "remarkable".
Academics, however, comment that Willow is a less sexual character than the others in the show. She is displayed as "cuddly" in earlier seasons, often dressing in pink fuzzy sweaters resulting in an innocent tomboyishness. She becomes more feminine in her relationship with Tara, who is already feminine; no issues with gender are present in their union. Their relationship is sanitized and unthreatening to male viewers. When the series moved broadcast networks from the WB to UPN in 2001, some of the restrictions were relaxed. Willow and Tara are shown in some scenes to be "intensely sexual", such as in the sixth season episode "Once More, with Feeling" where it is visually implied that Willow performs cunnilingus on Tara. When Willow and Tara reconcile, they spend part of the episode in "Seeing Red" unclothed in bed, covered by red sheets.
Willow is more demonstrative in the beginning of her relationship with Tara. Where in her relationship with Oz she described herself as belonging to him, Tara states that she belongs to Willow. Willow finds in Tara a place where she can be the focus of Tara's attention, not having to appease or sacrifice as she has in the past. Tara, however, eclipses Willow's role as the moral center of the Scoobies, and as Willow becomes more powerful and less ethical, Tara becomes a maternal figure for the group. Willow acts as a sort of middle child between Xander's immaturity and Buffy's weighty responsibilities. She becomes completely devoted to and enamored of Tara, and then manipulates her to avoid conflict when Tara does not conform to what she wants. Displeased with how Willow abuses her power, especially toward herself, Tara leaves Willow while continuing to counsel Dawn and Buffy. Long after Tara's death, Willow faces the choices she made: in the "Season Eight" episode "Anywhere But Here", Willow tells Buffy that she is responsible for Tara's death. Her ambition to bring back Buffy from the dead inevitably led to Tara getting shot and killed. In the one-shot comic, Willow is offered Tara as a guide for her mystical path to understanding her own powers, but rejects her as being an illusion, too much of a comfort, and not a guide who will force her to grow. She begins a relationship with Kennedy.
Kennedy.
Following protests angry about the death of Tara, Whedon and the writing team made a decision to keep Willow gay. In 2002, he told "The Advocate" about the possibility of Willow having a relationship with a man, "We do that now, and we will be burned alive. And possibly justifiably. We can't have Willow say, 'Oh, cured now, I can go back to cock!' Willow is not going to be straddling that particular fence. She will just be gay." Kennedy is markedly different from Tara. She is younger, outspoken, and aggressively pursues Willow, who hesitates to become involved again. When they first kiss in the episode "The Killer in Me", Willow's realization that she let Tara go reacts with a curse put upon her by another witch named Amy Madison (Elizabeth Anne Allen), turning Willow into Warren, Tara's murderer. The spell is broken when Willow acknowledges her guilt and Kennedy kisses her again. Kennedy expresses that she does not understand the value of magic and assumes it involves tricks, not the all-consuming energy that Willow is capable of. When Willow eventually exhibits what power she has, it briefly frightens Kennedy. Willow worries about becoming sexually intimate with Kennedy, unsure of what may transpire if she loses control of herself.
In season 7 episode 20, "Touched", in which practically all the main cast has sex (two by two) Willow and Kennedy take part in the first lesbian sex scene on primetime television.
In "Season Eight", Kennedy and Willow are still romantically involved, but separated during Willow's self-exploration. Unlike her relationship with Tara, Willow is able to hold a separate identity while with Kennedy. When she realizes her powers have gone at the end of "Season Eight", however, Willow ends her relationship with Kennedy, saying that there is someone else Willow is in love with, who she will never see again.
Kennedy's role split many Buffy fans into two groups. Many viewers hated Kennedy, because they saw her as a way of saying; "Tara's dead, let's move on." and they weren't ready to. After the emotional death of Tara and Willow's reaction (nearly ending all life on Earth) many fans thought that it was ridiculous for Willow to recover and move on so quickly. Kennedy overall, has received much hate, but there is the other side who say that she was exactly what Willow needed to recover and continue a happy life.
Cultural impact.
Willow Rosenberg is undoubtedly the most complexly represented girl in love and lust with other girls to be developed within a mainstream network television series.
Susan Driver in "Queer Girls and Popular Culture"
Willow's religion and sexuality have made her a role model for audiences. Whedon, however, has compared her Jewish identity to her sexuality, stating that they are rarely made a significant focus of the show. Willow at times reminds the other characters of her religion, wondering what her father might think of the crucifixes she must apply to her bedroom wall to keep out vampires, and commenting that Santa Claus misses her house every Christmas because of the "big honkin' menorah". "Buffy" essayist Matthew Pateman criticizes the show for presenting Willow's Jewish identity only when it opposes Christian declarations of holidays and other traditions. "The New York Times", however, named her as a positive example of a depiction of a Jewish woman, who stood out among portrayals of Jews as harsh, unfeminine, and shallow. Producer Gail Berman states that as a Jew, Willow "handles herself just fine, thank you".
In "Queer Girls and Popular Culture", Susan Driver states that television ascribes to viewers what lesbians look and act like, and that realistic portrayals of girls outside the norm of white, upper or middle class, and heterosexual are extremely rare. Realistic depictions of lesbians are so rare that they become strong role models and enable "hope and imagination" for girls limited by the conditions of their immediate surroundings, who may know of no other gay people. The time and space given to Willow to go from being a shy scared girl into a confident woman who falls in love with another woman is, as of 2007, unique in television; it does not occur in one flash or single moment. It is a progression that defies strict definition. Manda Scott in "The Herald" states that Willow's lack of panic or self-doubt when she realizes she is in love with Tara makes her "the best role model a teen could ask for".
When viewers realized that Willow was falling in love with Tara, Whedon remembered that some threatened to boycott the show, complaining "You made Willow a fag", to which he responded, "Bye. We'll miss you "a whole lot."" However, he also said, "For every (negative) post, there's somebody saying, 'You made my life a lot easier because I now have someone I can relate to on screen'." Gay characters had been portrayed before on television, and at the time the popular sitcom "Will & Grace" was on the air. Lesbian-themed HBO special "If These Walls Could Talk 2" won an Emmy. Twenty-three television shows depicted a gay character of some kind in 2000. However, these other characters were mostly desexualized, none were partnered or shown consistently affectionate towards the same person. Willow and Tara's relationship became the first long-term lesbian relationship on U.S. television. "Jane" magazine hailed Willow and Tara as a bold representation of gay relationship, remarking that "they hold hands, slow-dance and lay in bed at night. You won't find that kind of normalcy on "Will and Grace"." Despite Whedon's intentions of not making "Buffy" about overcoming issues, he said Willow's exploration of her sexuality "turned out to be one of the most important things we've done on the show".
Although the show's writers and producers received a minimal negative reaction from Willow choosing Tara over Oz, the response from viewers and critics alike was overwhelming towards Whedon for killing Tara, accusing him of homophobia. Particularly because Tara's death came at a point where Willow and Tara had reconciled and were shown following an apparent sexual encounter, the writers were criticized for representing the consequences of lesbian sex as punishable by death. Series writer and producer Marti Noxon—whose mother fell in love with another woman when Noxon was 13 years old—was unable to read some of the mail the writing team received because it was so upsetting. To her, the pain expressed in viewers' letters was a logical reaction to the lack of realistic lesbian role models on television.
Willow's cultural impact has been noted in several other ways. Patrick Krug, a biologist at California State University, Los Angeles named a sea slug with traits of sexual flexibility "Alderia willowi" partly for his grandmother and partly after Willow's character. Willow was included in AfterEllen.com's Top 50 Lesbian and Bisexual Characters, ranking at No. 7. She was also ranked No. 12 in their Top 50 Favorite Female TV Characters. UGO.com named her one of the best TV nerds. AOL also listed her as the #1 TV witch of all time, and one of the 100 Most Memorable Female TV Characters.

</doc>
<doc id="37732" url="http://en.wikipedia.org/wiki?curid=37732" title="True lover's knot">
True lover's knot

The true lover's knot (or true love knot) is a name which has been used for many distinct knots. The association of knots with the symbolism of love, friendship, and affection dates back to antiquity. Because of this, it is not possible to consider a single knot to be "the" "true love knot".
Naming.
Modern western knotting literature has the name for these related knots deriving from stories or legends in which the knots symbolize the connection between a couple in love. Many examples feature sailors separated from their beloved. Ashley notes that it was once common for sailors' wedding rings, where gold wire was wrought to incorporate the "true lovers" knot such that resultant ring would comprise two tori: each flexible to move about the other; yet nevertheless inseparable.
Variations.
In practical terms, these knots are generally shown as consisting of two interlocked overhand knots made in two parallel ropes or cords. The variations are differentiated by the way in which the overhand knots interweave and in the final arrangement of the knot.
To show if a young couple's love would last, each would take a small limb of a tree and tie a lovers knot. If the knot held and grew for approximately a year, their love would stay true.

</doc>
<doc id="37739" url="http://en.wikipedia.org/wiki?curid=37739" title="The Illustrated Man">
The Illustrated Man

The Illustrated Man is a 1951 book of eighteen science fiction short stories by Ray Bradbury that explores the nature of mankind. A recurring theme throughout the eighteen stories is the conflict of the cold mechanics of technology and the psychology of people. It was nominated for the International Fantasy Award in 1952.
The unrelated stories are tied together by the frame device of "the Illustrated Man", a vagrant former member of a carnival freak show with an extensively tattooed body whom the unnamed narrator meets. The man's tattoos, allegedly created by a time-traveling woman, are animated and each tell a different tale. All but one of the stories had been published previously elsewhere, although Bradbury revised some of the texts for the book's publication.
The book was made into the 1969 film starring Rod Steiger and Claire Bloom, adapted from the stories "The Veldt", "The Long Rain", and "The Last Night of the World".
A number of the stories, including "The Veldt", "The Fox and the Forest" (as "To the Future"), "Marionettes, Inc.", and "Zero Hour" were dramatized for the 1955-57 radio series "X Minus One". "The Veldt", "The Concrete Mixer", "The Long Rain", "Zero Hour", and "Marionettes Inc." were adapted for the TV series "The Ray Bradbury Theater".
Other versions.
The British edition, first published in 1952 by Hart-Davis omits "The Rocket Man", "The Fire Balloons", "The Exiles" and "The Concrete Mixer", and adds "Usher II" from "The Martian Chronicles" and "The Playground".
Editions published by Avon Books in 1997 and William Morrow in 2001 omit "The Fire Balloons" and add "The Illustrated Man" to the end of the book.
Reception.
Boucher and McComas gave "The Illustrated Man" a mixed review, faulting the framing story as "markedly ineffective" and the story selection for seeming "less than wisely chosen". However, they found the better stories "provide a feast [from] the finest traditions in imaginative fiction" and later named it among the year's top books. Villiers Gerson, reviewing the volume for "Astounding Science Fiction", praised it as "a book which demonstrates that its author is one of the most literate and spellbinding writers in science fiction today". In "The New York Times", Gerson also praised the book for its "three-dimensional people with whom it is easy to sympathize, to hate, and to admire".
Adaptations to other media.
1969 film.
A film adaptation of "The Illustrated Man" was released in 1969. It was directed by Jack Smight and starred Rod Steiger, Claire Bloom, and others, including Don Dubbins. The film contains adaptations of "The Veldt", "The Long Rain", "The Last Night of the World" and expands the prologue and epilogue with intermittent scenes and flashbacks of how the illustrations came to be. A short documentary, "Tattooed Steiger", details the process the filmmakers used to cover Steiger's body in mock tattoos and shows actors and filmmakers preparing for the movie.
2008 album.
A musical adaptation by Samuel Otten was released as a musical expression of the stories to go along with the reading.
Influence on "Dark Star", 1974.
Bradbury's "Kaleidoscope" inspired the 1974 science fiction movie "Dark Star", which ends in a similar final scene.
Influence on "To the Dark Side of the Moon", 2010.
A theater adaptation of "Kaleidoscope", with influence from music by Pink Floyd was used to produce "To the Dark Side of the Moon", in reference to the Pink Floyd album by the same name. This adaptation was produced by Stern-Theater, a Swiss-based theater company. The script was written by Daniel Rohr and was first shown at the Theater Rigiblick in Zurich, Switzerland on February 6, 2010. The music includes creative use of a string quartet and a piano.
BBC Radio, 2014.
A radio adaptation was broadcast on BBC Radio 4 on 14 June 2014 as part of the "Dangerous Visions" series adapted by Brian Sibley, directed by Gemma Jenkins and starring Iain Glen as "The Illustrated Man" and Jamie Parker as "The Youth". The stories adapted for this production were "Marionettes, Inc.", "Zero Hour" and "Kaleidoscope".
Film in development.
Director Zack Snyder is attached to direct, at least in part, a film adaptation of three stories from "The Illustrated Man": "The Illustrated Man", "Veldt", and "Concrete Mixer". Screenwriter Alex Tse is writing the screenplay.
"The Whispers" television series.
"The Whispers" is an upcoming American television series based on the short story "Zero Hour".

</doc>
<doc id="37745" url="http://en.wikipedia.org/wiki?curid=37745" title="Bell AH-1 Cobra">
Bell AH-1 Cobra

The Bell AH-1 Cobra (company designation: Model 209) is a two-blade, single engine attack helicopter manufactured by Bell Helicopter. It was developed using the engine, transmission and rotor system of the Bell's UH-1 Iroquois. The AH-1 is also referred to as the HueyCobra or Snake.
The AH-1 was the backbone of the United States Army's attack helicopter fleet, but has been replaced by the AH-64 Apache in Army service. Upgraded versions continue to fly with the militaries of several other nations. The AH-1 twin engine versions remain in service with United States Marine Corps (USMC) as the service's primary attack helicopter. Surplus AH-1 helicopters have been converted for fighting forest fires. The United States Forest Service refers to their program as the "Firewatch Cobra". Garlick Helicopters also converts surplus AH-1s for forest firefighting under the name, "FireSnake".
Development.
Background.
Closely related with the development of the Bell AH-1 is the story of the Bell UH-1 Iroquois—predecessor of the modern helicopter, icon of the Vietnam War and one of the most numerous helicopter types built. The UH-1 made the theory of air cavalry practical, as the new tactics called for US forces to be highly mobile across a wide area. Unlike before, they would not stand and fight long battles, and they would not stay and hold positions. Instead, the plan was that the troops carried by fleets of UH-1 "Hueys" would range across the country, to fight the enemy at times and places of their own choice.
It soon became clear that the unarmed troop helicopters were vulnerable against ground fire from Việt Cộng and North Vietnamese troops, particularly as they came down to drop their troops in a landing zone. Without friendly support from artillery or ground forces, the only way to pacify a landing zone was from the air, preferably with an aircraft that could closely escort the transport helicopters, and loiter over the landing zone as the battle progressed. By 1962 a small number of armed UH-1As were used as escorts, armed with multiple machine guns and rocket mounts.
The massive expansion of American military presence in Vietnam opened a new era of war from the air. The linchpin of US Army tactics were the helicopters, and the protection of those helicopters became a vital role.
Iroquois Warrior, Sioux Scout and AAFSS.
Bell had been investigating helicopter gunships since the late 1950s, and had created a mockup of its D-255 helicopter gunship concept, named "Iroquois Warrior". In June 1962, Bell displayed the mockup to Army officials, hoping to solicit funding for further development. The Iroquois Warrior was planned to be a purpose-built attack aircraft based on the UH-1B components with a new, slender airframe and a two-seat, tandem cockpit. It featured a grenade launcher in a ball turret on the nose, a 20 mm belly-mounted gun pod, and stub wings for mounting rockets or SS.10 anti-tank missiles.
The Army was interested and awarded Bell a proof of concept contract in December 1962. Bell modified a Model 47 into the sleek Model 207 Sioux Scout which first flew in July 1963. The Sioux Scout had all the key features of a modern attack helicopter: a tandem cockpit, stub wings for weapons, and a chin-mounted gun turret. After evaluating the Sioux Scout in early 1964, the Army was impressed, but also felt the Sioux Scout was undersized, underpowered, and generally not suited for practical use.
Army's solution to the shortcomings of the Sioux Scout was to launch the Advanced Aerial Fire Support System (AAFSS) competition. The AAFSS requirement gave birth to the Lockheed AH-56 Cheyenne, a heavy attack helicopter with high speed capability. It proved to be too sophisticated, and was canceled after 10 years of development in 1972. The Army sought greater survivability in a conventional attack helicopter.
Model 209.
At the same time, despite the Army's preference for the AAFSS–for which Bell Helicopter was not selected to compete–Bell stuck with their own idea of a smaller and lighter gunship. In January 1965 Bell invested $1 million to proceed with a new design. Mating the proven transmission, the "540" rotor system of the UH-1C augmented by a Stability Control Augmentation System (SCAS), and the T53 turboshaft engine of the UH-1 with the design philosophy of the Sioux Scout, Bell produced the Model 209. Bell's Model 209 largely resembled the "Iroquois Warrior" mockup.
In Vietnam, events were also advancing in favor of the Model 209. Attacks on US forces were increasing, and by the end of June 1965 there were already 50,000 US ground troops in Vietnam. 1965 was also the deadline for AAFSS selection, but the program would become stuck in technical difficulties and political bickering. The U.S. Army needed an interim gunship for Vietnam and it asked five companies to provide a quick solution. Submissions came in for armed variants of the Boeing-Vertol ACH-47A, Kaman HH-2C Tomahawk, Piasecki 16H Pathfinder, Sikorsky S-61, and the Bell 209.
On 3 September 1965 Bell rolled out its Model 209 prototype, and four days later it made its maiden flight, only eight months after the go-ahead. In April 1966, the model won an evaluation against the other rival helicopters. Then the Army signed the first production contract for 110 aircraft. Bell added Cobra to the UH-1's Huey nickname to produce its "HueyCobra" name for the 209. The Army applied the "Cobra" name to its AH-1G designation for the helicopter.
The Bell 209 demonstrator was used for the next six years to test weapons and fit of equipment. It had been modified to the match AH-1 production standard by the early 1970s. The demonstrator was retired to the Patton Museum at Fort Knox, Kentucky and converted to approximately its original appearance.
Into production.
The Bell 209 design was modified for production. The retractable skids were replaced by simpler fixed skids. A new wide-blade rotor was featured. For production, a plexiglass canopy replaced the 209's armored glass canopy which was heavy enough to harm performance. Other changes were incorporated after entering service. The main one of these was moving the tail rotor from the helicopter's left side to the right for improved effectiveness of the rotor.
The U.S. Marine Corps was interested in the Cobra and ordered an improved twin-engined version in 1968 under the designation AH-1J. This would lead to more twin-engine variants. In 1972, the Army sought improved anti-armor capability. Under the Improved Cobra Armament Program (ICAP), trials of eight AH-1s fitted with TOW missiles were conducted in October 1973. After passing qualification tests the following year, Bell was contracted with upgrading 101 AH-1Gs to the TOW-capable AH-1Q configuration. Following AH-1Q operational tests, a more powerful T53 engine and transmission were added from 1976 resulting in the AH-1S version. The AH-1S was upgraded in three steps, culminating with the AH-1F.
Operational history.
United States.
By June 1967, the first AH-1G HueyCobras had been delivered. Originally designated as UH-1H, the "A" for attack designation was soon adopted and when the improved UH-1D became the UH-1H, the HueyCobra became the AH-1G. The AH-1 was initially considered a variant of the H-1 line, resulting in the G series letter.
AH-1 Cobras were in use by the Army during the Tet offensive in 1968 and through to the end of the Vietnam War. Cobras provided fire support for ground forces, escorted transport helicopters and other roles, including aerial rocket artillery (ARA) battalions in the two Airmobile divisions. They also formed "hunter killer" teams by pairing with OH-6A scout helicopters. A team featured one OH-6 flying slow and low to find enemy forces. If the OH-6 drew fire, the Cobra could strike at the then revealed enemy. On 12 September 1968, Capt. Ronald Fogleman was flying an F-100 Super Saber when the aircraft was shot down and he ejected 200 miles north of Bien Hoa. Fogleman became the only pilot to be rescued by holding on to an Army AH-1G's deployed gun-panel door. Bell built 1,116 AH-1Gs for the U.S. Army between 1967 and 1973, and the Cobras chalked up over a million operational hours in Vietnam; the number of Cobras in service peaked at 1,081. Out of nearly 1,110 AH-1s delivered from 1967 to 1973 approximately 300 were lost to combat and accidents during the war. The U.S. Marine Corps used AH-1G Cobras in Vietnam for a short time before acquiring twin-engine AH-1J Cobras.
AH-1 Cobras were deployed for Operation Urgent Fury, the invasion of Grenada in 1983, flying close-support and helicopter escort missions. Army Cobras participated in Operation Just Cause, the U.S. invasion of Panama in 1989.
During Operations Desert Shield and Desert Storm in the Gulf War (1990–91), the Cobras and SuperCobras deployed in a support role. The USMC deployed 91 AH-1W SuperCobras and the US Army 140 AH-1 Cobras; these were operated from forward, dispersed desert bases. Three AH-1s were lost in accidents during fighting and afterward. Cobras destroyed many Iraqi armored vehicles and various targets in the fighting.
Army Cobras provided support for the US humanitarian intervention during Operation Restore Hope in Somalia in 1993. They were also employed during the US invasion of Haiti in 1994. US Cobras were also used in operations later in the 1990s.
The US Army phased out the AH-1 during the 1990s and retired the AH-1 from active service in March 1999, offering them to NATO allies. The Army retired the AH-1 from reserves in September 2001. The retired AH-1s have been passed to other nations and to the USDA Forest Service. The AH-1 continues to be in service with the US military, by the US Marine Corps, which operate the twin-engine AH-1W SuperCobra and AH-1Z Viper.
Israel.
The Israeli Air Force named its Cobras as the "Tzefa" (Hebrew: צפע‎, for Viper). Since the mid-1970s Lebanon has been Israel's most active front; IAF Cobras had been fighting there for more than 20 years.
Cobra helicopters were also used widely by the Israeli Air Force in the 1982 Lebanon War to destroy Syrian armor and fortification. IAF Cobras destroyed dozens of Syrian ground vehicles. The Cobras were also used in major operations against Hezbollah in Operations "Accountability" and "Grapes of Wrath" in southern Lebanon.
Israel retired its fleet of some 33 AH-1 Cobras in late 2013 due to budget cuts. The attack helicopter role was taken up entirely by two squadrons of Israeli AH-64 Apache helicopters, and the fleet of unmanned aerial vehicles took over the role of patrolling combat zones. The Cobra fleet was older than the Apaches and responsible for some fatal crashes, and more expensive to maintain than UAVs and more vulnerable for pilots to MANPADS fired by guerrilla groups.
Japan.
Japan manufactured 89 AH-1S Cobras which were licensed by Fuji Heavy Industries from 1984 to 2000. The type is used by the Japan Ground Self-Defense Force, and are Step 3 models, which are roughly the equivalent to the United States Army's AH-1Fs. The engine is the T53-K-703 turboshaft, which Kawasaki Heavy Industries produced under license.
Pakistan.
Pakistan was supplied with 20 AH-1S gunships by the U.S. between 1984 and 1986, these were later upgraded with the C-NITE thermal imaging package. AH-1s were used as Pakistan's main gunship helicopters against insurgents during the Balochistan conflict. The ongoing War in North-West Pakistan has seen Pakistani AH-1s in action against Taliban and Al Qaeda fighters as well as their tribal allies. The U.S. delivered 12 AH-1Fs to Pakistan in 2007, with 14 more AH-1F Cobras supplied in 2010.
Pakistan has 35 AH-1F helicopters in use. Maintaining these aircraft has been difficult, but possible through commercial channels. Additionally, the U.S. Government has given $750,000 to update a portion of Pakistan Army Aviation's existing AH-1F/S Cobra fleet. Turkey has also supplied spare parts of Cobra helicopters to Pakistan free of cost. Pakistan has repeatedly sought Bell AH-1 SuperCobra attack helicopters from the U.S. to supplement and replace its current AH-1 Cobras. Pakistan has lost 3 aircraft in recent years. Attempts to acquire the AH-1Z Viper or AH-64 Apache from the U.S. have been rejected, so Pakistan turned to buying other foreign attack helicopters to replace its aging AH-1F fleet. Possible candidates included the Turkish TAI T-129, the Chinese CAIC Z-10, and the Russian Mi-35 Hind. In November 2014, Russia approved the sale of Mi-35M helicopters to Pakistan. In April 2015, China delivered to 3 Z-10s to Pakistan. During the same month, the US State Department approved the sale of 15 AH-1Zs to Pakistan along with associated equipment, worth up to $952 million. 
U.S. Forest Service.
In 2003, the U.S. Forest Service acquired 25 retired AH-1Fs from the U.S. Army. These have been designated Bell 209 and are being converted into Firewatch Cobras with infrared and low light sensors and systems for real time fire monitoring. The Florida Division of Forestry has also acquired 3 AH-1Ps from the U.S. Army. These are called Bell 209 "Firesnakes" and are equipped to carry a water/fire retardant system.
Operators.
"For operators of AH-1J, AH-1T, AH-1W, and other AH-1 twin-engine variants, see AH-1 SuperCobra"
former operators.
A small number of former military helicopters are operated by civil organizations for display and demonstration, for example by Red Bull

</doc>
<doc id="37749" url="http://en.wikipedia.org/wiki?curid=37749" title="Marquis de Condorcet">
Marquis de Condorcet

Marie Jean Antoine Nicolas de Caritat, marquis de Condorcet (]; 17 September 1743 – 28 March 1794), known as Nicolas de Condorcet, was a French philosopher, mathematician, and early political scientist whose "Condorcet method" in voting tally selects the candidate who would beat each of the other candidates in a run-off election. Unlike many of his contemporaries, he advocated a liberal economy, free and equal public education, constitutionalism, and equal rights for women and people of all races. His ideas and writings were said to embody the ideals of the Age of Enlightenment and rationalism, and remain influential to this day. He died a mysterious death in prison after a period of flight from French Revolutionary authorities.
Early years.
Condorcet was born in Ribemont (in present-day Aisne), and descended from the ancient family of Caritat, who took their title from the town of Condorcet in Dauphiné, of which they were long-time residents. Fatherless at a young age, he was raised by his devoutly religious mother. He was educated at the Jesuit College in Reims and at the "Collège de Navarre" in Paris, where he quickly showed his intellectual ability, and gained his first public distinctions in mathematics. When he was sixteen, his analytical abilities gained the praise of Jean le Rond d'Alembert and Alexis Clairaut; soon, Condorcet would study under d'Alembert.
From 1765 to 1774, he focused on science. In 1765, he published his first work on mathematics entitled "Essai sur le calcul intégral", which was well received, launching his career as a mathematician. He would go on to publish more papers, and on 25 February 1769, he was elected to the "Académie royale des Sciences" (French Royal Academy of Sciences).
In 1772, he published another paper on integral calculus. Soon after, he met Jacques Turgot, a French economist, and the two became friends. Turgot was to be an administrator under King Louis XV in 1772, and became Controller-General of Finance under Louis XVI in 1774.
Condorcet worked with Leonhard Euler and Benjamin Franklin. He soon became an honorary member of many foreign academies and philosophic societies including the Royal Swedish Academy of Sciences (1785), Foreign Honorary Member of the American Academy of Arts and Sciences (1792),
and also in Prussia and Russia .
His political ideas, however, many of them in continuity with Turgot's, were criticized heavily in the English-speaking world, most notably by John Adams, who wrote two of his principal works of political philosophy to oppose Turgot and Condorcet's unicameral legislature and radical democracy.
Early political career.
In 1774, Condorcet was appointed Inspector General of the Paris mint by Turgot. From this point on, Condorcet shifted his focus from the purely mathematical to philosophy and political matters. In the following years, he took up the defense of human rights in general, and of women's and Blacks' rights in particular (an abolitionist, he became active in the Society of the Friends of the Blacks in the 1780s). He supported the ideals embodied by the newly formed United States, and proposed projects of political, administrative and economic reforms intended to transform France.
In 1776, Turgot was dismissed as Controller General. Consequently, Condorcet submitted his resignation as Inspector General of the "Monnaie", but the request was refused, and he continued serving in this post until 1791. Condorcet later wrote "Vie de M. Turgot" (1786), a biography which spoke fondly of Turgot and advocated Turgot's economic theories. Condorcet continued to receive prestigious appointments: in 1777, he became Permanent Secretary of the Académie des Sciences, holding the post until the abolition of the Académie in 1793, and in 1782 secretary of the "Académie française".
Condorcet's paradox and the Condorcet method.
In 1785, Condorcet wrote "Essai sur l’application de l’analyse à la probabilité des décisions rendues à la pluralité des voix" ("Essay on the Application of Analysis to the Probability of Majority Decisions"), one of his most important works. This work described several now famous results, including Condorcet's jury theorem, which states that if each member of a voting group is more likely than not to make a correct decision, the probability that the highest vote of the group is the correct decision increases as the number of members of the group increases, and Condorcet's paradox, which shows that majority preferences become intransitive with three or more options – it is possible for a certain electorate to express a preference for A over B, a preference for B over C, and a preference for C over A, all from the same set of ballots.
The paper also outlines a generic Condorcet method, designed to simulate pair-wise elections between all candidates in an election. He disagreed strongly with the alternative method of aggregating preferences put forth by Jean-Charles de Borda (based on summed rankings of alternatives). Condorcet was one of the first to systematically apply mathematics in the social sciences.
Other works.
In 1786, Condorcet worked on ideas for the differential and integral calculus, giving a new treatment of infinitesimals – a work which was never printed. In 1789, he published "Vie de Voltaire (1789)", which agreed with Voltaire in his opposition to the Church. In 1798, Thomas Malthus wrote "An Essay on the Principle of Population" partly in response to Condorcet's views on the "perfectibility of society" as outlined in the "Sketch for a Historical Picture of the Progress of the Human Mind". In 1781, Condorcet wrote a pamphlet, "Reflections on Negro Slavery", in which he denounced slavery.
French Revolution.
Deputy.
Condorcet took a leading role when the French Revolution swept France in 1789, hoping for a rationalist reconstruction of society, and championed many liberal causes. As a result, in 1791 he was elected as a Paris representative in the Assemblée, and then became the secretary of the Assembly. 
In April 1792 Condorcet presented a project for the reformation of the education system, aiming to create a hierarchical system, under the authority of experts, who would work as the guardians of the Enlightenment and who, independent of power, would be the guarantors of public liberties. The project was judged to be contrary to the republican and egalitarian virtues, giving the education of the Nation over to an aristocracy of savants. The institution adopted Condorcet's design for the state education system, and he drafted a proposed Bourbon Constitution for the new France. He advocated women's suffrage for the new government, writing an article for "Journal de la Société de 1789", and by publishing "De l'admission des femmes au droit de cité" in 1790.
There were three competing views on which direction France should go, embodied by three political parties: the moderate royalists or Feuillants, republican Girondins, and the more radical Montagnards, led by Maximilien Robespierre. The Feuillants wished to keep the constitutional monarchy as it was developed by the Assemblée, the latter two favored purging France of its royal past ("Ancien Régime"), each in their own way. Condorcet was quite independent, but still counted many friends in the Girondin party. He presided over the Assembly as the Girondins held the majority, until it was replaced by the National Convention, elected in order to design a new constitution. He led the Constitution Committee which drafted the Girondin constitutional project. The constitution was ordered to be printed, but was not put to a vote. When the Montagnards gained control of the Convention, they wrote their own, the "French Constitution of 1793".
At the time of the Trial of Louis XVI, the Girondins had, however, lost their majority in the Convention. Condorcet, who opposed the death penalty but still supported the trial itself, spoke out against the execution of the King during the public vote at the Convention – he proposed to send the king to the galleys. From that moment on, he was usually considered a Girondin. The Montagnards were becoming more and more influential in the Convention as the King's "betrayal" was confirming their theories. One of them, Marie-Jean Hérault de Séchelles, a member, like Condorcet, of the Constitution's Commission, misrepresented many ideas from Condorcet's draft and presented what was called a "Montagnard Constitution". Condorcet criticized the new work, and as a result, he was branded a traitor. On 3 October 1793, a warrant was issued for Condorcet's arrest.
Arrest and death.
The warrant forced Condorcet into hiding. He hid for five (or eight) months in the house of Mme. Vernet, on Rue Servandoni, in Paris. It was there that he wrote "Esquisse d'un tableau historique des progrès de l'esprit humain" ("Sketch for a Historical Picture of the Progress of the Human Spirit"), which was published posthumously in 1795 and is considered one of the major texts of the Enlightenment and of historical thought. It narrates the history of civilization as one of progress in the sciences, shows the intimate connection between scientific progress and the development of human rights and justice, and outlines the features of a future rational society entirely shaped by scientific knowledge.
On 25 March 1794 Condorcet, convinced he was no longer safe, left his hideout and attempted to flee Paris. Two days later he was arrested in Clamart and imprisoned in Bourg-la-Reine (or, as it was known during the Revolution, "Bourg-l'Égalité", "Equality Borough" rather than "Queen's Borough"). Two days after that, he was found dead in his cell. The most widely accepted theory is that his friend, Pierre Jean George Cabanis, gave him a poison which he eventually used. However, some historians believe that he may have been murdered (perhaps because he was too loved and respected to be executed). Jean-Pierre Brancourt (in his work "L'élite, la mort et la révolution") claims that Condorcet was killed with a mixture of "Datura stramonium" and opium.
Condorcet was symbolically interred in the Panthéon in 1989, in honor of the bicentennial of the French Revolution and Condorcet's role as a central figure in the Enlightenment. However his coffin was empty. Interred in the common cemetery of Bourg-la-Reine, his remains were lost during the nineteenth century.
Family.
In 1786 Condorcet married Sophie de Grouchy, who was more than twenty years his junior. His wife, reckoned one of the most beautiful women of the day, became an accomplished salon hostess as Madame de Condorcet, and also an accomplished translator of Thomas Paine and Adam Smith. She was intelligent and well-educated, fluent in both English and Italian. The marriage was a strong one, and Sophie visited her husband regularly while he remained in hiding. Although she began proceedings for divorce in January 1794, it was at the insistence of Condorcet and Cabanis, who wished to protect their property from expropriation and to provide financially for Sophie and their young daughter, Louise 'Eliza' Alexandrine.
Condorcet was survived by his widow and their four-year-old daughter Eliza. Sophie died in 1822, never having remarried, and having published all her husband's works between 1801 and 1804. Her work was carried on by their daughter Eliza Condorcet-O'Connor, wife of former United Irishman Arthur O'Connor. The Condorcet-O'Connors brought out a revised edition between 1847 and 1849.
The Idea of Progress.
Condorcet's "Sketch for a Historical Picture of the Progress of the Human Spirit" (1795) was perhaps the most influential formulation of the idea of progress ever written. It made the Idea of Progress a central concern of Enlightenment thought. He argued that expanding knowledge in the natural and social sciences would lead to an ever more just world of individual freedom, material affluence, and moral compassion. He argued for three general propositions: that the past revealed an order that could be understood in terms of the progressive development of human capabilities, showing that humanity's "present state, and those through which it has passed, are a necessary constitution of the moral composition of humankind"; that the progress of the natural sciences must be followed by progress in the moral and political sciences "no less certain, no less secure from political revolutions"; that social evils are the result of ignorance and error rather than an inevitable consequence of human nature.
Condorcet's writings were a key contribution to the French Enlightenment, particularly his work on the Idea of Progress. Condorcet believed that through the use of our senses and communication with others, knowledge could be compared and contrasted as a way of analyzing our systems of belief and understanding. None of Condorcet's writings refer to a belief in a religion or a god who intervenes in human affairs. Condorcet instead frequently had written of his faith in humanity itself and its ability to progress with the help of philosophers such as Aristotle. Through this accumulation and sharing of knowledge he believed it was possible for any man to comprehend all the known facts of the natural world. The enlightenment of the natural world spurred the desire for enlightenment of the social and political world. Condorcet believed that there was no definition of the perfect human existence and thus believed that the progression of the human race would inevitably continue throughout the course of our existence. He envisioned man as continually progressing toward a perfectly utopian society. However, he stressed that for this to be a possibility man must unify regardless of race, religion, culture or gender.
Civic duty.
For Condorcet's republicanism the nation needed enlightened citizens and education needed democracy to become truly public. Democracy implied free citizens, and ignorance was the source of servitude. Citizens had to be provided with the necessary knowledge to exercise their freedom and understand the rights and laws that guaranteed their enjoyment. Although education could not eliminate disparities in talent, all citizens, including women, had the right to free education. In opposition to those who relied on revolutionary enthusiasm to form the new citizens, Condorcet maintained that revolution was not made to last and that revolutionary institutions were not intended to prolong the revolutionary experience but to establish political rules and legal mechanisms that would insure future changes without revolution. In a democratic city there would be no Bastille to be seized. Public education would form free and responsible citizens, not revolutionaries.
Evaluation.
Rothschild (2001) argues that Condorcet has been seen since the 1790s as the embodiment of the cold, rational Enlightenment. However she suggests his writings on economic policy, voting, and public instruction indicate different views both of Condorcet and of the Enlightenment. Condorcet was concerned with individual diversity; he was opposed to proto-utilitarian theories; he considered individual independence, which he described as the characteristic liberty of the moderns, to be of central political importance; and he opposed the imposition of universal and eternal principles. His efforts to reconcile the universality of some values with the diversity of individual opinions are of continuing interest. He emphasizes the institutions of civilized or constitutional conflict, recognizes conflicts or inconsistencies within individuals, and sees moral sentiments as the foundation of universal values. His difficulties call into question some familiar distinctions, for example between French, German, and English-Scottish thought, and between the Enlightenment and the counter-Enlightenment. There was substantial continuity between Condorcet's criticism of the economic ideas of the 1760s and the liberal thought of the early 19th century.
The Lycée Condorcet in the rue du Havre, in the 9th arrondissement of Paris is named in his honour.

</doc>
<doc id="37756" url="http://en.wikipedia.org/wiki?curid=37756" title="Delhi">
Delhi

Delhi (, ] Dilli), officially the National Capital Territory of Delhi, is the Capital territory of India. It has a population of about 11 million and a metropolitan population of about 16.3 million, making it the second most populous city and second most populous urban agglomeration in India. Such is the nature of urban expansion in Delhi that its growth has expanded beyond the NCT to incorporate towns in neighbouring states and at its largest extent can count a population of about 25 million residents as of 2014.
The NCT and its urban region have been given the special status of National Capital Region (NCR) under the Constitution of India's 69th amendment act of 1991. The NCR includes the neighbouring cities of Gurgaon, Noida, Ghaziabad, Faridabad, Neharpar (Greater Faridabad), Greater Noida, Bahadurgarh, Sonepat, Panipat, Karnal, Rohtak, Bhiwani, Rewari, Baghpat, Meerut, Alwar, Bharatpur and other nearby towns. A union territory, the political administration of the NCT of Delhi today more closely resembles that of a state of India, with its own legislature, high court and an executive council of ministers headed by a Chief Minister. New Delhi is jointly administered by the federal government of India and the local government of Delhi, and is the capital of the NCT of Delhi.
Delhi has been continuously inhabited since the 6th century BC. Through most of its history, Delhi has served as a capital of various kingdoms and empires. It has been captured, ransacked and rebuilt several times, particularly during the medieval period, and modern Delhi is a cluster of a number of cities spread across the metropolitan region.
Toponymy and idioms.
There are a number of legends associated with the origin of the name "Delhi". One is that it is derived from "Dhillu" or "Dilu", a king who built a city at this location in 50 BC and named it after himself. Another legend holds that the name of the city is based on the Hindi/Prakrit word "dhili" ("loose") and that it was used by the Tomaras to refer to the city because the Iron Pillar of Delhi had a weak foundation and had to be moved. The coins in circulation in the region under the Tomaras were called "dehliwal". According to the Bhavishya Purana, King Prithiviraja,of Indraprastha built a new fort in the modern-day Purana Qila area for the convenience of all four castes in his kingdom. He ordered the construction of a gateway to the fort and later named the fort "dehali". Some historians believe that the name is derived from "Dilli", a corruption of "dehleez" or "dehali"—both terms meaning 'threshold' or 'gateway'—and symbolic of the city as a gateway to the Gangetic Plain. Another theory suggests that the city's original name was Dhillika.
The people of Delhi are referred to as "Delhiites" or "Dilliwalas". The city is referenced in various idioms of the Northern Indo-Aryan languages. Examples include:
History.
The area around Delhi was probably inhabited before the second millennium BC, and there is evidence of continuous inhabitation since at least the 6th century BC. The city is believed to be the site of Indraprastha, the legendary capital of the Pandavas in the Indian epic Mahabharata. According to this epic this land was initially a huge mass of forests called 'Kandavaprastha' which was burnt down to build the city of Indraprastha. The earliest architectural relics date back to the Maurya period (c. 300 BC); in 1966, an inscription of the Mauryan Emperor Ashoka (273–235 BC) was discovered near Srinivaspuri. Remains of eight major cities have been discovered in Delhi. The first five cities were in the southern part of present-day Delhi. Anang Pal of the Tomara dynasty founded the city of Lal Kot in AD 736. The Chauhans conquered Lal Kot in 1180 and renamed it Qila Rai Pithora.
The king Prithviraj Chauhan was defeated in 1192 by Muhammad Ghori, a Tajik invader from Afghanistan, who made a concerted effort to conquer northern India. By 1200, native Hindu resistance had begun to crumble, the dominance of foreign Turkic Muslim dynasties in India was to last for the next five centuries. On the death of Muhammad in 1206, the Turkic slave-general, Qutb-ud-din Aibak, broke away from the Ghurid Dynasty and became the first Sultan of Delhi. He began construction of the Qutb Minar and Quwwat-al-Islam (might of Islam) mosque, the earliest extant mosque in India. Qutb-ud-din faced widespread Hindu rebellions and it was his successor, Iltutmish (1211–36), who consolidated the Turkic conquest of northern India.
For the next three hundred years, Delhi was ruled by a succession of Turkic and an Afghan, Lodhi dynasty. They built a number of forts and townships that are part of the seven cities of Delhi. Delhi was a major centre of Sufism during this period. The Mamluk Sultanate (Delhi) was overthrown in 1290 by the Khilji dynasty (1290–1320). Under the second Khilji ruler, Ala-ud-din Khilji, the Delhi sultanate extended its control south of the Narmada River in the Deccan. The Delhi sultanate reached its greatest extent during the reign of Muhammad bin Tughluq (1325–1351). In an attempt to bring the whole of the Deccan under control, he moved his capital to Daulatabad, Maharashtra in central India, but by moving away from Delhi he lost control of the north and was forced to return to Delhi to restore order. The southern provinces then broke away. In the years following the reign of Firoz Shah Tughlaq (1351–1388), the Delhi sultanate rapidly began to lose its hold over its northern provinces. Delhi was captured and sacked by Timur Lenk in 1398. Near Delhi, Timur massacred 100,000 captives. Delhi's decline continued under the Sayyid dynasty (1414–1451), until the sultanate was reduced to Delhi and its hinterland. Under the Afghan Lodhi dynasty (1451–1526), the Delhi sultanate recovered control of the Punjab and the Gangetic plain to once again achieve domination over northern India. However, the recovery was short-lived and in 1526 the sultanate was destroyed by Babur, founder of the Mughal dynasty.
In 1526, Babur, a descendant of Genghis Khan and Timur, from the Fergana Valley in modern-day Uzbekistan, invaded India, defeated the last Lodhi sultan in the First Battle of Panipat and founded the Mughal Empire that ruled from Delhi and Agra. The Mughal dynasty ruled Delhi for more than three centuries, with a sixteen-year hiatus during the reign of Sher Shah Suri, from 1540 to 1556. In 1553, the Hindu king, Hemu Vikramaditya acceded to the throne of Delhi by defeating forces of Mughal Emperor Humayun at Agra and Delhi. However, the Mughals re-established their rule after Akbar's army defeated Hemu during the Second Battle of Panipat in 1556. Shah Jahan built the seventh city of Delhi that bears his name "Shahjahanabad", which served as the capital of the Mughal Empire from 1638 and is today known as the "Old City" or "Old Delhi".
After the death of Aurangzeb in 1707, the Mughal Empire's influence declined rapidly as the Hindu Maratha Empire rose to prominence. In 1737, Maratha forces sacked Delhi following their victory against the Mughals in the First Battle of Delhi. In 1739, the Mughal Empire lost the huge Battle of Karnal in less than three hours against the numerically outnumbered but military superior Persian army led by Nader Shah of Persia during his invasion after which he completely sacked and looted Delhi, the Mughal capital, carrying away immense wealth including the Peacock Throne, the Daria-i-Noor, and Koh-i-Noor. The Mughals, severely further weakened, would never overcome this crushing defeat and humiliation which would also let the way open for more invaders to come, including eventually the British. Nader eventually agreed to leave the city and India after forcing the Mughal emperor Muhammad Shah I to beg him for mercy and granting him the keys of the city and the royal treasury. A treaty signed in 1752 made Marathas the protectors of the Mughal throne in Delhi. 
In 1757, the Afghan ruler, Ahmad Shah Durrani, sacked Delhi. He returned to Afghanistan leaving a Mughal puppet ruler in nominal control. The Marathas again occupied Delhi in 1758, and were in control before their defeat in 1761 at the third battle of Panipat, and the city was captured again by Ahmad Shah. However, in 1771, the Marathas established a protectorate over Delhi when the Maratha ruler, Mahadji Shinde, recaptured Delhi and the Mughal Emperor Shah Alam II was installed as a puppet ruler in 1772. In 1783, Sikhs under Baghel Singh captured Delhi and Red Fort, however due to treaty Sikhs withdraw from Red Fort and agreed to restore Shah Alam as the emperor.In 1803, during the Second Anglo-Maratha War, the forces of British East India Company defeated the Maratha forces in the Battle of Delhi. During the Indian Rebellion of 1857, Delhi fell to the forces of East India Company after a bloody fight known as the Siege of Delhi. The city came under the direct control of the British Government in 1858. It was made a district province of the Punjab. In 1911, it was announced that the capital of British held territories in India was to be transferred from Calcutta to Delhi. The name "New Delhi" was given in 1927, and the new capital was inaugurated on 13 February 1931. New Delhi, also known as "Lutyens' Delhi", was officially declared as the capital of the Union of India after the country gained independence on 15 August 1947.
During the partition of India, thousands of Hindu and Sikh refugees, mainly from West Punjab fled to Delhi, while many Muslim residents of the city migrated to Pakistan. Migration to Delhi from the rest of India continues (as of 2013), contributing more to the rise of Delhi's population than the birth rate, which is declining.
The Constitution (Sixty-ninth Amendment) Act, 1991 declared the Union Territory of Delhi to be formally known as the National Capital Territory of Delhi. The Act gave Delhi its own legislative assembly along Civil lines, though with limited powers. In December 2001, the Parliament of India building in New Delhi was attacked by armed militants, killing six security personnel. India suspected Pakistan-based militant groups were behind the attack, which caused a major diplomatic crisis between the two countries. There were further terrorist attacks in Delhi in October 2005 and September 2008, resulting in a total of 103 deaths.
Ecology.
Delhi is located at , and lies in Northern India. It borders the Indian states of Haryana on the north, west and south and Uttar Pradesh (UP) to the east. During the British Raj, Delhi was part of the province of Punjab and is still historically and culturally connected to the Punjab region. Two prominent features of the geography of Delhi are the Yamuna flood plains and the Delhi ridge. The Yamuna river was the historical boundary between Punjab and UP, and its flood plains provide fertile alluvial soil suitable for agriculture but are prone to recurrent floods. The Yamuna, a sacred river in Hinduism, is the only major river flowing through Delhi. The Hindon River separates Ghaziabad from the eastern part of Delhi. The Delhi ridge originates from the Aravalli Range in the south and encircles the west, north-east and north-west parts of the city. It reaches a height of 318 m and is a dominant feature of the region.
The National Capital Territory of Delhi covers an area of 1484 km2, of which 783 km2 is designated rural, and 700 km2 urban therefore making it the largest city in terms of area in the country. It has a length of 51.9 km and a width of 48.48 km.
Delhi is included in India's seismic zone-IV, indicating its vulnerability to major earthquakes, but earthquakes have not been common in recent history.
Climate.
Delhi features an atypical version of the humid subtropical climate (Köppen "Cwa"). The warm season lasts from 9 April to 8 July with an average daily high temperature above 36 °C. The hottest day of the year is 22 May, with an average high of 38 °C and low of 25 °C. The cold season lasts from 11 December to 11 February with an average daily high temperature below 18 °C. The coldest day of the year is 4 January, with an average low of 2 °C and high of 15 °C. In early March, the wind direction changes from north-westerly to south-westerly. From April to October the weather is hot. The monsoon arrives at the end of June, along with an increase in humidity. The brief, mild winter starts in late November, peaks in January and heavy fog often occurs.
Temperatures in Delhi usually range from 5 to, with the lowest and highest temperatures ever recorded being -6.7 and respectively. The annual mean temperature is 25 °C; monthly mean temperatures range from 13 to. The highest temperature recorded in July was 45 °C in 1931. The average annual rainfall is approximately 714 mm, most of which falls during the monsoon in July and August. The average date of the advent of monsoon winds in Delhi is 29 June.
Air pollution.
Delhi is the most polluted city in the world and according to one estimate, air pollution causes the death of about 10,500 people in Delhi every year. During 2013-14, peak levels of fine particulate matter (PM) in Delhi increased by about 44%, primarily due to high vehicular and industrial emissions, construction work and crop burning in adjoining states. Delhi has the highest level of the airborne particulate matter, PM2.5 considered most harmful to health, with 153 micrograms. Rising air pollution level has significantly increased lung-related ailments (especially asthma and lung cancer) among Delhi's children and women. The dense smog in Delhi during winter season results in major air and rail traffic disruptions every year. According to Indian meteorologists, the average maximum temperature in Delhi during winters has declined notably since 1998 due to rising air pollution.
Environmentalists have criticised the Delhi government for not doing enough to curb air pollution and to inform people about air quality issues. Most of Delhi's residents are unaware of alarming levels of air pollution in the city and the health risks associated with it; however, as of 2015, awareness, particularly among the foreign diplomatic community and high-income Indians, was noticeably increasing. Since the mid-1990s, Delhi has undertaken some measures to curb air pollution – Delhi has the third highest quantity of trees among Indian cities and the Delhi Transport Corporation operates the world's largest fleet of environmentally friendly compressed natural gas (CNG) buses. In 1996, the Centre for Science and Environment (CSE) started a public interest litigation in the Supreme Court of India that ordered the conversion of Delhi's fleet of buses and taxis to run on compressed natural gas (CNG) and banned the use of leaded petrol in 1998. In 2003, Delhi won the United States Department of Energy's first 'Clean Cities International Partner of the Year' award for its "bold efforts to curb air pollution and support alternative fuel initiatives". The Delhi Metro has also been credited for significantly reducing air pollutants in the city.
However, according several authors, most of these gains have been lost, especially due to stubble burning, a rise in the market share of diesel cars and a considerable decline in bus ridership. According to CSE and System of Air Quality Weather Forecasting and Research (SAFAR), burning of agricultural waste in nearby Punjab, Haryana and Uttar Pradesh regions results in severe intensification of smog over Delhi. The state government of Uttar Pradesh is considering imposing a ban on crop burning to reduce pollution in Delhi NCR and an environmental panel has appealed to India's Supreme Court to impose a 30% cess on diesel cars.
The Circles of Sustainability assessment of Delhi gives a marginally more favourable impression of the ecological sustainability of the city only because it is based on a more comprehensive series of measures than only air pollution. Part of the reason that the city remains assessed at basic sustainability is because of the low resource-use and carbon emissions of its poorer neighbourhoods.
Civic administration.
As of July 2007, the National Capital Territory of Delhi comprises nine districts, 27 tehsils, 59 census towns, 300 villages, and three statutory towns, the Municipal Corporation of Delhi (MCD) – 1397.3 km2, the New Delhi Municipal Council (NDMC) – 42.7 km2 and the Delhi Cantonment Board (DCB) – 43 km2). On 16 July 2012, the Delhi Government decided to increase the number of districts from nine to 11.
The Delhi metropolitan area lies within the National Capital Territory of Delhi (NCT), which has five local municipal corporations; , , , NDMC and DCB. The former MCD was divided into three smaller Municipal Corporations – North Delhi, South Delhi and East Delhi. According to the 2011 census, MCD is among the largest municipal bodies in the world, providing civic services to about 11 million people.
Delhi (civic administration) was ranked 5th out of 21 Cities for best governance & administrative practices in India in 2014. It scored 3.6 on 10 compared to the national average of 3.3.
Delhi houses the Supreme Court of India, and the regional Delhi High Court, along with the Small Causes Court for civil cases; the Magistrate Court and the Sessions Court for criminal cases, has jurisdiction over Delhi. The city is administratively divided into eleven police-zones, which are subdivided into 95 local police stations.
Government and politics.
The National Capital Territory of Delhi has its own Legislative Assembly, Lieutenant Governor, council of ministers and Chief Minister. Members of the legislative assembly are directly elected from territorial constituencies in the NCT. The legislative assembly was abolished in 1956, after which direct federal control was implemented until it was re-established in 1993. The Municipal Corporation of Delhi (MCD) handles civic administration for the city as part of the Panchayati Raj Act. The Government of India and the Government of National Capital Territory of Delhi jointly administer New Delhi, where both bodies are located. The Parliament of India, the Rashtrapati Bhavan (Presidential Palace), Cabinet Secretariat and the Supreme Court of India are located in the municipal district of New Delhi. There are 70 assembly constituencies and seven Lok Sabha (Indian parliament's lower house) constituencies in Delhi.
The Indian National Congress (Congress) formed all the governments in Delhi until the 1990s, when the Bharatiya Janata Party (BJP), led by Madan Lal Khurana, came to power. In 1998, the Congress returned to power under the leadership of Sheila Dikshit, who was subsequently re-elected for 3 consecutive terms. But in 2013, the Congress was ousted from power, with the newly formed Aam Aadmi Party (AAP) led by Arvind Kejriwal forming the government with outside support from the Congress. However, that government was short-lived, collapsing only after 49 days. Delhi was then under President's rule till February, 2015. On February 10, 2015, the Aam Aadmi Party returned to power after a landslide victory, winning 67 out of the 70 seats in Delhi Legislative Assembly.
Economy.
Delhi is the largest commercial centre in northern India; it has an estimated net State Domestic Product (FY 2010) of ₹1595 billion () in nominal terms and ~₹6800 billion () in PPP terms. As of 2013, the per capita income of Delhi was Rs. 230000, highest in India. GSDP in Delhi at the current prices for 2012-13 is estimated at Rs 3.88 trillion (short scale) against Rs 3.11 trillion (short scale) in 2011-12.
As per the Economic survey of Delhi (2005–2006), the tertiary sector contributes 70.95% of Delhi's gross SDP followed by secondary and primary sectors, with 25.20% and 3.85% contributions respectively. Delhi's workforce constitutes 32.82% of the population, and increased by 52.52% between 1991 and 2001. Delhi's unemployment rate decreased from 12.57% in 1999–2000 to 4.63% in 2003. In December 2004, 636,000 people were registered with various employment exchange programs in Delhi.
In 2001 the total workforce in national and state governments and the quasi-government sector was 620,000, and the private sector employed 219,000. Key service industries are information technology, telecommunications, hotels, banking, media and tourism. Construction, power, health and community services, and real estate are also important to the city's economy. Delhi has one of India's largest and fastest growing retail industries. Manufacturing also grew considerably as consumer goods companies established manufacturing units and headquarters in the city. Delhi's large consumer market and the availability of skilled labour has attracted foreign investment. In 2001, the manufacturing sector employed 1,440,000 workers and the city had 129,000 industrial units.
Utility services.
Delhi's municipal water supply is managed by the Delhi Jal Board (DJB). As of 2005–06, it supplied 650 million gallons per day (MGD), whereas the estimated consumption requirement is 963 MGD. The shortfall is met by private and public tube wells and hand pumps. At 240 MGD, the Bhakra storage is DJB's largest water source, followed by the Yamuna and Ganges rivers. Delhi's groundwater level is falling and its population density is increasing, so residents often encounter acute water shortage. In Delhi, daily domestic solid waste production is 8000 tonnes which is dumped at three landfill locations by MCD. The daily domestic waste water production is 470 MGD and industrial waste water is 70 MGD. A large portion of the sewage flows untreated into the Yamuna river.
The city's electricity consumption is about 1,265 kWh per capita, but actual demand is higher. In Delhi power distribution is managed by Tata Power Distribution and BSES Rajdhani since 2002. The Delhi Fire Service runs 43 fire stations that attend about 15,000 fire and rescue calls per year. The state-owned Mahanagar Telephone Nigam Limited (MTNL) and private enterprises Vodafone, Airtel, Idea cellular, Reliance Infocomm, Aircel and Tata Docomo provide telephone and cell phone service to the city. Cellular coverage is available in GSM, CDMA, 3G and 4G.
Transport.
Air.
Indira Gandhi International Airport, situated to the southwest of Delhi, is the main gateway for the city's domestic and international civilian air traffic. In 2012-13, the airport was used by more than 35 million passengers, making it one of the busiest airports in South Asia. Terminal 3, which cost ₹96.8 billion () to construct between 2007 and 2010, handles an additional 37 million passengers annually.
The "Delhi Flying Club", established in 1928 with two de Havilland Moth aircraft named "Delhi" and "Roshanara", was based at "Safdarjung Airport" which started operations in 1929, when it was the Delhi's only airport and the second in India. The airport functioned until 2001, however in January 2002 the government closed the airport for flying activities because of security concerns following the New York attacks in September 2001. Since then, the club only carries out aircraft maintenance courses, and is used for helicopter rides to Indira Gandhi International Airport for VIP including the president and the prime minister.
A second airport open for commercial flights has been suggested, by expansion of Meerut Airport or construction of a new airport in Greater Noida.
Road.
Delhi has the highest road density of 2103 km/100 sq. km in India.
Buses are the most popular means of road transport catering to about 60% of Delhi's total demand. Delhi has one of India's largest bus transport systems. Buses are operated by the state-owned Delhi Transport Corporation (DTC), which owns largest fleet of Compressed Natural Gas (CNG)-fueled buses in the world. Personal vehicles especially cars also form a major chunk of vehicles plying on Delhi roads. Delhi has the highest number of registered cars compared to any other metropolitan city in India. Taxis, Auto Rickshaws and Cycle Rickshaws also ply on Delhi roads in large numbers.
Important Roads in Delhi
Some roads and expressways serve as important pillars of Delhi's road infrastructure:
National Highways Passing Through Delhi
Delhi is connected by Road to various parts of the country through several National highways:
Railway.
Delhi is a major junction in the Indian railway network and is the headquarters of the Northern Railway. The five main railway stations are New Delhi railway station, Old Delhi, Nizamuddin Railway Station, Anand Vihar Railway Terminal and Sarai Rohilla. The Delhi Metro, a mass rapid transit system built and operated by Delhi Metro Rail Corporation (DMRC), serves many parts of Delhi and the neighbouring cities Gurgaon, Noida and Ghaziabad. As of August 2011, the metro consists of six operational lines with a total length of 189 km and 146 stations, and several other lines are under construction. The Phase-I was built at a cost of US$2.3 billion and the Phase-II was expected to cost an additional ₹216 billion (). Phase-II has a total length of 128 km and was completed by 2010. Delhi Metro completed 10 years of operation on 25 December 2012. It carries millions of passengers every day. In addition to the Delhi Metro, a suburban railway, the Delhi Suburban Railway exists.
Metro.
The Delhi Metro is a rapid transit system serving Delhi, Gurgaon, Faridabad, Noida, and Ghaziabad in the National Capital Region of India. Delhi Metro is the world's 13th largest metro system in terms of length. Delhi Metro was India's first modern public transportation system, which has revolutionised travel by providing a fast, reliable, safe, and comfortable means of transport. The network consists of six lines with a total length of 189.63 km with 142 stations, of which 35 are underground, five are at-grade, and the remainder are elevated. All stations have escalators, elevators, and tactile tiles to guide the visually impaired from station entrances to trains. It has a combination of elevated, at-grade, and underground lines, and uses both broad gauge and standard gauge rolling stock. Four types of rolling stock are used: Mitsubishi-ROTEM Broad gauge, Bombardier MOVIA, Mitsubishi-ROTEM Standard gauge, and CAF Beasain Standard gauge. The Phase-I of Delhi Metro was built at a cost of US$2.3 billion and the Phase-II was expected to cost an additional ₹216 billion (). Phase-II has a total length of 128 km and was completed by 2010. Delhi Metro completed 10 years of operation on 25 December 2012. It carries millions of passengers every day. In addition to the Delhi Metro, a suburban railway, the Delhi Suburban Railway exists.
Delhi Metro is being built and operated by the Delhi Metro Rail Corporation Limited (DMRC), a state-owned company with equal equity participation from Government of India and Government of National Capital Territory of Delhi. However, the organisation is under administrative control of Ministry of Urban Development, Government of India. Besides construction and operation of Delhi metro, DMRC is also involved in the planning and implementation of metro rail, monorail and high-speed rail projects in India and providing consultancy services to other metro projects in the country as well as abroad. The Delhi Metro project was spearheaded by Padma Vibhushan E. Sreedharan, the Managing Director of DMRC and popularly known as the "Metro Man" of India. He famously resigned from DMRC, taking moral responsibility for a metro bridge collapse which took five lives. Sreedharan was awarded with the prestigious Legion of Honour by the French Government for his contribution to Delhi Metro.
Regional Rapid Transit System (RRTS).
The 08 RRTS Corridors have been proposed by National Capital Region Planning Board (NCRPB) to facilitate the people travelling from nearby cities in NCR to Delhi. The three main corridors in first phase are as follows which are expected to become operational before 2019:
Remaining five corridors are also approved by National Capital Region Planning Board but are planned in the second phase.
To make the project operational NCRPB has formed a separate body named as "National Capital Region Transport Corporation" on the lines of DMRC to independently formalise and monitor its progress.
Roads of 2006 and 2007.
As of 2007, private vehicles account for 30% of the total demand for transport. Delhi has 1922.32 km of road length per 100 km2, one of the highest road densities in India. It is connected to other parts of India by five National Highways: NH 1, 2, 8, 10 and 24. The city's road network is maintained by MCD, NDMC, Delhi Cantonment Board, Public Works Department (PWD) and Delhi Development Authority. The Delhi-Gurgaon Expressway connects Delhi with Gurgaon and the international airport. connects Delhi with the neighbouring industrial town of Faridabad. The DND Flyway and Noida-Greater Noida Expressway connect Delhi with the suburbs of Noida and Greater Noida. Delhi's rapid rate of economic development and population growth has resulted in an increasing demand for transport, creating excessive pressure on the city's transport infrastructure. As of 2008, the number of vehicles in the metropolitan region, Delhi NCR, is 11.2 million (11.2 million). In 2008, there were 85 cars in Delhi for every 1,000 of its residents.
To meet the transport demand, the State and Union government constructed a mass rapid transit system, including the Delhi Metro. In 1998, the Supreme Court of India ordered that all public transport vehicles in Delhi must be fuelled by compressed natural gas (CNG). Buses are the most popular means of public transport, catering for about 60% of the total demand. The state-owned Delhi Transport Corporation (DTC) is a major bus service provider which operates the world's largest fleet of CNG-fuelled buses. Delhi Bus Rapid Transit System runs between Ambedkar Nagar and Delhi Gate.
Demographics.
According to the 2011 census of India, the population of Delhi is 16,753,235. The corresponding population density was 11,297 persons per km2, with a sex ratio of 866 women per 1000 men, and a literacy rate of 86.34%. In 2004, the birth rate, death rate and infant mortality rate per 1000 population were 20.03, 5.59 and 13.08, respectively. In 2001, the population of Delhi increased by 285,000 as a result of migration and by 215,000 as a result of natural population growth – this made Delhi one of the fastest growing cities in the world. By 2015, Delhi is expected to be the third-largest conurbation in the world after Tokyo and Mumbai. Dwarka Sub City, Asia's largest planned residential area, is located within the National Capital Territory of Delhi.
Hinduism is Delhi's predominant religious faith, with approximately 81% of Delhi's population, followed by Islam(9.9%), Sikhism(5%), Jainism(1.1%), and others(1.2%). Other minority religions include Buddhism, Zoroastrianism, Christianity, Baha'ism and Judaism. Punjabi & Hindi are the most widely spoken languages in Delhi. English is the principal written language of the city and the most commonly used language for the official purposes. In addition to Hindi and English, Punjabi with Gurmukhī alphabets and Urdu also have official language status in Delhi.
According a 1999–2000 estimate, the total number of people living below the poverty line, defined as living on US$11 or less per month, in Delhi was 1,149,000, or 8.23% of the total population, compared to 27.5% of India as a whole. 52% of Delhi residents who live in slums without basic services like water, electricity, sanitation, sewage system or proper housing. In 2005, Delhi accounted for the highest percentage (16.2%) of the crimes reported in 35 Indian cities with populations of one million or more. The city has the highest rate of kidnapping and abduction cases with 9.3%; the national rate is 2.2%. Delhi accounts for 15.4% of crime against women in Indian cities.
Findings from surveys conducted by the Centre for the Study of Developing Societies (CSDS) in Delhi estimate an average of 40% of the voters in Delhi belong to the upper castes. About 12% are Brahmins, 7% are Punjabi Khatris, 7% are Rajputs, 6% belong to the Vaishya (Bania) and Jain communities and 8% are from other upper castes.Jat community, roughly 5% of Delhi's population and located mostly in the rural parts of outer Delhi. OBC communities such as the Gujjars, Yadavs and the lower OBCs together form about 18% of Delhi's population. The Dalit communities 17% of Delhi's population.
Culture.
Delhi's culture has been influenced by its lengthy history and historic association as the capital of India. This is exemplified by many significant monuments in the city. Delhi is also identified as the location of Indraprastha, the ancient capital of the Pandavas. The Archaeological Survey of India recognises 1200 heritage buildings and 175 monuments as national heritage sites. In the Old City, the Mughals and the Turkic rulers constructed several architecturally significant buildings, such as the Jama Masjid – India's largest mosque and the Red Fort. Three World Heritage Sites – the Red Fort, Qutab Minar and Humayun's Tomb – are located in Delhi. Other monuments include the India Gate, the Jantar Mantar – an 18th-century astronomical observatory – and the Purana Qila – a 16th-century fortress. The Laxminarayan temple, Akshardham temple, the Bahá'í Lotus temple and the ISKCON temple are examples of modern architecture. Raj Ghat and associated memorials houses memorials of Mahatma Gandhi and other notable personalities. New Delhi houses several government buildings and official residences reminiscent of British colonial architecture, including the Rashtrapati Bhavan, the Secretariat, Rajpath, the Parliament of India and Vijay Chowk. Safdarjung's Tomb is an example of the Mughal gardens style. Some regal "havelis" (palatial residences) are in the Old City.
Lotus Temple, is a Bahá'í House of Worship completed in 1986. Notable for its flowerlike shape, it serves as the Mother Temple of the Indian subcontinent and has become a prominent attraction in the city. The Lotus Temple has won numerous architectural awards and been featured in hundreds of newspaper and magazine articles. Like all other Bahá'í Houses of Worship, is open to all regardless of religion, or any other distinction, as emphasised in Bahá'í texts. The Bahá'í laws emphasise that the spirit of the House of Worship be that it is a gathering place where people of all religions may worship God without denominational restrictions. The Bahá'í laws also stipulate that only the holy scriptures of the Bahá'í Faith and other religions can be read or chanted inside in any language; while readings and prayers can be set to music by choirs, no musical instruments can be played inside. Furthermore no sermons can be delivered, and there can be no ritualistic ceremonies practised.
Chandni Chowk, a 17th-century market, is one of the most popular shopping areas in Delhi for jewellery and "Zari" saris. Delhi's arts and crafts include, "Zardozi"  – an embroidery done with gold thread – and "Meenakari" – the art of enamelling.
Festivals.
Delhi's association and geographic proximity to the capital, New Delhi, has amplified the importance of national events and holidays like Republic Day, Independence Day (15 August) and "Gandhi Jayanti". On Independence Day, the Prime Minister addresses the nation from the Red Fort. Most Delhiites celebrate the day by flying kites, which are considered a symbol of freedom. The Republic Day Parade is a large cultural and military parade showcasing India's cultural diversity and military strength. Over the centuries, Delhi has become known for its composite culture, and a festival that symbolises this is the "Phool Walon Ki Sair", which takes place in September. Flowers and "pankhe" – fans embroidered with flowers – are offered to the shrine of 13th century Sufi saint Khwaja Bakhtiyar Kaki and the Yogmaya temple, both situated in Mehrauli.
Religious festivals include "Diwali" (the festival of lights), "Mahavir Jayanti", Guru Nanak's Birthday, "Raksha Bandhan", "Durga Puja", "Holi", "Lohri", "Chauth", "Krishna Janmastami", "Maha Shivratri", Eid ul-Fitr, "Moharram" and "Buddha Jayanti". The Qutub Festival is a cultural event during which performances of musicians and dancers from all over India are showcased at night, with the Qutub Minar as a backdrop. Other events such as Kite Flying Festival, International Mango Festival and "Vasant Panchami" (the Spring Festival) are held every year in Delhi. The Auto Expo, Asia's largest auto show, is held in Delhi biennially. The World Book Fair, held biannually at the Pragati Maidan, is the second largest exhibition of books in the world. Delhi is often regarded as the "Book Capital" of India because of high readership.
Cuisine.
As India's national capital and centuries old Mughal capital, Delhi influenced the food habits of its residents and is where Mughlai cuisine originated. Along with Indian cuisine, a variety of international cuisines are popular among the residents. The dearth of food habits among the city's residents created a unique style of cooking which became popular throughout the world, with dishes such as "Kebab", "biryani", "tandoori". The city's classic dishes include Butter chicken, "Aloo Chaat", "chaat", "dahi vada", "kachori", "chole bhature", Chole kulche, "jalebi" and "lassi".:40–50, 189–196
The fast living habits of Delhi's people has motivated the growth of street food outlets.:41 A trend of dining at local "dhabas" is popular among the residents. High profile restaurants have gained popularity in recent years, among the popular restaurants are the Karim Hotel, the Punjab Grill and Bukhara. The "Gali Paranthe Wali" (the street of fried bread) is a street in Chandni Chowk particularly for food eateries since the 1870s. Almost the entire street is occupied by fast food stalls or street vendors. It has nearly become a tradition that almost every prime minister of India has visited the street to eat "paratha" at least once. Other Indian cuisines are also available in this area even though the street specializes in north Indian food .:40–50
Education.
Private schools in Delhi – which use either English or Hindi as the language of instruction – are affiliated to one of three administering bodies, the "Council for the Indian School Certificate Examinations" (CISCE), the "Central Board for Secondary Education" (NCERT (CBSE)) or the "National Institute of Open Schooling" (NIOS). In 2004–05, approximately 15.29 lakh (1.529 million) students were enrolled in primary schools, 8.22 lakh (0.822 million) in middle schools and 6.69 lakh (0.669 million) in secondary schools across Delhi. Female students represented 49% of the total enrolment. The same year, the Delhi government spent between 1.58% and 1.95% of its gross state domestic product on education.
Schools and higher educational institutions in Delhi are administered either by the Directorate of Education, the NCT government or private organisations. In 2006, Delhi had 165 colleges, five medical colleges and eight engineering colleges, seven major universities and nine deemed universities. Indraprastha Institute of Information Technology, Delhi Technological University, Guru Gobind Singh Indraprastha University and National Law University are the only state universities, Indira Gandhi National Open University is for distance education and the rest are central universities.
As of 2008, about 16% of all Delhi residents possessed at least a college graduate degree.
Media.
As the capital of India, Delhi is the focus of political reportage, including regular television broadcasts of Parliament sessions. Many national media agencies, including the state-owned Press Trust of India, Media Trust of India and Doordarshan, is based in the city. Television programming includes two free terrestrial television channels offered by Doordarshan, and several Hindi, English and regional-language cable channels offered by multi system operators. Satellite television has yet to gain a large quantity of subscribers in the city.
Print journalism remains a popular news medium in Delhi. The city's Hindi newspapers include "Navbharat Times", "Hindustan Dainik", "Punjab Kesari", "Pavitra Bharat", "Dainik Jagran", "Dainik Bhaskar" and "Dainik Desbandhu". Amongst the English language newspapers, "The Hindustan Times", with a daily circulation of over a million copies, is the single largest daily. Other major English newspapers include "Times of India", "The Hindu", "Indian Express", "Business Standard", "The Pioneer" and "The Asian Age'Top Story (Daily). Regional language newspapers include the Malayalam daily "Malayala Manorama" and the Tamil dailies "Dinamalar" and "Dinakaran".
Radio is a less popular mass medium in Delhi, although FM radio has gained popularity since the inauguration of several new stations in 2006.
A number of state-owned and private radio stations broadcast from Delhi.
Sports.
Delhi has hosted many major international sporting events, including the first and also the ninth Asian Games, the 2010 Hockey World Cup, the 2010 Commonwealth Games and the 2011 Cricket World Cup. Delhi lost bidding for the 2014 Asian Games, and considered making a bid for the 2020 Summer Olympics. However, sports minister Manohar Singh Gill later stated that funding infrastructure would come before a 2020 bid. There are indications of a possible 2028 bid.
The 2010 Commonwealth Games, which ran from 3 to 14 October 2010, was one of the largest sports event held in India. The opening ceremony of the 2010 Commonwealth Games was held at the Jawaharlal Nehru Stadium, the main stadium of the event, in New Delhi at 7:00 pm Indian Standard Time on 3 October 2010. The ceremony featured over 8,000 performers and lasted for two and a half hours. It is estimated that ₹3.5 billion () were spent to produce the ceremony. Events took place at 12 competition venues. 20 training venues were used in the Games, including seven venues within Delhi University. The rugby stadium in Delhi University North Campus hosted rugby games for Commonwealth Games. The mess left behind after the Commonwealth Games prompted Prime Minister Manmohan Singh to replace Sports and Youth Affairs minister Manohar Singh Gill with Ajay Maken in 19 January 2011 Cabinet reshuffle.
Cricket and football are the most popular sports in Delhi. There are several cricket grounds, or "maidans", located across the city. The Feroz Shah Kotla Ground (known commonly as the "Kotla") is one of the oldest cricket grounds in India and is a venue for international cricket matches. It is the home ground of the Delhi cricket team, which represents the city in the Ranji Trophy, the premier Indian domestic first-class cricket championship. The Delhi cricket team has produced several world-class international cricketers such as Virender Sehwag, Gautam Gambhir, Virat Kohli, Madan Lal, Chetan Chauhan and Bishan Singh Bedi to name a few. The Railways and Services cricket teams in the Ranji Trophy also play their home matches in Delhi, in the Karnail Singh Stadium and the Harbax Singh Stadium respectively. The city is also home to the Indian Premier League team Delhi Daredevils, who play their home matches at the Kotla, and was the home to the Delhi Giants team (previously Delhi Jets) of the now defunct Indian Cricket League.
Ambedkar Stadium, a football stadium in Delhi which holds 21,000 people, was the venue for the Indian football team's World Cup qualifier against UAE on 28 July 2012. Delhi hosted the Nehru Cup in 2007 and 2009, in both of which India defeated Syria 1–0. In the Elite Football League of India, Delhi's first professional American football franchise, the Delhi Defenders played its first season in Pune. Buddh International Circuit in Greater Noida, a suburb of Delhi, hosts the annual Formula 1 Indian Grand Prix. The Indira Gandhi Arena is also in Delhi.
Delhi is a member of the Asian Network of Major Cities 21.
World Heritage status.
In February 2014, Government of India approved Delhi's bid for World Heritage City status. The historical city of Shahjahanabad and Lutyens' Bungalow Zone in New Delhi have been cited in the bid. A team from UNESCO is scheduled to visit Delhi in September, 2014 to validate its claims. Indian National Trust for Art and Cultural Heritage (INTACH) has acted as the nodal agency for the bid. The announcement of accepted cities will be made in June, 2015.

</doc>
<doc id="37800" url="http://en.wikipedia.org/wiki?curid=37800" title="Dendrochronology">
Dendrochronology

Dendrochronology (from δένδρον, "dendron", "tree limb"; χρόνος, "khronos", "time"; and -λογία, "-logia") or tree-ring dating, is the scientific method of dating based on the analysis of patterns of "tree rings", also known as "growth rings". Dendrochronology can date the time at which tree rings were formed, in many types of wood, to the exact calendar year. This has three main areas of application: paleoecology, where it is used to determine certain aspects of past ecologies (most prominently climate); archaeology and the history of art and architecture, where it is used to date old panel paintings on wood, buildings, etc.; and radiocarbon dating, where it is used to calibrate radiocarbon ages (see below). In some areas of the world, it is possible to date wood back a few thousand years, or even many thousands. As of 2013, fully anchored chronologies in the northern hemisphere extend back 13,900 years.
History.
The Greek botanist Theophrastus (ca. 371 – ca. 287 BC) first mentioned that the wood of trees has rings. In his "Trattato della Pittura" (Treatise on Painting), Leonardo da Vinci was the first person to mention that trees form rings annually and that their thickness is determined by the conditions under which they grew. In 1737, French investigators Henri-Louis Duhamel du Monceau and Georges-Louis Leclerc de Buffon examined the effect of growing conditions on the shape of tree rings. They found that in 1709, a severe winter produced a distinctly dark tree ring, which served as a reference for subsequent European naturalists.
In the U.S., Alexander Catlin Twining (1801-1884) suggested in 1833 that patterns among tree rings could be used to synchronize the dendrochronologies of various trees and thereby to reconstruct past climates across entire regions. The English polymath Charles Babbage proposed using dendrochronology to date the remains of trees in peat bogs or even in geological strata (1835, 1838).
During the latter half of the nineteenth century, the scientific study of tree rings and the application of dendrochronology began. In 1859, the German-American Jacob Kuechler (1823-1893) used crossdating to examine oaks ("Quercus stellata") in order to study the record of climate in western Texas. In 1866, the German botanist, entomologist, and forester Julius Ratzeburg (1801-1871) observed the effects on tree rings of defoliation caused by insect infestations. By 1882, this observation was already appearing in forestry textbooks. In the 1870s, the Dutch astronomer Jacobus C. Kapteyn (1851-1922) was using crossdating to reconstruct the climates of Holland and Germany. In 1881, the Swiss-Austrian forester Arthur von Seckendorff-Gudent (1845-1886) was using crossdating. From 1869 to 1901, Robert Hartig (1839-1901), a German professor of forest pathology, wrote a series of papers on the anatomy and ecology of tree rings. In 1892, the Russian physicist Fedor Nikiforovich Shvedov (Фёдор Никифорович Шведов) (1841-1905) wrote that he had used patterns found in tree rings to predict droughts in 1882 and 1891.
During the first half of the 20th century, the astronomer A. E. Douglass founded the Laboratory of Tree-Ring Research at the University of Arizona. Douglass sought to better understand cycles of sunspot activity and reasoned that changes in solar activity would affect climate patterns on earth which would subsequently be recorded by tree-ring growth patterns ("i.e.", sunspots → climate → tree rings).
Growth rings.
Growth rings, also referred to as "tree rings" or "annual rings", can be seen in a horizontal cross section cut through the trunk of a tree. Growth rings are the result of new growth in the vascular cambium, a layer of cells near the bark that is classified as a lateral meristem; this growth in diameter is known as secondary growth. Visible rings result from the change in growth speed through the seasons of the year; thus, critical for the title method, one ring generally marks the passage of one year in the life of the tree.
The rings are more visible in temperate zones, where the seasons differ more markedly. The inner portion of a growth ring is formed early in the growing season, when growth is comparatively rapid (hence the wood is less dense) and is known as "early wood" (or "spring wood", or "late-spring wood"); the outer portion is the "late wood" (and has sometimes been termed "summer wood", often being produced in the summer, though sometimes in the autumn) and is denser.
Many trees in temperate zones make one growth ring each year, with the newest adjacent to the bark. Hence, for these, for the entire period of a tree's life, a year-by-year record or ring pattern is formed that reflects the age of the tree and the climatic conditions in which the tree grew. Adequate moisture and a long growing season result in a wide ring, while a drought year may result in a very narrow one.
Direct reading of tree ring chronologies is a learned science, for several reasons. First, contrary to the single ring per year paradigm, alternating poor and favorable conditions, such as mid-summer droughts, can result in several rings forming in a given year. In addition, particular tree species may present "missing rings", and this influences the selection of trees for study of long time spans. For instance, missing rings are rare in oak and elm trees.
Critical to the science, trees from the same region tend to develop the same patterns of ring widths for a given period of historical study. These patterns can be compared and matched ring for ring with trees growing at the same time, in the same geographical zone (and therefore under similar climatic conditions). When these tree-ring patterns are carried back, from tree to tree in the same locale, in overlapping fashion, chronologies can be built up—both for entire geographical regions and sub-regions. Moreover, wood from ancient structures with known chronologies can be matched to the tree ring data (a technique called "cross-dating"), and the age of the wood can thereby be determined precisely. Cross-dating was originally done by visual inspection; computers have been harnessed to do the task, applying statistical techniques to assess the matching.
To eliminate individual variations in tree-ring growth, dendrochronologists take the smoothed average of the tree-ring widths of multiple tree samples to build up a ring history, a process termed replication. A tree-ring history whose beginning and end dates are not known is called a "floating chronology". It can be anchored by cross-matching a section against another chronology (tree-ring history) whose dates are known. Fully anchored chronologies extending back more than 11,000 years exist for river oak trees from South Germany (from the Main and Rhine rivers) and for pine from Northern Ireland. The consistency of these two independent dendrochronological sequences has been supported through comparison of their radiocarbon and dendrochronological ages. Another fully anchored chronology which extends back 8500 years exists for the bristlecone pine in the Southwest US (White Mountains of California).
Sampling and dating.
Dendrochronology makes available specimens of once-living material accurately dated to a specific year. Dates are often represented as estimated calendar years B.P., for before present, where "present" refers to 1 January 1950. 
Timber core samples are sampled and used to measure the width of annual growth rings; by taking samples from different sites within a particular region, researchers can build a comprehensive historical sequence. The techniques of dendrochronology are more consistent in areas where trees grew in marginal conditions such as aridity or semi-aridity where the ring growth is more sensitive to the environment, rather than in humid areas where tree-ring growth is more uniform (complacent). In addition, some genera of trees are more suitable than others for this type of analysis. For instance, the bristlecone pine is exceptionally long-lived and slow growing, and has been used extensively for chronologies; still-living and dead specimens of this species provide tree-ring patterns going back thousands of years, in some regions more than 10,000 years. Currently, the maximum span for fully anchored chronology is a little over 11,000 years B.P.
In 2004 a new calibration curve, INTCAL04, was internationally ratified to provide calibrated dates back to 26,000 B.P. (based on an agreed worldwide data set of trees and marine sediments). The part of the new calibration curves that relies on tree-ring evidence spans 12,410 calendar B.P., and a further 14,700 calendar years prior to that.
Dendrochronology practice faces many obstacles, including the existence of species of ants that inhabit trees and extend their galleries into the wood, thus destroying ring structure.
Reference sequences.
European chronologies derived from wooden structures initially found it difficult to bridge the gap in the 14th century when there was a building hiatus which coincided with the Black Death, however there do exist unbroken chronologies dating back to prehistoric times, for example the Danish chronology dating back to 352 BC.
Given a sample of wood, the variation of the tree-ring growths provides not only a match by year, it can also match location because the climate across a continent is not consistent. This makes it possible to determine the source of ships as well as smaller artifacts made from wood but which were transported long distances, such as panels for paintings and ship timbers.
Applications.
Radiocarbon dating calibration.
Dates from dendrochronology can be used as a calibration and check of radiocarbon dating
Climatology.
In areas where the climate is reasonably predictable, trees develop annual rings of different properties depending on weather, rain, temperature, soil pH, plant nutrition, CO2 concentration, etc. in different years. These variations are used in dendroclimatology to infer past climate variations.
Art history.
Dendrochronology has become important to art historians in the dating of panel paintings. However, unlike analysis of samples from buildings which are typically sent to a laboratory, wooden supports for paintings usually have to be measured in a museum conservation department, which places limitations on the techniques that can be used.
In addition to dating, dendrochronology can also provide information as to the source of the panel. Many Early Netherlandish paintings have turned out to be painted on panels of "Baltic oak" shipped from the Vistula region via ports of the Hanseatic League. Oak panels were used in a number of northern countries such as England, France and Germany. Wooden supports other than oak were rarely used by Netherlandish painters.
Since panels of seasoned wood were used, an uncertain number of years has to be allowed for seasoning when estimating dates. Panels were trimmed of the outer rings, and often each panel only uses a small part of the radius of the trunk. Consequently, dating studies usually result in a "terminus post quem" (earliest possible) date, and a tentative date for the actual arrival of a seasoned raw panel using assumptions as to these factors. As a result of establishing numerous sequences, it was possible to date 85% - 90% of the 250 paintings from the 14th to 17th century analysed between 1971 and 1982; by now a much greater number have been analysed.
A portrait of Mary, Queen of Scots in the National Portrait Gallery, London was believed to be an 18th-century copy. However, dendrochronology revealed that the wood dated from the second half of the 16th century. It is now regarded as an original 16th century painting by an unknown artist.
On the other hand, dendrochronology was applied to four paintings depicting the same subject, that of Christ expelling the money-lenders from the Temple. The results showed that the age of the wood was too late for any of them to have been painted by Hieronymus Bosch.
While dendrochronology has become an important tool for dating oak panels, it is not effective in dating the poplar panels often used by Italian painters because of the erratic growth rings in poplar.
The 16th century saw a gradual replacement of wooden panels by canvas as the support for paintings which means the technique is less often applicable to later paintings. In addition, many panel paintings were transferred onto canvas or other supports during the 19th and 20th centuries.
Building history.
The dating of buildings with wooden structures and components has also been done by using dendrochronology. While archaeologists can date wood and when it was felled, it may be difficult to definitively determine the age of a building or structure in which the wood was used; the wood could have been reused from an older structure, may have been felled and left for many years before use, or could have been used to replace a damaged piece of wood. The dating of building via dendrochronology thus requires knowledge of the history of building technology.
Examples:
Related chronologies.
Similar seasonal patterns also occur in ice cores and in varves (layers of sediment deposition in a lake, river, or sea bed). The deposition pattern in the core will vary for a frozen-over lake versus an ice-free lake, and with the fineness of the sediment. 
Some columnar cactus also exhibit similar seasonal patterns in the isotopes of carbon and oxygen in their spines (acanthochronology). These are used for dating in a manner similar to dendrochronology, and such techniques are used in combination with dendrochronology, to plug gaps and to extend the range of the seasonal data available to archaeologists and paleoclimatologists.
A related technique is used to analyse fish stocks through the analysis of growth rings in the otolith bones of fish.

</doc>
<doc id="37807" url="http://en.wikipedia.org/wiki?curid=37807" title="Colin Turnbull">
Colin Turnbull

Colin Macmillan Turnbull (November 23, 1924 – July 28, 1994) was a British-American anthropologist who came to public attention with the popular books "The Forest People" (on the Mbuti Pygmies of Zaire) and "The Mountain People" (on the Ik people of Uganda), and one of the first anthropologists to work in the field of ethnomusicology.
Early life.
Turnbull was born in London and educated at Westminster School and Magdalen College, Oxford, where he studied politics and philosophy. During World War II he was in the Royal Naval Volunteer Reserve after which he was awarded a two year grant in the Department of Indian Religion and Philosophy, Banaras Hindu University, India, from which he graduated with a master's degree in Indian Religion and Philosophy.
Career.
In 1951, after his graduation from Banaras, he traveled to the Belgian Congo (present-day Democratic Republic of the Congo) with Newton Beal, a schoolteacher from Ohio he met in India. Turnbull and Beal first studied the Mbuti pygmies during this time, though that was not the complete goal of the trip.
An "odd job" Turnbull picked up while in Africa at this time was working for the Hollywood producer Sam Spiegel. Spiegel hired Turnbull to assist in the construction and transportation of a boat needed for his film. This boat was the "African Queen", which was used for the feature film "The African Queen" (starring Humphrey Bogart and Katharine Hepburn; 1951). After his first trip to Africa, Turnbull traveled to Yellowknife in the northwest territories of Canada, where he worked as a geologist and gold miner for approximately a year, before he went back to school to obtain another degree.
Upon returning to Oxford in 1954, he began specializing in the anthropology of Africa. Turnbull remained in Oxford for two years before another field trip to Africa, finally focusing on the Belgian Congo (1957–58) and Uganda. After years of fieldwork, he finally achieved his anthropology doctorate from Oxford in 1964.
Turnbull became a naturalized citizen of the United States in 1965, after he moved to New York City to become curator in charge of African Ethnology at the American Museum of Natural History in 1959. He later resided in Lancaster County, Virginia, and was on staff in the Department of Sociology and Anthropology, Virginia Commonwealth University, Richmond. Other professional associations included Corresponding Membership of Royal Museum for Central Africa and fellowship in the British Royal Anthropological Institute. He first gained prominence with his book "The Forest People" (1961), an admiring study of the Mbuti. 
In 1972, having been commissioned to come up with an explanation and solution, the highly controversial "The Mountain People" was published. It concerned the Ik, a hunter/gatherer tribe who had been forced to stop moving around ancestral lands, through the seasons, because it now involved the three national borders of Uganda, Kenya and Sudan. Forced to become stationary in Uganda, and without a knowledge base and culture for doing so, they failed to thrive, even to the point of starvation and death. The book remains in print and on the reading list of University courses teaching anthropology.
Turnbull was in unique position to study and document what happens to a society and culture. His book documented the society he witnessed and described the culture of the Ik using the recollections of older Ik from the pre-stressed society. He extrapolated his observations to warn that civilization is shallow.
He later worked on a theatrical adaptation of "The Mountain People" with his friend, playwright Peter Brook.
Contributions to music.
Some of Turnbull's recordings of Mbuti music were commercially released, and his works have inspired other ethnomusicological studies, such as those of Simha Arom and Mauro Campagnoli. His most famous recording is "Music of the Rainforest Pygmies", recorded in 1961, now released on CD by Lyrichord Discs, Inc. His recording of a Zaire pygmy girls' initiation song was used on the Voyager Golden Record.
Joseph Towles.
Joseph Allen Towles was born in Senora, Virginia, on August 17, 1937. In 1957 he moved to New York City to pursue a career as an actor and writer. He met Turnbull in 1959 and they exchanged marriage vows the following year.
Towles' initiation into anthropology occurred as a volunteer in the Anthropology Department at the American Museum of Natural History with Turnbull. From 1965 to 1967, he assisted with the creation of the "Man in Africa Hall", a permanent exhibit (re-titled in 1990 as "Hall of African Peoples"). He also researched and constructed the "Slavery in the New World" subsection of the museum. In 1963, he entered Pace College to study history and anthropology, graduating in 1968. He received his Ph.D. from Makerere University in 1979.
From 1965 to 1967, Turnbull and Towles conducted fieldwork among the Ik of Northern Uganda in Africa. In the Congo in 1970, they conducted fieldwork on the Nkumbi circumcision initiation ritual for boys and the Asa myth of origin among the Mbo of the Ituri forest.
In 1979, they traveled the world studying the concept of tourism as pilgrimage. Towles next turned to biblical research and writing plays and novels. He reacted angrily to Turnbull's semi-autobiographical work "The Human Cycle" (1983), which omitted all references to their relationship. Towles' health declined slowly from that time. He died from complications of AIDS in 1988.
Turnbull arranged for Towles' research to be published posthumously. It appeared in 1993 as "Nkumbi initiation ritual and structure among the Mbo of Zaïre" and as "Asa: Myth of Origin of the Blood Brotherhood Among the Mbo of the Ituri Forest", both in "Annales" of the Royal Museum for Central Africa (Tervuren, Belgium), vol. 137.
Later years.
Late in life Turnbull took up the political cause of death row inmates. After his partner's death, Turnbull donated all his belongings to the United Negro College Fund. He donated all their research materials, most of which were the product of his career, to the College of Charleston, insisting that the collection be known under Towles' name alone.
In 1989, Turnbull moved to Bloomington, Indiana, to participate to the building of Tibetan Cultural Center with his friend Thupten Jigme Norbu, elder brother of the 14th Dalai Lama. Later Turnbull moved to Dharamsala, India where he took the monks' vow of Tibetan Buddhism, given to him by the Dalai Lama.
Controversy.
Many people found Turnbull's description of the Ik disturbing. Turnbull was explicitly describing what happens to a society that is forced to abandon its culture, and the fierce individualism and hardship that results. His graphic descriptions were placed into context by the careful interviews he did with older Ik to contrast the older society that existed prior to displacement. 
Bernd Heine exemplifies the strong reaction evoked by Turnbull's evaluation of the Ik in a scathing 1985 article in , , using information gained 20 years afer Turnbull's researches. He provided new information that appeared to discredit the portrayal of the Ik provided by Turnbull. It calls the issue of informants into question. Was Turnbull misled or just careless? Had 20 years changed the Ik to an extent that would make Heine's account less accurately comparable?
Sources.
</dl>

</doc>
<doc id="37815" url="http://en.wikipedia.org/wiki?curid=37815" title="Bulgakov">
Bulgakov

Bulgakov (Russian: Булгаков) is a Russian surname. Notable people with the surname include:

</doc>
<doc id="37824" url="http://en.wikipedia.org/wiki?curid=37824" title="SAT (disambiguation)">
SAT (disambiguation)

The SAT (formerly Scholastic Aptitude Test, Scholastic Assessment Test, and SAT Reasoning Test) is a college admissions test in the United States.
SAT or Sat may also refer to:

</doc>
<doc id="37828" url="http://en.wikipedia.org/wiki?curid=37828" title="DNF">
DNF

DNF may refer to:

</doc>
<doc id="37832" url="http://en.wikipedia.org/wiki?curid=37832" title="Monopropellant rocket">
Monopropellant rocket

A monopropellant rocket (or "monoprop rocket") is a rocket that uses a single chemical as its propellant.
Chemical-reaction monopropellant rockets.
For monopropellant rockets that depend on a chemical reaction, the power for the propulsive reaction and resultant thrust is provided by the chemical itself. That is, the energy needed to propel the spacecraft is contained within the chemical bonds of the chemical molecules involved in the reaction.
The most commonly used monopropellant is hydrazine (N2H4), a chemical which is a strong reducing agent. The most common catalyst is granular alumina coated with iridium (e.g. S-405 or KC 12 GA). There is no igniter with hydrazine. Shell 405 is a spontaneous catalyst, that is, hydrazine decomposes on contact with the catalyst. The decomposition is highly exothermic and produces an 1800 °F (1000 °C) gas that is a mixture of nitrogen, hydrogen and ammonia.
Another monopropellant is hydrogen peroxide, which, when purified to 90% or higher concentration, is self-decomposing at high temperatures or when a catalyst is present.
Most chemical-reaction monopropellant rocket systems consist of a fuel tank, usually a titanium or aluminium sphere, with an ethylene-propylene rubber container or a surface tension propellant management device filled with the fuel. The tank is then pressurized with helium or nitrogen, which pushes the fuel out to the motors. A pipe leads from the tank to a poppet valve, and then to the decomposition chamber of the rocket motor. Typically, a satellite will have not just one motor, but two to twelve, each with its own valve.
The attitude control rocket motors for satellites and space probes are often very small, 25mm (1 inch) or so in diameter, and mounted in groups that point in four directions (within a plane).
The rocket is fired when the computer sends direct current through a small electromagnet that opens the poppet valve. The firing is often very brief, a few thousandths of a second, and — if operated in air — would sound like a pebble thrown against a metal trash can; if on for long, it would make a piercing hiss.
Chemical-reaction monopropellants are not as efficient as some other propulsion technologies. Engineers choose monopropellant systems when the need for simplicity and reliability outweigh the need for high delivered impulse. If the propulsion system must produce large amounts of thrust, or have a high specific impulse, as on the main motor of an interplanetary spacecraft, other technologies are used.
Solar-thermal monopropellant thrusters.
A concept to provide low Earth orbit (LEO) propellant depots that could be used as way-stations for other spacecraft to stop and refuel on the way to beyond-LEO missions has proposed that waste gaseous hydrogen—an inevitable byproduct of long-term liquid hydrogen storage in the radiative heat environment of space—would be usable as a monopropellant in a solar-thermal propulsion system. The waste hydrogen would be productively utilized for both orbital stationkeeping and attitude control, as well as providing limited propellant and thrust to use for orbital maneuvers to better rendezvous with other spacecraft that would be inbound to receive fuel from the depot.
Solar-thermal monoprop thrusters are also integral to the design of a next-generation cryogenic upper stage rocket proposed by U.S. company United Launch Alliance (ULA). The Advanced Common Evolved Stage (ACES) is intended as a lower-cost, more-capable and more-flexible upper stage that would supplement, and perhaps replace, the existing ULA Centaur and ULA Delta Cryogenic Second Stage (DCSS) upper stage vehicles. The ACES Integrated Vehicle Fluids option eliminates all hydrazine and helium from the space vehicle—normally used for attitude control and station keeping—and depends instead on solar-thermal monoprop thrusters using waste hydrogen.
New Developments.
NASA is developing a new monopropellant propulsion system for small, cost-driven spacecraft with delta-v requirements in the range of 10–150 m/s. This system is based on a hydroxylammonium nitrate (HAN)/water/fuel monopropellant blend which is extremely dense, environmentally benign, and promises good performance and simplicity.
The EURENCO Bofors company produced LMP-103S as a 1-to-1 substitute for hydrazine by dissolving 65% ammonium dinitramide, NH4N(NO2)2, in 35% water solution of methanol and ammonia. LMP-103S has 6% higher specific impulse and 30% higher impulse density than hydrazine monopropellant. Additionally, hydrazine is highly toxic and carcinogenic, while LMP-103S is only moderately toxic. LMP-103S is UN Class 1.4S allowing for transport on commercial aircraft, and was demonstrated on the Prisma satellite in 2010. Special handling is not required. LMP-103S could replace hydrazine as the most commonly used monopropellant.

</doc>
<doc id="37834" url="http://en.wikipedia.org/wiki?curid=37834" title="Dual mode propulsion rocket">
Dual mode propulsion rocket

Dual mode propulsion systems combine the high efficiency of bipropellant rockets with the reliability and simplicity of monopropellant rockets. Dual mode systems are either hydrazine/N2O4, or MMH/hydrogen peroxide (the former is much more common). Typically, this system works as follows: During the initial high-impulse orbit-raising maneuvers, the system operates in a bipropellant fashion, providing high thrust at high efficiency; when it arrives on orbit, it closes off either the fuel or oxidizer, and conducts the remainder of its mission in a simple, predictable monopropellant
fashion.

</doc>
<doc id="37836" url="http://en.wikipedia.org/wiki?curid=37836" title="Arcjet rocket">
Arcjet rocket

An arcjet rocket or arcjet thruster is a form of electrically powered spacecraft propulsion, in which an electrical discharge (arc) is created in a flow of propellant (typically hydrazine or ammonia). This imparts additional energy to the propellant, so that one can extract more work out of each kilogram of propellant, at the expense of increased power consumption and (usually) higher cost. Also, the thrust levels available from typically used arcjet engines are very low compared with chemical engines.
When the energy is available, arcjets are well suited to keeping stations in orbit and can replace monopropellant rockets.
In Germany, researchers at the University of Stuttgart's Institute of Space Aviation Systems have been looking into these challenges for years and have developed various hydrogen-powered arcjet engines capable of power outputs from 1 to 100 kW. The heated hydrogen reaches exit speeds of 16 km/s. An arcjet-propelled test satellite by the name of Baden-Württemberg 1 (BW1) is scheduled to go to the Moon. Baden-Württemberg 1 would use polytetrafluoroethylene PTFE propellant.

</doc>
<doc id="37839" url="http://en.wikipedia.org/wiki?curid=37839" title="Ion thruster">
Ion thruster

An ion thruster is a form of electric propulsion used for spacecraft propulsion that creates thrust by accelerating ions. The term is strictly used to refer to gridded electrostatic ion thrusters, but may often more loosely be applied to all electric propulsion systems that accelerate plasma, since plasma consists of ions.
Ion thrusters are categorized by how they accelerate the ions, using either electrostatic or electromagnetic force. Electrostatic ion thrusters use the Coulomb force and accelerate the ions in the direction of the electric field. Electromagnetic ion thrusters use the Lorentz force to accelerate the ions. In either case, when an ion passes through an electrostatic grid engine, the potential difference of the electric field converts to the ion's kinetic energy.
Ion thrusters have an input power spanning 1–7 kilowatts, exhaust velocity 20–50 kilometers per second, thrust 20–250 millinewtons and efficiency 60–80%.
The Deep Space 1 spacecraft, powered by an ion thruster, changed velocity by 4.3 km/s while consuming less than 74 kilograms of xenon. The Dawn spacecraft broke the record, reaching 10 km/s.
Applications include control of the orientation and position of orbiting satellites (some satellites have dozens of low-power ion thrusters) and use as a main propulsion engine for low-mass robotic space vehicles (for example Deep Space 1 and Dawn).
Ion thrusters are not the most promising type of electrically powered spacecraft propulsion (although in practice they have been more successful than others). The ion drive is comparable to a car that takes two days to accelerate from zero to 60 miles per hour; a real ion engine's technical characteristics, and especially its thrust, are considerably inferior to its literary prototypes. Technical capabilities of the ion engine are limited by the space charge created by ions. This limits the thrust density (force per cross-sectional area of the engine). Ion thrusters create small thrust levels (for example the thrust of Deep Space 1's engine approximately equals the weight of one sheet of paper) compared to conventional chemical rockets, but achieve very high specific impulse, or propellant mass efficiency, by accelerating their exhaust to high speed. However, ion thrusters carry a fundamental price: the power imparted to the exhaust increases with the square of its velocity while thrust increases linearly. Chemical rockets, on the other hand, can provide high thrust, but are limited in total impulse by the small amount of energy that can be stored chemically in the propellants. Given the practical weight of suitable power sources, the accelerations given by ion thrusters are frequently less than one thousandth of standard gravity. However, since they operate as electric (or electrostatic) motors, a greater fraction of the input power is converted into kinetic exhaust power than in a chemical rocket. Chemical rockets operate as heat engines, hence Carnot's theorem bounds their possible exhaust velocity.
Due to their relatively high power needs, given the specific power of power supplies and the requirement of an environment void of other ionized particles, ion thrust propulsion is currently only practical on spacecraft that have already reached space, and is unable to take vehicles from Earth to space. Spacecraft rely on conventional chemical rockets to initially reach orbit.
Origins.
The first person to publish mention of the idea was Konstantin Tsiolkovsky in 1911. However, the first documented instance where the possibility of electric propulsion was considered is found in Robert H. Goddard's handwritten notebook in an entry dated September 6, 1906.
The first experiments with ion thrusters were carried out by Goddard at Clark University from 1916–1917. The technique was recommended for near-vacuum conditions at high altitude, but thrust was demonstrated with ionized air streams at atmospheric pressure. The idea appeared again in Hermann Oberth's "Wege zur Raumschiffahrt” (Ways to Spaceflight), published in 1923, where he explained his thoughts on the mass savings of electric propulsion, predicted its use in spacecraft propulsion and attitude control, and advocated electrostatic acceleration of charged gases.
A working ion thruster was built by Harold R. Kaufman in 1959 at the NASA Glenn Research Center facilities. It was similar to the general design of a gridded electrostatic ion thruster with mercury as its fuel. Suborbital tests of the engine followed during the 1960s and in 1964 the engine was sent into a suborbital flight aboard the Space Electric Rocket Test 1 (SERT 1). It successfully operated for the planned 31 minutes before falling back to Earth. This test was followed by an orbital test, SERT-2, in 1970.
An alternate form of electric propulsion, the Hall effect thruster was studied independently in the U.S. and the Soviet Union in the 1950s and 1960s. Hall effect thrusters had operated on Soviet satellites since 1972. Until the 1990s they were mainly used for satellite stabilization in North-South and in East-West directions. Some 100–200 engines completed their mission on Soviet and Russian satellites until the late 1990s. Soviet thruster design was introduced to the West in 1992 after a team of electric propulsion specialists, under the support of the Ballistic Missile Defense Organization, visited Soviet laboratories.
General description.
Ion thrusters use beams of ions (electrically charged atoms or molecules) to create thrust in accordance with momentum conservation. The method of accelerating the ions varies, but all designs take advantage of the charge/mass ratio of the ions. This ratio means that relatively small potential differences can create very high exhaust velocities. This reduces the amount of reaction mass or fuel required, but increases the amount of specific power required compared to chemical rockets. Ion thrusters are therefore able to achieve extremely high specific impulses. The drawback of the low thrust is low spacecraft acceleration, because the mass of current electric power units is directly correlated with the amount of power given. This low thrust makes ion thrusters unsuited for launching spacecraft into orbit, but they are ideal for in-space propulsion applications.
Various ion thrusters have been designed and they all generally fit under two categories. The thrusters are categorized as either electrostatic or electromagnetic. The main difference is how the ions are accelerated.
Electric power supplies for ion thrusters are usually solar panels but, at sufficiently large distances from the Sun, nuclear power is used. In each case the power supply mass is essentially proportional to the peak power that can be supplied, and they both essentially give, for this application, no limit to the energy.
Electric thrusters tend to produce low thrust, which results in low acceleration. Using 1 g is 9.81 m/s2; F = m a ⇒ a = F/m. An NSTAR thruster producing a thrust (force) of 92 mN will accelerate a satellite with a mass of 1,000 kg by 0.092 N / 1,000 kg =  m/s2 (or 9.38×10−6 g).
thrust = 2*η*power/(g * Isp)
Where
Electrostatic ion thrusters.
Gridded electrostatic ion thrusters.
Gridded electrostatic ion thrusters commonly utilize xenon gas. This gas has no charge and is ionized by bombarding it with energetic electrons. These electrons can be provided from a hot cathode filament and when accelerated in the electrical field of the cathode, fall to the anode. Alternatively, the electrons can be accelerated by the oscillating electric field induced by an alternating magnetic field of a coil, which results in a self-sustaining discharge and omits any cathode (radio frequency ion thruster).
The positively charged ions are extracted by an extraction system consisting of 2 or 3 multi-aperture grids. After entering the grid system via the plasma sheath the ions are accelerated due to the potential difference between the first and second grid (named screen and accelerator grid) to the final ion energy of typically 1–2 keV, thereby generating the thrust.
Ion thrusters emit a beam of positive charged xenon ions only. To avoid charging up the spacecraft, another cathode is placed near the engine, which emits electrons (basically the electron current is the same as the ion current) into the ion beam. This also prevents the beam of ions from returning to the spacecraft and cancelling the thrust.
Gridded electrostatic ion thruster research (past/present):
Hall effect thrusters.
Hall effect thrusters accelerate ions with the use of an electric potential maintained between a cylindrical anode and a negatively charged plasma that forms the cathode. The bulk of the propellant (typically xenon gas) is introduced near the anode, where it becomes ionized, and the ions are attracted towards the cathode; they accelerate towards and through it, picking up electrons as they leave to neutralize the beam and leave the thruster at high velocity.
The anode is at one end of a cylindrical tube, and in the center is a spike that is wound to produce a radial magnetic field between it and the surrounding tube. The ions are largely unaffected by the magnetic field, since they are too massive. However, the electrons produced near the end of the spike to create the cathode are far more affected and are trapped by the magnetic field, and held in place by their attraction to the anode. Some of the electrons spiral down towards the anode, circulating around the spike in a Hall current. When they reach the anode they impact the uncharged propellant and cause it to be ionized, before finally reaching the anode and closing the circuit.
Field-emission electric propulsion.
Field-emission electric propulsion (FEEP) thrusters use a very simple system of accelerating ions to create thrust. Most designs use either caesium or indium as the propellant. The design comprises a small propellant reservoir that stores the liquid metal, a narrow tube or a system of parallel plates that the liquid flows through, and an accelerator (a ring or an elongated aperture in a metallic plate) about a millimeter past the tube end. Caesium and indium are used due to their high atomic weights, low ionization potentials, and low melting points. Once the liquid metal reaches the end of the tube, an electric field applied between the emitter and the accelerator causes the liquid surface to deform into a series of protruding cusps ("Taylor cones"). At a sufficiently high applied voltage, positive ions are extracted from the tips of the cones. The electric field created by the emitter and the accelerator then accelerates the ions. An external source of electrons neutralizes the positively charged ion stream to prevent charging of the spacecraft.
Electromagnetic thrusters.
Pulsed inductive thrusters (PIT).
Pulsed inductive thrusters (PIT) use pulses of thrust instead of one continuous thrust, and have the ability to run on power levels in the order of Megawatts (MW). PITs consist of a large coil encircling a cone shaped tube that emits the propellant gas. Ammonia is the gas commonly used in PIT engines. For each pulse of thrust the PIT gives, a large charge first builds up in a group of capacitors behind the coil and is then released. This creates a current that moves circularly in the direction of jθ. The current then creates a magnetic field in the outward radial direction (Br), which then creates a current in the ammonia gas that has just been released in the opposite direction of the original current. This opposite current ionizes the ammonia and these positively charged ions are accelerated away from the PIT engine due to the electric field jθ crossing with the magnetic field Br, which is due to the Lorentz Force.
Magnetoplasmadynamic (MPD) / lithium Lorentz force accelerator (LiLFA).
Magnetoplasmadynamic (MPD) thrusters and lithium Lorentz force accelerator (LiLFA) thrusters use roughly the same idea with the LiLFA thruster building off of the MPD thruster. Hydrogen, argon, ammonia, and nitrogen gas can be used as propellant. In a certain configuration, the ambient gas in Low Earth Orbit (LEO) can be used as a propellant. The gas first enters the main chamber where it is ionized into plasma by the electric field between the anode and the cathode. This plasma then conducts electricity between the anode and the cathode. This new current creates a magnetic field around the cathode, which crosses with the electric field, thereby accelerating the plasma due to the Lorentz force. The LiLFA thruster uses the same general idea as the MPD thruster, except for two main differences. The first difference is that the LiLFA uses lithium vapor, which has the advantage of being able to be stored as a solid. The other difference is that the cathode is replaced by multiple smaller cathode rods packed into a hollow cathode tube. The cathode in the MPD thruster is easily corroded due to constant contact with the plasma. In the LiLFA thruster the lithium vapor is injected into the hollow cathode and is not ionized to its plasma form/corrode the cathode rods until it exits the tube. The plasma is then accelerated using the same Lorentz Force.
Electrodeless plasma thrusters.
Electrodeless plasma thrusters have two unique features: the removal of the anode and cathode electrodes and the ability to throttle the engine. The removal of the electrodes takes away the factor of erosion, which limits lifetime on other ion engines. Neutral gas is first ionized by electromagnetic waves and then transferred to another chamber where it is accelerated by an oscillating electric and magnetic field, also known as the ponderomotive force. This separation of the ionization and acceleration stage give the engine the ability to throttle the speed of propellant flow, which then changes the thrust magnitude and specific impulse values.
Helicon double layer thruster.
A helicon double layer thruster is a type of plasma thruster, which ejects high velocity ionized gas to provide thrust to a spacecraft. In this thruster design, gas is injected into a tubular chamber (the "source tube") with one open end. Radio frequency AC power (at 13.56 MHz in the prototype design) is coupled into a specially shaped antenna wrapped around the chamber. The electromagnetic wave emitted by the antenna causes the gas to break down and form a plasma. The antenna then excites a helicon wave in the plasma, which further heats the plasma. The device has a roughly constant magnetic field in the source tube (supplied by solenoids in the prototype), but the magnetic field diverges and rapidly decreases in magnitude away from the source region, and might be thought of as a kind of magnetic nozzle. In operation, there is a sharp boundary between the high density plasma inside the source region, and the low density plasma in the exhaust, which is associated with a sharp change in electrical potential. The plasma properties change rapidly across this boundary, which is known as a "current-free electric double layer". The electrical potential is much higher inside the source region than in the exhaust, and this serves both to confine most of the electrons, and to accelerate the ions away from the source region. Enough electrons escape the source region to ensure that the plasma in the exhaust is neutral overall.
Comparisons.
The following table compares actual test data of some ion thrusters:
The following thrusters are highly experimental and have been tested only in pulse mode.
Lifetime.
A major limiting factor of ion thrusters is their small thrust; however, it is generated at a high propellant efficiency (mass utilisation, specific impulse). The efficiency comes from the high exhaust velocity, which in turn demands high energy, and the performance is ultimately limited by the available spacecraft power.
The low thrust requires ion thrusters to provide continuous thrust for a long time to achieve the needed change in velocity (delta-v) for a particular mission. To cause enough change in momentum, ion thrusters are designed to last for periods of weeks to years.
In practice the lifetime of electrostatic ion thrusters is limited by several processes:
A test of the NASA Solar Technology Application Readiness (NSTAR) electrostatic ion thruster resulted in 30,472 hours (roughly 3.5 years) of continuous thrust at maximum power. The test was concluded prior to any failure and examination indicated the engine was not approaching failure either.
More recently, the NASA Evolutionary Xenon Thruster (NEXT) Project, conducted at NASA's Glenn Research Center in Cleveland, Ohio, operated continuously for more than 48,000 hours. The test was conducted in a high vacuum test chamber at Glenn Research Center. Over the course of the 5 1/2 + year test, the engine consumed approximately 870 kilograms of xenon propellant. The total impulse provided by the engine would require over 10,000 kilograms of conventional rocket propellant for similar application. The engine was designed by Aerojet Rocketdyne of Sacramento, California.
NASA's Jet Propulsion Laboratory has created ion drives with a time of continuous operation of more than 3 years.
Propellants.
Ionization energy represents a very large percentage of the energy needed to run ion drives. The ideal propellant for ion drives is thus a propellant molecule or atom that is easy to ionize, that has a high mass/ionization energy ratio. In addition, the propellant should not cause erosion of the thruster to any great degree to permit long life; and should not contaminate the vehicle.
Many current designs use xenon gas, as it is easy to ionize, has a reasonably high atomic number, its inert nature, and low erosion. However, xenon is globally in short supply and very expensive.
Older designs used mercury, but this is toxic and expensive, tended to contaminate the vehicle with the metal and was difficult to feed accurately.
Other propellants, such as bismuth, show promise and are areas of research, particularly for gridless designs, such as Hall effect thrusters.
VASIMR design (and other plasma-based engines) are theoretically able to use practically any material for propellant. However, in current tests the most practical propellant is argon, which is a relatively abundant and inexpensive gas.
Energy efficiency.
Ion thrusters are frequently quoted with an efficiency metric. This efficiency is the kinetic energy of the exhaust jet emitted per second divided by the electrical power into the device.
The actual overall system energy efficiency in use is determined by the propulsive efficiency, which depends on vehicle speed and exhaust speed. Some thrusters can vary exhaust speed in operation, but all can be designed with different exhaust speeds. At the lower end of Isps the overall efficiency drops, because the ionization takes up a larger percentage energy, and at the high end propulsive efficiency is reduced.
Optimal efficiencies and exhaust velocities can thus be calculated for any given mission to give minimum overall cost.
Applications.
Ion thrusters have many applications for in-space propulsion. The best applications of the thrusters make use of the long lifetime when significant thrust is not needed. Examples of this include orbit transfers, attitude adjustments, drag compensation for low Earth orbits, transporting cargo such as chemical fuels between propellant depots and ultra-fine adjustments for scientific missions. Ion thrusters can also be used for interplanetary and deep-space missions where time is not crucial. Continuous thrust over a very long time can build up a larger velocity than traditional chemical rockets.
Missions.
Of all the electric thrusters, ion thrusters have been the most seriously considered commercially and academically in the quest for interplanetary missions and orbit raising maneuvers. Ion thrusters are seen as the best solution for these missions, as they require very high change in velocity overall that can be built up over long periods of time.
Pure demonstration vehicles.
Ion propulsion systems were first demonstrated in space by the NASA Lewis (now Glenn Research Center) missions "Space Electric Rocket Test" (SERT) I and II. The first was SERT-1, launched July 20, 1964, successfully proved that the technology operated as predicted in space. These were electrostatic ion thrusters using mercury and cesium as the reaction mass. The second test, SERT-II, launched on February 3, 1970, verified the operation of two mercury ion engines for thousands of running hours.
Operational missions.
Ion thrusters are routinely used for station-keeping on commercial and military communication satellites in geosynchronous orbit, including satellites manufactured by Boeing and by Hughes Aerospace. The pioneers in this field were the Soviet Union, who used SPT thrusters on a variety of satellites starting in the early 1970s.
Two geostationary satellites (ESA's Artemis in 2001–03 and the US military's AEHF-1 in 2010–12) have used the ion thruster for orbit raising after the failure of the chemical-propellant engine. Boeing have been using ion thrusters for station-keeping since 1997, and plan in 2013–14 to offer a variant on their 702 platform, which will have no chemical engine and use ion thrusters for orbit raising; this enables a significantly lower launch mass for a given satellite capability. AEHF-2 used a chemical engine to raise perigee to 10150 miles and is then proceeding to geosynchronous orbit using electric propulsion.
In Earth orbit.
ESA's Gravity Field and Steady-State Ocean Circulation Explorer was launched on March 16, 2009. It used ion propulsion throughout its twenty-month mission to combat the air-drag it experienced in its low orbit before intentionally deorbiting on November 11, 2013.
In deep space.
NASA developed the NSTAR ion engine for use in their interplanetary science missions beginning in the late-1990s. This xenon-propelled ion thruster was first space-tested in the highly successful space probe Deep Space 1, launched in 1998. This was the first use of electric propulsion as the interplanetary propulsion system on a science mission.
Based on the NASA design criteria, Hughes Research Labs, developed the XIPS (Xenon Ion Propulsion System) for performing station keeping on geosynchronous satellites.. Hughes (EDD) manufactured the NSTAR thruster used on the spacecraft.
The Japanese space agency's Hayabusa, which was launched in 2003 and successfully rendezvoused with the asteroid 25143 Itokawa and remained in close proximity for many months to collect samples and information, was powered by four xenon ion engines. It used xenon ions generated by microwave electron cyclotron resonance, and a carbon / carbon-composite material (which is resistant to erosion) for its acceleration grid. Although the ion engines on Hayabusa had some technical difficulties, in-flight reconfiguration allowed one of the four engines to be repaired, and allowed the mission to successfully return to Earth.
The European Space Agency's satellite SMART-1, launched in 2003, used a Snecma PPS-1350-G Hall thruster to get from GTO to lunar orbit. This satellite completed its mission on September 3, 2006, in a controlled collision on the Moon's surface, after a trajectory deviation so scientists could see the 3 meter crater the impact created on the visible side of the moon.
Dawn was launched on September 27, 2007, to explore the asteroid Vesta and the dwarf planet Ceres. To cruise from Earth to its targets it uses three Deep Space 1 heritage xenon ion thrusters (firing only one at a time) to take it in a long outward spiral. An extended mission in which Dawn explores other asteroids after Ceres is also possible. Dawn's ion drive is capable of accelerating from 0 to 60 mi/h in 4 days, firing continuously.
Planned missions.
In addition, several missions are planned to use ion thrusters in the next few years.
ESA will launch the BepiColombo mission to Mercury in 2016. It uses ion thrusters in combination with swing-bys to get to Mercury, where a chemical rocket will be fired for orbit insertion.
LISA Pathfinder is an ESA spacecraft to be launched in 2015. It will not use ion thrusters as its primary propulsion system, but will use both colloid thrusters and FEEP for very precise attitude control -— the low thrusts of these propulsion devices make it possible to move the spacecraft incremental distances very accurately. It is a test for the possible LISA mission.
s of March 2011[ [update]], a future launch of an Ad Astra VF-200 200 kW VASIMR electromagnetic thruster was being considered for placement and testing on the International Space Station. The VF-200 is a flight version of the VX-200.
Since the available power from the ISS is less than 200 kW, the ISS VASIMR will include a trickle-charged battery system allowing for 15 min pulses of thrust. Testing of the engine on ISS is valuable, because ISS orbits at a relatively low altitude and experiences fairly high levels of atmospheric drag, making periodic boosts of altitude necessary. Currently, altitude reboosting by chemical rockets fulfills this requirement. If the tests of VASIMR reboosting of the ISS goes according to plan, the increase in specific impulse could mean that the cost of fuel for altitude reboosting will be one-twentieth of the current $210 million annual cost. Hydrogen is generated by the ISS as a by-product, which is currently vented into space.
In June 2011, NASA launched a request-for-proposals for a test mission (from context probably using the NEXT engine) capable of being extended to 300 kW electrical power; this was awarded to Northrop Grumman in February 2012.
Proposal.
Geoffrey A. Landis proposed for interstellar travel future-technology project interstellar probe with supplying the energy from an external source (laser of base station) and ion thruster.
References.
</dl>

</doc>
<doc id="37845" url="http://en.wikipedia.org/wiki?curid=37845" title="Magnetic sail">
Magnetic sail

A magnetic sail or magsail is a proposed method of spacecraft propulsion which would use a static magnetic field to deflect charged particles radiated by the Sun as a plasma wind, and thus impart momentum to accelerate the spacecraft. A magnetic sail could also thrust directly against planetary and solar magnetospheres.
History.
The magnetic sail was invented by Dana Andrews and Robert Zubrin working in collaboration in 1988. At that time, Andrews was working on a concept to use a magnetic scoop to gather ions to provide propellant for a nuclear electric ion drive spacecraft, allowing the craft to operate in the same manner of a Bussard ramjet, but without the need for a proton-proton fusion propulsion drive. He asked Zubrin to help him compute the drag that the magnetic scoop would create against the interplanetary medium. Zubrin agreed, but found that the drag created by the scoop would be much greater than the thrust created by the ion drive. He therefore proposed that the ion drive component of the system be dropped, and the device simply used as a sail. Andrews agreed, and the magsail was born. The two then proceeded to elaborate their analysis of the magsail for interplanetary, interstellar, and planetary orbital propulsion in a series of papers published from 1988 through the 1990s.
Principles of operation and design.
The "magsail" operates by creating drag against the local medium (planet's magnetic field, solar wind, or interstellar winds), thereby allowing a spacecraft accelerated to very high velocities by other means, such as a fusion rocket or laser pushed lightsail, to slow down - even from relativistic velocities - without requiring the use of onboard propellant. It can thus reduce the delta-V propulsion required for an interstellar mission by a factor of two. This capability is the most unique feature of the magsail, and perhaps the most significant in the long term.
In typical magnetic sail designs, the magnetic field is generated by a loop of superconducting wire. Because loops of current-carrying conductors tend to be forced outwards towards a circular shape by their own magnetic field, the sail could be deployed simply by unspooling the conductor and applying a current through it.
Solar wind example.
The solar wind is a continuous stream of plasma that flows outwards from the Sun: near the Earth's orbit, it contains several million protons and electrons per cubic meter and flows at 400 to. The magnetic sail introduces a magnetic field into this plasma flow which can deflect the particles from their original trajectory: the momentum of the particles is then transferred to the sail, leading to a thrust on the sail. One advantage of magnetic or solar sails over (chemical or ion) reaction thrusters is that no reaction mass is depleted or carried in the craft.
For a sail in the solar wind one AU away from the Sun, the field strength required to resist the dynamic pressure of the solar wind is 50 nT. Zubrin's proposed magnetic sail design would create a bubble of space of 100 km where solar-wind ions are substantially deflected using a hoop 50 km in radius. The minimum mass of such a coil is constrained by material strength limitations at roughly 40 tonne and it would generate 70 N of thrust, giving a mass/thrust ratio of 600 kg/N. If operated within the solar system, high temperature superconducting wire would be required to make the magsail practical. If operated in interstellar space conventional superconductors would be adequate.
The operation of magnetic sails using plasma wind is analogous to the operation of solar sails using the radiation pressure of photons emitted by the Sun. Although solar wind particles have rest mass and photons do not, sunlight has thousands of times more momentum than the solar wind. Therefore, a magnetic sail must deflect a proportionally larger area of the solar wind than a comparable solar sail to generate the same amount of thrust. However, it need not be as massive as a solar sail because the solar wind is deflected by a magnetic field instead of a large physical sail. Conventional materials for solar sails weigh around 7 g/m2, giving a thrust of 0.01 mPa at 1 AU. This gives a mass/thrust ratio of at least 700 kg/N, similar to a magnetic sail, neglecting other structural components.
The solar and magnetic sails have a thrust that falls off as the square of the distance from the Sun.
When close to a planet with a strong magnetosphere such as Earth or a gas giant, the magnetic sail could generate more thrust by interacting with the magnetosphere instead of the solar wind, and may therefore be more efficient.
Mini-magnetospheric plasma propulsion (M2P2).
In order to reduce the size and weight of the magnet of the magnetic sail, it may be possible to "inflate" the magnetic field using a plasma in the same way that the plasma around the Earth stretches out the Earth's magnetic field in the magnetosphere. In this approach, called mini-magnetospheric plasma propulsion (M2P2), currents that run through the plasma will augment and partially replace the currents in the coil. This is expected to be especially useful far from the Sun, where the increased effective size of a M2P2 sail compensates for the reduced dynamic pressure of the solar wind. The original NASA design proposes a spacecraft containing a can-shaped electromagnet into which a plasma is injected. The plasma pressure stretches the magnetic field and inflates a bubble of plasma around the spacecraft. The plasma then generates a kind of miniaturized magnetosphere around the spacecraft, analogous to the magnetosphere that surrounds the Earth. The protons and electrons which make up the solar wind are deflected by this magnetosphere and the reaction accelerates the spacecraft. The thrust of the M2P2 device would be steerable to some extent, potentially allowing the spacecraft to 'tack' into the solar wind and allowing efficient changes of orbit.
In the case of the (M2P2) system the spacecraft releases gas to create the plasma needed to maintain the somewhat leaky plasma bubble. The M2P2 system therefore has an effective "specific impulse" which is the amount of gas consumed per newton second of thrust. This is a figure of merit usually used for rockets, where the fuel is actually reaction mass. Robert Winglee, who originally proposed the M2P2 technique, calculates a "specific impulse" of 200 kN·s/kg (roughly 50 times better than the space shuttle main engine). These calculations suggest that the system requires on the order of a kilowatt of power per newton of thrust, considerably lower than electric thrusters, and that the system generates the same thrust anywhere within the heliopause because the sail spreads automatically as the solar wind becomes less dense. However, this technique is less understood than the simpler magnetic sail and issues of how large and heavy the magnetic coil would have to be or whether the momentum from the solar wind can be efficiently transferred to the spacecraft are under dispute.
The expansion of the magnetic field using plasma injected has been successfully tested in a large vacuum chamber on Earth, but the development of thrust was not part of the experiment. A beam-powered variant, MagBeam, is also under development.
Modes of operation.
In a plasma wind.
When operating away from planetary magnetospheres, a magnetic sail would force the positively charged protons of the solar wind to curve as they passed through the magnetic field. The change of momentum of the protons would thrust against the magnetic field, and thus against the field coil.
Just as with solar sails, magnetic sails can "tack." If a magnetic sail orients at an angle relative to the solar wind, charged particles are deflected preferentially to one side and the magnetic sail is pushed laterally. This means that magnetic sails could maneuver to most orbits.
In this mode, the amount of thrust generated by a magnetic sail falls off with the square of its distance from the Sun as the flux density of charged particles reduces. Solar weather also has major effects on the sail. It is possible that the plasma eruption from a severe solar flare could damage an efficient, fragile sail.
A common misconception is that a magnetic sail cannot exceed the speed of the plasma pushing it. As the speed of a magnetic sail increases, its acceleration becomes more dependent on its ability to tack efficiently. At high speeds, the plasma wind's direction will seem to come increasingly from the front of the spacecraft. Advanced sailing spacecraft might deploy field coils as "keels," so the spacecraft could use the difference in vector between the solar magnetic field and the solar wind, much as sailing yachts do.
Inside a planetary magnetosphere.
Inside a planetary magnetosphere, a magnetic sail can thrust against a planet's magnetic field, especially in an orbit that passes over the planet's magnetic poles, in a similar manner to an electrodynamic tether.
The range of maneuvers available to a magnetic sail inside a planetary magnetosphere are more limited than in a plasma wind. Just as with the more familiar small-scale magnets used on Earth, a magnetic sail can only be attracted towards the magnetosphere's poles or repelled from them, depending on its orientation.
When the magnetic sail's field is oriented in the opposite direction to the magnetosphere it experiences a force inward and toward the nearest pole, and when it is oriented in the same direction as the magnetosphere it experiences the opposite effect. A magnetic sail oriented in the same direction as the magnetosphere is not stable, and will have to prevent itself from being flipped over to the opposite orientation by some other means.
The thrust that a magnetic sail delivers within a magnetosphere decreases with the fourth power of its distance from the planet's internal magnetic dynamo.
This limited maneuvering capability is still quite useful. By varying the magnetic sail's field strength over the course of its orbit, a magnetic sail can give itself a "perigee kick" raising the altitude of its orbit's apogee.
Repeating this process with each orbit can drive the magnetic sail's apogee higher and higher, until the magnetic sail is able to leave the planetary magnetosphere and catch the solar wind. The same process in reverse can be used to lower or circularize the apogee of a magsail's orbit when it arrives at a destination planet.
In theory, it is possible for a magnetic sail to launch directly from the surface of a planet near one of its magnetic poles, repelling itself from the planet's magnetic field. However, this requires the magnetic sail to be maintained in its "unstable" orientation. A launch from Earth requires superconductors with 80 times the current density of the best known high-temperature superconductors.
Interstellar travel.
Interstellar space contains very small amounts of hydrogen. A fast-moving sail would ionize this hydrogen by accelerating the electrons in one direction and the oppositely charged protons in the other direction. The energy for the ionization and cyclotron radiation would come from the spacecraft's kinetic energy, slowing the spacecraft. The cyclotron radiation from the acceleration of particles would be an easily detected howl in radio frequencies.
Thus, in interstellar spaceflight outside the heliopause of a star a magnetic sail could act as a parachute to decelerate a spacecraft. This removes any fuel requirements for the deceleration half of an interstellar journey, which would benefit interstellar travel enormously. The magsail was first proposed for this purpose in 1988 by Robert Zubrin and Dana Andrews, predating other uses, and evolved from a concept of the Bussard ramjet which used a magnetic scoop to collect interstellar material.
Magnetic sails could also be used with beam-powered propulsion by using a high-power particle accelerator to fire a beam of charged particles at the spacecraft. The magsail would deflect this beam, transferring momentum to the vehicle. This would provide much higher acceleration than a solar sail driven by a laser, but a charged particle beam would disperse in a shorter distance than a laser due to the electrostatic repulsion of its component particles. This dispersion problem could potentially be resolved by accelerating a stream of sails which then in turn transfer their momentum to a magsail vehicle, as proposed by Jordin Kare.
Fictional uses.
The ancestor of the magsail, the Bussard magnetic scoop, first appeared in science-fiction in Poul Anderson's 1967 short story "To Outlive Eternity", which was followed by the novel "Tau Zero" in 1970. The magsail appears as a crucial plot device in "The Children's Hour", a "Man-Kzin Wars" novel by Jerry Pournelle and S.M. Stirling (1991). It also features prominently in the science-fiction novels of Michael Flynn, particularly in "The Wreck of the River of Stars" (2003); this book is the tale of the last flight of a magnetic sail ship when fusion rockets based on the Farnsworth-Hirsch Fusor have become the preferred technology.

</doc>
<doc id="37849" url="http://en.wikipedia.org/wiki?curid=37849" title="Nuclear salt-water rocket">
Nuclear salt-water rocket

A nuclear salt-water rocket (or NSWR) is a proposed type of nuclear thermal rocket designed by Robert Zubrin that would be fueled by water bearing dissolved salts of plutonium or U235. These would be stored in tanks that would prevent a critical mass from forming by some combination of geometry or neutron absorption (for example: long tubes made out of boron in an array with considerable spacing between tubes). Thrust would be generated by nuclear fission reactions from the nuclear salts heating the water and being expelled through a nozzle. The water would serve as both a neutron moderator and propellant.
Design.
In a conventional chemical rocket, chemical reactions of the fuel and oxidizer (e.g. oxygen and kerosene) heat the by-products of the chemical reaction (e.g. CO2 and H2O) to high temperatures as they are forced through a rocket nozzle. The fast moving molecules in the exhaust focused in one direction create thrust. In a nuclear thermal rocket (or NTR) a nuclear fission reactor would serve as a source of heat which would be transferred to a propellant that is then exhausted through a rocket nozzle. The propellant in this case can be any material with suitable properties, it need not react during the operation of the rocket, it is simply a source of mass to be heated up and exhausted out of the rocket at high speeds. In an NSWR the nuclear salt-water would be made to flow through a reaction chamber and out an exhaust nozzle in such a way and at such speeds that the peak neutron flux in the fission reaction would occur outside the vehicle.
Advantages.
There are several advantages relative to conventional NTR designs. As the peak neutron flux and fission reaction rates would occur outside the vehicle, these activities could be much more vigorous than they could be if it was necessary to house them in a vessel (which would have temperature limits due to materials constraints). Additionally, a contained reactor can only allow a small percentage of its fuel to undergo fission at any given time, otherwise it would overheat and meltdown (or explode in a runaway fission chain reaction). The fission reaction in an NSWR is dynamic and because the reaction products are exhausted into space it doesn't have a limit on the proportion of fission fuel that reacts. In many ways this makes NSWRs like a hybrid between fission reactors and fission bombs.
Due to their ability to harness the power of what is essentially a continuous nuclear fission explosion, NSWRs would have both very high thrust and very high exhaust velocity, a rare combination of traits in the rocket world, meaning that the rocket would be able to accelerate quickly as well as be extremely efficient in terms of propellant usage. One design would generate 13 meganewtons of thrust at 66 km/s exhaust velocity (compared to ~4.5 km/s exhaust velocity for the best chemical rockets of today). Another design would achieve much higher exhaust velocities (4,700 km/s) and use 2,700 tonnes of highly enriched uranium salts in water to propel a 300 tonne spacecraft up to 3.6% of the speed of light.
NSWRs share many of the features of Orion propulsion systems, except that NSWRs would generate continuous rather than pulsed thrust and may be workable on much smaller scales than the smallest feasible Orion designs (which are generally large, due to the requirements of the shock-absorber system and the minimum size of efficient nuclear explosives) "
Limitations.
The vessel's exhaust would contain radioactive isotopes, but these would be rapidly dispersed after travelling only a short distance; the exhaust would also be travelling at high speed (in Zubrin's scenario, faster than Solar escape velocity, allowing it to eventually leave the Solar System). This is however, little use on the surface of a planet, where a NSWR would eject massive quantities of superheated steam, still containing fissioning nuclear salts. Terrestrial testing might be subject to reasonable objections; as one physicist wrote, "Writing the environmental impact statement for such tests [...] might present an interesting problem ..."
It is also not certain that fission in a NSWR could be controlled: "Whether fast criticality can be controlled in a rocket engine remains an open question.".

</doc>
<doc id="37852" url="http://en.wikipedia.org/wiki?curid=37852" title="Fusion rocket">
Fusion rocket

A fusion rocket is a theoretical design for a rocket driven by fusion power which could provide efficient and long-term acceleration in space without the need to carry a large fuel supply. The design relies on the development of fusion power technology beyond current capabilities, and the construction of rockets much larger and more complex than any current spacecraft. A smaller and lighter fusion reactor might be possible in the future when more sophisticated methods have been devised to control magnetic confinement and prevent plasma instabilities. Fusion power could provide a lighter and more compact alternative.
For space flight, the main advantage of fusion would be the very high specific impulse, and the main disadvantage the (likely) large mass of the reactor. However, a fusion rocket may produce less radiation than a fission rocket, reducing the mass needed for shielding. The surest way of building a fusion rocket with current technology is to use hydrogen bombs as proposed in Project Orion, but such a spacecraft would also be massive and the Partial Nuclear Test Ban Treaty prohibits the use of nuclear bombs. Therefore, the use of nuclear bombs to propel rockets on Earth is problematic, but possible in space in theory. An alternate approach would be electrical (e.g. ion) propulsion with electric power generation via fusion power instead of direct thrust.
Electricity generation vs. direct thrust.
Many spacecraft propulsion methods such as ion thrusters require an input of electric power to run but are highly efficient. In some cases their maximum thrust is limited by the amount of power that can be generated (for example, a mass driver). An electric generator that ran on fusion power could be installed purely to drive such a ship. One disadvantage is that conventional electricity production requires a low-temperature energy sink, which is difficult (i.e. heavy) in a spacecraft. Direct conversion of the kinetic energy of the fusion products into electricity is in principle possible and would mitigate this problem.
An attractive possibility is to simply direct the exhaust of fusion product out the back of the rocket to provide thrust without the intermediate production of electricity. This would be easier with some confinement schemes (e.g. magnetic mirrors) than with others (e.g. tokamaks). It is also more attractive for "advanced fuels" (see aneutronic fusion). Helium-3 propulsion is a proposed method of spacecraft propulsion that uses the fusion of helium-3 atoms as a power source. Helium-3, an isotope of helium with two protons and one neutron, could be fused with deuterium in a reactor. The resulting energy release could be used to expel propellant out the back of the spacecraft. Helium-3 is proposed as a power source for spacecraft mainly because of its abundance on the moon. Currently, scientists estimate that there are 1 million tons of helium-3 present on the moon, mainly due to solar wind colliding with the moon's surface and depositing it, among other elements, into the soil. Only 20% of the power produced by the D-T reaction could be used this way; the other 80% is released in the form of neutrons which, because they cannot be directed by magnetic fields or solid walls, would be very difficult to use for thrust. Helium-3 is also produced via beta decay of tritium, which in turn can be produced from deuterium, lithium, or boron.
Even if a self-sustaining fusion reaction cannot be produced, it might be possible to use fusion to boost the efficiency of another propulsion system, such as a VASIMR engine.
Confinement concept.
To sustain a fusion reaction, the plasma must be confined. The most widely studied configuration for terrestrial fusion is the tokamak, a form of magnetic confinement fusion. Currently tokamaks weigh a great deal, so the thrust to weight ratio would seem unacceptable. NASA's Glenn Research Center has proposed a small aspect ratio spherical torus reactor for its "Discovery II" conceptual vehicle design. "Discovery II" could deliver a manned 172 000-kilogram payload to Jupiter in 118 days (or 212 days to Saturn) using 861 metric tons of hydrogen propellant, plus 11 metric tons of Helium-3-Deuterium (D-He3) fusion fuel. The hydrogen is heated by the fusion plasma debris to increase thrust, at a cost of reduced exhaust velocity (348–463 km/s) and hence increased propellant mass.
The main alternative to magnetic confinement is inertial confinement fusion (ICF), such as that proposed by Project Daedalus. A small pellet of fusion fuel (with a diameter of a couple of millimeters) would be ignited by an electron beam or a laser. To produce direct thrust, a magnetic field would form the pusher plate. In principle, the Helium-3-Deuterium reaction or an aneutronic fusion reaction could be used to maximize the energy in charged particles and to minimize radiation, but it is highly questionable whether it is technically feasible to use these reactions. Both the detailed design studies in the 1970s, the Orion drive and Project Daedalus, used inertial confinement. In the 1980s, Lawrence Livermore National Laboratory and NASA studied an ICF-powered "Vehicle for Interplanetary Transport Applications" (VISTA). The conical VISTA spacecraft could deliver a 100-tonne payload to Mars orbit and return to Earth in 130 days, or to Jupiter orbit and back in 403 days. 41 tonnes of deuterium/tritium (D-T) fusion fuel would be required, plus 4,124 tonnes of hydrogen expellant. The exhaust velocity would be 157 km/s.
Magnetized target fusion (MTF) is a relatively new approach that combines the best features of the more widely studied magnetic confinement fusion (i.e. good energy confinement) and inertial confinement fusion (i.e. efficient compression heating and wall free containment of the fusing plasma) approaches. Like the magnetic approach, the fusion fuel is confined at low density by magnetic fields while it is heated into a plasma, but like the inertial confinement approach, fusion is initiated by rapidly squeezing the target to dramatically increase fuel density, and thus temperature. MTF uses "plasma guns" (i.e. electromagnetic acceleration techniques) instead of powerful lasers, leading to low cost and low weight compact reactors. The NASA/MSFC Human Outer Planets Exploration (HOPE) group has investigated a manned MTF propulsion spacecraft capable of delivering a 163933-kilogram payload to Jupiter's moon Callisto using 106-165 metric tons of propellant (hydrogen plus either D-T or D-He3 fusion fuel) in 249–330 days. This design would thus be considerably smaller and more fuel efficient due to its higher exhaust velocity (700 km/s) than the previously mentioned "Discovery II", "VISTA" concepts.
Another popular confinement concept for fusion rockets is inertial electrostatic confinement (IEC), such as in the Farnsworth-Hirsch Fusor or the Polywell variation being researched by the Energy-Matter Conversion Corporation. The University of Illinois has defined a 500-tonne "Fusion Ship II" concept capable of delivering a 100,000 kg manned payload to Jupiter's moon Europa in 210 days. Fusion Ship II utilizes ion rocket thrusters (343 km/s exhaust velocity) powered by ten D-He3 IEC fusion reactors. The concept would need 300 tonnes of argon propellant for a 1-year round trip to the Jupiter system. Dr. Robert Bussard published a series of technical articles discussing its application to spaceflight throughout the 1990s. His work was popularised by an article in the Analog Science Fiction and Fact publication, where Tom Ligon (who has also written several science fiction stories) described how the fusor would make for a highly effective fusion rocket. It was also featured in this role in the science fiction novel "The Wreck of the River of Stars", by Michael Flynn.
A still more speculative concept is antimatter catalyzed nuclear pulse propulsion, which would use tiny quantities of antimatter to catalyze a fission and fusion reaction, allowing much smaller fusion explosions to be created.
Development projects.
See MSNW Magneto-Inertial Fusion Driven Rocket.

</doc>
<doc id="37861" url="http://en.wikipedia.org/wiki?curid=37861" title="Ante-Nicene Fathers">
Ante-Nicene Fathers

The Ante-Nicene Fathers, subtitled "The Writings of the Fathers Down to A.D. 325", is a collection of books in 10 volumes (one volume is indexes) containing English translations of the majority of Early Christian writings. The period covers the beginning of Christianity until before the promulgation of the Nicene Creed at the First Council of Nicaea. The translations are very faithful, but sometimes rather old-fashioned.
Publication.
The series was originally published between 1867 and 1873 by the Presbyterian publishing house T. & T. Clark in Edinburgh under the title Ante-Nicene Christian Library, as a response to the Oxford movement's Library of the Fathers which was perceived as too Roman Catholic. The volumes were edited by Rev. Alexander Roberts and James Donaldson. This series was available by subscription but the editors were unable to interest enough subscribers to commission a translation of the homilies of Origen.
In 1885 a US firm, the Christian Literature Company, first of Buffalo, then New York, began to issue the volumes in a reorganised form, edited by the Episcopalian bishop of New York, A. Cleveland Coxe. Coxe gave his "new" series the title: "The Ante-Nicene Fathers".
In 1896, the American edition/revision was complete. In 1897, the volume 9 of this one which contained new translations, was published by T. & T. Clark as an additional volume to complete the original ANCL.
Apart from this volume 9, the contents entirely derived thus from the ANCL, but in a more chronological order. However Coxe took the liberty to add his own introductions and notes, which were criticised by many academic authorities as well as Roman Catholic reviewers.
Surely convinced by the commercial success of the cheaper American version/revision of the ANCL - although of lesser quality on some minor points - the T. & T. Clark get associated with the Christian Literature Company and with others American editors for the publication of sequel: "Nicene and Post-Nicene Fathers".
Contents.
The volumes include the following:
Volume I. Apostolic Fathers with Justin Martyr and Irenaeus
Volume II. Fathers of the Second Century
Volume III. Latin Christianity: Its Founder, Tertullian
Volume IV. The Fathers of the Third Century
Volume V. The Fathers of the Third Century
Volume VI. The Fathers of the Third Century
Volume VII. Fathers of the Third and Fourth Centuries
Volume VIII. Fathers of the Third and Fourth Centuries
Volume IX. Recently Discovered Additions to Early Christian Literature; Commentaries of Origen

</doc>
<doc id="37868" url="http://en.wikipedia.org/wiki?curid=37868" title="Porphyry">
Porphyry

Porphyry (; Greek: Πορφύριος, "Porphyrios" "purple-clad") may refer to:

</doc>
<doc id="37882" url="http://en.wikipedia.org/wiki?curid=37882" title="Crocodile">
Crocodile

Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes "Tomistoma", is not used in this article. The term crocodile here applies only to the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes "Tomistoma", the alligators and caimans (family Alligatoridae), the gharials (family Gavialidae), and all other living and fossil Crocodylomorpha.
Although they appear to be similar to the untrained eye, crocodiles, alligators and the gharial belong to separate biological families. The gharial having a narrow snout is easier to distinguish, while morphological differences are more difficult to spot in crocodiles and alligators. The most obvious external differences are visible in the head with crocodiles having narrower and longer heads, with a more V-shaped than a U-shaped snout compared to alligators and caimans. Another obvious trait is the upper and lower jaws of the crocodiles are the same width, and teeth in the lower jaw fall along the edge or outside the upper jaw when the mouth is closed; therefore all teeth are visible unlike an alligator; which possesses small depressions in the upper jaw where the lower teeth fit into. Also when the crocodile's mouth is closed, the large fourth tooth in the lower jaw fits into a constriction in the upper jaw. For hard-to-distinguish specimens, the protruding tooth is the most reliable feature to define the family that the species belongs to. Crocodiles have more webbing on the toes of the hind feet and can better tolerate saltwater due to specialized salt glands for filtering out salt, which are present but non-functioning in alligators. Another trait that separates crocodiles from other crocodilians is their much higher levels of aggression.
Crocodile size, morphology, behavior and ecology somewhat differs between species. However, they have many similarities in these areas as well. All crocodiles are semiaquatic and tend to congregate in freshwater habitats such as rivers, lakes, wetlands and sometimes in brackish water and saltwater. They are carnivorous animals, feeding mostly on vertebrates such as fish, reptiles, birds and mammals, and sometimes on invertebrates such as molluscs and crustaceans, depending on species and age. All crocodiles are tropical species that unlike alligators, are very sensitive to cold. They first separated from other crocodilians during the Eocene epoch, about 55 million years ago. Many species are at the risk of extinction, some being classified as critically endangered.
Etymology.
The word "crocodile" comes from the Ancient Greek κροκόδιλος ("crocodilos"), "lizard," used in the phrase "ho krokódilos tou potamoú", "the lizard of the (Nile) river". There are several variant Greek forms of the word attested, including the later form κροκόδειλος ("crocodeilos") found cited in many English reference works. In the Koine Greek of Roman times, "crocodilos" and "crocodeilos" would have been pronounced identically, and either or both may be the source of the Latinized form "crocodīlus" used by the ancient Romans. "Crocodilos" or "crocodeilos" is a compound of "krokè" ("pebbles"), and "drilos/dreilos" ("worm"), although "drilos" is only attested as a colloquial term for "penis". It is ascribed to Herodotus, and supposedly describes the basking habits of the Egyptian crocodile.
The form "crocodrillus" is attested in Medieval Latin. It is not clear whether this is a medieval corruption or derives from alternate Greco-Latin forms (late Greek "corcodrillos" and "corcodrillion" are attested). A (further) corrupted form "cocodrille" is found in Old French and was borrowed into Middle English as "cocodril(le)". The Modern English form "crocodile" was adapted directly from the Classical Latin "crocodīlus" in the 16th century, replacing the earlier form. The use of -y- in the scientific name "Crocodylus" (and forms derived from it) is a corruption introduced by Laurenti (1768).
Species.
A total of 14 extant species have been recognized. Further genetic study is needed for the confirmation of proposed species under the genus Osteolaemus, which is currently monotypic.
Characteristics.
A crocodile’s physical traits allow it to be a successful predator. Its external morphology is a sign of its aquatic and predatory lifestyle. Its streamlined body enables it to swim swiftly, it also tucks its feet to the side while swimming, which makes it faster by decreasing water resistance. They have webbed feet which, though not used to propel the animal through the water, allow them to make fast turns and sudden moves in the water or initiate swimming. Webbed feet are an advantage in shallower water, where the animal sometimes moves around by walking. Crocodiles have a palatal flap, a rigid tissue at the back of the mouth that blocks the entry of water. The palate has a special path from the nostril to the glottis that bypasses the mouth. The nostrils are closed during submergence.
Like other archosaurs, crocodilians are diapsid, although their post-temporal fenestrae are reduced. The walls of the braincase are bony, but lack supratemporal and postfrontal bones. Their tongues are not free, but held in place by a membrane that limits movement; as a result, crocodiles are unable to stick out their tongues. Crocodiles have smooth skin on their bellies and sides, while their dorsal surfaces are armoured with large osteoderms. The armoured skin has scales and is thick and rugged, providing some protection. They are still able to absorb heat through this armour, as a network of small capillaries allows blood through the scales to absorb heat. Crocodilian scales have pores believed to be sensory in function, analogous to the lateral line in fishes. They are particularly seen on their upper and lower jaws. Another possibility is that they are secretory, as they produce an oily substance which appears to flush mud off.
Size.
Size greatly varies between species, from the dwarf crocodile to the saltwater crocodile. Species of "Osteolaemus" grow to an adult size of just 1.5 to, whereas the saltwater crocodile can grow to sizes over 7 m and weigh 1000 kg. Several other large species can reach over 5.2 m long and weigh over 900 kg. Crocodilians show pronounced sexual dimorphism, with males growing much larger and more rapidly than females. Despite their large adult sizes, crocodiles start their lives at around 20 cm long. The largest species of crocodile is the saltwater crocodile, found in eastern India, northern Australia, throughout South-east Asia, and in the surrounding waters.
The largest crocodile ever held in captivity is an estuarine–Siamese hybrid named Yai (Thai: ใหญ่, meaning big) (born 10 June 1972) at the Samutprakarn Crocodile Farm and Zoo, Thailand. This animal measures 6 m in length and weighs 1114.27 kg.
The longest crocodile captured alive is Lolong, which was measured at 6.17 m and weighed at 1075 kg by a National Geographic team in Agusan del Sur Province, Philippines.
Teeth.
Crocodiles are polyphyodonts; they are able to replace each of their 80 teeth up to 50 times in their 35 to 75-year lifespan. Next to each full grown tooth, there is a small replacement tooth and a odontogenic stem cell in the dental lamina in standby that can be activated if required.
Biology and behavior.
Crocodilians are more closely related to birds and dinosaurs than to most animals classified as reptiles, the three families being included in the group Archosauria ('ruling reptiles'). Despite their prehistoric look, crocodiles are among the more biologically complex reptiles. Unlike other reptiles, a crocodile has a cerebral cortex and a four-chambered heart. Crocodilians also have the functional equivalent of a diaphragm by incorporating muscles used for aquatic locomotion into respiration. Salt glands are present in the tongues of crocodiles and they have a pore opening on the surface of the tongue, which is a trait that separates them from alligators. Salt glands are dysfunctional in Alligatoridae. Their function appears to be similar to that of salt glands in marine turtles. Crocodiles do not have sweat glands and release heat through their mouths. They often sleep with their mouths open and may pant like a dog. Four species of freshwater crocodile climb trees to bask in areas lacking a shoreline.
Senses.
Crocodiles have acute senses, an evolutionary advantage that makes them successful predators. The eyes, ears and nostrils are located on top of the head, allowing the crocodile to lie low in the water, almost totally submerged and hidden from prey.
Vision.
Crocodiles have very good night vision, and are mostly nocturnal hunters. They use the disadvantage of most prey animals' poor nocturnal vision, to their advantage. The light receptors in crocodilians’ eyes include both cones and numerous rods, so it is assumed all crocodilians can see colors. Crocodiles have vertical-slit shaped pupils, similar to domestic cats. One explanation for the evolution of slit pupils is that they exclude light more effectively than a circular pupil, helping to protect their eyes during daylight. On the rear wall of the eye is a tapetum lucidum, which reflects incoming light back onto the retina, thus utilizing the small amount of light available at night to best advantage. In addition to the protection of the upper and lower eyelids, crocodiles have a nictitating membrane that can be drawn over the eye from the inner corner while the lids are open. The eyeball surface is thus protected under the water while a certain degree of vision is still possible.
Olfaction.
Crocodilian sense of smell is also very well developed, aiding them to detect prey or animal carcasses that are either on land or in water, from far away. It is possible that crocodiles use olfaction in the egg prior to hatching.
Chemoreception in crocodiles is especially interesting because they hunt in both terrestrial and aquatic surroundings. Crocodiles have only one olfactory chamber and the vomeronasal organ is absent in the adults indicating all olfactory perception is limited to the olfactory system. Behavioral and olfactometer experiments indicate that crocodiles detect both air-borne and water-soluble chemicals and use their olfactory system for hunting. When above water, crocodiles enhance their ability to detect volatile odorants by gular pumping, a rhythmic movement of the floor of the pharynx. Unlike turtles, crocodiles close their nostrils when submerged, so olfaction underwater is unlikely. Underwater food detection is presumably gustatory and tactile.
Hearing.
Crocodiles can hear well; their tympanic membranes are concealed by flat flaps that may be raised or lowered by muscles.
Touch.
Caudal: The upper and lower jaws are covered with sensory pits, visible as small, black speckles on the skin, the crocodilian version of the lateral line organs seen in fish and many amphibians, though arising from a completely different origin. These pigmented nodules encase bundles of nerve fibers innervated beneath by branches of the trigeminal nerve. They respond to the slightest disturbance in surface water, detecting vibrations and small pressure changes as small as a single drop. This makes it possible for crocodiles to detect prey, danger and intruders, even in total darkness. These sense organs are known as "Domed Pressure Receptors" (DPRs).
Post-Caudal: While alligators and caimans have DPRs only on their jaws, crocodiles have similar organs on almost every scale on their bodies. The function of the DPRs on the jaws is clear; to catch prey, but it is still not clear what is the function of the organs on the rest of the body. The receptors flatten when exposed to increased osmotic pressure, such as that experienced when swimming in sea water hyper-osmotic to the body fluids. When contact between the integument and the surrounding sea water solution is blocked, crocodiles are found to lose their ability to discriminate salinities. It has been proposed that the flattening of the sensory organ in hyper-osmotic sea water is sensed by the animal as “touch”, but interpreted as chemical information about its surroundings. This might be why in alligators they are absent on the rest of the body.
Hunting and diet.
Crocodiles are ambush predators, waiting for fish or land animals to come close, then rushing out to attack. Crocodiles mostly eat fish, amphibians, crustaceans, molluscs, birds, reptiles, and mammals, and they occasionally cannibalize on smaller crocodiles. What a crocodile eats varies greatly with species, size and age. From the mostly fish-eating species, like the slender-snouted and freshwater crocodiles, to the larger species like the Nile crocodile and the saltwater crocodile that prey on large mammals, such as buffalo, deer and wild boar, diet shows great diversity. Diet is also greatly affected by the size and age of the individual within the same species. All young crocodiles hunt mostly invertebrates and small fish, gradually moving onto larger prey. As cold-blooded predators, they have a very slow metabolism, so they can survive long periods without food. Despite their appearance of being slow, crocodiles have a very fast strike and are top predators in their environment, and various species have been observed attacking and killing other predators such as sharks and big cats.
Crocodiles have the most acidic stomach of any vertebrate. They can easily digest bones, hooves and horns. The BBC TV reported that a Nile crocodile that has lurked a long time underwater to catch prey builds up a large oxygen debt. When it has caught and eaten that prey, it closes its right aortic arch and uses its left aortic arch to flush blood loaded with carbon dioxide from its muscles directly to its stomach; the resulting excess acidity in its blood supply makes it much easier for the stomach lining to secrete more stomach acid to quickly dissolve bulks of swallowed prey flesh and bone. Many large crocodilians swallow stones (called gastroliths or stomach stones), which may act as ballast to balance their bodies or assist in crushing food, similar to grit ingested by birds. Herodotus claimed that Nile crocodiles had a symbiotic relationship with certain birds, such as the Egyptian plover, which enter the crocodile's mouth and pick leeches feeding on the crocodile's blood; with no evidence of this interaction actually occurring in any crocodile species, it is most likely mythical or allegorical fiction.
Bite.
Since they feed by grabbing and holding onto their prey, they have evolved sharp teeth for piercing and holding onto flesh, and powerful muscles to close the jaws and hold them shut. The teeth are not well-suited to tearing flesh off of large prey items as is the dentition and claws of many mammalian carnivores, the hooked bills and talons of raptorial birds, or the serrated teeth of sharks. However, this is an advantage rather than a disadvantage to the crocodile since the properties of the teeth allow it to hold onto prey with the least possibility of the prey animal to escape. Otherwise combined with the exceptionally high bite force, the flesh would easily cut through; thus creating an escape opportunity for the prey item. The jaws can bite down with immense force, by far the strongest bite of any animal. The force of a large crocodile's bite is more than 5000 lbf, which was measured in a 5.5 m Nile crocodile, on the field, compared to just 335 lbf for a Rottweiler, 670 lbf for a great white shark, 800 lbf for a hyena, or 2200 lbf for an American alligator. A 5.2 m long saltwater crocodile has been confirmed as having the strongest bite force ever recorded for an animal in a laboratory setting. It was able to apply a bite force value of 3700 lbf, and thus surpassed the previous record of 2125 lbf made by a 3.9 m long American alligator. Taking the measurements of several 5.2 m crocodiles as reference, the bite forces of 6-m individuals were estimated at 7700 lbf. The study, lead by Dr. Gregory M. Erickson, also shed light to the larger, extinct species of crocodilians. Since crocodile anatomy has changed only slightly for the last 80 million years, current data on modern crocodilians can be used to estimate the bite force of extinct species. An 11 to(-) long Deinosuchus would apply a force of 23100 lbf, twice that of the latest, higher bite force estimations of Tyrannosaurus. The extraordinary bite of crocodilians is a result of their anatomy. The space for the jaw muscle in the skull is very large, which is easily visible from the outside as a bulge at each side. The nature of the muscle is so stiff, it is almost as hard as bone to touch, as if it were the continuum of the skull. Another trait is that most of the muscle in a crocodile's jaw is arranged for clamping down. Despite the strong muscles to close the jaw, crocodiles have extremely small and weak muscles to open the jaw. Crocodiles can thus be subdued for study or transport by taping their jaws or holding their jaws shut with large rubber bands cut from automobile inner tubes.
Locomotion.
Crocodiles are very fast over short distances, even out of water. The land speed record for a crocodile is 17 km/h measured in a galloping Australian freshwater crocodile. Maximum speed varies from species to species. Certain species can indeed gallop, including Cuban crocodiles, New Guinea crocodiles, African dwarf crocodiles, and even small Nile crocodiles. The fastest means by which most species can move is a kind of "belly run", where the body moves in a snake-like fashion, limbs splayed out to either side paddling away frantically while the tail whips to and fro. Crocodiles can reach speeds of 10 - when they "belly run", and often faster if slipping down muddy riverbanks. Another form of locomotion is the "high walk", where the body is raised clear of the ground. Crocodiles may possess a form of homing instinct. In northern Australia, three rogue saltwater crocodiles were relocated 400 km by helicopter, but had returned to their original locations within three weeks, based on data obtained from tracking devices attached to the reptiles.
Longevity.
Measuring crocodile age is unreliable, although several techniques are used to derive a reasonable guess. The most common method is to measure lamellar growth rings in bones and teeth—each ring corresponds to a change in growth rate which typically occurs once a year between dry and wet seasons. Bearing these inaccuracies in mind, it can be safely said that all crocodile species have an average lifespan of at least 30–40 years, and in the case of larger species an average of 60–70 years. The oldest crocodiles appear to be the largest species. "C. porosus" is estimated to live around 70 years on average, with limited evidence of some individuals exceeding 100 years.
In captivity, some individuals are claimed to have lived for over a century. A male crocodile lived to an estimated age of 110–115 years in a Russian zoo in Yekaterinburg. Named Kolya, he joined the zoo around 1913 to 1915, fully grown, after touring in an animal show, and lived until 1995. A male freshwater crocodile lived to an estimated age of 120–140 years at the Australia Zoo. Known affectionately as “Mr. Freshie”, he was rescued around 1970 by Bob Irwin and Steve Irwin, after being shot twice by hunters and losing an eye as a result, and lived until 2010. Crocworld Conservation Centre, in Scottburgh, South Africa, claims to have a male Nile crocodile that was born in 1900 (age 114–115). Named Henry, the crocodile is said to have lived in Botswana along the Okavango River, according to centre director Martin Rodrigues.
Social behavior and vocalization.
Crocodiles are the most social of reptiles. Even though they do not form social groups, many species congregate in certain section of a rivers, tolerating each other at times of feeding and basking. Most species are not highly territorial, with the exception of the saltwater crocodile; which is a highly territorial and aggressive species. A mature male will not tolerate any other males at any time of the year. Most of the species however, are more flexible. There is a certain form of hierarchy in crocodiles, where the largest and heaviest males are at the top; having access to the best basking site, females and priority during a group feeding of a big kill or carcass. A good example to the hierarchy in crocodiles would be the case of the Nile crocodile. This species clearly displays all of these behaviors. Studies in this area are not thorough, and many species are yet to be studied in greater detail. Mugger crocodiles are also known to show toleration in group feedings and tend to congregate to certain areas. However, males of all species are aggressive towards each other during mating season, to gain access to females.
Crocodiles are also the most vocal of all reptiles, producing a wide variety of sounds during various situations and conditions, depending on species, age, size and sex. Depending on the context, some species can communicate over 20 different messages through vocalizations alone. Some of these vocalizations are made during social communication, especially during territorial displays towards the same sex and courtship with the opposite sex; the common concern being reproduction. Therefore most conspecific vocalization is made during the breeding season, with the exception being year-round territorial behavior in some species and quarrels during feeding. Crocodiles also produce different distress calls and in aggressive displays to their own kind and other animals; notably other predators during interspecific predatory confrontations over carcasses and terrestrial kills.
Specific vocalisations include -
Chirp: When about to hatch, the young make a “peeping” noise, which encourages the female to excavate the nest. The female then gathers the hatchlings in her mouth and transports them to the water, where they remain in a group for several months, protected by the female
Distress call: A high-pitched call mostly used by younger animals that alerts other crocodiles to imminent danger or an animal being attacked.
Threat call: A hissing sound that has also been described as a coughing noise.
Hatching call: Emitted by females when breeding to alert other crocodiles that she has laid eggs in her nest.
Bellowing: Male crocodiles are especially vociferous. Bellowing choruses occur most often in the spring when breeding groups congregate, but can occur at any time of year. To bellow, males noticeably inflate as they raise the tail and head out of water, slowly waving the tail back and forth. They then puff out the throat and with a closed mouth, begin to vibrate air. Just before bellowing, males project an infrasonic signal at about 10 Hz through the water which vibrates the ground and nearby objects. These low-frequency vibrations travel great distances through both air and water to advertise the male's presence and are so powerful they result in the water appearing to 'dance’.
Reproduction.
Crocodiles reproduce by laying eggs, which are either laid in hole or mound nests, depending on species. A hole nest is usually excavated in sand and a mound nest is usually constructed out of vegetation. Nesting period ranges from a few weeks up to six months. Courtship takes place in a series of behavioral interactions that include a variety of snout rubbing and submissive display that can take a long time. Mating always takes place in water, where the pair can be observed mating several times. Females can build or dig several trial nests which appear incomplete and abandoned later. Egg laying usually takes place at night and about 30–40 minutes. Females are highly protective of their nests and young. The egg are hard shelled but translucent at the time of egg-laying. Depending on the species crocodile, a number of 7-95 eggs are laid. Crocodile embryos do not have sex chromosomes, and unlike humans, sex is not determined genetically. Sex is determined by temperature, where at 30 °C or less most hatchlings are females and at 31 °C, offspring are of both sexes. A temperature of 32 to gives mostly males whereas above 33 °C in some species continues to give males but in other species resulting in females, which are sometimes called as high-temperature females. Temperature also affects growth and survival rate of the young, which may explain the sexual dimorphism in crocodiles. The average incubation period is around 80 days, and also is dependent on temperature and species that usually ranges from 65 to 95 days. At the time of hatching, the young start calling within the eggs. They have an egg-tooth at the tip of their snouts, which is developed from the skin, helps them pierce out of the shell. Hearing the calls, the female usually excavates the nest and sometimes takes the unhatched eggs in her mouth, slowly rolling the eggs to help the process. The young is usually carried to the water in the mouth. A group of hatchlings is called a pod or crèche and may be protected for months.
The eggshell structure is very conservative through evolution but there are enough changes to tell different species apart by their eggshell microstructure.
Taxonomy and phylogeny.
Most species are grouped into the genus "Crocodylus". The other extant genus, "Osteolaemus", is monotypic (as is "Mecistops", if recognized).
Phylogeny.
The cladogram below follows the topology from a 2012 analysis of morphological traits by Christopher A. Brochu and Glenn W. Storrs. Many extinct species of "Crocodylus" might represent different genera. ""Crocodylus" pigotti", for example, was placed in the newly erected genus "Brochuchus" in 2013. "C. suchus" was not included because its morphological codings were identical to those of "C. niloticus". However, the authors suggested that the lack of differences was due to limited specimen sampling, and considered the two species to be distinct. This analysis found weak support for the clade Osteolaeminae. Brochu named Osteolaeminae in 2003 as a subfamily of Crocodylidae separate from Crocodylinae, but the group has since been classified within Crocodylinae. It includes the living genus "Osteolaemus" as well as the extinct species "Voay robustus" and "Rimasuchus lloydi".
A 2013 analysis by Jack L. Conrad, Kirsten Jenkins, Thomas Lehmann, and others did not support Osteolaeminae as a true clade but rather a paraphyletic group consisting of two smaller clades. They informally called these clades "osteolaemins" and "mecistopins". "Osteolaemins" include "Osteolaemus", "Voay", "Rimasuchus", and "Brochuchus" and "mecistopins" include "Mecistops" and "Euthecodon".
Relationship with humans.
Danger to humans.
The larger species of crocodiles are very dangerous to humans, mainly because of their ability to strike before the person can react. The saltwater crocodile and Nile crocodile are the most dangerous, killing hundreds of people each year in parts of Southeast Asia and Africa. The mugger crocodile, American crocodile, American alligator and black caiman are also dangerous to humans.
Crocodile products.
Crocodiles are protected in many parts of the world, but they also are farmed commercially. Their hides are tanned and used to make leather goods such as shoes and handbags; crocodile meat is also considered a delicacy. The most commonly farmed species are the saltwater and Nile crocodiles, while a hybrid of the saltwater and the rare Siamese crocodile is also bred in Asian farms. Farming has resulted in an increase in the saltwater crocodile population in Australia, as eggs are usually harvested from the wild, so landowners have an incentive to conserve their habitat. Crocodile leather can be made into goods such as wallets, briefcases, purses, handbags, belts, hats, and shoes. Crocodile oil has been used for various purposes.
Crocodiles in religion.
Crocodiles have appeared in various forms in religions across the world. Ancient Egypt had Sobek, the crocodile-headed god, with his cult-city Crocodilopolis, as well as Taweret, the goddess of childbirth and fertility, with the back and tail of a crocodile. The South Temple at Karanis in Graeco-Roman Egypt was dedicated to local crocodile gods in the 1st century BC.
The Jukun shrine in the Wukari Federation, Nigeria is dedicated to crocodiles in thanks for their aid during migration.
Crocodiles appear in different forms in Hinduism. Varuna, a Vedic and Hindu god, rides a part-crocodile makara; his consort Varuni rides a crocodile. Similarly the goddess personifications of the Ganga and Yamuna rivers are often depicted as riding crocodiles. Also in India, in Goa, crocodile worship is practised, including the annual "Mannge Thapnee" ceremony.
In Latin America, Cipactli was the giant earth crocodile of the Aztec and other Nahua peoples.
Crocodile tears.
The term "Crocodile tears" (and equivalents in other languages) refers to a false, insincere display of emotion, such as a hypocrite crying fake tears of grief. It is derived from an ancient anecdote that crocodiles weep in order to lure their prey, or that they cry for the victims they are eating, first told in the "Bibliotheca" by Photios. The story is repeated in bestiaries such as "De bestiis et aliis rebus". This tale was first spread widely in English in the stories of the travels of Sir John Mandeville in the 14th century, and appears in several of Shakespeare's plays.
In fact, crocodiles can and do generate tears, but they do not actually cry.

</doc>
<doc id="38062" url="http://en.wikipedia.org/wiki?curid=38062" title="Die Entführung aus dem Serail">
Die Entführung aus dem Serail

Die Entführung aus dem Serail (K. 384; The Abduction from the Seraglio; also known as Il Seraglio) is an opera Singspiel in three acts by Wolfgang Amadeus Mozart. The German libretto is by Christoph Friedrich Bretzner with adaptations by Gottlieb Stephanie. The plot concerns the attempt of the hero Belmonte, assisted by his servant Pedrillo, to rescue his beloved Konstanze from the seraglio of Pasha Selim. The work premiered on 16 July 1782 at the Vienna Burgtheater, with the composer conducting.
Origins.
The company that first sponsored the opera was the "Nationalsingspiel" ("national Singspiel"), a pet project (1778–1783) of the Austrian emperor Joseph II. The Emperor had set up the company to perform works in the German language (as opposed to the popular Italian opera style widely popular in Vienna). This project was ultimately given up as a failure, but along the way it produced a number of successes, mostly a series of translated works. Mozart's opera emerged as its outstanding original success.
The inspector of the "Nationalsingspiel" was Gottlieb Stephanie. When the 25-year-old Mozart arrived in Vienna in 1781, seeking professional opportunity, one of the first tasks to which he addressed himself was to become acquainted with Stephanie and lobby him for an opera commission. To this end, he brought a copy of his earlier opera "Zaide" and showed it to Stephanie, who was duly impressed. Mozart also made a strong impression on the manager of the theater, Count Franz Xaver Orsini-Rosenberg, when in the home of Mozart's friend and patroness Maria Wilhelmine Thun the Count heard him play excerpts from his opera "Idomeneo", premiered with great success the previous year in Munich. With this backing, it was agreed that Stephanie would find appropriate material and prepare a libretto for Mozart. Stephanie complied by first pirating and then altering "Belmont und Constanze, oder Die Entführung aus dem Serail", an earlier work by Bretzner, who later complained loudly and publicly about the theft.
Composition.
Mozart received the libretto from Stephanie on 29 July 1781. He had few opportunities to compose professionally during the summer and he set to work on the libretto at a very rapid pace, finishing three major numbers in just two days. A letter to his father Leopold indicates he was very excited about the prospect of having his opera performed in Vienna, and worked enthusiastically on his project.
At first Mozart thought he needed to finish his opera in only two months, because tentative plans were made to perform it at the September visit of the Russian Grand Duke Paul (son of Catherine the Great and heir to the Russian throne). However, it was ultimately decided to perform operas by Gluck instead, giving Mozart more time.
It was around this time that Mozart articulated his views about the role of the composer and the librettist in the preparation of an opera. He wrote to his father (13 October 1781):
I would say that in an opera the poetry must be altogether the obedient daughter of the music. Why are Italian comic operas popular everywhere – in spite of the miserable libretti? … Because the music reigns supreme, and when one listens to it all else is forgotten. An opera is sure of success when the plot is well worked out, the words written solely for the music and not shoved in here and there to suit some miserable rhyme ... The best thing of all is when a good composer, who understands the stage and is talented enough to make sound suggestions, meets an able poet, that true phoenix; in that case, no fears need be entertained as to the applause – even of the ignorant.
It would seem that something along these lines did happen—that is, Mozart decided to play a major role in the shaping of the libretto, insisting that Stephanie make changes for dramatic and musical effect. On 26 September Mozart wrote:
Now comes the rub! The first act was finished more than three weeks ago, as was also one aria in act 2 and the drunken duet ["Vivat Bacchus", act 2] ... But I cannot compose any more, because the whole story is being altered – and, to tell the truth, at my own request. At the beginning of act 3 there is a charming quintet or rather finale, but I would prefer to have it at the end of act 2. In order to make this practicable, great changes must be made, in fact an entirely new plot must be introduced – and Stephanie is up to his neck in other work. So we must have a little patience.
Mozart was evidently quite pleased to have in Stephanie a librettist who would listen to him. The September 26 letter also says:
Everyone abuses Stephanie. It may be the case he is only friendly to my face. But after all he is preparing the libretto for me – and, what is more, exactly as I want it – and by Heaven, I don't ask anything more of him.
With the delays for rewriting, the composition took several more months. The premiere took place on 16 July 1782, at the Burgtheater in Vienna.
Character.
"Die Entführung aus dem Serail" is in the genre of "Singspiel", meaning that much of the action is carried forward by spoken dialogue, thus the music lacks recitatives and consists entirely of set numbers.
The work is lighthearted and frequently comic, with little of the deep character exploration or darker feelings found in Mozart's later operas. Along with other contemporary works, such as Giovanni Paolo Marana's "Letters Writ by a Turkish Spy" and Montesquieu's "Persian Letters", the opera was inspired by a contemporary interest in the perceived "exotic" culture of the Ottoman Empire, a nation which had only recently ceased to be a military threat to the Austrian Empire. Mozart's opera includes a Westernized version of Turkish music, based very loosely on the Turkish Janissary band music that he had employed in earlier work. Like most comedies of the time, it incorporates many elements of plot and characterization established by the popular Commedia dell'arte.
Certain aspects of the opera conform to an eighteenth century European view of orientalism. The Pasha's titular harem, for example, reprised themes of sexual libertinage. And the comically sinister overseer, Osmin, is a send-up of earlier stereotypes of Turkish despotism. However, the opera also defies the stereotyped expectations of a despotic Turkish culture, since its climax entails a selfless act of forgiveness on the part of the Pasha.
The music includes some of the composer's most spectacular and difficult arias. Osmin's act 3 aria "O, wie will ich triumphieren" includes characteristic 18th century coloratura passage work, and twice goes down to a low D (D2), one of the lowest notes demanded of any voice in opera. Perhaps the most famous aria in the opera is the long and elaborate "Martern aller Arten" ("Tortures of all kinds") for Konstanze, an outstanding challenge for sopranos. Konstanze sings in a kind of sinfonia concertante with four solo players from the orchestra; the strikingly long orchestral introduction, without stage action, also poses problems for stage directors.
The virtuosity of these roles is perhaps attributable to the fact that when he took up the task of composing the opera, Mozart already knew the outstanding reputations of the singers for whom he was writing, and he tailored the arias to their strengths. The first Osmin was Ludwig Fischer, a bass noted for his wide range and skill in leaping over large intervals with ease. Similarly, Mozart wrote of the first Konstanze, Caterina Cavalieri, "I have sacrificed Konstanze's aria a little to the flexible throat of Mlle. Cavalieri."
Reception.
The opera was a huge success. The first two performances brought in the large sum of 1200 florins, three times what Mozart's salary had been for his old job in Salzburg. The work was repeatedly performed in Vienna during Mozart's lifetime, and throughout German-speaking Europe. In 1787, Goethe wrote (concerning his own efforts as a librettist):
All our endeavour ... to confine ourselves to what is simple and limited was lost when Mozart appeared. "Die Entführung aus dem Serail" conquered all, and our own carefully written piece has never been so much as mentioned in theater circles.
Although the opera greatly raised Mozart's standing with the public as a composer, it did not make him rich: he was paid a flat fee of 100 Imperial ducats (about 450 florins) for his work, and made no profits from the many subsequent performances.
The opera reached Paris in November 1801, when Frédéric Blasius conducted Ellmenreich's company in performances at the Théâtre de la Gaîté.
This opera is firmly ensconced in the repertoire today. According to Operabase.com, in the 2012/13 season it ranked 29th among operas in performance frequency. s of 2012[ [update]], at least 52 complete recordings were in circulation: 18 videos, 17 studio audio recordings, and at least 17 live audio recordings.
The "too many notes" tale.
The complexity of Mozart's work noted by Goethe, also plays a role in a well-known tale about the opera which appeared in the early (1798) biography of Mozart by Franz Xaver Niemetschek. In the version of the anecdote printed in "Bartlett's Book of Anecdotes", a reference work, the story is told like this:
The Emperor Joseph II commissioned the creation of "The Abduction from the Seraglio", but when he heard it, he complained to Mozart, 'That is too fine for my ears – there are too many notes.' Mozart replied, 'There are just as many notes as there should be.‍ '​
The authenticity of this story is not accepted by all scholars. Moreover, the version given by the Bartlett reference (and many other places) includes a translation of the original German that is dubious. The original reads as follows:
Zu schön für unsere Ohren, und gewaltig viel Noten, lieber Mozart!"
"Too many notes" is not a plausible translation of the German phrase "gewaltig viel Noten". Mautner (1956:33), translating Niemetschek, renders this as "an extraordinary number of notes", while Branscombe (2006) translates it simply as "very many notes". The anecdote, which is often repeated, may have unfairly given the Emperor a bad reputation, concerning both his own musical abilities and his appreciation and support of Mozart. For defense of Joseph from such criticisms, see Beales (2006).
Instrumentation.
The singers perform with a Classical-era orchestra: pairs of flutes (2nd doubling piccolo), oboes, clarinets, bassoons, horns, trumpets, a set of two timpani, and strings. They are augmented with the instruments needed for "Turkish" music: bass drum, cymbals, triangle, and piccolo. The aria, "Traurigkeit ward mir zum Lose", is augmented by basset horn. The part now played on a piccolo was meant for a recorder pitched in G.
The orchestra for the premiere included a number of eminent musicians of the day: first cellist Joseph Franz Weigl, first oboist Josef Triebensee, second horn Joseph Leutgeb, and the clarinettist brothers Anton and Johann Stadler. In the first violin section was Franz de Paula Hofer, who later became Mozart's brother-in-law. The four musicians who played the "Turkish" instruments remain anonymous, though it is known that they were recruited for this purpose by one Franz Tyron, Kapellmeister of the Austrian Second Field Artillery Regiment.
Synopsis.
Act 1.
Belmonte enters, looking for his betrothed, Konstanze, who with her English servant Blonde has fallen into the hands of pirates and been sold to Pasha Selim (Aria: "Hier soll ich dich denn sehen" – "Here surely I must find her"). Osmin, the Pasha's bad-tempered servant, comes to pluck figs in the garden and completely ignores Belmonte's questions (Aria: "Wer ein Liebchen hat gefunden" – "You may think, you've found a maiden"). Belmonte tries to obtain news of his servant, Pedrillo, who has been captured with the women and is serving as a servant in the Pasha's palace. Osmin replies with insults and abuse (Duet: "Verwünscht seist du samt deinem Liede!" – "The devil take you and your song, sir"). Belmonte leaves in disgust. Pedrillo enters and Osmin rages at him, vowing to get him tortured and killed in many different ways (Aria: "Solche hergelaufne Laffen" – "These young men who go a-spying"). Osmin leaves and Belmonte enters and happily reunites with Pedrillo. Together they resolve to rescue Konstanze and Pedrillo's fiancée, Blonde, who is Konstanze's servant (Aria: "Konstanze, Konstanze, dich wiederzusehen … O wie ängstlich" – "Konstanze, Konstanze, to see thee again … Oh what trembling").
Accompanied by a chorus of Janissaries ("Singt dem großen Bassa Lieder" – "Sing to the mighty Pasha Selim"), Pasha Selim appears with Konstanze, for whose love he strives in vain (Aria of Konstanze: "Ach ich liebte" – "How I loved him"). Pedrillo tricks the Pasha into hiring Belmonte as an architect. When Belmonte and Pedrillo try to enter the palace, Osmin bars their way, but they hurry past him anyway (Terzett: "Marsch! Trollt euch fort!" – "March! March! March!").
Act 2.
Blonde repulses the rough lovemaking attempts of Osmin (Aria: "Durch Zärtlichkeit und Schmeicheln" – "With smiles and kind caresses"), and threatens to scratch out his eyes. After a duet ("Ich gehe, doch rate ich dir" – "I'm going, but mark what I say"), Osmin departs. Konstanze greets Blonde in distress (Aria: "Welcher Wechsel herrscht in meiner Seele … Traurigkeit ward mir zum Lose" – "Oh what sorrow overwhelms my spirit … Endless grief tortures my spirit"), informing her that Selim demands her love and threatens to use force (Aria: "Martern aller Arten" – "Tortures unrelenting.")
When she has gone, Pedrillo comes to Blonde, who is his sweetheart, and informs her that Belmonte has come and is planning to rescue them. Blonde is filled with joy. (Aria: "Welche Wonne, welche Lust" – "Oh, the happy, happy day"). Pedrillo invites Osmin to drink, hoping that he will become intoxicated (Aria: "Frisch zum Kampfe" – "Now Pedrillo, now for battle!"; Duet: "Vivat Bacchus! Bacchus lebe!" – "'Here's to Bacchus, long live Bacchus"). When Osmin has drunk himself into a stupor, the two couples reunite (Quartet, Belmonte, Konstanze, Pedrillo, Blonde: "Ach Belmonte! Ach, mein Leben" – "Ah, Belmonte, ah my dear one!"). Belmonte and Pedrillo both question anxiously whether their respective fiancees have remained faithful during their forced separation; to their delight the women respond with indignation and dismay. They forgive the offensive questions and the curtain falls.
Act 3.
Belmonte and Pedrillo come to the garden with ladders (Aria, Belmonte: "Ich baue ganz auf deine Stärke" – "Love, only love, can now direct me"; Romanze, Pedrillo: "In Mohrenland gefangen war" – "In Moorish lands a maiden fair"). However, they and the women are caught by Osmin, who rouses the castle (Aria: "Ha, wie will ich triumphieren" – "My triumphant hour's approaching"). Belmonte pleads for their lives and tells Pasha Selim that his father is a Spanish Grandee and Governor of Oran, named Lostados, who will pay a generous ransom. Unfortunately, Pasha Selim and Lostados are long-standing enemies. The Pasha rejoices in the opportunity to kill his enemy's son. He leaves Belmonte and Konstanze to bid each other a last farewell (Duet: "Welch ein Geschick! O Qual der Seele" – "What dreadful fate conspires against us"), but when he returns, he decides he can make a better point against Lostados by releasing Belmonte and his friends. All are set at liberty – much to the dismay of Osmin, who would prefer to see them all brutally executed (Finale: "Nie werd' ich deine Huld verkennen" – "Your noble mercy passes measure").
Adaptations.
The Finnish composer Aulis Sallinen wrote an opera called "The Palace" (first performed 1995); it contains characters whose names are adapted from "Abduction" and loosely uses elements of the plot of Mozart's opera as the starting point of a satirical fantasy.
Music professor, composer, and humorist Peter Schickele claims to have "discovered" P. D. Q. Bach's "The Abduction of Figaro", a pastiche of the "Entführung" and Mozart's "The Marriage of Figaro".
References.
Notes
Sources

</doc>
<doc id="38076" url="http://en.wikipedia.org/wiki?curid=38076" title="Gender">
Gender

Gender is the range of characteristics pertaining to, and differentiating between, masculinity and femininity. Depending on the context, these characteristics may include biological sex (i.e. the state of being male, female or intersex), sex-based social structures (including gender roles and other social roles), or gender identity.
Sexologist John Money introduced the terminological distinction between biological sex and gender as a role in 1955. Before his work, it was uncommon to use the word "gender" to refer to anything but grammatical categories. However, Money's meaning of the word did not become widespread until the 1970s, when feminist theory embraced the concept of a distinction between biological sex and the social construct of gender. Today, the distinction is strictly followed in some contexts, especially the social sciences and documents written by the World Health Organization (WHO). However, in many other contexts, including some areas of social sciences, "gender" includes "sex" or replaces it. Although this change in the meaning of gender can be traced to the 1980s, a small acceleration of the process in the scientific literature was observed in 1993 when the USA's Food and Drug Administration (FDA) started to use "gender" instead of "sex". In 2011, the FDA reversed its position and began using "sex "as the biological classification and "gender" as "a person's self representation as male or female, or how that person is responded to by social institutions based on the individual's gender presentation." In non-human animal research, "gender" is also commonly used to refer to the physiology of the animals.
In the English literature, the trichotomy between biological sex, psychological gender, and social gender role first appeared in a feminist paper on transsexualism in 1978. Some cultures have specific gender-related social roles that can be considered distinct from male and female, such as the hijra of India and Pakistan.
The social sciences have a branch devoted to gender studies. Other sciences, such as sexology and neuroscience, are also interested in the subject. While the social sciences sometimes approach gender as a social construct, and gender studies particularly do, research in the natural sciences investigates whether biological differences in males and females influence the development of gender in humans; both inform debate about how far biological differences influence the formation of gender identity.
Etymology and usage.
The modern English word "gender" comes from the Middle English "gendre", a loanword from Norman-conquest-era Old French. This, in turn, came from Latin "". Both words mean "kind", "type", or "sort". They derive ultimately from a widely attested Proto-Indo-European (PIE) root "gen-",
which is also the source of "kin", "kind", "king", and many other English words.
Most uses of derivatives of this root in Indo-European languages refer either directly to what pertains to birth (for example "pre-gn-ant") or, by extension, to natural, innate qualities and their consequent social distinctions
(for example "gentry", "generation", "gentile", "genocide", and "eugenics").
It appears in Modern French in the word "genre" (type, kind, also "") and is related to the Greek root "gen-" (to produce), appearing in "gene", "genesis", and "oxygen".
The first edition of the Oxford English Dictionary (OED1, Volume 4, 1900) notes the original meaning of "gender" as "kind" had already become obsolete.
Gender (dʒe'ndəɹ), "sb". Also 4 gendre. [a. OF. "gen(d)re" (F. "genre") = Sp. "género", Pg. "gênero", It. "genere", ad. L. "gener"- stem form of "genus" race, kind = Gr. γένος, Skr. "jánas":— OAryan *"genes"-, f. root γεν- to produce; cf. K.]†1. Kind, sort, class; also, genus as opposed to species. "The general gender": the common sort (of people). "Obs."13.. "E.E.Allit. P." P. 434 Alle gendrez so ioyst wern ioyned wyth-inne. "c" 1384 C "H. Fame"* 1. 18 To knowe of hir signifiaunce The gendres. 1398 T "Barth. De P. K." . xxix. (1495) 34I Byshynynge and lyghte ben dyuers as species and gendre, for suery shinyng is lyght, but not ayenwarde. 1602 S. "Ham". . vii. 18 The great loue the generall gender beare him. 1604—"Oth". . iii. 326 Supplie it with one gender of Hearbes, or distract it with many. 1643 and so on.
The word was still widely attested, however, in the specific sense of grammatical gender (the assignment of nouns to categories such as "masculine", "feminine" and "neuter"). According to Aristotle, this concept was introduced by the Greek philosopher Protagoras.
τὰ γένη τῶν ὀνομάτων ἄρρενα καὶ θήλεα καὶ σκεύη The classes ("genē") of the nouns are males, females and things. "The Technique of Rhetoric" III v
In 1926, Henry Watson Fowler stated that the definition of the word pertains to this grammar-related meaning:
"Gender...is a grammatical term only. To talk of persons...of the masculine or feminine g[ender], meaning of the male or female sex, is either a jocularity (permissible or not according to context) or a blunder."
However examples of the use of "gender" to refer to masculinity and femininity as types are found throughout the history of Modern English (from about the 14th century).
As a verb, "gender" means "breed" in the King James Bible:
Thou shalt not let thy cattle gender with a diverse kind—1616
The modern academic sense of the word, in the context of social roles of men and women, dates from the work of John Money (1955), and was popularized and developed by the feminist movement from the 1970s onwards (see Feminism theory and gender studies below). The theory was that human nature is essentially epicene and social distinctions based on sex are arbitrarily constructed. Matters pertaining to this theoretical process of social construction were labelled matters of "gender".
The popular use of "gender" simply as an alternative to "sex" (as a biological category) is also widespread, although attempts are still made to preserve the distinction. The American Heritage Dictionary (2000) uses the following two sentences to illustrate the difference, noting that the distinction "is useful in principle, but it is by no means widely observed, and considerable variation in usage occurs at all levels."
The effectiveness of the medication appears to depend on the sex "(not gender)" of the patient.<br>In peasant societies, gender "(not sex)" roles are likely to be more clearly defined.
In the last two decades of the 20th century, the use of "gender" in academia increased greatly, outnumbering uses of "sex" in the social sciences. While the spread of the word in science publications can be attributed to the influence of feminism, its use as a euphemism for sex is attributed to the failure to grasp the distinction made in feminist theory, and the distinction has sometimes become blurred with the theory itself.
Among the reasons that working scientists have given me for choosing gender rather than sex in biological contexts are desires to signal sympathy with feminist goals, to use a more academic term, or to avoid the connotation of copulation—David Haig, "The Inexorable Rise of Gender and the Decline of Sex".
Analogous terms in other languages.
Gender activist Gopi Shankar coined the regional terms for Genderqueer people in Tamil. After English, Tamil is the only language that has been given names for all the genders identified so far.
Urdu recognizes hijra as a third gender in India and Pakistan since the mid to late 2000s.
Gender identity and gender roles.
"Gender identity" refers to a personal identification with a particular gender and gender role in society. The term "woman" has historically been used interchangeably with reference to the female body, though more recently this usage has been viewed as controversial by feminists. There are qualitative analyses that explore and present the representations of gender; however, feminists challenge these dominant ideologies concerning gender roles and biological sex. One's biological sex is directly tied to specific social roles and the expectations. Judith Butler considers the concept of being a woman to have more challenges, owing not only to society's viewing women as a social category but also as a felt sense of self, a culturally conditioned or constructed subjective identity. "Social identity" refers to the common identification with a collectivity or social category that creates a common culture among participants concerned. According to social identity theory, an important component of the self-concept is derived from memberships in social groups and categories; this is demonstrated by group processes and how inter-group relationships impact significantly on individuals' self perception and behaviors. The groups people belong to therefore provide members with the definition of who they are and how they should behave within their social sphere.
Categorizing males and females into social roles creates a problem, because individuals feel they have to be at one end of a linear spectrum and must identify themselves as man or woman, rather than being allowed to choose a section in between. Globally, communities interpret biological differences between men and women to create a set of social expectations that define the behaviors that are "appropriate" for men and women and determine women’s and men’s different access to rights, resources, power in society and health behaviors. Although the specific nature and degree of these differences vary from one society to the next, they still tend to typically favor men, creating an imbalance in power and gender inequalities within most societies. Many cultures have different systems of norms and beliefs based on gender, but there is no universal standard to a masculine or feminine role across all cultures. Social roles of men and women in relation to each other is based on the cultural norms of that society, which lead to the creation of gender systems. The gender system is the basis of social patterns in many societies, which include the separation of sexes, and the primacy of masculine norms.
Philosopher Michel Foucault said that as sexual subjects, humans are the object of power, which is not an institution or structure, rather it is a signifier or name attributed to "complex strategical situation". Because of this, "power" is what determines individual attributes, behaviors, etc. and people are a part of an ontologically and epistemologically constructed set of names and labels. Such as, being female characterizes one as a woman, and being a woman signifies one as weak, emotional, and irrational, and is incapable of actions attributed to a "man". Butler said that gender and sex are more like verbs than nouns. She reasoned that her actions are limited because she is female. "I am not permitted to construct my gender and sex willy-nilly," she said. "[This] is so because gender is politically and therefore socially controlled. Rather than 'woman' being something one is, it is something one does." More recent criticisms of Judith Butler's theories critique her writing for reinforcing the very conventional dichotomies of gender.
Social assignment and gender fluidity.
According to gender theorist Kate Bornstein, gender can have ambiguity and fluidity. There are two contrasting ideas regarding the definition of gender, and the intersection of both of them is definable as below:
The World Health Organization defines gender as the result of socially constructed ideas about the behavior, actions, and roles a particular sex performs. The beliefs, values and attitude taken up and exhibited by them is as per the agreeable norms of the society and the personal opinions of the person is not taken into the primary consideration of assignment of gender and imposition of gender roles as per the assigned gender. Intersections and crossing of the prescribed boundaries have no place in the arena of the social construct of the term "gender".
The assignment of gender involves taking into account the physiological and biological attributes assigned by nature followed by the imposition of the socially constructed conduct. The social label of being classified into one or the other sex is necessary for the medical stamp on birth certificates. "Gender" is a term used to exemplify the attributes that a society or culture constitutes as "masculine" or "feminine". Although a person's sex as male or female stands as a biological fact that is identical in any culture, what that specific sex means in reference to a person's gender role as a woman or a man in society varies cross culturally according to what things are considered to be masculine or feminine. The cultural traits typically coupled to a particular sex finalize the assignment of gender and the biological differences which play a role in classifying either sex as interchangeable with the definition of gender within the social context.
In this context, the socially constructed rules are at a cross road with the assignment of a particular gender to a person. Gender ambiguity deals with having the freedom to choose, manipulate and create a personal niche within any defined socially constructed code of conduct while gender fluidity is outlawing all the rules of cultural gender assignment. It does not accept the prevalence of the two rigidly defined genders "man" and "woman" and believes in freedom to choose any kind of gender with no rules, no defined boundaries and no fulfilling of expectations associated with any particular gender.
Both these definitions are facing opposite directions with their own defined set of rules and criteria on which the said systems are based.
Social categories.
Sexologist John Money coined the term "gender role" in 1955. The term "gender role" is defined as the actions or responses that may reveal their status as boy, man, girl or woman, respectively. It includes, but is not restricted to, sexuality in the sense of eroticism.
Elements surrounding gender roles include clothing, speech patterns, movement, occupations, and other factors not limited to biological sex. Because social aspects of gender can normally be presumed to be the ones of interest in sociology and closely related disciplines, "gender role" is often abbreviated to "gender" in their literature.
Most societies have only two distinct, broad classes of gender roles, masculine and feminine, that correspond with the biological sexes of male and female. When a baby is born, society allocates the child to one sex or the other, on the basis of what their genitals resemble. However, some societies explicitly incorporate people who adopt the gender role opposite to their biological sex; for example, the two-spirit people of some indigenous American peoples. Other societies include well-developed roles that are explicitly considered more or less distinct from archetypal female and male roles in those societies. In the language of the sociology of gender, they comprise a third gender,
more or less distinct from biological sex (sometimes the basis for the role does include intersexuality or incorporates eunuchs).
One such gender role is that adopted by the hijras of India and Pakistan. Another example may be the Muxe (pronounced ]), found in the state of Oaxaca, in southern Mexico, "beyond gay and straight."
The Bugis people of Sulawesi, Indonesia have a tradition that incorporates all the features above.
Joan Roughgarden argues that some non-human animal species also have more than two genders, in that there might be multiple templates for behavior available to individual organisms with a given biological sex.
In July 2012 Gopi Shankar, a Gender activist and a student from The American College in Madurai coined the regional terms for "genderqueer" people in Tamil, Gopi said apart from male and female, there are more than 20 types of genders, such as "transwoman," "transmen," "androgynous," "pangender," "trigender,", etc., and ancient India refers it as Trithiya prakirthi. "
Measurement of gender identity.
Early gender identity research hypothesized a single bipolar dimension of masculinity-femininity—that is masculinity and femininity were opposites on one continuum. As societal stereotypes changed, however, assumptions of the unidimensional model were challenged. This led to the development of a two-dimensional gender identity model, in which masculinity and femininity were conceptualized as two separate, orthogonal dimensions, coexisting in varying degrees within an individual. This conceptualization on femininity and masculinity remains the accepted standard today.
Two instruments incorporating the multidimensional of masculinity and femininity have dominated gender identity research: The Bem Sex Role Inventory (BSRI) and the Personal Attributes Questionnaire (PAQ). Both instruments categorize individuals as either being sex typed (males report themselves as identifying primarily with masculine traits, females report themselves as identifying primarily with feminine traits), cross sex-typed (males report themselves as identifying primarily with feminine traits, females report themselves as identifying primarily with masculine traits), androgynous (either males or females who report themselves as high on both masculine and feminine traits) or undifferentiated (either males or females who report themselves as low on both masculine and feminine traits). Twenge (1997) noted that, although men are generally more masculine than women and women generally more feminine than men, the association between biological sex and masculinity/femininity is waning.
Feminism theory and gender studies.
Biologist and feminist academic Anne Fausto-Sterling rejects the discourse of biological versus social determinism and advocates a deeper analysis of how interactions between the biological being and the social environment influence individuals' capacities. The philosopher and feminist Simone de Beauvoir applied existentialism to women's experience of life: "One is not born a woman, one becomes one." In context, this is a philosophical statement. However, it may be analyzed in terms of biology—a girl must pass puberty to become a woman—and sociology, as a great deal of mature relating in social contexts is learned rather than instinctive.
Within feminist theory, terminology for gender issues developed over the 1970s. In the 1974 edition of "Masculine/Feminine or Human", the author uses "innate gender" and "learned sex roles", but in the 1978 edition, the use of "sex" and "gender" is reversed.
By 1980, most feminist writings had agreed on using "gender" only for socioculturally adapted traits.
In gender studies the term "gender" refers to proposed social and cultural constructions of masculinities and femininities. In this context, "gender" explicitly excludes reference to biological differences, to focus on cultural differences. This emerged from a number of different areas: in sociology during the 1950s; from the theories of the psychoanalyst Jacques Lacan; and in the work of French psychoanalysts like Julia Kristeva, Luce Irigaray, and American feminists such as Judith Butler. Those who followed Butler came to regard gender roles as a practice, sometimes referred to as "performative".
Charles E. Hurst states that some people think sex will, "...automatically determine one's gender demeanor and role (social) as well as one's sexual orientation (sexual attractions and behavior). Gender sociologists believe that people have cultural origins and habits for dealing with gender. For example, Michael Schwalbe believes that humans must be taught how to act appropriately in their designated gender to fill the role properly, and that the way people behave as masculine or feminine interacts with social expectations. Schwalbe comments that humans "are the results of many people embracing and acting on similar ideas". People do this through everything from clothing and hairstyle to relationship and employment choices. Schwalbe believes that these distinctions are important, because society wants to identify and categorize people as soon as we see them. They need to place people into distinct categories to know how we should feel about them.
Hurst comments that in a society where we present our genders so distinctly, there can often be severe consequences for breaking these cultural norms. Many of these consequences are rooted in discrimination based on sexual orientation. Gays and lesbians are often discriminated against in our legal system because of societal prejudices. Hurst describes how this discrimination works against people for breaking gender norms, no matter what their sexual orientation is. He says that "courts often confuse sex, gender, and sexual orientation, and confuse them in a way that results in denying the rights not only of gays and lesbians, but also of those who do not present themselves or act in a manner traditionally expected of their sex". This prejudice plays out in our legal system when a person is judged differently because they do not present themselves as the "correct" gender.
Andrea Dworkin stated her "commitment to destroying male dominance and gender itself" while stating her belief in radical feminism.
Critiques of feminist theory by Warren Farrell have given broader consideration to findings from a ten-year study of courtship by Buss. Both perspectives on gendering are integrated in "Attraction Theory", a theoretical framework developed by Dr Rory Ridley-Duff illustrating how courtship and parenting obligations (rather than male dominance) act as a generative mechanism that produces and reproduces a range of gender identities.
Political scientist Mary Hawkesworth addresses gender and feminist theory, stating that since the 1970s the concept of gender has transformed and been used in significantly different ways within feminist scholarship. She notes that a transition occurred when several feminist scholars, such as Sandra Harding and Joan Scott, began to conceive of gender "as an analytic category within which humans think about and organize their social activity". Feminist scholars in Political Science began employing gender as an analytical category, which highlighted "social and political relations neglected by mainstream accounts". However, Hawkesworth states "feminist political science has not become a dominant paradigm within the discipline".
American political scientist Karen Beckwith addresses the concept of gender within political science arguing that a "common language of gender" exists and that it must be explicitly articulated in order to build upon it within the political science discipline. Beckwith describes two ways in which the political scientist may employ 'gender' when conducting empirical research: "gender as a category and as a process." Employing gender as a category allows for political scientists "to delineate specific contexts where behaviours, actions, attitudes and preferences considered masculine or feminine result in particular" political outcomes. It may also demonstrate how gender differences, not necessarily corresponding precisely with sex, may "constrain or facilitate political" actors. Gender as a process has two central manifestations in political science research, firstly in determining "the differential effects of structures and policies upon men and women," and secondly, the ways in which masculine and feminine political actors "actively work to produce favorable gendered outcomes".
With regard to gender studies, Jacquetta Newman states that although sex is determined biologically, the ways in which people express gender is not. Gendering is a socially constructed process based on culture, though often cultural expectations around women and men have a direct relationship to their biology. Because of this, Newman argues, many privilege sex as being a cause of oppression and ignore other issues like race, ability, poverty, etc. Current gender studies classes seek to move away from that and examine the intersectionality of these factors in determining people's lives. She also points out that other non-Western cultures do not necessarily have the same views of gender and gender roles. Newman also debates the meaning of equality, which is often considered the goal of feminism; she believes that "equality" is a problematic term because it can mean many different things, such as people being treated identically, differently, or fairly based on their gender. Newman believes this is problematic because there is no unified definition as to what equality means or looks like, and that this can be significantly important in areas like public policy.
Social construction of sex hypotheses.
Sociologists generally regard gender as a social construct, and various researchers, including many feminists, consider sex to only be a matter of biology and something that is not about social or cultural construction. For instance, sexologist John Money suggests the distinction between biological sex and gender as a role. Moreover, Ann Oakley, a professor of sociology and social policy, says "the constancy of sex must be admitted, but so also must the variability of gender." The World Health Organization states, "'[s]ex' refers to the biological and physiological characteristics that define men and women," and "'[g]ender' refers to the socially constructed roles, behaviours, activities, and attributes that a given society considers appropriate for men and women." Thus, sex is regarded as a category studied in biology (natural sciences), while gender is studied in humanities and social sciences. Lynda Birke, a feminist biologist, maintains "'biology' is not seen as something which might change." Therefore, it is stated that sex is something that does not change, while gender can change according to social structure.
However, there are scholars who argue that sex is also socially constructed. For example, Judith Butler, a professor of rhetoric and comparative literature, states that "perhaps this construct called 'sex' is as culturally constructed as gender; indeed, perhaps it was always already gender, with the consequence that the distinction between sex and gender turns out to be no distinction at all."
She continues:It would make no sense, then, to define gender as the cultural interpretation of sex, if sex is itself a gender-centered category. Gender should not be conceived merely as the cultural inscription of meaning based on a given sex (a juridical conception); gender must also designate the very apparatus of production whereby the sexes themselves are established. [...] This production of sex as the pre-discursive should be understood as the effect of the apparatus of cultural construction designated by gender.
Butler argues that "bodies only appear, only endure, only live within the productive constraints of certain highly gendered regulatory schemas," and sex is "no longer as a bodily given on which the construct of gender is artificially imposed, but as a cultural norm which governs the materialization of bodies." Marria Lugones states that, among the Yoruba people, there was no concept of gender and no gender system at all before colonialism. She argues that colonial powers used a gender system as a tool for domination and fundamentally changing social relations among the indigenous.
With regard to history, Linda Nicholson, a professor of history and women's studies, says that the notion of human bodies being separated into two sexes is not historically consistent. She argues that male genitals and female genitals were considered inherently the same in Western society until the 18th century. At that time, female genitals were regarded as incomplete male genitals, and the difference between the two was conceived as a matter of degree. In other words, there was a gradation of physical forms, or a spectrum. Therefore, the current perspective toward sex, which is to consider women and men and their typical genitalia as the only possible natural options, came into existence through historical, not biological roots.
In addition, drawing from the empirical research of intersex children, Anne Fausto-Sterling, a professor of biology and gender studies, describes how the doctors address the issues of intersexuality. She starts her argument with an example of the birth of an intersexual individual and maintains "[o]ur conceptions of the nature of gender difference shape, even as they reflect, the ways we structure our social system and polity; they also shape and reflect our understanding of our physical bodies." Then she adds how gender assumptions affects the scientific study of sex by presenting the research of intersexuals by John Money et al., and she concludes that "they never questioned the fundamental assumption that there are only two sexes, because their goal in studying intersexuals was to find out more about 'normal' development." She also mentions the language the doctors use when they talk with the parents of the intersexuals. After describing how the doctors inform parents about the intersexuality, she asserts that because the doctors believe that the intersexuals are actually male or female, they tell the parents of the intersexuals that it will take a little bit more time for the doctors to determine whether the infant is a boy or a girl. That is to say, the doctors' behavior is formulated by the cultural gender assumption that there are only two sexes. Lastly, she maintains that the differences in the ways in which the medical professionals in different regions treat intersexual people also give us a good example of how sex is socially constructed. In her book, titled "Sexing the body: gender politics and the construction of sexuality", she introduces the following example: A group of physicians from Saudi Arabia recently reported on several cases of XX intersex children with congenital adrenal hyperplasia (CAH), a genetically inherited malfunction of the enzymes that aid in making steroid hormones. [...] In the United States and Europe, such children, because they have the potential to bear children later in life, are usually raised as girls. Saudi doctors trained in this European tradition recommended such a course of action to the Saudi parents of CAH XX children. A number of parents, however, refused to accept the recommendation that their child, initially identified as a son, be raised instead as a daughter. Nor would they accept feminizing surgery for their child. [...] This was essentially an expression of local community attitudes with [...] the preference for male offspring.
Thus it may be said that determining the sex of children is actually a cultural act, and the sex of children is in fact socially constructed. Therefore, it is possible that although sex seems fixed and only related to biology, it may be actually deeply related to historical and social factors as well as biology and other natural sciences.
Another work of Ann Fausto-Sterling’s in which she discusses gender is "The Five Sexes: Why Male and Female Are Not Enough." In this article, Fausto-Sterling states that Western culture has only two sexes and that even their language restricts the presence of more than two sexes. She argues that instead of having a binomial nomenclature for organizing humans into two distinct sexes (male and female), there are at least five sexes in the broad spectrum of gender. These five sexes include male, female, hermaphrodite, female pseudohermaphrodites (individuals who have ovaries and some male genitalia but lack testes), and male pseudohermaphrodites (individuals who have testes and some female genitalia but lack ovaries). Fausto-Sterling additionally adds that in the category of hermaphrodites, there are additional degrees and levels in which the genitalia are developed; this means that there may be more intersexes that exist in this continuum of gender.
Fausto-Sterling argues that sex has been gradually institutionally disciplined into a binary system through medical advances. She brings up multiple instances where gender in history was not split into strictly male or female, Fausto-Sterling mentioned that by the end of the Middle Age, intersex individuals were forced to pick a side in the binary gender code and to adhere by it. She then adds on that "hermaphrodites have unruly bodies" and they need to fit into society's definition of gender. Thus, modern-day parents have been urged by medical doctors to decide the sex for their hermaphroditic child immediately after childbirth. She emphasizes that the role of the medical community is that of an institutionalized discipline on society that there can only be two sexes: male and female and only the two listed are considered “normal." Lastly, Fausto-Sterling argues that modern laws require humans to be labelled either as male or female and that "ironically, a more sophisticated knowledge of the complexity of sexual systems has led to the repression of such intricacy." She mentions this quote to inform the prevailing thought that hermaphrodites, without medical intervention, are assumed to live a life full of psychological pain when in fact, there is no evidence in which that is the case. She finishes up her argument asking what would happen if society started accepting intersex individuals.
In her article "Undoing Gender", Francine Deutsch gives her response to the article "Doing gender", written by Candace West and Don Zimmerman. She states that people have issues doing the roles that were meant for the other sex. Deutsch adds that many activities have been constructed to be either a male or female role in society. She additionally gives examples of how women in sports have to negotiate with training their bodies needed for the sport but at the same time retain parts of their femininity. For example, in the sport of soccer, Deutsch describes that female athletes are expected to have the same muscular strength but at the same time maintain their lean bodies; she points out that the women in elite soccer groups are “doing gender.” She also mentions that in the midst of these actions, people purposely enact a heterosexual position (usually masculinity) when individuals, especially males, feel that their masculinity has been questioned. Deutsche later turns to reflect on current research studies that study the process of "doing gender" and she mentions a few ways to better the research strategies. In Deutsch’s view, in future researches, researchers should focus on taking into account of how gender inequalities exist in various cultures and time which shifts into her assertion that gender differences need to be reduced. She finishes her article with stating that instead of “doing gender,” social interactions should employ “undoing gender” to minimize gender inequality.
The article "Adolescent Gender-Role Identity and Mental Health: Gender Intensification Revisited" focuses on the work of Heather A. Priess, Sara M. Lindberg, and Janet Shibley Hyde on whether or not girls and boys diverge in their gender identities during adolescent years. The researchers based their work on ideas previously mentioned by Hill and Lynch in their gender intensification hypothesis in that signals and messages from parents determine and affect their children’s gender role identities. This hypothesis argues that parents affect their children's gender role identities and that different interactions spent with either parents will affect gender intensification. Priess and among other’s study did not support the hypothesis of Hill and Lynch which stated "that as adolescents experience these and other socializing influences, they will become more stereotypical in their gender-role identities and gendered attitudes and behaviors." However, the researchers did state that perhaps the hypothesis Hill and Lynch proposed was true in the past but is not true now due to changes in the population of teens in respect to their gender-role identities.
Authors of "Unpacking the Gender System: A Theoretical Perspective on Gender Belief’s and Social Relations", Cecilia Ridgeway and Shelley Correll, argue that gender is more than an identity or role but is something that is institutionalized through "social relational contexts." Ridgeway and Correll define "social relational contexts" as "any situation in which individuals define themselves in relation to others in order to act." They also point out that in addition to social relational contexts, cultural beliefs plays a role in the gender system. The coauthors argue that daily people are forced to acknowledge and interact with others in ways that are related to gender. Every day, individuals are interacting with each other and comply with society's set standard of hegemonic beliefs, which includes gender roles. They state that society's hegemonic cultural beliefs sets the rules which in turn create the setting for which social relational contexts are to take place. Ridgeway and Correll then shift their topic towards sex categorization. The authors define sex categorization as "the sociocognitive process by which we label another as male or female." They describe how individuals automatically categorize each other into the pre-existing, simple binary category of either male or female. However, they do point out that a person’s gender is not the only identity attached to them but it is one of the main ones. They also add that gender can be furthermore described as being a background identity that is always present in a person regardless of their race or position. They end the article with stating that gender is not only determined by family members, but reinforced by society in ways that is hard to avoid on a daily basis. They additionally mention that many actions and thoughts of people are dictated by societal gender relations and expectations.
Biological factors and views.
The biology of gender became the subject of an expanding number of studies over the course of the late 20th century. One of the earliest areas of interest was what is now called gender identity disorder (GID). Studies in this, and related areas, inform the following summary of the subject by John Money. He stated:
The term "gender role" appeared in print first in 1955. The term "gender identity" was used in a press release, November 21, 1966, to announce the new clinic for transsexuals at The Johns Hopkins Hospital. It was disseminated in the media worldwide, and soon entered the vernacular. The definitions of gender and gender identity vary on a doctrinal basis. In popularized and scientifically debased usage, sex is what you are biologically; gender is what you become socially; gender identity is your own sense or conviction of maleness or femaleness; and gender role is the cultural stereotype of what is masculine and feminine. Causality with respect to gender identity disorder is sub-divisible into genetic, prenatal hormonal, postnatal social, and post-pubertal hormonal determinants, but there is, as yet, no comprehensive and detailed theory of causality. Gender coding in the brain is bipolar. In gender identity disorder, there is discordance between the natal sex of one's external genitalia and the brain coding of one's gender as masculine or feminine.
Money refers to attempts to distinguish a difference between biological sex and social gender as "scientifically debased", because of our increased knowledge of a continuum of dimorphic features (Money's word is "dipolar") that link biological and behavioral differences. These extend from the exclusively biological "genetic" and "prenatal hormonal" differences between men and women, to "postnatal" features, some of which are social, but others have been shown to result from "post-pubertal hormonal" effects.
Although causation from the biological—genetic and hormonal—to the behavioral has been broadly demonstrated and accepted, Money is careful to also note that understanding of the causal chains from biology to behavior in sex and gender issues is very far from complete. For example, the existence of a "gay gene" has not been proven, but such a gene remains an acknowledged possibility.
There are studies concerning women who have a condition called congenital adrenal hyperplasia, which leads to the overproduction of the masculine sex hormone, androgen. These women usually have ordinary female appearances (though nearly all girls with congenital adrenal hyperplasia (CAH) have corrective surgery performed on their genitals). However, despite taking hormone-balancing medication given to them at birth, these females are statistically more likely to be interested in activities traditionally linked to males than female activities. Psychology professor and CAH researcher Dr. Sheri Berenbaum attributes these differences to an exposure of higher levels of male sex hormones in utero.
Sexual reproduction.
Sexual differentiation demands the fusion of gametes that are morphologically different.—Cyril Dean Darlington, "Recent Advances in Cytology", 1937.
Sexual reproduction is a common method of producing a new individual within various species. In sexually reproducing species, individuals produce special kinds of cells (called "gametes") whose function is specifically to fuse with one "unlike" gamete and thereby to form a new individual. This fusion of two unlike gametes is called fertilization. By convention, where one type of gamete cell is physically larger than the other, it is associated with female sex. Thus an individual that produces exclusively large gametes (ova in humans) is called "female", and one that produces exclusively small gametes (spermatozoa in humans) is called "male".
An individual that produces both types of gametes is called "hermaphrodite" (a name applicable also to people with one testis and one ovary). In some species hermaphrodites can self-fertilize (see Selfing), in others they can achieve fertilization with females, males or both. Some species, like the Japanese Ash, "Fraxinus lanuginosa", only have males and hermaphrodites, a rare reproductive system called "androdioecy". Gynodioecy is also found in several species. Human hermaphrodites are typically, but not always, infertile.
What is considered defining of sexual reproduction is the "difference" between the gametes and the "binary" nature of fertilization. Multiplicity of gamete "types" within a species would still be considered a form of sexual reproduction. However, of more than 1.5 million living species,
recorded up to about the year 2000, "no third sex cell—and so no third sex—has appeared in multi-cellular animals." Why sexual reproduction has an exclusively binary gamete system is not yet known. A few rare species that push the boundaries of the definitions are the subject of active research for light they may shed on the mechanisms of the evolution of sex. For example, the most toxic insect, the harvester ant "Pogonomyrmex", has two kinds of female and two kinds of male. One hypothesis is that the species is a hybrid, evolved from two closely related preceding species.
Fossil records indicate that sexual reproduction has been occurring for at least one billion years.
However, the reason for the initial evolution of sex, and the reason it has survived to the present are still matters of debate, there are many plausible theories. It appears that the ability to reproduce sexually has evolved independently in various species on many occasions. There are cases where it has also been lost, notably among the Fungi Imperfecti.
The blacktip shark ("Carcharhinus limbatus"), flatworm ("Dugesia tigrina") and some other species can reproduce either sexually or asexually depending on various conditions.
Gender taxonomy.
The following systematic list gender taxonomy illustrates the kinds of diversity that have been studied and reported in medical literature. It is placed in roughly chronological order of biological and social development in the human life cycle. The earlier stages are more purely biological and the latter are more dominantly social. Causation is known to operate from chromosome to gonads, and from gonads to hormones. It is also significant from brain structure to gender identity (see Money quote above). Brain structure and processing (biological) that may explain erotic preference (social), however, is an area of ongoing research. Terminology in some areas changes quite rapidly as knowledge grows.
Sexual/gender dimorphism.
Although sexual reproduction is "defined" at the cellular level, key features of sexual reproduction operate "within" the structures of the gamete cells themselves. Notably, gametes carry very long molecules called DNA that the biological processes of reproduction can "read" like a book of instructions. In fact, there are typically many of these "books", called "chromosomes". Human gametes usually have 23 chromosomes, 22 of which are common to both sexes. The final chromosomes in the two human gametes are called "sex" chromosomes because of their role in sex determination. Ova always have the same sex chromosome, labelled "X". About half of spermatozoa also have this same X chromosome, the rest have a Y-chromosome. At fertilization the gametes fuse to form a cell, usually with 46 chromosomes, and either XX female or XY male, depending on whether the sperm carried an X or a Y chromosome. Some of the other possibilities are listed above.
The human XY system is not the only sex determination system. Birds typically have a reverse, ZW system—males are ZZ and females ZW. Whether male or female birds influence the sex of offspring is not known for all species. Several species of butterfly are known to have female parent sex determination.
The platypus has a complex hybrid system, the male has ten sex chromosomes, half X and half Y.
Gender studies.
Gender studies is a field of interdisciplinary study and academic field devoted to gender, gender identity and gendered representation as central categories of analysis. This field includes Women's studies (concerning women, feminity, their gender roles and politics, and feminism), Men's studies (concerning men, masculinity, their gender roles, and politics), and LGBT studies.
Sometimes Gender studies is offered together with Study of Sexuality.
These disciplines study gender and sexuality in the fields of literature and language, history, political science, sociology, anthropology, cinema and media studies, human development, law, and medicine.
It also analyses race, ethnicity, location, nationality, and disability.
General studies.
Genes.
Chromosomes were likened to books (above), also like books they have been studied at more detailed levels. They contain "sentences" called "genes". In fact, many of these sentences are common to multiple species. Sometimes they are organized in the same order, other times they have been "edited"—deleted, copied, changed, moved, even relocated to another "book", as species evolve. Genes are a particularly important part of understanding biological processes because they are directly associated with observable objects, outside chromosomes, called "proteins", whose influence on cell chemistry can be measured. In some cases genes can also be directly associated with differences clear to the naked eye, like eye-color itself. Some of these differences are sex specific, like hairy ears. The "hairy ear" gene might be found on the Y chromosome, which explains why only men tend to have hairy ears. However, sex-limited genes on "any" chromosome can be expressed and "say", for example, ""if" you are in a male body do X, otherwise do not." The same principle explains why chimpanzees and humans are distinct, despite sharing nearly all their genes.
The study of genetics is particularly inter-disciplinary. It is relevant to almost every biological science. It is investigated in detail by molecular level sciences, and itself contributes details to high level abstractions like evolutionary theory.
Brain.
"It is well established that men have a larger cerebrum than women by about 8–10% (Filipek et al., 1994; Nopoulos et al.,
2000; Passe et al., 1997a,b; Rabinowicz et al., 1999; Witelson et al., 1995)."
However, what is functionally relevant are differences in composition and "wiring". Richard J. Haier and colleagues at the universities of New Mexico and California (Irvine) found, using brain mapping, that men have more grey matter related to general intelligence than women, and women have more white matter related to intelligence than men – the ratio between grey and white matter is 4% higher for men than women.
Grey matter is used for information processing, while white matter consists of the connections between processing centers. Other differences are measurable but less pronounced.
Most of these differences are produced by hormonal activity, ultimately derived from the Y chromosome and sexual differentiation. However, differences that arise directly from gene activity have also been observed.
A sexual dimorphism in levels of expression in brain tissue was observed by quantitative real-time PCR, with females presenting an up to 2-fold excess in the abundance of PCDH11X transcripts. We relate these findings to sexually dimorphic traits in the human brain. Interestingly, PCDH11X/Y gene pair is unique to "Homo sapiens", since the X-linked gene was transposed to the Y chromosome after the human–chimpanzee lineages split.—
It has also been demonstrated that brain processing responds to the external environment. Learning, both of ideas and behaviors, appears to be coded in brain processes. It also appears that in several simplified cases this coding operates differently, but in some ways equivalently, in the brains of men and women. For example, both men and women learn and use language; however, bio-chemically, they appear to process it differently. Differences in female and male use of language are likely reflections "both" of biological preferences and aptitudes, "and" of learned patterns.
Two of the main fields that study brain structure, biological (and other) causes and behavioral (and other) results are brain neurology and biological psychology. Cognitive science is another important discipline in the field of brain research.
Society and behaviors.
Many of the more complicated human behaviors are influenced by both innate factors and by environmental ones, which include everything from genes, gene expression, and body chemistry, through diet and social pressures. A large area of research in behavioral psychology collates evidence in an effort to discover correlations between behavior and various possible antecedents such as genetics, gene regulation, access to food and vitamins, culture, gender, hormones, physical and social development, and physical and social environments.
A core research area within sociology is the way human behavior operates on "itself", in other words, how the behavior of one group or individual influences the behavior of other groups or individuals. Starting in the late 20th century, the feminist movement has contributed extensive study of gender and theories about it, notably within sociology but not restricted to it.
Social theorists have sought to determine the specific nature of gender in relation to biological sex and sexuality, with the result being that culturally established gender and sex have become interchangeable identifications that signify the allocation of a specific 'biological' sex within a categorical gender. The second wave feminist view that gender is socially constructed and hegemonic in all societies, remains current in some literary theoretical circles, Kira Hall and Mary Bucholtz publishing new perspectives as recently as 2008.
Contemporary socialisation theory proposes the notion that when a child is first born it has a biological sex but no social gender. As the child grows, "...society provides a string of prescriptions, templates, or models of behaviors appropriate to the one sex or the other," which socialises the child into belonging to a culturally specific gender. There is huge incentive for a child to concede to their socialisation with gender shaping the individual’s opportunities for education, work, family, sexuality, reproduction, authority, and to make an impact on the production of culture and knowledge. Adults who do not perform these ascribed roles are perceived from this perspective as deviant and improperly socialized.
Some believe society is constructed in a way that splits gender into a dichotomy via social organisations that constantly invent and reproduce cultural images of gender. Joan Acker believes gendering occurs in at least five different interacting social processes:
Looking at gender through a Foucauldian lens, gender is transfigured into a vehicle for the social division of power. Gender difference is merely a construct of society used to enforce the distinctions made between what is assumed to be female and male, and allow for the domination of masculinity over femininity through the attribution of specific gender-related characteristics. "The idea that men and women are more different from one another than either is from anything else, must come from something other than nature… far from being an expression of natural differences, exclusive gender identity is the suppression of natural similarities."
Gender conventions play a large role in attributing masculine and feminine characteristics to a fundamental biological sex. Socio-cultural codes and conventions, the rules by which society functions, and which are both a creation of society as well as a constituting element of it, determine the allocation of these specific traits to the sexes. These traits provide the foundations for the creation of hegemonic gender difference. It follows then, that gender can be assumed as the acquisition and internalisation of social norms. Individuals are therefore socialized through their receipt of society’s expectations of 'acceptable' gender attributes that are flaunted within institutions such as the family, the state and the media. Such a notion of 'gender' then becomes naturalized into a person’s sense of self or identity, effectively imposing a gendered social category upon a sexed body.
The conception that people are gendered rather than sexed also coincides with Judith Butler’s theories of gender performativity. Butler argues that gender is not an expression of what one is, but rather something that one does. It follows then, that if gender is acted out in a repetitive manner it is in fact re-creating and effectively embedding itself within the social consciousness. Contemporary sociological reference to male and female gender roles typically uses "masculinities" and "femininities" in the plural rather than singular, suggesting diversity both within cultures as well as across them.
The difference between the sociological and popular definitions of gender involve a different dichotomy and focus. For example, the sociological approach to "gender" (social roles: female versus male) focuses on the difference in (economic/power) position between a male CEO (disregarding the fact that he is heterosexual or homosexual) to female workers in his employ (disregarding whether they are straight or gay). However the popular sexual self-conception approach (self-conception: gay versus straight) focuses on the different self-conceptions and social conceptions of those who are gay/straight, in comparison with those who are straight (disregarding what might be vastly differing economic and power positions between female and male groups in each category). There is then, in relation to definition of and approaches to "gender", a tension between historic feminist sociology and contemporary homosexual sociology.
Legal status.
General.
A person's sex as male or female has legal significance—sex is indicated on government documents, and laws provide differently for men and women. Many pension systems have different retirement ages for men or women. Marriage is usually only available to opposite-sex couples; in some countries, there are same-sex marriage laws.
The question then arises as to what legally determines whether someone is female or male. In most cases this can appear obvious, but the matter is complicated for intersex or transgender people. Different jurisdictions have adopted different answers to this question. Almost all countries permit changes of legal gender status in cases of intersexualism, when the gender assignment made at birth is determined upon further investigation to be biologically inaccurate—technically, however, this is not a change of status "per se". Rather, it is recognition of a status deemed to exist but unknown from birth. Increasingly, jurisdictions also provide a procedure for changes of legal gender for transgender people.
Gender assignment, when there are indications that genital sex might not be decisive in a particular case, is normally not defined by a single definition, but by a combination of conditions, including chromosomes and gonads. Thus, for example, in many jurisdictions a person with XY chromosomes but female gonads could be recognized as female at birth.
The ability to change legal gender for transgender people in particular has given rise to the phenomena in some jurisdictions of the same person having different genders for the purposes of different areas of the law. For example, in Australia prior to the Re Kevin decisions, transsexual people could be recognized as having the genders they identified with under many areas of the law, including social security law, but not for the law of marriage. Thus, for a period, it was possible for the same person to have two different genders under Australian law.
It is also possible in federal systems for the same person to have one gender under state law and a different gender under federal law.
The first person known to be legally of indeterminate gender (that is, neither man or woman in legal terms) is Alex MacFarlane, from Australia, whose status was reported in January 2003.
Australian government policy on indeterminate gender.
Alex MacFarlane was reported as receiving a passport with an 'X' sex descriptor in early 2003. This was stated by the West Australian to be on the basis of a challenge by MacFarlane, using an indeterminate birth certificate issued by the State of Victoria.
Australian government policy between 2003 and 2011 was to issue passports with an 'X' marker only to people who could "present a birth certificate that notes their sex as indeterminate".
In 2011, the Australian Passport Office introduced new guidelines for issuing of passports with a new gender, and broadened availability of an X descriptor to all individuals with certified "indeterminate" sex or gender, issued by a medical doctor. The revised policy stated that "sex reassignment surgery is not a prerequisite to issue a passport in a new gender. Birth or citizenship certificates do not need to be amended."
Australian Commonwealth guidelines on the recognition of sex and gender, passed the Sex Discrimination Amendment Act on 25 June 2013 without a vote. It defined the 'X' as a gender marker which would encompass "indeterminate/intersex/unspecified" categories. The policy extends the use an 'X' gender marker to any adult who chooses that option and can obtain a certifying letter from a doctor or psychologist, in all dealings with the Commonwealth government and its agencies. The option is being introduced over a three-year period. The guidelines also clarify that the federal government will now collect data on gender, rather than sex. The "gender identity" attribute protects trans people with non-binary identities. However, it does contain an exemption in record keeping. Federal guidelines on sex and gender recognition provide for the implementation of the "X" classification across federal departments and agencies.
Recognition of an intermediate gender is controversial even amongst intersex organisations, such as Organisation Intersex International Australia, who oppose such a classification of infants and children.
India's recognition of a third gender.
On 15 April 2014, India's Supreme Court declared transgender individuals as a third gender.These third gendered individuals are referred to as "hijras" within their community, which is made up of transsexuals, eunuchs, and cross-dressers. This decision would allow transgender individuals the opportunity to gain access to jobs and education, because a quota must now be fulfilled to incorporate these individuals. Those who identify with this third gender will also not be required to submit medical evidence of their sexuality to be accepted by the government as a member of that gender. Females who identify as males and males who identify as females will not be forced to undergo a gender reassignment surgery in order to identify with that specific gender. The Indian Supreme Court hopes that this modification will help the "hijras" in mainstream society as it gains widespread recognition.
Gender and society.
Gender and languages.
Natural languages often make gender distinctions. These may be of various kinds, more or less loosely associated by analogy with various actual or perceived differences between men and women. Some grammatical gender systems go beyond, or ignore, the masculine-feminine distinction.
Gender and science gender.
According to Londa Schiebinger, many have argued that science "should" have a gender. Additionally, "Sir Francis Bacon, the seventeenth-century English ideologue, called for the Royal Society of London to "raise a masculine philosophy". Karl Joel, 19th century German historian of philosophy, desired to return to "manly philosophy" and "applauded the arrival of a masculine epoch". On a different perspective, specifically, the female perspective, Mary Wollstonecraft, "in her efforts to create equality between the sexes, encouraged women to become "more masculine and respectable". On board of supporting the notion that science was masculine, was Evelyn Fox Keller, a feminist American physicist, "declared that science is "masculine," not only in the person of its practitioners but in its ethos and substance." Gender is the prime reason in which women feel estranged and left out of the realm of science. As for women who did participate within science, shadowed the masculine voice in their publications or utilized their male partners to carry out their own findings of science. Society played a leading and influential role into women in the public and private sphere. As more women entered the primatology sciences, in which they were to leave society behind and delve deep into adapting within the dark premises of the wild jungles where years passed by them. Once women were allowed within the public sphere of science, they became secretive about their pregnancies and "took trips for their work", to indulge in giving birth without experiencing the negative stigma of society. Some women disguised themselves in looking like men and experienced the outside societal judgments of working alongside a male scientists.
Gender and religion.
This topic includes internal and external religious issues such as gender of God and deities creation myths about human gender, roles and rights (for instance, leadership roles especially ordination of women, sex segregation, gender equality, marriage, abortion, homosexuality)
According to Kati Niemelä of the Church Research Institute, women are universally more religious than men. They believe that the difference in religiousity between genders is due to biological differences, for instance usually people seeking security in life are more religious, and as men are considered to be greater risk takers than women, they are less religious. Although religious fanaticism is more often seen in men than women.
In Taoism, yin and yang are considered feminine and masculine, respectively. The Taijitu and concept of the Zhou period reach into family and gender relations. Yin is female and yang is male. They fit together as two parts of a whole.
The male principle was equated with the sun: active, bright, and shining; the female principle corresponds to the moon: passive, shaded, and reflective. Male toughness was balanced by female gentleness, male action and initiative by female endurance and need for completion, and male leadership by female supportiveness.
In Judaism, God is traditionally described in the masculine, but in the mystical tradition of the Kabbalah, the Shekhinah represents the feminine aspect of God's essence. However, Judaism traditionally holds that God is completely non-corporeal, and thus neither male nor female. Conceptions of the gender of God notwithstanding, traditional Judaism places a strong emphasis on individuals following Judaism's traditional gender roles, though many modern denominations of Judaism strive for greater egalitarianism.
In Christianity, God is described in masculine terms and the Church has historically been described in feminine terms. On the other hand, Christian theology in many churches distinguishes between the masculine images used of God (Father, King, God the Son) and the reality they signify, which transcends gender, embodies all the virtues of both genders perfectly, which may be seen through the doctrine of Imago Dei. In the New Testament, Jesus at several times mentions with the masculine pronoun i.e. John 15:26 among other verses. Hence, the Father, the Son and the Holy Spirit (i.e. Trinity) are all mentioned with the masculine pronoun; though the exact meaning of the masculinity of the Christian triune God is contended.
In Hinduism
One of the several forms of the Hindu God Shiva, is Ardhanarishwar (literally half-female God). Here Shiva manifests himself so that the left half is Female and the right half is Male. The left represents Shakti (energy, power) in the form of Goddess Parvati (otherwise his consort) and the right half Shiva. Whereas Parvati is the cause of arousal of Kama (desires), Shiva is the killer. Shiva is pervaded by the power of Parvati and Parvati is pervaded by the power of Shiva.
While the stone images may seem to represent a half-male and half-female God, the true symbolic representation is of a being the whole of which is Shiva and the whole of which is Shakti at the same time. It is a 3-D representation of only shakti from one angle and only Shiva from the other. Shiva and Shakti are hence the same being representing a collective of Jnana (knowledge) and Kriya (activity).
Adi Shankaracharya, the founder of non-dualistic philosophy (Advaita–"not two") in Hindu thought says in his "Saundaryalahari"—"Shivah Shaktayaa yukto yadi bhavati shaktah prabhavitum na che devum devona khalu kushalah spanditam api" " i.e., It is only when Shiva is united with Shakti that He acquires the capability of becoming the Lord of the Universe. In the absence of Shakti, He is not even able to stir. In fact, the term "Shiva" originated from "Shva," which implies a dead body. It is only through his inherent shakti that Shiva realizes his true nature.
This mythology projects the inherent view in ancient Hinduism, that each human carries within himself both female and male components, which are forces rather than sexes, and it is the harmony between the creative and the annihilative, the strong and the soft, the proactive and the passive, that makes a true person. Such thought, leave alone entail gender equality, in fact obliterates any material distinction between the male and female altogether. This may explain why in ancient India we find evidence of homosexuality, bisexuality, androgyny, multiple sex partners and open representation of sexual pleasures in artworks like the Khajuraho temples, being accepted within prevalent social frameworks.—
Gender and poverty.
Gender inequality is most common in women dealing with poverty. Many women must shoulder all the responsibility of the household because they must take care of the family. Oftentimes this may include tasks such as tilling land, grinding grain, carrying water and cooking. Also, women are more likely to earn low incomes because of gender discrimination, as men are more likely to receive higher pay, have more opportunities, and have overall more political and social capital then women. Approximately 75% of world's women are unable to get authorize bank loans because they have unstable jobs. It shows that there are many women in the world's population but only a few represent world's wealth. In many countries, the financial sector largely neglects women even though they play an important role in the economy, as Nena Stoiljkovic pointed out in "D+C Development and Cooperation". In 1978 Diana M. Pearce coined the term feminization of poverty to describe the problem of women having higher rates of poverty. Women are more vulnerable to chronic poverty because of gender inequalities in the distribution of income, property ownership, credit, and control over earned income. Resource allocation is typically gender-biased within households, and continue on a higher level regarding state institutions.
Gender and Development (GAD) is a holistic approach to give aid to countries where gender inequality has a great effect of not improving the social and economic development. It is a program focused on the gender development of women to empower them and decrease the level of inequality between men and women.
General strain theory.
According to general strain theory, studies suggest that gender differences between individuals can lead to externalized anger that may result in violent outbursts. These violent actions related to gender inequality can be measured by comparing violent neighborhoods to non-violent neighborhoods. By noticing the independent variables (neighborhood violence) and the dependent variable (individual violence), it's possible to analyze gender roles. The strain in the general strain theory is the removal of a positive stimulus and or the introduction of a negative stimulus, which would create a negative effect (strain) within individual, which is either inner-directed (depression/guilt) or outer-directed (anger/frustration), which depends on whether the individual blames themselves or their environment. Studies reveal that even though males and females are equally likely to react to a strain with anger, the origin of the anger and their means of coping with it can vary drastically. Males are likely to put the blame on others for adversity and therefore externalize feelings of anger. Females typically internalize their angers and tend to blame themselves instead. Female internalized anger is accompanied by feelings of guilt, fear, anxiety and depression. Women view anger as a sign that they've somehow lost control, and thus worry that this anger may lead them to harm others and/or damage relationships. On the other end of the spectrum, men are less concerned with damaging relationships and more focused on using anger as a means of affirming their masculinity. According to the general strain theory, men would more likely engage in aggressive behavior directed towards others due to externalized anger whereas women would direct their anger towards themselves rather than others.
Gender and economic development.
Gender, and particularly the role of women is widely recognized as vitally important to international development issues. This often means a focus on gender-equality, ensuring participation, but includes an understanding of the different roles and expectation of the genders within the community.
In modern times, the study of gender and development has become a broad field that involves politicians, economists, and human rights activists. Gender and Development, unlike previous theories concerning women in development, includes a broader view of the effects of development on gender including economic, political, and social issues. The theory takes a holistic approach to development and its effects on women and recognizes the negative effects gender blind development policies have had on women. Prior to 1970, it was believed that development affected men and women in the same way and no gendered perspective existed for development studies. However, the 1970s saw a transformation in development theory that sought to incorporate women into existing development paradigms. When Ester Boserup published her book, "Woman’s Role in Economic Development", there was a realization that development affected men and women differently and there began to be more of a focus on women and development. Boserup argued that women were marginalized in the modernization process and practices of growth, development, and development policy threatened to actually make women worse off. 
Boserup’s work translated into the beginning of a larger discourse termed Women in Development (WID) coined by the Women’s Committee of the Washington DC Chapter of the Society for International Development, a network of female development professionals. The primary goal of WID was to include women into existing development initiatives, since it was argued that women were marginalized and excluded from the benefits of development. In so doing, the WID approach pointed out that the major problem to women’s unequal representation and participation were male biased and patriarchal development policies. In short, the WID approach blamed patriarchy, which did not consider women’s productive and reproductive work. In fact, women were tied to domestic work hence were almost invisible in development programs. The WID approach, however, began to gain criticism as ignoring how women’s economic marginalization was linked to the development model itself. Some feminists argued that the key concept for women and development should be subordination in the context of new capitalist forms of insecure and hierarchical job structures, rather than marginalization as WID approaches emphasized. The rise of criticism against the WID approach led to the emergence of a new theory, that of Women and Development (WAD).
However, just as WID had its critics, so did WAD. Many critics of WAD argued that it failed to sufficiently address the differential power relations between women and men, and tended to overemphasize women’s productive as opposed to reproductive roles. Also, rising criticism of the exclusion of men in WID and WAD led to a new theory termed Gender and Development (GAD). Drawing from insights developed in psychology, sociology, and gender studies, GAD theorists shifted from understanding women’s problems as based on their sex (i.e. their biological differences from men) to understanding them as based on gender – the social relations between women and men, their social construction, and how women have been systematically subordinated in this relationship. 
At their most fundamental, GAD perspectives link the social relations of production with the social relations of reproduction – exploring why and how women and men are assigned to different roles and responsibilities in society, how these dynamics are reflected in social, economic, and political theories and institutions, and how these relationships affect development policy effectiveness. According to proponents of GAD, women are cast not as passive recipients of development aid, but rather as active agents of change whose empowerment should be a central goal of development policy. In contemporary times, most literature and institutions that are concerned with women's role in development incorporate a GAD perspective, with the United Nations taking the lead of mainstreaming the GAD approach through its system and development policies.
Researchers at the Overseas Development Institute have highlighted that policy dialogue on the Millennium Development Goals needs to recognize that the gender dynamics of power, poverty, vulnerability and care link all the goals. 
The various United Nations international women’s conferences in Beijing, Mexico City, Copenhagen, and Nairobi, as well as the development of the Millennium Development Goals in 2000 have taken a GAD approach and holistic view of development. The United Nations Millennium Declaration signed at the United Nations Millennium Summit in 2000 including eight goals that were to be reached by 2015, and although it would be a difficult task to reach them, all of them could be monitored. The eight goals are:
The MDGs have three goals specifically focused on women: Goal 3, 4 and 5 but women’s issues also cut across all of the goals. These goals overall comprise all aspects of women’s lives including economic, health, and political participation.
Gender equality is also strongly linked to education. The Dakar Framework for Action (2000) set out ambitious goals: to eliminate gender disparities in primary and secondary education by 2005, and to achieve gender equality in education by 2015. The focus was on ensuring girls’ full and equal access to and achievement in good quality basic education. The gender objective of the Dakar Framework for Action is somewhat different from the MDG Goal 3 (Target 1): “Eliminate gender disparity in primary and secondary education, preferably by 2005, and in all levels of education no later than 2015”. MDG Goal 3 does not comprise a reference to learner achievement and good quality basic education, but goes beyond the school level. Studies demonstrate the positive impact of girls’ education on child and maternal health, fertility rates, poverty reduction and economic growth. Educated mothers are more likely to send their children to school.
Some organizations working in developing countries and in the development field have incorporated advocacy and empowerment for women into their work. The Food and Agriculture Organization of the United Nations (FAO) adopted a 10-year strategic framework in November 2009 that includes the strategic objective of gender equity in access to resources, goods, services and decision-making in rural areas, and mainstreams gender equity in all FAO's programs for agriculture and rural development. The Association for Progressive Communications (APC) has developed a Gender Evaluation Methodology for planning and evaluating development projects to ensure they benefit all sectors of society including women.
The Gender-related Development Index (GDI), developed by the United Nations, aims to show the inequalities between men and women in the following areas: long and healthy life, knowledge, and a decent standard of living. The United Nations Development Programme (UNDP) has introduced indicators designed to add a gendered dimension to the Human Development Index (HDI). Additionally, in 1995, the Gender-related Development Index (GDI) and the Gender Empowerment Measure (GEM) were introduced. More recently, in 2010, UNDP introduced a new indicator, the Gender Inequality Index (GII), which was designed to be a better measurement of gender inequality and to improve the shortcomings of GDI and GEM.
Gender and climate change.
Gender is a topic of increasing concern within climate change policy and science. Generally, gender approaches to climate change address gender- differentiated consequences of climate change, as well as unequal adaptation capacities and gendered contribution to climate change. Furthermore, the intersection of climate change and gender raises questions regarding the complex and intersecting power relations arising from it. These differences, however, are mostly not due to biological or physical differences, but are formed by the social, institutional and legal context. Subsequently, vulnerability is less an intrinsic feature of women and girls but rather a product of their marginalization. 
Roehr notes that, while the United Nations officially committed to gender mainstreaming, in practice gender equality is not reached in the context of climate change policies. This is reflected in the fact that discourses of and negotiations over climate change are mostly dominated by men.
Some feminist scholars hold that the debate on climate change is not only dominated by men but also primarily shaped in ‘masculine’ principles, which limits discussions about climate change to a perspective that focuses on technical solutions. This perception of climate change hides subjectivity and power relations that actually condition climate-change policy and science, leading to a phenomenon that Tuana terms ‘epistemic injustice’.
Similarly, MacGregor attests that by framing climate change as an issue of ‘hard’ natural scientific conduct and natural security, it is kept within the traditional domains of hegemonic masculinity.

</doc>
<doc id="38099" url="http://en.wikipedia.org/wiki?curid=38099" title="Giovanni Battista Pergolesi">
Giovanni Battista Pergolesi

Giovanni Battista Draghi (]) (4 January 1710 – 16 March 1736), best known as Pergolesi (]) or Giovanni Battista Pergolesi, was an Italian composer, violinist and organist.
Biography.
Born in Jesi in what is now the Province of Ancona (but was then the Papal States), he was commonly called with the nickname "Pergolesi", a demonym indicating in Italian the residents of Pergola, Marche, the birthplace of his ancestors. He studied music in Iesi under a local musician, Francesco Santini, before going to Naples in 1725, where he studied under Gaetano Greco and Francesco Feo among others. He spent most of his brief life working for aristocratic patrons like the Colonna principe di Stigliano, and duca Marzio IV Maddaloni Carafa.
Pergolesi was one of the most important early composers of "opera buffa" (comic opera). His "opera seria", "Il prigionier superbo", contained the two act "buffa" intermezzo, "La serva padrona" ("The Servant Mistress", 28 August 1733), which became a very popular work in its own right. When it was performed in Paris in 1752, it prompted the so-called Querelle des Bouffons ("quarrel of the comic actors") between supporters of serious French opera by the likes of Jean-Baptiste Lully and Jean-Philippe Rameau and supporters of new Italian comic opera. Pergolesi was held up as a model of the Italian style during this quarrel, which divided Paris's musical community for two years.
Among Pergolesi's other operatic works are his first opera "La conversione e morte di San Guglielmo" (1731), "Lo frate 'nnamorato" ("The brother in love", 1732, to a text in the Neapolitan language), L'Olimpiade (31 January 1735) and "Il Flaminio" (1735). All his operas were premiered in Naples, apart from L'Olimpiade, which was first given in Rome.
Pergolesi also wrote sacred music, including a Mass in F and his "Magnificat" in C major. It is his "Stabat Mater" (1736), however, for soprano, alto, string orchestra and basso continuo, which is his best-known sacred work. It was commissioned by the Confraternità dei Cavalieri di San Luigi di Palazzo who presented an annual Good Friday meditation in honor of the Virgin Mary. Pergolesi's work replaced one composed by Alessandro Scarlatti only nine years before, but which was already perceived as "old-fashioned," so rapidly had public tastes changed.
While classical in scope, the opening section of the setting demonstrates Pergolesi's mastery of the Italian baroque "durezze e ligature" style, characterized by numerous suspensions over a faster, conjunct bassline. The work remained popular, becoming the most frequently printed work of the 18th century, and being arranged by a number of other composers, including Johann Sebastian Bach, who used it as the basis for his cantata "Tilge, Höchster, meine Sünden" ("Root out my sins, Highest One"), BWV 1083.
Pergolesi wrote a number of secular instrumental works, including a violin sonata and a violin concerto. A considerable number of instrumental and sacred works once attributed to Pergolesi have since been shown to be misattributed. Much of Igor Stravinsky's ballet "Pulcinella", which ostensibly reworks pieces by Pergolesi, is actually based on works by other composers, especially Domenico Gallo. The "Concerti Armonici" are now known to have been composed by Unico Wilhelm van Wassenaer. Many colorful anecdotes related by Pergolesi's 19th-century biographer, Francesco Florimo, were later revealed as fabrication, though they furnished material for two nineteenth-century operas broadly based on Pergolesi's career.
Giovanni Battista Pergolesi died on March 16, 1736 at the age of 26 in Pozzuoli from tuberculosis and was buried at the Franciscan monastery one day later.
Pergolesi's works on screen.
Pergolesi's "Salve Regina" is a highlighted performance in the movie "Farinelli" (1994), in which Farinelli also performs "Stabat Mater Dolorosa" in the only duet.
The first and last parts of Pergolesi's Stabat Mater have been used in the soundtrack of the movie "Jésus de Montréal"; the third part (Quis est homo) is used in the soundtrack of the movie "Smilla's Sense of Snow"; the last part is also used in the movie "Amadeus" and in the movie "The Mirror" by Andrei Tarkovsky. 
The film "Cactus" by the Australian director Paul Cox is also said to have Pergolesi's Stabat Mater on the soundtrack.

</doc>
<doc id="38103" url="http://en.wikipedia.org/wiki?curid=38103" title="BMX">
BMX

BMX, an abbreviation for bicycle motocross, is a cycle sport performed on BMX bikes, either in competitive BMX racing or freestyle BMX, or else in general on- or off-road recreation. BMX began when young cyclists appropriated motocross tracks for fun, racing and stunts, eventually evolving specialized BMX bikes and competitions.
History.
BMX began in the early 1970s when children began racing their bicycles on dirt tracks in southern California, inspired by the motocross stars of the time. The size and availability of the Schwinn Sting-Ray and other wheelie bikes made them the natural bike of choice for these races, since they were easily customized for better handling and performance. BMX racing was a phenomenon by the mid-1970s. Children were racing standard road bikes off-road, around purpose-built tracks in California. The 1972 motorcycle racing documentary "On Any Sunday" is generally credited with inspiring the movement nationally in the United States; its opening scene shows kids riding their Sting-Rays off-road. By the middle of that decade the sport achieved critical mass, and manufacturers began creating bicycles designed especially for the sport. 
George E. Esser founded the National Bicycle League as a non-profit bicycle motocross sanctioning organization in 1974. Before they set up the NBL, Esser and his wife, Mary, sanctioned motorcycle races with the American Motocross Association (AMA). Their two sons, Greg and Brian, raced motorcycles, but also enjoyed riding and racing BMX with their friends. It was their sons’ interest, and the lack of BMX organizations in the East, which prompted Esser to start the NBL in Florida.
By 1977, the American Bicycle Association (ABA) was organized as a national sanctioning body for the growing sport. 
In April 1981, the International BMX Federation was founded, and the first world championships were held in 1982. Since January 1993 BMX has been integrated into the Union Cycliste Internationale.
Freestyle BMX is now one of the staple events at the annual Summer X Games Extreme Sports competition and the Etnies Backyard Jam, held primarily on the East and West coasts of the United States. The popularity of the sport has increased due to its relative ease and availability of places to ride and do tricks.
In 2003, the International Olympic Committee made BMX a full medal Olympic sport for 2008 Summer Olympic Games in Beijing, China, and Māris Štrombergs (male, for Latvia) and Anne-Caroline Chausson (female, for France) became the first Olympic champions.
Many great BMX riders go on to other cycling sports like downhill, including Australian Olympian Jared Graves, former "golden child" Eric Carter, and youth BMX racer Aaron Gwin.
References.
</dl>

</doc>
<doc id="38114" url="http://en.wikipedia.org/wiki?curid=38114" title="Keep the Aspidistra Flying">
Keep the Aspidistra Flying

Keep the Aspidistra Flying, first published in 1936, is a socially critical novel by George Orwell. It is set in 1930s London. The main theme is Gordon Comstock's romantic ambition to defy worship of the money-god and status, and the dismal life that results.
Background.
Orwell wrote the book in 1934 and 1935 when he was living at various locations near Hampstead in London, and drew on his experiences in these and the preceding few years. At the beginning of 1928 he lived in lodgings in Portobello Road from where he started his tramping expeditions, sleeping rough and roaming in the poorer parts of London. At this time he wrote a fragment of a play in which the protagonist Stone needs money for his child's life-saving operation. Stone would prefer to prostitute his wife rather than prostitute his artistic integrity by writing advertising copy.
Orwell's early publications appeared in "The Adelphi", a left-wing literary journal edited by Sir Richard Rees, a wealthy and idealistic baronet who made Orwell one of his protégés. The character of Ravelston the wealthy publisher in "Keep the Aspidistra Flying" has much in common with Rees. Ravelston is acutely self-conscious of his upper-class status and defensive about his unearned income. Comstock speculates that Ravelston receives nearly two thousand pounds a year after tax—a very comfortable sum in those days—and Rees, in a volume of autobiography published in 1963 wrote: "... I have never had the spending of much less than £1,000 a year of unearned income, and sometimes considerably more [...] Before the war, this was wealth, especially for an unmarried man. Many of my socialist and intellectual friends were paupers compared to me..." In quoting this, Orwell's biographer Michael Shelden commented "One of these 'paupers'—at least in 1935—was Orwell, who was lucky if he made £200 that year [...] He appreciated Rees's editorial support at the "Adelphi" and sincerely enjoyed having him as a friend, but he could not have avoided feeling some degree of resentment toward a man who had no real job but who enjoyed an income four or five times greater than his."
In 1932 Orwell took a job as a teacher in a small school in West London. From there he would take journeys into the country at places like Burnham Beeches. There are allusions to Burnham Beeches and walks in the country in Orwell's correspondence at this time with Brenda Salkeld and Eleanor Jacques.
In October 1934, after nine months at his home in Southwold, Orwell's Aunt Nellie Limouzin found him a job as a part-time assistant in Booklovers' Corner, a second-hand bookshop in Hampstead run by Francis and Myfanwy Westrope. The Westropes, who were friends of Nellie in the Esperanto movement, had an easy-going outlook and provided him with comfortable accommodation at Warwick Mansions, Pond Street. He was job sharing with Jon Kimche who also lived with the Westropes. Orwell worked at the shop in the afternoons, having the mornings free to write and the evenings to socialize. He was at Booklovers' Corner for fifteen months. His essay "Bookshop Memories," published in November 1936, recalled aspects of his time at the bookshop, and in "Keep the Aspidistra Flying", "he described it, or revenged himself upon it, with acerbity and wit and spleen." In their study of Orwell the writers Stansky & Abrahams remarked upon the improvement on the "stumbling attempts at female portraiture in his first two novels: the stereotyped Elizabeth Lackersteen in "Burmese Days" and the hapless Dorothy in "A Clergyman's Daughter"" and contended that, in contrast, "Rosemary is a credible female portrait." Through his work in the bookshop Orwell was in a position to become acquainted with women, "first as a clerk, then as a friend [...] and with whom, if circumstances were favorable, he might eventually embark upon a 'relationship' [...] This for Orwell the author and Blair the man, was the chief reward of working at Booklovers' Corner." In particular, Orwell met Sally Jerome, at this time working for an advertising agency (like Rosemary in "Keep the Aspidistra Flying"), and Kay Ekevall, who ran a small typing and secretarial service which did work for "Adelphi" magazine. 
By the end of February 1935 he had moved into a flat in Parliament Hill; his landlady, Rosalind Obermeyer, was studying at the University of London. It was through a joint party with his landlady here that Orwell met his future wife Eileen O'Shaughnessy. In August Orwell moved into a flat in Kentish Town, which he shared with Michael Sayers and Rayner Heppenstall. Over this period he was working on "Keep the Aspidistra Flying" and had two novels, "Burmese Days" and "A Clergyman's Daughter", published. At the beginning of 1936 Orwell was dealing with pre-publication issues for "Keep the Aspidistra Flying" while on his tour in the North of England collecting material for "The Road to Wigan Pier". The novel was published by Victor Gollancz Ltd on 20 April 1936.
The aspidistra is a hardy, long-living plant that is used as a house plant in England. It was especially popular in the Victorian era, in large part because it could not only tolerate weak sunlight but also could tolerate the poor indoor air quality that resulted from the use of oil lamps and, later, coal gas lamps. They had fallen out of favor by the 20th century, not coincidentally paralleling the advent of electric lighting. Their use had been so widespread among the middle class that they had become a music hall joke appearing in songs such as "Biggest Aspidistra in the World," of which Gracie Fields made a recording.
Plot summary.
Gordon Comstock has "declared war" on what he sees as an "overarching dependence" on money by leaving a promising job as a copywriter for an advertising company called "New Albion"—at which he shows great dexterity—and taking a low-paying job instead, ostensibly so he can write poetry. Coming from a respectable family background in which the inherited wealth has now become dissipated, Gordon resents having to work for a living. The "war" (and the poetry), however, aren't going particularly well and, under the stress of his "self-imposed exile" from affluence, Gordon has become absurd, petty and deeply neurotic.
Comstock lives without luxuries in a bedsit in London, which he affords by working in a small bookshop owned by a Scot, McKechnie. He works intermittently at a magnum opus he plans to call "London Pleasures", describing a day in London; meanwhile, his only published work, a slim volume of poetry entitled "Mice", collects dust on the remainder shelf. He is simultaneously content with his meager existence and also disdainful of it. He lives without financial ambition and the need for a "good job," but his living conditions are uncomfortable and his job is boring.
Comstock is "obsessed" by what he sees as a pervasion of money (the "Money God," as he calls it) behind social relationships, feeling sure that women would find him more attractive if he were better off. At the beginning of the novel, he senses that his girlfriend Rosemary Waterlow (whom he met at New Albion, and who continues to work there) is dissatisfied with him because of his poverty. An example of his financial embarrassment is when he is desperate for a pint of beer at his local pub, but has run out of pocket money and is ashamed to cadge a drink off his fellow lodger, Flaxman.
One of Comstock's last remaining friends, Philip Ravelston, a Marxist who publishes a magazine called "Anti-Christ", agrees with Comstock in principle, but is comfortably well-off himself and this causes strains when the practical miseries of Comstock's life become apparent. He does, however, endeavor to publish some of Comstock's work and his efforts had resulted in "Mice" being published via one of his publisher contacts (unbeknownst to Comstock).
Gordon and Rosemary have little time together—she works late and lives in a hostel, and his "bitch of a landlady" forbids female visitors to her tenants. Then one evening, having headed southward and having been thinking about women—"this women business" in general, and Rosemary in particular—he happens to see Rosemary in a street market. Rosemary won't have sex with him but she wants to spend a Sunday with him, right out in the country, near Burnham Beeches. At their parting, as he takes the tram from Tottenham Court Road back to his bedsit, he is happy and feels that somehow it is agreed between them that Rosemary is going to be his mistress. However, what is intended to be a pleasant day out away from London's grime turns into a disaster when, though hungry, they opt to pass by a "rather low-looking" pub, and can then not find another pub, and are forced to eat an unappetizing lunch at a fancy, overpriced hotel instead. Gordon has to pay the bill with all the money he had set aside for their jaunt and worries about having to borrow money from Rosemary. Out in the countryside again, they are about to have sex for the first time when she violently pushes him back—he wasn't going to use contraception. He rails at her; "Money again, you see! […] You say you 'can't' have a baby. […] You mean you daren't; because you'd lose your job and I've got no money and all of us would starve."
Having sent a poem to an American publication, Gordon suddenly receives from them a check worth ten pounds—a considerable sum for him at the time. He intends to set aside half for his sister Julia, who has always been there to lend him money and support. He treats Rosemary and Ravelston to dinner, which begins well, but the evening deteriorates as it proceeds. Gordon, drunk, tries to force himself upon Rosemary but she angrily rebukes him and leaves. Gordon continues drinking, drags Ravelston with him to visit a pair of prostitutes, and ends up broke and in a police cell the next morning. He is guilt-ridden over the thought of being unable to pay his sister back the money he owes her, because his £5 note is gone, given to, or stolen by, one of the tarts.
Ravelston pays Gordon's fine after a brief appearance before the magistrate, but a reporter hears about the case and writes about it in the local paper. The ensuing publicity results in Gordon losing his job at the bookshop, and, consequently, his relatively "comfortable" lifestyle. As Gordon searches for another job, his life deteriorates, and his poetry stagnates. After living with his friend Ravelston, and his girlfriend Hermione, during his time of unemployment, Gordon ends up working at another book shop and cheap two-penny lending library, this time in Lambeth, owned by the sinister Mr. Cheeseman, for an even smaller wage of 30 shillings a week. This is 10 shillings less than he was earning before, but Gordon is satisfied: "The job would do. There was no "trouble" about a job like this; no room for ambition, no effort, no hope." Determined to sink to the lowest level of society Gordon takes a furnished bed-sitting-room in a filthy alley parallel to Lambeth Cut. Both Julia and Rosemary, "in feminine league against him," seek to get Gordon to go back to his "good" job at the New Albion advertising agency.
Rosemary, having avoided Gordon for some time, suddenly comes to visit him one day at his dismal lodgings. Despite his terrible poverty and shabbiness, they have sex but it is without any emotion or passion. Later, Rosemary drops in one day unexpectedly at the library, having not been in touch with Gordon for some time, and tells him that she is pregnant. Gordon is presented with the choice between leaving Rosemary to a life of social shame at the hands of her family—since both of them reject the idea of an abortion—or marrying her and returning to a life of respectability by taking back the job he once so deplored at the New Albion with its £4 weekly salary.
He chooses Rosemary and respectability and then experiences a feeling of relief at having abandoned his anti-money principles with such comparative ease. After two years of abject failure and poverty, he throws his poetic work "London Pleasures" down a drain, marries Rosemary, resumes his advertising career, and plunges into a campaign to promote a new product to prevent foot odour. In his lonely walks around mean streets, aspidistras seem to appear in every lower-middle class window. As the book closes, Gordon wins an argument with Rosemary to install an aspidistra in their new small but comfortable flat off the Edgware Road.
Extracts.
No need to repeat the blasphemous comments which everyone who had known Gran'pa Comstock made on that last sentence. But it is worth pointing out that the chunk of granite on which it was inscribed weighed close on five tons and was quite certainly put there with the intention, though not the conscious intention, of making sure that Gran'pa Comstock shouldn't get up from underneath it. If you want to know what a dead man's relatives really think of him, a good rough test is the weight of his tombstone.
Gordon put his hand against the swing door. He even pushed it open a few inches. The warm fog of smoke and beer slipped through the crack. A familiar, reviving smell; nevertheless as he smelled it his nerve failed him. No! Impossible to go in. He turned away. He couldn't go shoving into that saloon bar with only fourpence halfpenny in his pocket. Never let other people buy your drinks for you! The first commandment of the moneyless. He made off down the dark pavement.
This woman business! What a bore it is! What a pity we can't cut it right out, or at least be like the animals—minutes of ferocious lust and months of icy chastity. Take a cock pheasant, for example. He jumps up on the hen's backs without so much as a with your leave or by your leave. And no sooner is it over than the whole subject is out of his mind. He hardly even notices his hens any longer; he ignores them, or simply pecks them if they come too near his food.
Literary significance and criticism.
Cyril Connolly wrote two reviews at the time of the novel's publication. In the "Daily Telegraph" he described it as a "savage and bitter book" and said "the truths which the author propounds are so disagreeable that one ends by dreading their mention." In the "New Statesman" he wrote "a harrowing and stark account of poverty" and referred to "clear and violent language, at times making the reader feel he is in a dentist's chair with the drill whirring."
For an edition of "Omnibus", ("The Road to the Left", broadcast 10 January 1971), Melvyn Bragg interviewed Norman Mailer. Bragg said he "just assumed Mailer had read Orwell. In fact he's mad on him." Of "Keep the Aspidistra Flying", Mailer said, "It is perfect from the first page to the last."
In a letter to George Woodcock on 28 September 1946 referring to "Keep the Aspidistra Flying", Orwell noted that it was one of the two or three books of which he was ashamed. He dittoed his comment on "A Clergyman's Daughter" that it "was written simply as an exercise and I oughtn't to have published it, but I was desperate for money [-] At that time I simply hadn't a book in me, but I was half starved and had to turn out something to bring in £100 or so." Orwell biographer Jeffrey Meyers found the novel flawed by weaknesses in plot, style and characterization but praised "a poignant and moving quality [-] that comes from Orwell's perceptive portrayal of the alienation and loneliness of poverty, and from Rosemary's tender response to Gordon's mean misery." In spite of negative judgments the novel has won its admirers, notably Lionel Trilling, who called it "a "summa" of all the criticisms of a commercial civilization that have ever been made."
Tosco Fyvel, literary editor of "Tribune" from 1945–49, and a friend and colleague of Orwell during the last decade of the writer's life, found it interesting that "through Gordon Comstock Orwell expressed violent dislike of London's crowded life [and Orwell moved to the small isolated village of Wallington in rural Hertfordshire in 1936] and mass advertising—a foretaste here of "Nineteen Eighty-Four". He has Gordon reacting to a poster saying "Corner Table Enjoys His Meal With Bovex" in a manner already suggesting that of the later novel:
"Gordon examined the thing with the intimacy of hatred...Corner Table grins at you, seemingly optimistic, with a flash of false teeth. But what is behind the grin? Desolation, emptiness, prophecies of doom. – For can you not see [-] Behind that slick self-satisfaction, that tittering fat-bellied triviality, there's nothing but a frightful emptiness, a secret despair? And the reverberations of future wars."
Film adaptation.
A film adaptation of "Keep the Aspidistra Flying" was released in 1997, directed by Robert Bierman, and starring Richard E. Grant and Helena Bonham Carter. The film appeared in North America and New Zealand under the alternative title of "A Merry War".

</doc>
<doc id="38121" url="http://en.wikipedia.org/wiki?curid=38121" title="Peperomia">
Peperomia

Peperomia (radiator plant) is one of the 2 large genera of the Piperaceae family, with more than 1000 recorded species. Most of them are compact, small perennial epiphytes growing on rotten wood. More than 1500 species have been recorded, occurring in all tropical and subtropical regions of the world, though concentrated in Central America and northern South America. A limited number of species (around 17) are found in Africa.
Description.
Though varying considerably in appearance (see gallery below), these species generally have thick, stout stems and fleshy leaves, sometimes with leaf windows. "Peperomia" flowers typically come in yellow to brown conical spikes. 
These tropical perennials are grown for their ornamental foliage.They are mostly natives of tropical America. They are compact and usually do not exceed 12 in in height. They vary considerably in appearance. Some have threadlike, trailing stems and some have fleshy, stout stems. The leaves are smooth and fleshy and may be oval with the leafstalk at or near the center of the leaf blade, or they may be heart-shaped or lance-shaped; their size may vary from 1 - long. They may be green or striped, marbled or bordered with pale green, red or gray, and the petioles of some kinds are red. The tiny flowers are unnoticeable and they grow in the form of cordlike spikes
Horticulture.
"Peperomias" are grown for their ornamental foliage and sometimes for their attractive flowers ("Peperomia fraseri"). Except for the succulent species, they are generally easy to grow in a greenhouse. Different species (e.g. "Peperomia caperata") and cultivars are found in the trade.
Propagation.
These plants are usually propagated by seeds. They are also commercially propagated by cuttings. "Peperomia" cuttings root easily.
Plants can be divided at potting time. They are removed and separated into smaller pieces, each with a few roots attached. Leaf or stem cuttings can also be taken in the spring or summer. The lower leaves of the shoots are removed and a cut is made below the bottom node (joint). They are then laid on a bench for an hour or two to allow a protective callus tissue to form over the cuts. They are then inserted in a propagating case with bottom heat of 70-75 degrees F. It is best not to seal the top completely, as the plants are semi-succulent in nature and excessive humidity is detrimental. When enough roots have formed, cuttings can be planted in 3-inch pots or in hanging baskets.

</doc>
<doc id="38145" url="http://en.wikipedia.org/wiki?curid=38145" title="Los Alamos National Laboratory">
Los Alamos National Laboratory

Los Alamos National Laboratory (or LANL; previously known at various times as Project Y, Los Alamos Laboratory, and Los Alamos Scientific Laboratory) is one of two laboratories in the United States where classified work towards the design of nuclear weapons is undertaken. The other, since 1952, is Lawrence Livermore National Laboratory. LANL is a United States Department of Energy (DOE) national laboratory, managed and operated by Los Alamos National Security (LANS), located in Los Alamos, New Mexico. The laboratory is one of the largest science and technology institutions in the world. It conducts multidisciplinary research in fields such as national security, space exploration, renewable energy, medicine, nanotechnology, and supercomputing.
LANL is the largest institution and the largest employer in northern New Mexico, with approximately 9,000 direct employees and around 650 contractor personnel. Additionally, there are roughly 120 DOE employees stationed at the laboratory to provide federal oversight of LANL's work and operations. Approximately one-third of the laboratory's technical staff members are physicists, one quarter are engineers, one-sixth are chemists and materials scientists, and the remainder work in mathematics and computational science, biology, geoscience, and other disciplines. Professional scientists and students also come to Los Alamos as visitors to participate in scientific projects. The staff collaborates with universities and industry in both basic and applied research to develop resources for the future. The annual budget is approximately US$2.2 billion.
History.
The Manhattan Project.
The laboratory was founded during World War II as a secret, centralized facility to coordinate the scientific research of the Manhattan Project, the Allied project to develop the first nuclear weapons. In September 1942, the difficulties encountered in conducting preliminary studies on nuclear weapons at universities scattered across the country indicated the need for a laboratory dedicated solely to that purpose.
General Leslie Groves wanted a central laboratory at an isolated location for safety, and to keep the scientists away from the populace. It should be at least 200 miles from international boundaries and west of the Mississippi. Major John Dudley suggested Oak City, Utah or Jemez Springs, New Mexico but both were rejected. Manhattan Project scientific director J. Robert Oppenheimer had spent much time in his youth in the New Mexico area, and suggested the Los Alamos Ranch School on the mesa. Dudley had rejected the school as not meeting Groves’ criteria, but as soon as Groves saw it he said in effect "This is the place". Oppenheimer became the laboratory's first director.
During the Manhattan Project, Los Alamos hosted thousands of employees, including many Nobel Prize-winning scientists. The location was a total secret. Its only mailing address was a post office box, number 1663, in Santa Fe, New Mexico. Eventually two other post office boxes were used, 180 and 1539, also in Santa Fe. Though its contract with the University of California was initially intended to be temporary, the relationship was maintained long after the war. Until the atomic bombings of Hiroshima and Nagasaki, Japan, University of California president Robert Sproul did not know what the purpose of the laboratory was and thought it might be producing a "death ray". The only member of the UC administration who knew its true purpose—indeed, the only one who knew its exact physical location—was the Secretary-Treasurer Robert Underhill, who was in charge of wartime contracts and liabilities.
The work of the laboratory culminated in the creation of several atomic devices, one of which was used in the first nuclear test near Alamogordo, New Mexico, codenamed "Trinity", on July 16, 1945. The other two were weapons, "Little Boy" and "Fat Man", which were used in the attacks on Hiroshima and Nagasaki. The Laboratory received the Army-Navy ‘E’ Award for Excellence in production on October 16, 1945.
After the war, Oppenheimer retired from the directorship, and it was taken over by Norris Bradbury, whose initial mission was to make the previously hand-assembled atomic bombs "G.I. proof" so that they could be mass-produced and used without the assistance of highly trained scientists. Many of the original Los Alamos "luminaries" chose to leave the laboratory, and some even became outspoken opponents to the further development of nuclear weapons.
In the years since the 1940s, Los Alamos was responsible for the development of the hydrogen bomb, and many other variants of nuclear weapons. Several female scientists also made contributions to work at the lab at this time. In 1952, Lawrence Livermore National Laboratory was founded to act as Los Alamos' "competitor", with the hope that two laboratories for the design of nuclear weapons would spur innovation. Los Alamos and Livermore served as the primary classified laboratories in the U.S. national laboratory system, designing all the country's nuclear arsenal. Additional work included basic scientific research, particle accelerator development, health physics, and fusion power research as part of Project Sherwood. Many nuclear tests were undertaken in the Marshall Islands and at the Nevada Test Site. During the late-1950s, a number of scientists including Dr. J. Robert "Bob" Beyster left Los Alamos to work for General Atomics (GA) in San Diego.
Three major nuclear-related accidents have occurred at LANL. Criticality accidents occurred in August 1945 and May 1946, and a third accident occurred during an annual physical inventory in December 1958.
Several buildings associated with the Manhattan Project at Los Alamos were declared a National Historic Landmark in 1965.
Post-Cold War.
At the end of the Cold War, both labs went through a process of intense scientific diversification in their research programs to adapt to the changing political conditions that no longer required as much research towards developing new nuclear weapons and has led the lab to increase research for “non-war” science and technology. Los Alamos' nuclear work is currently thought to relate primarily to computer simulations and stockpile stewardship. The development of the Dual-Axis Radiographic Hydrodynamic Test Facility will allow complex simulations of nuclear tests to take place without full explosive yields.
The lab has made intense efforts for humanitarian causes through its scientific research in medicine. Three vaccines for the AIDS virus are being tested by lab scientist Bette Korber and her team. "These vaccines might finally deal a lethal blow to the AIDS virus", says Chang-Shung Tung, leader of the Lab's Theoretical Biology and Biophysics group.
There is also development for a safer, more comfortable and accurate test for breast cancer by Lab scientists Lianjie Huang and Kenneth M. Hanson and collaborators. The new technique, called ultrasound-computed tomography (ultrasound CT), uses sound waves to accurately detect small tumors that traditional mammography cannot..
The laboratory contributed to the early development of the flow cytometry technology. In the 1950s, researcher Mack Fulwyler developed a technique for sorting erythrocytes that combined the Coulter Principle of Coulter counter technologies, which measures the presence of cells and their size, with ink jet technology, which produces a laminar flow of liquid that breaks up into separate, fine drops. In 1969, Los Alamos reported the first fluorescence detector apparatus, which accurately measured the number and size of ovarian cells and blood cells.
Other research performed at the lab includes developing cheaper, cleaner bio-fuels and advancing scientific understanding around renewable energy.
Non-nuclear national security and defense development is also a priority at the lab. This includes preventing outbreaks of deadly diseases by improving detection tools and the monitoring the effectiveness of the United States’ vaccine distribution infrastructure. Additional advancements include the ASPECT airplane that can detect bio threats from the sky.
The laboratory has attracted negative publicity from a number of events. In 1999, Los Alamos scientist Wen Ho Lee was accused of 59 counts of mishandling classified information by downloading nuclear secrets—"weapons codes" used for computer simulations of nuclear weapons tests—to data tapes and removing them from the lab. After ten months in jail, Lee pleaded guilty to a single count and the other 58 were dismissed with an apology from U.S. District Judge James Parker for his incarceration. Lee had been suspected for having shared U.S. nuclear secrets with China, but investigators were never able to establish what Lee did with the downloaded data. In 2000, two computer hard drives containing classified data were announced to have gone missing from a secure area within the laboratory, but were later found behind a photocopier; in 2003, the laboratory's director John Browne, and deputy director, resigned following accusations that they had improperly dismissed two whistleblowers who had alleged widespread theft at the lab. The year 2000 brought additional hardship for the laboratory in the form of the Cerro Grande Fire, a severe forest fire that destroyed several buildings (and employees' homes) and forced the laboratory to close for two weeks.
In July 2004, an inventory of classified weapons data revealed that four hard disk drives were missing: two of the drives were subsequently found to have been improperly moved to a different building, but another two remained unaccounted for. In response, director Peter Nanos shut down large parts of the laboratory and publicly rebuked scientists working there for a lax attitude to security procedures. In the laboratory's August 2004 newsletter he wrote, "This willful flouting of the rules must stop, and I don't care how many people I have to fire to make it stop". Nanos is also quoted as saying, "If I have to restart the laboratory with 10 people, I will". However, a report released in January 2005 found that the drives were in fact an artifact of an inconsistent inventory system: the report concludes that 12 barcodes were issued to a group of disk drives that needed only 10, but the two surplus barcodes nevertheless appeared on a master list. Thus, auditors wrongly concluded that two disks were missing. The report states, "The allegedly missing disks never existed and no compromise of classified material has occurred". This incident is widely reported as contributing to continuing distrust of management at the lab. In May 2005, Nanos stepped down as director.
Contract changes.
Continuing efforts to make the laboratory more efficient led the Department of Energy to open its contract with the University of California to bids from other vendors in 2003. Though the university and the laboratory had difficult relations many times since their first World War II contract, this was the first time that the university ever had to compete for management of the laboratory. The University of California decided to create a private company with the Bechtel Corporation, Washington Group International, and the BWX Technologies to bid on the contract to operate the laboratory. The UC/Bechtel led corporation—Los Alamos National Security, LLC (LANS)—was pitted against a team formed by the University of Texas System partnered with Lockheed-Martin. In December 2005, the Department of Energy announced that LANS had won the next seven-year contract to manage and operate the laboratory.
On June 1, 2006, the University of California ended its sixty years of direct involvement in operating Los Alamos National Laboratory, and management control of the laboratory was taken over by Los Alamos National Security, LLC. Approximately 95% of the former 10,000 plus UC employees at LANL were rehired by LANS to continue working at LANL. Other than UC appointing three members to the eleven member board of directors that oversees LANS, UC now has virtually no responsibility or direct involvement in LANL. UC policies and regulations that apply to UC campuses and its two national laboratories in California (Lawrence Berkeley and Lawrence Livermore) no longer apply to LANL, and the LANL director no longer reports to the UC Regents or UC Office of the President. Also, LANL employees were removed from the UC's 403(b) retirement savings and defined benefits pension program and placed in a LANS run program. While the LANS retirement program provides rehired UC employees with pensions similar to those UC would have given them, LANS no longer guarantees full pensions to newly hired LANL employees, it now provides basic 401(k) retirement saving options.
Concern has been voiced about the new contractor's effectiveness in correcting the perceived problems in safety, security and financial management that were cited as the reasons for bidding the contract, and Bechtel's lack of transparency (as a private corporation) and increasing control of national nuclear facilities.
Award of the Lawrence Livermore National Laboratory contract to LLNS LLC took effect October 1, 2007. LLNS is a joint venture of the University of California, Bechtel, Babcock & Wilcox, URS and Batelle.
Extended operations.
With support of the National Science Foundation, LANL operates one of the three National High Magnetic Field Laboratories in conjunction with and located at two other sites Florida State University in Tallahassee, Florida, and University of Florida in Gainesville, Florida.
Los Alamos National Laboratory is a partner in the Joint Genome Institute (JGI) located in Walnut Creek, California. JGI was founded in 1997 to unite the expertise and resources in genome mapping, DNA sequencing, technology development, and information sciences pioneered at the three genome centers at University of California's Lawrence Berkeley National Laboratory (LBNL), Lawrence Livermore National Laboratory (LLNL), and LANL.
The Integrated Computing Network (ICN), in 2008 the fastest supercomputer, is a multi-security level network at the LANL integrating large host supercomputers, a file server, a batch server, a printer and graphics output server and numerous other general purpose and specialized systems.
The Los Alamos National Laboratory also used to host the arXiv e-print archive. The arXiv is currently operated and funded by Cornell University.
In the recent years, the Laboratory has developed a major research program in systems biology modeling, known at LANL under the name q-bio.
Controversy and criticism.
In 2005, Congress held new hearings on lingering security issues at Los Alamos National Weapons Laboratory in New Mexico. But documented problems continued to be ignored.
In 2009, 69 computers which did not contain classified information were lost. 2009 also saw a scare in which 2.2 pounds of missing plutonium prompted a Department of Energy investigation into the laboratory. The investigation found that the "missing plutonium" was a result of miscalculation by LANL's statisticians and did not actually exist; but, the investigation did lead to heavy criticism of the laboratory by the DOE for security flaws and weaknesses that the DOE claimed to have found.

</doc>
<doc id="38213" url="http://en.wikipedia.org/wiki?curid=38213" title="Hosiery">
Hosiery

Hosiery, also referred to as legwear, describes garments worn directly on the feet and legs. The term originated as the collective term for products of which a maker or seller is termed a hosier; and those products are also known generically as hose. The term is also used for all types of knitted fabric, and its thickness and weight is defined by denier or opacity. Lower denier measurements of 5 to 15 describe a hose which may be sheer in appearance, whereas styles of 40 and above are dense, with little to no light able to come through on 100 denier items.
The first references to hosiery can be found in works of Hesiod, where Romans are said to have used leather or cloth in forms of strips to cover their lower body parts. Even the Egyptians are speculated to have used hosiery as socks have been found in certain tombs.
Most hosiery garments are made by knitting methods. Modern hosiery is usually tight-fitting by virtue of stretchy fabrics and meshes. Older forms include binding to achieve a tight fit. Due to its close fit, most hosiery can be worn as an undergarment, but it is more commonly worn as a combined under/outer garment.
In 2011, the hosiery and sock mills market was over $1.2 billion in the United States.
References.
Notes

</doc>
<doc id="38221" url="http://en.wikipedia.org/wiki?curid=38221" title="Shit">
Shit

Shit is an English word that is usually considered vulgar and profane in Modern English. As a noun it refers to fecal matter, and as a verb it means to defecate; in the plural ("the shits") it means diarrhea. Shite is also a common variant in British English and Irish English.
As a slang term, it has many meanings, including: nonsense, foolishness, something of little value or quality, trivial and usually boastful or inaccurate talk, or a contemptible person. It may also be used as an expression of annoyance, surprise, or anger.
Etymology.
The word is likely derived from Old English, having the nouns "scite" (dung, attested only in place names) and "scitte" (diarrhoea), and the verb "scītan" (to defecate, attested only in "bescītan", to cover with excrement); eventually it morphed into Middle English "schītte" (excrement), "schyt" (diarrhoea) and "shiten" (to defecate), and it is virtually certain that it was used in some form by preliterate Germanic tribes at the time of the Roman Empire. The word may be further traced to Proto-Germanic *"skit"-, and ultimately to Proto-Indo-European *"skheid"- "cut, separate", the same root believed to have become the word "shed". The word has several cognates in modern Germanic languages, such as German "Scheiße", Dutch "schijt", Swedish "skit", Icelandic "skítur", Norwegian "skitt" etc. Ancient Greek had 'skōr' (gen. 'skatos' hence 'scato-'), from Proto-Indo-European *"sker"-, which is likely unrelated.
Usage.
The word "shit" (also "shite" in British and Hiberno-English) is usually avoided in formal speech. Minced oath substitutes for the word "shit" in English include "sugar", "shoot", and "shucks".
In the word's literal sense, it has a rather small range of common usages. An unspecified or collective occurrence of feces is generally "shit" or "some shit"; a single deposit of feces is sometimes "a shit" or "a piece of shit", and to defecate is "to shit", "to take a shit". While it is common to speak of shit as existing in "a pile", "a load", "a hunk" and other quantities and configurations, such expressions flourish most strongly in the figurative. For practical purposes, when actual defecation and excreta are spoken of, it is either through creative euphemism or with a vague and fairly rigid literalism.
"Piece of shit" may also be used figuratively to describe a particularly loathsome individual.
Vague noun.
"Shit" can be used as a generic mass noun similar to "stuff"; for instance, "This show is funny shit" or "This test is hard shit", or "That was stupid shit". These three usages (with "funny", "hard", and "stupid" or another synonym of "stupid") are heard most commonly in the United States.
In the expression "Get your shit together!" the word "shit" may refer to some set of personal belongings or tools, or to one's wits, composure, or attention to the task at hand. "He doesn't have his shit together" suggests he is failing rather broadly, with the onus laid to multiple personal shortcomings, rather than bad luck or outside forces.
To "shoot the shit" is to have a friendly but pointless conversation, as in "Come by my place some time and we'll shoot the shit."
Surprise.
To "shit oneself", or to "shit bricks" can be used to refer to surprise or fear, often in the future. The latter form can be commonly seen in a form of Internet meme which goes by the phrase "when you see it, you will shit bricks", used in connection with an image of a busy scene with an often unnoticed laughing face or disturbing object which is hard to see until you study the picture.
The word can also be used to represent anger, as in "Jim is totally going to flip his shit when he sees that we wrecked his marriage."
Trouble.
"Shit" can be used to denote trouble, by saying one is in "a lot of shit" or "deep shit". It's common for someone to refer to an unpleasant thing as "hard shit" ("You got a speeding ticket? Man, that's some hard shit"), but the phrase "tough shit" is used as an unsympathetic way of saying "too bad" to whoever is having problems ("You got arrested? Tough shit, man!") or as a way of expressing to someone that they need to stop complaining about something and cope with it instead (Billy: "I got arrested because of you!" Tommy: "Tough shit, dude, you knew you might get arrested when you chose to come with me.") Note that in this case, as in many cases with the term, "tough shit" is often said as a way of pointing out someone's fault in his/her own current problem. It's also common to express annoyance by simply saying "Shit".
"When the shit hits the fan" is usually used to refer to a specific time of confrontation or trouble, which requires decisive action. This is often used in reference to combat situations and the action scenes in movies, but can also be used for everyday instances that one might be apprehensive about. "I don't want to be here when the shit hits the fan!" indicates that the speaker is dreading this moment (which can be anything from an enemy attack to confronting an angry parent or friend). "He's the one to turn to when the shit hits the fan" is an indication that the person being talked about is dependable and will not run from trouble or abandon their allies in tough situations. The concept of this phrase is simple enough, as the actual substance striking the rotating blades of a fan would cause a messy and unpleasant situation (much like being in the presence of a manure spreader). Whether or not this has actually happened, or if the concept is simply feasible enough for most people to imagine the result without needing it to be demonstrated, is unknown. Another example might be the saying "shit rolls downhill", a metaphor suggesting that trouble for a manager may be transferred to the subordinates. There are a number of anecdotes and jokes about such situations, as the imagery of these situations is considered to be funny. This is generally tied-in with the concept that disgusting and messy substances spilled onto someone else are humorous.
Displeasure.
"Shit" can comfortably stand in for the terms "bad" and "anything" in many instances ("Dinner was good, but the movie was shit." "You're all mad at me, but I didn't do shit!"). A comparison can also be used, as in "Those pants look like shit", or "This stuff tastes like shit". Many usages are idiomatic. "I'm shit out of luck" usually refers to someone who is at the end of their wits or who has no remaining viable options. "That little shit shot me in the ass", suggests a mischievous or contemptuous person. Euphemisms such as "crap" are not used in this context.
The term "piece of shit" is generally used to classify a product or service as being sufficiently below the writer's understanding of generally accepted quality standards to be of negligible and perhaps even negative value. The term "piece of shit" has greater precision than "shit" or "shitty" in that "piece of shit" identifies the low quality of a specific component or output of a process without applying a derogatory slant to the entire process. For example, if one said "The inner city youth orchestra has been a remarkably successful initiative in that it has kept young people off the streets after school and exposed them to culture and discipline, thereby improving their self esteem and future prospects. The fact that the orchestra's recent rendition of Tchaikovsky's Manfred Symphony in B minor was pretty much a piece of shit should not in any way detract from this." The substitution of "shit" or "shitty" for "pretty much a piece of shit" would imply irony and would therefore undermine the strength of the statement.
The term "(blank) don't/doesn't give a shit" can be used when one does not care about something, or has a passive attitude toward said thing, as it denotes indifference. In context, one can say: "You're offended? Well, I don't give a shit!"
Dominance.
"Shit" can also be used to establish superiority over another being. The most common phrase is "eat shit!" expressing hatred of the addressee. Some other personal word may be added such as "eat my shit" implying truly personal connotations. As an aside, the above is actually a contraction of the phrase "eat shit and die!". It is often said without commas as a curse; they command the other party to perform exactly those actions in that order. However, the term was originally "Eat, Shit, and Die" naming the three most basic things humans have to do, and it is common among soldiers.
The phrase "You ain't shit", expresses an air of intimidation over the addressee, expressing that they mean nothing or are worthless.
Positive attitude.
In slang, prefixing the article "the" to "shit" gives it a completely opposite definition, meaning "the best", as in "Altered Beast is the shit", or ""The Oregon Trail" is the shit". Again, other slang words of the same meaning, "crap" for example, are not used in such locutions.
Shortening of "bullshit".
The expression "no shit?" (a contraction of "no bullshit?") is used in response to a statement that is extraordinary or hard to believe. Alternatively the maker of the hard-to-believe statement may add "no shit" to reinforce the sincerity or truthfulness of their statement, particularly in response to someone expressing disbelief at their statement. "No shit" is also used sarcastically in response to a statement of the obvious, as in "no shit, Sherlock".
In this form the word can also be used in phrases such as "don't give me that shit" or "you're full of shit". The term "full of shit" is often used as an exclamation to charge someone who is believed to be prone to dishonesty, exaggeration or is thought to be "phoney" with an accusation. For example:
The word "bullshit" also denotes false or insincere discourse. ("Horseshit" is roughly equivalent, while "chickenshit" means "cowardly", "batshit" indicates a person is crazy, and "going apeshit" indicates a person is entering a state of high excitement or unbridled rage.) "Are you shitting me?!" is a question sometimes given in response to an incredible assertion. An answer that reasserts the veracity of the claim is, "I shit you not".
Emphasis.
Perhaps the only constant connotation that "shit" reliably carries is that its referent holds some degree of emotional intensity for the speaker. Whether offense is taken at hearing the word varies greatly according to listener and situation, and is related to age and social class: elderly speakers and those of (or aspiring to) higher socioeconomic strata tend to use it more privately and selectively than younger and more blue-collar speakers.
Like the word "fuck", "shit" is often used to add emphasis more than to add meaning, for example, "shit! I was so shit-scared of that shithead that I shit-talked him into dropping out of the karate match!" The term "to shit-talk" connotes bragging or exaggeration (whereas "to talk shit" primarily means "to gossip [about someone in a damaging way]" or to talk in a boastful way about things which are erroneous in nature), but in such constructions as the above, the word "shit" often functions as an interjection.
Unlike the word "fuck", "shit" is not used emphatically with "-ing" or as an infix. For example; "I lost the shitting karate match" would be replaced with "...the fucking karate match". Similarly, while "in-fucking-credible" is generally acceptable, "in-shitting-credible" is not.
The verb "to shit".
The preterite and past participle of "shit" are attested as "shat", "shit", or "shitted", depending on dialect and, sometimes, the rhythm of the sentence. In the prologue of "The Canterbury Tales", "shitten" is used as the past participle; however this form is very rare in modern English. In American English "shit" as a past participle is often correct, while "shat" is generally acceptable and "shitted" is uncommon and missing from the "Random House" and "American Heritage" dictionaries.
Backronyms.
The backronym form "S.H.I.T." often figures into jokes, like "Special High Intensity Training" (a well-known joke used in job applications), "Special Hot Interdiction Team" (a mockery on SWAT), "Super Hackers Invitational Tournament," and any college name that begins with an S-H (like "Sam Houston Institute of Technology" or "South Harmon Institute of Technology" in the 2006 film "Accepted" or "Store High In Transit" in the 2006 film "Kenny"). "South Hudson Institute of Technology" has sometimes been used to describe the United States Military Academy at West Point. "The Simpsons"' Apu was a graduate student at "Springfield Heights Institute of Technology".
In polite company, sometimes backronyms such as "Sugar Honey in Tea" or "Sugar Honey Iced Tea" are used.
Usage in media and campaigns.
Television.
The word has become increasingly acceptable on American cable television and satellite radio, which are not subject to FCC regulation. In other English-speaking countries, such as Canada, the United Kingdom, the Republic of Ireland, Australia and New Zealand the word is allowed to be used in broadcast television by the regulative councils of each area, as long as it is used in late hours when young people are not expected to be watching. It has appeared on ABC News' 20/20.
United Kingdom.
It is believed that the first person to say "shit" on British TV was John Cleese of the "Monty Python" comedy troupe in the late 1960s, as he, himself, says in his eulogy for Graham Chapman.
Canada.
In Canada, it is one of the words considered by the Canadian Broadcast Standards Council to be "coarse, offensive language intended for adults", acceptable for broadcast only after 9:00pm.
The Canadian Showcase television show "Trailer Park Boys" frequently uses the term "shit". Trailer park supervisor James "Jim" Lahey employs many metaphors with the negative slang "shit" bizarrely worked in. For example, in one episode, Mr. Lahey likens Ricky's growing ignorance to that of a "shit tsunami", while in another episode, Mr. Lahey tells Bubbles that the "shit hawks are swooping in low" due to his deplorable behavior and company.
The term "shit" is also used in the titles of the episodes themselves. Some of which include "The Winds of Shit," "A Shit Leopard Can't Change Its Spots," and "Never Cry Shitwolf".
United States.
"Shit" was one of the original "Seven Words You Can Never Say On TV", a comedy routine by American Comedian George Carlin. In the United States, although the use of the word is censored on broadcast network television (while its synonym "crap" is not usually subject to censorship), the FCC permitted some exceptions. The 14 October 1999 episode of "Chicago Hope" is believed to be the first show (excluding documentaries) on U.S. network television to contain the word "shit" in uncensored form. The word also is used in a later "ER" episode "On the Beach" by Dr. Mark Greene, experiencing the final stages of a deadly brain tumor. Although the episode was originally aired uncensored, the "shit" utterance has since been edited out in syndicated reruns.
An episode of "South Park", "It Hits the Fan", originally aired on 20 June 2001, was a parody of the hype over the "Chicago Hope" episode. "Shit" is used 162 times, and a counter in the corner of the screen tallies the repetitions, excluding the word in written forms — 38 times, totaling up to an even 200 times. "South Park" airs on American cable networks, outside the regulatory jurisdiction of the Federal Communications Commission (FCC), where censorship of vulgar dialogue is at the discretion of the cable operators. Since then, the word has become a mainstay of "South Park", along with programming on other cable networks including FX and as of March 2014, Adult Swim.
American terrestrial radio stations must abide by FCC guidelines on obscenity to avoid punitive fines, unlike satellite radio. These guidelines do not define exactly what constitutes obscenity, but it has been interpreted by some commissioners as including any form of words like "shit" and "fuck", for whatever use.
Despite this, the word has been featured in popular songs that have appeared on broadcast radio in cases where the usage of the word is not audibly clear to the casual listener, or on live television. In the song "Man in the Box" by Alice in Chains, the line "Buried in my shit" was played unedited over most rock radio stations. The 1973 Pink Floyd song "Money" from the album "Dark Side of the Moon" contains the line "Don't give me that do goody good bullshit," and has frequently been broadcast unedited on US radio. The 1980 hit album "Hi Infidelity" by REO Speedwagon contained the song "Tough Guys" which had the line "she thinks they're full of shit," which was played on broadcast radio. On 3 December 1994, Green Day performed "Geek Stink Breath", on "Saturday Night Live", "shit" was not edited from tape delay live broadcast. The band did not appear on the show again until 9 April 2005.
Some notable instances of censorship of the word from broadcast television and radio include Steve Miller's "Jet Airliner." Although radio stations have sometimes played an unedited version containing the line "funky shit going down in the city." The songs was also released with a "radio edit" version, replacing the "funky shit" with "funky kicks". Another version of "Jet Airliner" exists in which the word "shit" is faded out. Likewise, the Bob Dylan song "Hurricane" has a line about having no idea "what kind of shit was about to go down," and has a radio edit version without the word. Gwen Stefani's "Hollaback Girl" video had the original album's use of the word censored in its video. The music video title "...On the Radio (Remember the Days)" by Nelly Furtado replaced by the original title "Shit on the Radio (Remember the Days)." This also happened to "That's That Shit" by Snoop Dogg featuring R. Kelly, which became "That's That". In Avril Lavigne's song "My Happy Ending," the Radio Disney edit of the song replaces "all the shit that you do" with "all the stuff that you do." Likewise, in the song "London Bridge" by the Black Eyed Peas member Fergie, the phrase "Oh Shit" is repeatedly used as a background line. A radio edit of this song replaced "Oh Shit" with "Oh Snap."
Japan.
On Japanese TV such as anime that is aired on American television "shit" is uncensored. The Japanese word for shit as well as bullshit is"kuso" but it also means milder swear words like damn, crap or even darn and crud. It comes up in the subtitles from inaccurate translations or unsure of which word it means.
Sanitation campaigns.
Using the term "shit" (or other locally used crude words) - rather than feces or excreta - during campaigns and triggering events is a deliberate aspect of the community-led total sanitation approach which aims to stop open defecation, a massive public health problem in developing countries.

</doc>
<doc id="38234" url="http://en.wikipedia.org/wiki?curid=38234" title="Diana Wynne Jones">
Diana Wynne Jones

Diana Wynne Jones (16 August 1934 – 26 March 2011) was a British writer, principally of fantasy novels for children and adults.
Some of her better-known works are the Chrestomanci series, the Dalemark series; the novels "Howl's Moving Castle" and "Dark Lord of Derkholm"; and "The Tough Guide To Fantasyland".
Early life and marriage.
Jones was born in London, the daughter of Marjorie (née Jackson) and Richard Aneurin Jones, both of whom were educators. When war was announced, shortly after her fifth birthday, she was evacuated to Wales, and thereafter moved several times, including periods in Coniston Water, in York, and back in London. In 1943 her family finally settled in Thaxted, Essex, where her parents worked running an educational conference centre. There, Jones and her two younger sisters Isobel (later Professor Isobel Armstrong, the literary critic) and Ursula (later an actress and a children's writer) spent a childhood left chiefly to their own devices. After attending the Friends School Saffron Walden, she studied English at St Anne's College in Oxford, where she attended lectures by both C. S. Lewis and J. R. R. Tolkien before graduating in 1956. In the same year she married John Burrow, a scholar of medieval literature, with whom she had three sons, Richard, Michael and Colin. After a brief period in London, in 1957 the couple returned to Oxford, where they stayed until moving to Bristol in 1976.
According to her autobiography, Jones decided she was an atheist when she was a child.
Career.
Jones started writing during the mid-1960s "mostly to keep my sanity", when the youngest of her three children was about two years old and the family lived in a house owned by an Oxford college. Beside the children, she felt harried by the crises of adults in the household: a sick husband, a mother-in-law, a sister, and a friend with daughter. Her first book was a novel for adults published by Macmillan in 1970, entitled "Changeover". It originated as the British Empire was divesting colonies; she recalled in 2004 that it had "seemed like every month, we would hear that yet another small island or tiny country had been granted independence." "Changeover" is set in a fictional African colony during transition, and begins as a memo about the problem of how to "mark changeover" ceremonially is misunderstood to be about the threat of a terrorist named Mark Changeover. It is a farce with a large cast of characters, featuring government, police, and army bureaucracies; sex, politics, and news. In 1965, when Rhodesia declared independence unilaterally (one of the last colonies and not tiny), "I felt as if the book were coming true as I wrote it."
Jones' books range from amusing slapstick situations to sharp social observation ("Changeover" is both), to witty parody of literary forms. Foremost amongst the latter are "The Tough Guide To Fantasyland", and its fictional companion-pieces "Dark Lord of Derkholm" (1998) and "Year of the Griffin" (2000), which provide a merciless (though not unaffectionate) critique of formulaic sword-and-sorcery epics.
The "Harry Potter" books are frequently compared to the works of Diana Wynne Jones. Many of her earlier children's books were out of print in recent years, but have now been re-issued for the young audience whose interest in fantasy and reading was spurred by "Harry Potter".
Jones' works are also compared to those of Robin McKinley and Neil Gaiman. She was friends with both McKinley and Gaiman, and Jones and Gaiman are fans of each other's work; she dedicated her 1993 novel "Hexwood" to him after something he said in conversation inspired a key part of the plot. Gaiman had already dedicated his 1991 four-part comic book mini-series "The Books of Magic" to "four witches", of whom Jones was one.
For "Charmed Life", the first Chrestomanci novel, Jones won the 1978 Guardian Children's Fiction Prize, a once-in-a-lifetime award by "The Guardian" newspaper that is judged by a panel of children's writers. Three times she was a commended runner-up for the Carnegie Medal from the Library Association, recognising the year's best children's book: for "Dogsbody" (1975), "Charmed Life" (1977), and the fourth Chrestomanci book "The Lives of Christopher Chant" (1988). She won the Mythopoeic Fantasy Award, children's section, in 1996 for "The Crown of Dalemark" (concluding that series) and in 1999 for "Dark Lord of Derkholm"; in four other years she was a finalist for that annual literary award by the Mythopoeic Society.
The 1986 novel "Howl's Moving Castle" was inspired by a boy who asked for a story about a moving castle. It was published first by Greenwillow in the U.S., where it was a runner-up for the annual Boston Globe–Horn Book Award in children's fiction. In 2004, Hayao Miyazaki made the Japanese-language animated movie "Howl's Moving Castle", which was nominated for the Academy Award for Best Animated Feature. A version dubbed in English was released in the UK and US in 2005, with the voice of Howl performed by Christian Bale. Next year Jones and the novel won the annual Phoenix Award from the Children's Literature Association, recognising the best children's book published twenty years earlier that did not win a major award (named for mythical bird phoenix to suggest the book's rise from obscurity).
"Fire and Hemlock" had been the 2005 Phoenix runner-up. It is a novel based on Scottish ballads, and was a Mythopoeic Fantasy finalist in its own time.
"Archer's Goon" (1984) was a runner-up for that year's Horn Book Award. It was adapted for television in 1992. One Jones fansite believes it to be "the only tv adaptation (so far) of one of Diana's books".
Jones' book on clichés in fantasy fiction, "The Tough Guide To Fantasyland" (nonfiction), has a cult following among writers and critics, despite being difficult to find due to an erratic printing history. It was recently reissued in the UK, and has been reissued in the USA in 2006 by Firebird Books. The Firebird edition has additional material and a completely new design, including a new map.
The British Fantasy Society recognized her significant impact on fantasy with its occasional Karl Edward Wagner Award in 1999.
She received an honorary D.Litt from the University of Bristol in July 2006 and the World Fantasy Award for Life Achievement in 2007.
Shortly after her death in March 2011, it was reported that "Earwig and the Witch" and a collection of Jones' articles would be published later – as they were in June 2011 and September 2012.
The story in progress when she became too ill to write was completed by her sister Ursula Jones: "The Islands of Chaldea" (HarperCollins, 2014).
Interviewed by "The Guardian" in June 2013, after she finished the Chaldea story, Ursula Jones said that "other things were coming to light ... She left behind a mass of stuff."
Illness and death.
Jones was diagnosed with lung cancer in the early summer of 2009. She underwent surgery in July and reported to friends that the procedure had been successful. However, in June 2010 she announced that she would be discontinuing chemotherapy because it only made her feel ill. In mid-2010 she was halfway through a new book with plans for another to follow. She died on 26 March 2011 from the disease. She was survived by her husband, three sons, and five grandchildren.

</doc>
<doc id="38246" url="http://en.wikipedia.org/wiki?curid=38246" title="Madama Butterfly">
Madama Butterfly

Madama Butterfly ("Madame Butterfly") is an opera in three acts (originally two acts) by Giacomo Puccini, with an Italian libretto by Luigi Illica and Giuseppe Giacosa. The libretto of the opera is based in part on the short story "Madame Butterfly" (1898) by John Luther Long—which in turn was based partially on stories told to Long by his sister Jennie Correll and partially on the semi-autobiographical 1887 French novel "Madame Chrysanthème" by Pierre Loti.
Long's short story was dramatized by David Belasco as a one-act play, "Madame Butterfly: A Tragedy of Japan" (1900). After premiering in New York, Belasco's play moved to London, where Puccini saw it in the summer of 1900.
The original version of the opera, in two acts, had its premiere on 17 February 1904 at La Scala in Milan. It was very poorly received despite the presence of such notable singers as soprano Rosina Storchio, tenor Giovanni Zenatello and baritone Giuseppe De Luca in the lead roles. This was due in large part to the late completion and inadequate time for rehearsals. Puccini revised the opera, splitting the second act into two acts and making other changes. On May 28, 1904, this version was performed in Brescia and was a huge success.
Between 1915 and 1920, Japan's best-known opera singer Tamaki Miura won international fame for her performances as Cio-Cio San. Her statue, along with that of Puccini, can be found in the Glover Garden in Nagasaki, the city where the opera is set.
"Madama Butterfly" is a staple of the standard operatic repertoire for companies around the world, ranking 6th in the Operabase list of the most-performed operas worldwide (Puccini's La Bohème and Tosca rank 3rd and 5th resp.) .
Production history.
Puccini wrote five versions of the opera; the original two-act version, which was presented at the world premiere at La Scala on 17 February 1904, was withdrawn after the disastrous premiere. Puccini then substantially rewrote it, this time in three acts. This second version was performed on 28 May 1904 in Brescia, where it was a great success. It was this second version that premiered in the United States in 1906, first in Washington, D.C., in October, and then in New York in November, performed by Henry Savage's New English Opera Company (so named because it performed in English-language translations).
In 1906, Puccini wrote a third version, which was performed at the Metropolitan Opera in New York. In 1907, Puccini made several changes in the orchestral and vocal scores, and this became the fourth version, which was performed in Paris.
In 1907, Puccini made his final revisions to the opera in a fifth version, which has become known as the "Standard Version" and is the one which is most often performed around the world. However, the original 1904 version is occasionally performed as well.
Performance history.
Premieres of the standard version in major opera houses throughout the world include those in Buenos Aires on 2 July 1904, this being the first performance in Argentina. Its first performance in Britain was in London on 10 July 1905 at the Royal Opera House, Covent Garden, while the first US performance was presented in English on October 15, 1906, in Washington, D.C., at the Columbia Theater. The first performance in New York took place on 12 November of the same year at the Garden Theater. The Metropolitan Opera first performed the work on February 11, 1907 in the presence of the composer with Geraldine Farrar as Cio-Cio San, Enrico Caruso as Pinkerton, Louise Homer as Suzuki, Antonio Scotti as Sharpless, and Arturo Vigna conducting. Three years later, the first Australian performance was presented at the Theatre Royal in Sydney on 26 March 1910, starring Amy Castles.
Synopsis.
Act 1.
In 1904, a U.S. Naval officer named Pinkerton rents a house on a hill in Nagasaki, Japan, for him and his soon-to-be wife, "Butterfly". Her real name is Ciocio-san, ("cio-cio", pronounced "chocho": the Japanese word for "butterfly" is "chō" 蝶). She is a 15-year-old Japanese girl whom he is marrying for convenience, since he intends to leave her once he finds a proper American wife, and since Japanese divorce laws are very lax. The wedding is to take place at the house. Butterfly had been so excited to marry an American that she had earlier secretly converted to Christianity. After the wedding ceremony, her uninvited uncle, a "bonze", who has found out about her conversion, comes to the house, curses her and orders all the guests to leave, which they do while renouncing her. Pinkerton and Butterfly sing a love duet and prepare to spend their first night together.
Act 2.
Three years later, Butterfly is still waiting for Pinkerton to return, as he had left shortly after their wedding. Her maid Suzuki keeps trying to convince her that he is not coming back, but Butterfly will not listen to her. Goro, the marriage broker who arranged her marriage, keeps trying to marry her off again, but she won't listen to him either. The American Consul, Sharpless, comes to the house with a letter which he has received from Pinkerton which asks him to break some news to Butterfly: that Pinkerton is coming back to Japan, but Sharpless cannot bring himself to finish it because Butterfly becomes very excited to hear that Pinkerton is coming back. Sharpless asks Butterfly what she would do if Pinkerton were not to return. She then reveals that she gave birth to Pinkerton's son after he had left and asks Sharpless to tell him.
From the hill house, Butterfly sees Pinkerton's ship arriving in the harbour. She and Suzuki prepare for his arrival, and then they wait. Suzuki and the child fall asleep, but Butterfly stays up all night waiting for him to arrive.
Act 3.
Suzuki wakes up in the morning and Butterfly finally falls asleep. Sharpless and Pinkerton arrive at the house, along with Pinkerton's new American wife, Kate. They have come because Kate has agreed to raise the child. But, as Pinkerton sees how Butterfly has decorated the house for his return, he realizes he has made a huge mistake. He admits that he is a coward and cannot face her, leaving Suzuki, Sharpless and Kate to break the news to Butterfly. Agreeing to give up her child if Pinkerton comes himself to see her, she then prays to statues of her ancestral gods, says goodbye to her son, and blindfolds him. She places a small American flag into his hands and goes behind a screen, cutting her throat with her father's hara-kiri knife. Pinkerton rushes in, but he is too late, and Butterfly dies.
Synopsis (musical numbers).
This is a synopsis of the standard version of the opera, with its arias, duets, trios, choruses, etc. The synopsis is organized into the 34 tracks that constitute most recordings.
Act 1.
1. A short orchestral prelude with a busy, fugal opening theme, followed by a second theme of more overtly Japanese character, leads straight into the opening scene.
2. "E soffitto e pareti" ("And ceiling and walls"). Pinkerton, a U.S. Naval Officer on USS "Abraham Lincoln", and Goro, a Japanese marriage broker, are inspecting a small house which sits on a hill and overlooks the bay. Goro has found the house for Pinkerton and his bride, and is showing him the house, with its sliding doors and small garden. The butler, the cook and the bride's maid, Suzuki, enter the garden and are introduced to Pinkerton. After they leave, Goro tells Pinkerton that everything is now ready and that his intended bride, a girl of 15 called Cio-Cio San (nicknamed Butterfly), will arrive soon, as will the American Consul, the marriage Registrar and all the bride's relatives, except her uncle. Her uncle is a priest and refuses to attend the wedding ceremony. Sharpless, the American Consul, has climbed up the hill from the city. He enters the garden, greets Pinkerton and Goro, and admires the view that overlooks Nagasaki's harbor and the sea. Pinkerton tells Sharpless that he has just purchased the little house for 999 years, with the right every month to cancel the agreement. Pinkerton explains that, in Japan, the law is very loose.
3. "Dovunque al mondo" ("Throughout the world"). As the orchestra plays the opening flourish to "The Star-Spangled Banner" (a musical theme which will characterize Pinkerton throughout the opera), Pinkerton tells Sharpless that, throughout the world, the Yankee wanderer is not satisfied until he captures the flowers of every shore and the love of every beautiful woman. "So I am marrying in the Japanese style: for 999 years, but with the right to cancel the marriage each month". Sharpless is critical of Pinkerton's beliefs, but they stand and agree, "America forever". Pinkerton tells Goro to bring Butterfly to him. When Goro leaves, Sharpless asks Pinkerton if he is really in love.
4. "Amore o grillo" ("Love or fancy"). Pinkerton admits to Sharpless that he does not know whether he is really in love or just infatuated, but he is bewitched with Butterfly's innocence, charm and beauty; she is like a butterfly fluttering around and then landing with silent grace, so beautiful "that I must have her, even though I injure her butterfly wings". Sharpless tells Pinkerton that he heard Butterfly speak, when she visited the Consulate, and he asks Pinkerton not to pluck off her delicate wings. However, Pinkerton tells Sharpless that he will do "no great harm, even if Butterfly falls in love." Sharpless takes his glass of whisky and offers a toast to Pinkerton's family at home, to which Pinkerton adds, "and to the day when I will have a real wedding and marry a real American bride." Goro re-enters to tell Pinkerton and Sharpless that Butterfly's friends are coming.
5. "Ancora un passo" ("One step more"). Butterfly can be heard guiding her friends to the top of the hill, jubilantly telling them that "Over land and sea, there floats the joyful breath of spring. I am the happiest girl in Japan, or rather in the world." Butterfly and her friends enter the garden. She recognizes Pinkerton and points him out to her friends, and all bow down before him.
6. "Gran ventura" ("May good fortune attend you"). Butterfly greets Pinkerton, who asks about her difficult climb up the hill. Butterfly says that, for a happy bride, the wait is even more difficult. Pinkerton thanks her for the compliment but cuts her off as she continues to compliment him further. Butterfly tells Pinkerton and Sharpless that her family is from Nagasaki and was once very wealthy.
7. "L'Imperial Commissario" ("The Imperial Commissioner"). Goro announces the arrival of both the Grand Commissioner and the Registrar of marriages. Butterfly greets her relatives, who have arrived for the wedding. Pinkerton laughs at the sight and whispers to Sharpless, "This is a farce: all these will be my new relatives for only a month." Sharpless tells him that, even though he considers the marriage contract a farce, she considers it very real. Meanwhile, Butterfly tells her relatives how much she loves Pinkerton. One of her cousins says that Goro first offered Pinkerton to her, but she refused. Butterfly's relatives say that he is like a king, so rich and so handsome, and then, at a sign from Butterfly, all her friends and relatives bow to Pinkerton and walk out to the garden. Pinkerton takes Butterfly's hand and leads her into the house.
8. "Vieni, amor mio!" ("Come, my love!"). From her sleeve, Butterfly brings out to show Pinkerton all of her treasures, which include only a few handkerchiefs, a mirror, a sash, and other trinkets. Then she shows him a long, narrow case, which she tells him holds her only sacred treasure, but she cannot open it, because there are too many people around. Goro whispers to Pinkerton that the case contains a "gift" from the Mikado to Butterfly's father, inviting him to commit seppuku. Butterfly continues to show Pinkerton her other little treasures, including several little statues: "They are the spirits of my ancestors."
9. "Ieri son salita tutta sola" ("Yesterday, I went all alone"). Butterfly tells Pinkerton that yesterday, in secret and without telling her uncle, who is a Buddhist priest, the Bonze, she went to the Consulate, where she abandoned her ancestral religion and converted to Pinkerton's religion. "I am following my destiny and, full of humility, bow to Mr. Pinkerton's God."
10. "Tutti zitti" ("Quiet everyone"). Everything is ready, and Goro tells everyone to be quiet. The Commissioner conducts the brief ceremony and witnesses Pinkerton and Butterfly sign the official papers.
11. "Madama Butterfly" ("Madam Butterfly"). The wedding celebration begins, and everyone wishes happiness to the new couple. After a short while, Sharpless pleads with Pinkerton not to be cruel, and he leaves with the Commissioner and the Registrar. Pinkerton, Butterfly and their guests continue the celebration with many toasts.
12. "Cio-Cio San!" ("Cio-Cio San"). The toasts are interrupted by an angry voice offstage, saying "Cio-Cio San! Cio-Cio San! You are damned." Butterfly's uncle, the Bonze, has discovered that Butterfly has renounced her ancestral religion, and he has arrived to deliver his curse. He stands over Butterfly, shouting his curses at her, when Pinkerton intervenes to stop him. The Bonze is shocked at the American, and he orders all the guests to leave with him, saying to Butterfly, "You have renounced us, and we renounce you." All the guests shout their renunciation as they rush away. The night is falling. Butterfly is weeping. Pinkerton consoles her.
13. "Bimba, Bimba, non piangere" ("Sweetheart, sweetheart, do not weep"). (This begins the famous long love duet, which ends act 1.) Pinkerton tells Butterfly that "All your relatives and all the priests in Japan are not worth the tears from your loving, beautiful eyes." Butterfly smiles through her tears, "You mean that? I won't cry any more. And I do not worry about their curses, because your words sound so sweet." They hear Suzuki offstage, saying her evening prayers.
13A. "Viene la sera" ("Night is falling"). (The long duet continues.) Pinkerton tells Butterfly that the "Night is falling", and Butterfly answers that "with it comes darkness and peace." Pinkerton claps his hands, and the three servants enter and close up the house. Then Suzuki helps Butterfly dress for her wedding night. Pinkerton watches Butterfly, as she watches him, but her happiness is tempered, as "still the angry voice curses me. Butterfly is renounced – renounced but happy".
14. "Bimba dagli occhi" ("Sweetheart, with eyes..."). (The long duet continues.) Pinkerton admires the beautiful Butterfly and tells her, "you have not yet told me that you love me." Butterfly replies that she does not want to say the words, "for fear of dying at hearing them!" She tells him that now she is happy.
15. "Vogliatemi bene" ("Love me, please."). (The long duet concludes.) Butterfly pleads with Pinkerton to "Love me, please." She asks whether it is true that, in foreign lands, a man will catch a butterfly and pin its wings to a table. Pinkerton admits that it is true but explains, "Do you know why? So that she'll not fly away." He embraces her and says, "I have caught you. You are mine." She replies, "Yes, for life."
Act 2.
16. "E Izaghi ed Izanami" ("And Izanagi and Izanami"). As the curtain opens, three years have passed. Suzuki kneels in front of a Buddha, praying that Butterfly will stop crying. Butterfly hears and tells her that the Japanese gods are fat and lazy, and that the American God will answer quickly, if only He knows where they are living. Suzuki tells Butterfly that their money has almost run out and, if Pinkerton does not return quickly, they will suffer in a bad way. Butterfly assures Suzuki that Pinkerton will return, because he took care to arrange for the Consul to pay the rent and to fit the house with locks to keep out the mosquitoes, relatives and troubles. Suzuki tells Butterfly that foreign husbands never return to their Japanese wives, but Butterfly replies furiously that Pinkerton had assured her, on the very last morning they were together, "Oh, Butterfly, my little wife, I shall return with the roses, when the earth is full of joy, when the robin makes his nest." Suzuki begins quietly to weep.
17. "Un bel dì" ("One beautiful day"). In this, the opera's most famous aria (and one of the most popular works in the soprano repertoire), Butterfly says that, "one beautiful day", they will see a puff of smoke on the far horizon. Then a ship will appear and enter the harbor. She will not go down to meet him but will wait on the hill for him to come. After a long time, she will see in the far distance a man beginning the walk out of the city and up the hill. When he arrives, he will call "Butterfly" from a distance, but she will not answer, partly for fun and partly not to die from the excitement of the first meeting. Then he will speak the names he used to call her: "Little one. Dear wife. Orange blossom." Butterfly promises Suzuki that this will happen. Suzuki departs, as Sharpless and Goro arrive in the garden.
18. "C'e. Entrate." ("She is there. Go in."). Sharpless greets her, "Excuse me, Madam Butterfly." Without looking to see who is speaking, Butterfly corrects him, "Madam Pinkerton, please." As she turns and sees that it is Sharpless who has spoken, she exclaims in happiness, "My very dear Consul. Welcome to this American home." Sharpless draws a letter from his pocket and tells her, "Benjamin Franklin Pinkerton has written to me." Sharpless tells her that Pinkerton is perfectly well, and she says, "I am the happiest woman in Japan." Butterfly asks him, "When do the robins make their nests in America?" The question confuses Sharpless, so Butterfly explains that Pinkerton promised to return to her "when the robin builds his nest again." She says that, in Japan, the robin has already built his nest three times, and she asks if "over there he nests less frequently." Sharpless, mortified, tells her that he does not know because he has not studied ornithology. At this, Butterfly hears Goro laugh, and she whispers to Sharpless that Goro is a bad man. She tells him that, after Pinkerton left, Goro came to her many times "with presents to palm off this or that husband on me." She says that Goro now wants her to agree to marry the wealthy man Yamadori, who then is arriving with his entourage to a musical accompaniment that quotes the same Japanese folk tune ("Miyasan") that Gilbert and Sullivan set as "Mi-ya sama" in "The Mikado".
19. "Yamadori, ancor le pene" ("Yamadori, are you not yet…"). Butterfly sees Yamadori and asks him if he is not going to give up pursuing her, because "You have already had many different wives." Yamadori admits that he married all of them, but says that he divorced them too. In the meantime, Sharpless gives up trying to read Pinkerton's letter to Butterfly, and he puts the letter back in his pocket. Goro tells Sharpless that Butterfly thinks that she is still married. Butterfly hears this and says, "I don"t think I am; I am." When Goro tries to tell her about the Japanese law of marriage, Butterfly interrupts and tells him that the Japanese law is not the law of her country, the United States. She tells Goro that she understands how easy divorce is under Japanese law, "but in America, you cannot do that." She turns sharply and asks Sharpless, "Am I correct?" Sharpless is embarrassed and must admit that she is correct. Butterfly turns triumphantly to Suzuki and asks that she serve tea. Yamadori, Sharpless and Goro quietly discuss Butterfly's blindness. Goro whispers that Pinkerton's ship is expected to arrive soon, and Sharpless explains that Pinkerton is too embarrassed to meet Butterfly and has asked Sharpless to handle it. Yamadori, offended, departs with his grand entourage and Goro. Sharpless remains, sits next to Butterfly, and takes the letter out of his pocket once more.
20. "Ora a noi." ("Now for us."). Sharpless begins to read Pinkerton's letter to Butterfly: "My friend, will you find that lovely flower of a girl…" Butterfly cannot control her happiness, as he continues, "since that happy times, three years have passed, and Butterfly perhaps does not remember me anymore." Butterfly looks at Suzuki and says, "I don't remember him? Suzuki, you tell him!" Sharpless continues, "If she still loves me, if she awaits me, I place myself in your hands so that you may carefully and considerately prepare her …" Butterfly exclaims, "He is coming! When? Soon! Soon!" Sharpless cannot bear to continue. He puts the letter away, muttering to himself, "that devil Pinkerton!" Sharpless asks her gently, "Butterfly, what would you do if he never returned?" Butterfly is shocked.
21. "Due cose potrei far" ("Two things I could do"). Butterfly cries that, if Pinkerton never returned, she would go back to entertaining people with her songs, or, better, die. Sharpless pleads with her to accept the rich offer from Yamadori. Butterfly is upset with Sharpless and instructs Suzuki to show him out. As he begins to leave, Butterfly stops him, apologizes for her anger, and explains that his questions have hurt her "so very, very much!" Then she goes into another room and returns, bringing with her the blonde-haired two-year-old boy who is her constant reminder of her American husband.
22. "Ah! M'ha scordata?" ("Ah! He has forgotten me?"). Butterfly shows Sharpless her child, and Sharpless asks if Pinkerton knows. Butterfly replies, "No. The child was born when he was away in his big country." She asks Sharpless to write and tell him that his son waits for him. "And then we shall see if he does not hurry over land and sea!" Butterfly kneels in front of her son and asks him, "Do you know that that gentleman had dared to think that your mother would take you in her arms and walk to town, through the wind and rain, to earn your bread and clothes. And she would stretch out her arms to the pitying crowd, crying ‘Listen! Listen to my sad song, For an unhappy mother, your charity. Take pity! And Butterfly – oh, horrible destiny – will dance for you! And as she used to do, the Geisha will sing for you. And her joyful, happy song will end in a sob!" She kneels in front of Sharpless and says that she will never do that, "that trade which leads to dishonor. Death! Death! Never more to dance! Rather would I cut short my life! Ah! Death!"
23. "Io scendo al piano." ("I will go now.") Sharpless finally says, "I will go now." Butterfly gives him her hand and this her child's. Sharpless asks the child his name, and Butterfly answers for him, "Today my name is Sorrow. But write and tell Daddy that, the day he returns, my name will be Joy." Sharpless promises to tell Pinkerton. Offstage, Suzuki can be heard shouting, "Snake. Damned toad!" Suzuki enters, pulling Goro with her, and she tells Butterfly, "He buzzes around, the snake. Every day he tells the four winds that no one knows who is the child's father!" Goro explains that, in America, when a child is born with a curse, he will always be rejected by everyone. In a rage, Butterfly runs to the shrine, seizes the dagger and threatens to stab him, "You are lying! You are lying! Say that again, and I will kill you!" Goro flees. Suzuki takes the child to the other room. Butterfly replaces the dagger, goes to her son and says, "You will see, my darling, my Sorrow. You will see, your savior will take us far, far away to his land."
24. "Il cannone del porto!" ("The cannon at the harbor!", often known as The Flower Duet). Just then a cannon shot is heard. Suzuki and Butterfly watch from the hill as the ship enters the harbor and drops anchor. Then Butterfly sees that the ship is the "Abraham Lincoln", and she tells Suzuki, "They were all lying! All of them! I alone knew. Only I, who love him." She continues, "My love, my faith, triumphs completely! He has returned, and he loves me!" She tells Suzuki to prepare a fragrant bath and asks how long she will have to wait for him. "An hour? Two hours, perhaps? The house must be filled with flowers. Everywhere. As the night is full of stars!" Butterfly tells Suzuki to gather all the flowers.
25. "Tutti i fior?" ("All the flowers?"). Suzuki asks, "All the flowers?" Butterfly says yes, all the flowers from all the bushes and plants and trees. "I want the whole fragrance of Spring in here." They continue to gather flowers and place them everywhere.
26. "Or vienmi ad adornar" ("Now come to adorn me"). Finally, Butterfly sits at her dressing table and tells Suzuki, "Now, come and adorn me. No, first bring me the child." She puts a touch of rouge on her own and on her child's cheeks and then, as Suzuki does her hair, asks her, "What will they say? My uncle, the priest? All so happy at my misery! And Yamadori, with his pursuit? Ridiculed, disgraced, made foolish, the hateful things!" Butterfly dons the same dress that she wore as a bride, while Suzuki dresses her child. Butterfly tells Suzuki that she wants Pinkerton to see her dressed as she was on the first day "and a red poppy in my hair."
27. "Coro a bocca chiusa" ("Humming Chorus"). As the off-stage chorus hums a wordless, melancholy tune, Butterfly, her child and Suzuki begin the long wait for Pinkerton to come. Night falls. Suzuki and the baby are soon asleep, but Butterfly keeps her vigil. (There is no intermission between acts 2 and 3 – the action continues without interruption as the "Humming Chorus" ends and morning light appears.)
Act 3.
28. "Oh eh! Oh eh!" ("Heave-ho! Heave-ho!"). Suzuki and the baby are asleep, but Butterfly remains standing and waiting. Distant voices are heard from the bay. Sailors are singing, "Heave-ho! Heave-ho!" The sun rises and fills Butterfly's house with light.
29. "Già il sole!" ("The Sun's come up!"). Suzuki awakes and is very sad. Butterfly tells her that "He will come." Then she carries her sleeping child into the other room and tells him to sleep, while she too falls asleep. Suzuki waits in the front room and hears a knock at the door. Pinkerton and Sharpless have arrived, but Pinkerton tells Suzuki not to wake Butterfly and asks how Butterfly knew that he had arrived. Suzuki tells him that, for the last three years, Butterfly has studied every ship that entered the port. Sharpless tells Pinkerton, "Did I not tell you so?" Suzuki sees a strange woman in the garden, learns from Sharpless that she is Pinkerton's American wife and collapses to her knees in shock.
30. "Io so che sue dolore" ("I know that her pain"). While Pinkerton looks at the flowers, the picture of himself and the room that has remained unchanged for three years, Sharpless tells Suzuki that they can do nothing for Butterfly but that they must help her child. Sharpless tells her that Pinkerton's new wife, Kate, wants to care for the child. Suzuki goes into the garden to meet Pinkerton's new wife, while Sharpless reminds Pinkerton, "I told you, didn't I? Do you remember? When she gave you her hand: 'Take care', I said, 'she believes in you'. She has been waiting for you." Pinkerton admits his wrong and leaves Sharpless to tell Butterfly the shameful news.
31. "Addio, fiorito asil" ("Farewell, flowery refuge"). Pinkerton says "Farewell, flowery refuge of happiness and of love, her gentle face will always haunt me, torturing me endlessly." He admits that he is a coward and cannot face her, and quickly leaves as Suzuki and Kate enter from the garden. Kate is telling Suzuki to assure Butterfly that Kate will look after her child like her own son.
32. "Suzuki! Suzuki!" ("Suzuki! Suzuki!"). From offstage, Butterfly calls for Suzuki and then enters the room. As she enters, Kate retreats to the garden, so that she will not be seen. She asks Suzuki why she is crying, and then she sees Sharpless and the woman in the garden. She tells Suzuki, "Suzuki, you are so kind. Do not cry. You love me so much. Tell me softly, just 'yes' or 'no' … Is he alive?" When Suzuki answers, "yes", Butterfly understands that Pinkerton is not coming for her and that Kate is his new wife. Butterfly realizes that she must give up her son, and Kate asks her forgiveness. Finally, Butterfly tells Kate, "I will give my child to her only if he comes himself. In half an hour, come up the hill again." Suzuki escorts Kate and Sharpless out, and Butterfly falls weeping.
33. "Come una mosca" ("Like a little fly"). Butterfly stands, sees Suzuki and tells her to close up the house, because it is too light and spring-like. Then she orders her to go to the other room where the child is playing. Butterfly then kneels before the statue of Buddha and prays to her ancestral gods. She rises, takes down her father's knife, kisses the blade, and reads the inscription.
34. "Con onor muore" ("To die with honor"). Butterfly reads the inscription on her father's knife: "Who cannot live with honor must die with honor." Butterfly's child enters, but Suzuki does not. Butterfly tells her child not to feel sorrow for his mother's desertion but to keep a faint memory of his mother's face. She bids him farewell, seats him on the floor and blindfolds him gently. She gives him a miniature American flag to wave in greeting to his father, which he does, blindfolded, throughout the following action. Butterfly takes the knife and walks behind the screen. The knife clatters to the floor as Butterfly staggers from behind the screen with a scarf around her neck. She kisses her child and collapses. From outside, Pinkerton cries, "Butterfly!" and rushes in – but it is too late: Butterfly is dead.
References.
Notes
Sources

</doc>
<doc id="38265" url="http://en.wikipedia.org/wiki?curid=38265" title="Viceroyalty of New Granada">
Viceroyalty of New Granada

 |style="vertical-align:bottom;"| 
 |  Brazil Colombia Ecuador Guyana Panama Peru Trinidad and Tobago Venezuela
The Viceroyalty of New Granada (Spanish: "Virreinato de la Nueva Granada") was the name given on 27 May 1717, to the jurisdiction of the Spanish Empire in northern South America, corresponding mainly to modern Colombia, Ecuador, Panama, and Venezuela. The territory corresponding to Panama was incorporated later in 1739. In addition to these core areas, the territory of the Viceroyalty of New Granada included Guyana, southwestern Suriname, parts of northwestern Brazil, northern Peru, Costa Rica and Nicaragua.
Colonial history.
Nearly two centuries after the establishment of the New Kingdom of Granada in the 16th century, whose governor was dependent upon the Viceroy of Peru at Lima, and an "audiencia" at Santa Fé de Bogotá (today capital of the republic of Colombia), the slowness of communications between the two capitals led to the creation of an independent Viceroyalty of New Granada in 1717 (and its reestablishment in 1739 after a short interruption). Other provinces corresponding to modern Ecuador, the eastern and southern parts of today's Venezuela, and Panama came together in a political unit under the jurisdiction of Bogotá, confirming that city as one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City. Sporadic attempts at reform were directed at increasing efficiency and centralizing authority, but control from Spain was never very effective.
The rough and diverse geography of northern South America and the limited range of proper roads made travel and communications within the viceroyalty difficult. The establishment of an autonomous Captaincy General in Caracas in 1777 and the preservation of the older Audiencia of Quito, nominally subject to the Viceroy but for most purposes independent, was a response to the necessities of effectively governing the peripheral regions. Some analysts also consider that these measures reflected a degree of local traditions that eventually contributed to the differing political and national differences among these territories once they became independent in the nineteenth century and which the unifying efforts of Simón Bolívar could not overcome.
Guajira rebellion.
The Wayuu had never been subjugated by the Spanish. The two groups were in a more or less permanent state of war. There had been rebellions in 1701 (when they destroyed a Capuchin mission), 1727 (when more than 2,000 Wayuus attacked the Spanish), 1741, 1757, 1761 and 1768. In 1718, Governor Soto de Herrera called them "barbarians, horse thieves, worthy of death, without God, without law and without a king". Of all the Indians in the territory of Colombia, the Wayuu were unique in having learned the use of firearms and horses.
In 1769 the Spanish took 22 Wayuus captive, in order to put them to work building the fortifications of Cartagena. The reaction of the Wayuus was unexpected. On 2 May 1769, at El Rincón, near Riohacha, they set their village afire, burning the church and two Spaniards who had taken refuge in it. They also captured the priest. The Spanish immediately dispatched an expedition from El Rincón to capture the Wayuus. At the head of this force was José Antonio de Sierra, a mestizo who had also headed the party that had taken the 22 Guajiro captives. The Guajiros recognized him and forced his party to take refuge in the house of the curate, which they then set afire. Sierra and eight of his men were killed.
This success was soon known in other Guajiro areas, and more men joined the revolt. According to Messía, at the peak there were 20,000 Wayuus under arms. Many had firearms acquired from English and Dutch smugglers, sometimes even from the Spanish. This enabled the rebels to take nearly all the settlements of the region, which they burned. According to the authorities, more than 100 Spaniards were killed and many others taken prisoner. Many cattle were also taken by the rebels. The Spaniards took refuge in Riohacha and sent urgent messages to Maracaibo, Valledupar, Santa Marta and Cartagena, the latter responding by sending 100 troops. The rebels themselves were not unified. Sierra's relatives among the Indians took up arms against the rebels to avenge his death. A battle between the two groups of Wayuus was fought at La Soledad. That and the arrival of the Spanish reinforcements caused the rebellion to fade away, but not before the Guajiro had regained much territory.
Demographics.
New Granada was estimated of having 4,345,000 inhabitants in 1819.
Main cities.
By population
Independent history.
The territories of the viceroyalty gained full de facto independence from Spain between 1819 and 1822 after a series of military and political struggles, uniting in a republic now known as Gran Colombia.
When Ecuador and Venezuela seceded from Gran Colombia, a "Republic of New Granada", with its capital at Bogotá, lasted from 1831 to 1856. The name "Colombia" reappeared in the "United States of Colombia", the new name for the country introduced by a liberal government after a civil war. The use of the term "New Granada" survived in conservative circles, such as among ecclesiastics.
As is typical in Spanish, older adjectives of places are used as demonyms for people from those areas. Today, it is typical in Spanish to refer to Colombians as "neogranadinos" ("New Granadians"), especially in neighboring Venezuela.

</doc>
<doc id="38300" url="http://en.wikipedia.org/wiki?curid=38300" title="Pancreas">
Pancreas

The pancreas is a glandular organ in the digestive system and endocrine system of vertebrates. In humans, it is located in the abdominal cavity behind the stomach. It is an endocrine gland producing several important hormones, including insulin, glucagon, somatostatin, and pancreatic polypeptide which circulate in the blood. The pancreas is also a digestive organ, secreting pancreatic juice containing digestive enzymes that assist digestion and absorption of nutrients in the small intestine. These enzymes help to further break down the carbohydrates, proteins, and lipids in the chyme.
Structure.
The pancreas is an endocrine organ that lies in the abdomen, specifically the upper left abdomen. It is found behind the stomach, with the head of the pancreas surrounded by the duodenum. The pancreas is about 15 cm (6 in) long.
Anatomically, the pancreas is divided into a "head", which rests within the concavity of the duodenum, a "body" lying behind the base of the stomach, and a "tail", which ends abutting the spleen. The "neck" of the pancreas lies between the body and head, and lies anterior to the superior mesenteric artery and vein. The head of the pancreas surrounds these two vessels, and a small "uncinate process" emerges from the lower part of the head, lying behind the superior mesenteric artery.
The pancreas is a secretory structure with a internal hormonal role (endocrine) and an external digestive role (exocrine). It has two main ducts, the main pancreatic duct, and the accessory pancreatic duct. These drain enzymes through the ampulla of Vater into the duodenum.
Margins.
The superior margin of pancreas is blunt and flat to the right; narrow and sharp to the left, near the tail.
It commences on the right in the tuber omentale, and is in relation with the celiac artery, from which the hepatic artery courses to the right just above the gland, while the lienal artery runs toward the left in a groove along this border.
The inferior margin of pancreas separates the posterior from the inferior surface; the superior mesenteric vessels emerge under its right extremity. 
The anterior margin of pancreas separates the anterior from the inferior surface of the pancreas, and along this border the two layers of the transverse mesocolon diverge from one another; one passing upward over the anterior surface, the other backward over the inferior surface.
Surfaces.
The inferior surface of pancreas is narrow on the right but broader on the left, and is covered by peritoneum; it lies upon the duodenojejunal flexure and on some coils of the jejunum; its left extremity rests on the left colic flexure.
The anterior surface of the pancreas faces the front of the abdomen. Most of the right half of this surface is in contact with the transverse colon, with only areolar tissue intervening.
From its upper part it joins to the neck of the pancreas at a well-marked prominence, the tuber omentale which abuts the lesser omentum. Its right limit being marked by a groove for the gastroduodenal artery.
The lower part of the right half, below the transverse colon, is covered by peritoneum continuous with the inferior layer of the transverse mesocolon, and is in contact with the coils of the small intestine.
The superior mesenteric artery passes down in front of the left half across the uncinate process; the superior mesenteric vein runs upward on the right side of the artery and, behind the neck, joins with the lienal vein to form the portal vein.
Blood supply.
The pancreas receives blood from branches of both the coeliac artery and superior mesenteric artery. The splenic artery runs along the top margin of the pancreas, and supplies the neck, body and tail of the pancreas through its pancreatic branches, the largest of which is called the greater pancreatic artery. The superior pancreaticoduodenal artery and inferior pancreaticoduodenal artery run along the anterior and posterior surfaces of the head of the pancreas at its border with the duodenum. These supply the head of the pancreas.
The body and neck of the pancreas drain into splenic vein; the head drains into the superior mesenteric and portal veins.
Histology.
The pancreas contains tissue with an endocrine and exocrine role, and this division is also visible when the pancreas is viewed under a microscope.
The tissues with an endocrine role can be seen under staining as lightly-stained clusters of cells, called islets of Langerhans.
Darker-staining cells form clusters called acini, which are arranged in lobes separated by a thin fibrous barrier. The secretory cells of each acinus surround a small "intercalated duct". Because of their secretory function, these cells have many small granules of zymogens that are visible. The intercalated ducts drains into larger ducts within the lobule, and finally "interlobular ducts". The ducts are lined by a single layer of columnar cells. With increasing diameter, several layers of columnar cells may be seen.
Variation.
The size of the pancreas varies considerably. Several anatomical variations exist, relating to the embryological development of the two buds of the pancreas. The pancreas develops as two buds on either side of the duodenum. The ventral bud eventually rotates to lie next to the dorsal bud, eventually fusing. If the two buds do not fuse, a pancreas may exist as two separate lobes. This is also called pancreatic divisum. If the ventral bud does not fully rotate, an annular pancreas may exist. This is where sections of the pancreas completely encircle the duodenum, and may even lead to duodenal atresia.
An accessory pancreatic duct may exist if the main duct of pancreas does not regress.
Development.
The pancreas forms from the embryonic foregut and is therefore of endodermal origin. Pancreatic development begins with the formation of a ventral and a dorsal bud, which are the anlages of the pancreas. Each structure communicates with the foregut through a duct. The dorsal pancreatic bud forms the head, body and tail, whereas the ventral pancreatic bud forms the uncinate process.
Differential rotation and fusion of the ventral and dorsal pancreatic buds results in the formation of the definitive pancreas. As the duodenum rotates to the right, it carries with it the ventral pancreatic bud and common bile duct. Upon reaching its final destination, the ventral pancreatic bud fuses with the much larger dorsal pancreatic bud. At this point of fusion, the main ducts of the ventral and dorsal pancreatic buds fuse, forming the main pancreatic duct. The duct of the dorsal bud regresses, leaving the main pancreatic duct.
Differentiation of cells of the pancreas proceeds through two different pathways, corresponding to the dual endocrine and exocrine functions of the pancreas. In progenitor cells of the exocrine pancreas, important molecules that induce differentiation include follistatin, fibroblast growth factors, and activation of the Notch receptor system. Development of the exocrine acini progresses through three successive stages. These are the predifferentiated, protodifferentiated, and differentiated stages, which correspond to undetectable, low, and high levels of digestive enzyme activity, respectively.
Progenitor cells of the endocrine pancreas arise from cells of the protodifferentiated stage of the exocrine pancreas. Under the influence of neurogenin-3 and ISL1, but in the absence of notch receptor signaling, these cells differentiate to form two lines of committed endocrine precursor cells. The first line, under the direction of a Pax gene, forms α- and γ- cells, which produce glucagon and pancreatic polypeptides, respectively. The second line, influenced by Pax-6, produces beta cells (β-) and delta cells (δ-), which secrete insulin and somatostatin, respectively.
Insulin and glucagon can be detected in the human fetal circulation by the fourth or fifth month of fetal development.
Function.
The pancreas is a dual-function gland, having features of both endocrine and exocrine glands.
Endocrine.
The part of the pancreas with endocrine function is made up of approximately a million cell clusters called islets of Langerhans. Four main cell types exist in the islets. They are relatively difficult to distinguish using standard staining techniques, but they can be classified by their secretion: α alpha cells secrete glucagon (increase glucose in blood), β beta cells secrete insulin (decrease glucose in blood), Δ delta cells secrete somatostatin (regulates/stops α and β cells) and PP cells, or γ (gamma) cells, secrete pancreatic polypeptide.
The islets are a compact collection of endocrine cells arranged in clusters and cords and are crisscrossed by a dense network of capillaries. The capillaries of the islets are lined by layers of endocrine cells in direct contact with vessels, and most endocrine cells are in direct contact with blood vessels, either by cytoplasmic processes or by direct apposition. According to the volume "The Body," by Alan E. Nourse, the islets are "busily manufacturing their hormone and generally disregarding the pancreatic cells all around them, as though they were located in some completely different part of the body." The islets of Langerhans play an imperative role in glucose metabolism and regulation of blood glucose concentration.
Exocrine.
The pancreas also functions as an exocrine gland that assists the digestive system. It secretes pancreatic fluid that contains digestive enzymes that pass to the small intestine. These enzymes help to further break down the carbohydrates, proteins and lipids (fats) in the chyme.
In humans, the secretory activity of the pancreas is regulated directly via the effect of hormones in the blood on the islets of Langerhans and indirectly through the effect of the autonomic nervous system on the blood flow.
The exocrine component of the pancreas, often called simply the exocrine pancreas, is the portion of the pancreas that performs exocrine functions. It has ducts that are arranged in clusters called "acini" (singular "acinus"). Pancreatic secretions are secreted into the lumen of the acinus, and then accumulate in intralobular ducts that drain to the main pancreatic duct, which drains directly into the duodenum.
Control of the exocrine function of the pancreas is via the hormones gastrin, cholecystokinin and secretin, which are hormones secreted by cells in the stomach and duodenum, in response to distension and/or food and which cause secretion of pancreatic juices.
There are two main classes of exocrine pancreatic secretions:
Pancreatic secretions from ductal cells contain bicarbonate ions and are alkaline in order to neutralize the acidic chyme that the stomach churns out.
The pancreas is also the main source of enzymes for digesting fats (lipids) and proteins. (The enzymes that digest polysaccharides, by contrast, are primarily produced by the walls of the intestines.)
The cells are filled with secretory granules containing the precursor digestive enzymes. 
The major proteases which the pancreas secretes are trypsinogen and chymotrypsinogen. Secreted to a lesser degree are pancreatic lipase and pancreatic amylase. The pancreas also secretes phospholipase A2, lysophospholipase, and cholesterol esterase.
The precursor enzymes (termed zymogens or proenzymes) are inactive variants of the enzymes; thus autodegradation, which can lead to pancreatitis, is avoided. Once released in the intestine, the enzyme enteropeptidase (formerly, and incorrectly, called enterokinase) present in the intestinal mucosa activates trypsinogen by cleaving it to form trypsin. The free trypsin then cleaves the rest of the trypsinogen, as well as chymotrypsinogen to its active form chymotrypsin.
Clinical relevance.
A puncture of the pancreas, which may lead to the secretion of digestive enzymes such as lipase and amylase into the abdominal cavity as well as subsequent pancreatic self-digestion and digestion and damage to organs within the abdomen, generally requires prompt and experienced medical intervention.
It is possible for one to live without a pancreas, provided that the person takes insulin for proper regulation of blood glucose concentration and pancreatic enzyme supplements to aid digestion.
Inflammation.
Inflammation of the pancreas is known as pancreatitis. Pancreatitis is most often associated with recurrent gallstones or chronic alcohol use, although a variety of other causes, including measles, mumps, some medications, the congenital condition alpha-1 antitrypsin deficiency and even some scorpion stings, may cause pancreatitis. Pancreatitis is likely to cause intense pain in the central abdomen, that often radiates to the back, and may be associated with jaundice. In addition, due to causing problems with fat digestion and bilirubin excretion, pancreatitis often presents with pale stools and dark urine.
In pancreatitis, enzymes of the exocrine pancreas damage the structure and tissue of the pancreas. Detection of some of these enyzmes, such as amylase and lipase in the blood, along with symptoms and findings on X-ray, are often used to indicate that a person has pancreatitis. A person with pancreatitis is also at risk of shock. Pancreatitis is often managed medically with analgesics, removal of gallstones or treatment of other causes, and monitoring to ensure a patient does not develop shock.
Cancer.
Pancreatic cancers, particularly the most common type, pancreatic adenocarcinoma, remain very difficult to treat, and are mostly diagnosed only at a stage that is too late for surgery, which is the only curative treatment. Pancreatic cancer is rare in those younger than 40, and the median age of diagnosis is 71. Risk factors include: smoking, obesity, diabetes, and certain rare genetic conditions including: multiple endocrine neoplasia type 1 and hereditary nonpolyposis colon cancer among others. About 25% of cases are attributable to tobacco smoking, while 5-10% of cases are linked to inherited genes.
There are several types of pancreatic cancer, involving both the endocrine and exocrine tissue. Pancreatic adenocarcinoma, which affects the exocrine part of the pancreas, is by far the most common form. The many types of pancreatic endocrine tumors are all uncommon or rare, and have varied outlooks. However the incidence of these cancers has been rising sharply; it is not clear to what extent this reflects increased detection, especially through medical imaging, of tumors that would be very slow to develop. Insulinomas (largely benign) and gastrinomas are the most common types. In the United States pancreatic cancer is the fourth most common cause of deaths due to cancer. The disease occurs more often in the developed world, which had 68% of new cases in 2012. Pancreatic adenocarcinoma typically has poor outcomes with the average percentage alive for at least one and five years after diagnosis being 25% and 5% respectively. In localized disease where the cancer is small (< 2 cm) the number alive at five years is approximately 20%. For those with neuroendocrine cancers the number alive after five years is much better at 65%, varying considerably with type.
Diabetes.
Type 1 diabetes.
Diabetes mellitus type 1 is a chronic autoimmune disorder in which the immune system attacks the insulin-secreting cells of the pancreas. People with type 1 diabetes therefore lack the insulin needed to keep blood sugar levels within optimal ranges. If left untreated, this leads to high blood sugar and the array of associated symptoms. Type 1 diabetes develops in people of all ages but is most often diagnosed before adulthood. For type 1 diabetics, insulin injections are critical for survival.
Type 2 diabetes.
Diabetes mellitus type 2 is the most common form of diabetes. The causes for high blood sugar in this form of diabetes usually are a combination of insulin resistance and impaired insulin secretion, with both genetic and environmental factors playing an important role in the development of the disease. The management of type 2 diabetes relies on a series of changes in diet and physical activity with the purpose of reducing blood sugar levels to normal ranges and increasing insulin sensitivity. Biguanides such as metformin are also used as part of the treatment along with insulin therapy.
History.
The pancreas was first identified for western civilization by Herophilus (335–280 BC), a Greek anatomist and surgeon. Only a few hundred years later, Rufus of Ephesus, another Greek anatomist, gave the pancreas its name. Etymologically, the term "pancreas", a modern Latin adaptation of Greek πάγκρεας, [πᾶν ("all", "whole"), and κρέας ("flesh")], originally means sweetbread, although literally meaning all-flesh, presumably because of its fleshy consistency. It was only in 1889 when Oskar Minkowski discovered that removing the pancreas from a dog caused it to become diabetic (insulin was later discovered by Frederick Banting and Charles Herbert Best in 1921).
Other animals.
Pancreatic tissue is present in all vertebrate species, but its precise form and arrangement vary widely. There may be up to three separate pancreases, two of which arise from ventral buds, and the other dorsally. In most species (including humans), these fuse in the adult, but there are several exceptions. Even when a single pancreas is present, two or three pancreatic ducts may persist, each draining separately into the duodenum (or equivalent part of the foregut). Birds, for example, typically have three such ducts.
In teleosts, and a few other species (such as rabbits), there is no discrete pancreas at all, with pancreatic tissue being distributed diffusely across the mesentery and even within other nearby organs, such as the liver or spleen. In a few teleost species, the endocrine tissue has fused to form a distinct gland within the abdominal cavity, but otherwise it is distributed among the exocrine components. The most primitive arrangement, however, appears to be that of lampreys and lungfish, in which pancreatic tissue is found as a number of discrete nodules within the wall of the gut itself, with the exocrine portions being little different from other glandular structures of the intestine.

</doc>
<doc id="38310" url="http://en.wikipedia.org/wiki?curid=38310" title="Cannabis">
Cannabis

Cannabis () is a genus of flowering plants that includes three different species, "Cannabis sativa", "Cannabis indica" and "Cannabis ruderalis". These species are indigenous to Central and South Asia. "Cannabis" has long been used for hemp fibre, for hemp oils, for medicinal purposes, and as a recreational drug. Industrial hemp products are made from "Cannabis" plants selected to produce an abundance of fiber. To satisfy the UN Narcotics Convention, some "Cannabis" strains have been bred to produce minimal levels of tetrahydrocannabinol (THC), the principal psychoactive constituent responsible for the high associated with it and which is obtained through the dried flowers of "Cannabis" plants selectively bred to produce high levels of THC and other psychoactive cannabinoids. Various extracts including hashish and hash oil are also produced from the plant.
Etymology.
The word "cannabis" is from Greek κάνναβις ("kánnabis") (see Latin "cannabis"), which was originally Scythian or Thracian. It is related to the Persian "kanab", the English "canvas" and possibly even to the English "hemp" (Old English "hænep"). In modern Hebrew, קַנַּבּוֹס "qannabōs" ( ]) is used but there are those who have theorized that it was referred to in antiquity as קני בושם "q'nei bosem", a component of the biblical anointing oil. Old Akkadian "qunnabtu", Neo-Assyrian and Neo-Babylonian "qunnabu" were used to refer to the plant meaning "a way to produce smoke."
Description.
"Cannabis" is an annual, dioecious, flowering herb. The leaves are palmately compound or digitate, with serrate leaflets. The first pair of leaves usually have a single leaflet, the number gradually increasing up to a maximum of about thirteen leaflets per leaf (usually seven or nine), depending on variety and growing conditions. At the top of a flowering plant, this number again diminishes to a single leaflet per leaf. The lower leaf pairs usually occur in an opposite leaf arrangement and the upper leaf pairs in an alternate arrangement on the main stem of a mature plant.
The leaves have a peculiar and diagnostic venation pattern that enables persons poorly familiar with the plant to distinguish a "Cannabis" leaf from unrelated species that have confusingly similar leaves (see illustration). As is common in serrated leaves, each serration has a central vein extending to its tip. However, the serration vein originates from lower down the central vein of the leaflet, typically opposite to the position of, not the first notch down, but the next notch. This means that on its way from the midrib of the leaflet to the point of the serration, the vein serving the tip of the serration passes close by the intervening notch. Sometimes the vein will actually pass tangent to the notch, but often it will pass by at a small distance, and when that happens a spur vein (occasionally a pair of such spur veins) branches off and joins the leaf margin at the deepest point of the notch. This venation pattern varies slightly among varieties, but in general it enables one to tell "Cannabis" leaves from superficially similar leaves without difficulty and without special equipment. Tiny samples of "Cannabis" plants also can be identified with precision by microscopic examination of leaf cells and similar features, but that requires special expertise and equipment.
The plant is believed to have originated in the mountainous regions northwest of the Himalayas. It is also known as hemp, although this term is often used to refer only to varieties of "Cannabis" cultivated for non-drug use.
Reproduction.
"Cannabis" normally has imperfect flowers, with staminate "male" and pistillate "female" flowers occurring on separate plants. It is not unusual, however, for individual plants to bear both male and female flowers. Although monoecious plants are often referred to as "hermaphrodites," true hermaphrodites (which are less common) bear staminate and pistillate structures on individual flowers, whereas monoecious plants bear male and female flowers at different locations on the same plant. Male flowers are normally borne on loose panicles, and female flowers are borne on racemes. "At a very early period the Chinese recognized the "Cannabis" plant as dioecious," and the (c. 3rd century BCE) "Erya" dictionary defined "xi" "male "Cannabis"" and "fu" (or "ju" ) "female "Cannabis".
All known strains of "Cannabis" are wind-pollinated and the fruit is an achene. Most strains of "Cannabis" are short day plants, with the possible exception of "C. sativa" subsp. "sativa" var. "spontanea" (= "C. ruderalis"), which is commonly described as "auto-flowering" and may be day-neutral.
Biochemistry and drugs.
"Cannabis" plants produce a group of chemicals called cannabinoids, which produce mental and physical effects when consumed.
Cannabinoids, terpenoids, and other compounds are secreted by glandular trichomes that occur most abundantly on the floral calyxes and bracts of female plants. As a drug it usually comes in the form of dried flower buds (marijuana), resin (hashish), or various extracts collectively known as hashish oil. In the early 20th century, it became illegal in most of the world to cultivate or possess "Cannabis" for sale or personal use.
Chromosomes and genome.
"Cannabis", like many organisms, is diploid, having a chromosome complement of 2n=20, although polyploid individuals have been artificially produced. The first genome sequence of "Cannabis", which is estimated to be 820 Mb in size, was published in 2011 by a team of Canadian scientists.
Taxonomy.
The genus "Cannabis" was formerly placed in the Nettle (Urticaceae) or Mulberry (Moraceae) family, and later, along with the "Humulus" genus (hops), in a separate family, the Hemp family (Cannabaceae sensu stricto). Recent phylogenetic studies based on cpDNA restriction site analysis and gene sequencing strongly suggest that the Cannabaceae sensu stricto arose from within the former Celtidaceae family, and that the two families should be merged to form a single monophyletic family, the Cannabaceae sensu lato.
Various types of "Cannabis" have been described, and variously classified as species, subspecies, or varieties:
"Cannabis" plants produce a unique family of terpeno-phenolic compounds called cannabinoids, which produce the "high" one experiences from consuming marijuana. There are 483 identifiable chemical constituents known to exist in the cannabis plant, and at least 85 different cannabinoids have been isolated from the plant. The two cannabinoids usually produced in greatest abundance are cannabidiol (CBD) and/or Δ9-tetrahydrocannabinol (THC), but only THC is psychoactive. Since the early 1970s, "Cannabis" plants have been categorized by their chemical phenotype or "chemotype," based on the overall amount of THC produced, and on the ratio of THC to CBD. Although overall cannabinoid production is influenced by environmental factors, the THC/CBD ratio is genetically determined and remains fixed throughout the life of a plant. Non-drug plants produce relatively low levels of THC and high levels of CBD, while drug plants produce high levels of THC and low levels of CBD. When plants of these two chemotypes cross-pollinate, the plants in the first filial (F1) generation have an intermediate chemotype and produce similar amounts of CBD and THC. Female plants of this chemotype may produce enough THC to be utilized for drug production.
Whether the drug and non-drug, cultivated and wild types of "Cannabis" constitute a single, highly variable species, or the genus is polytypic with more than one species, has been a subject of debate for well over two centuries. This is a contentious issue because there is no universally accepted definition of a species. One widely applied criterion for species recognition is that species are "groups of actually or potentially interbreeding natural populations which are reproductively isolated from other such groups." Populations that are physiologically capable of interbreeding, but morphologically or genetically divergent and isolated by geography or ecology, are sometimes considered to be separate species. Physiological barriers to reproduction are not known to occur within "Cannabis", and plants from widely divergent sources are interfertile. However, physical barriers to gene exchange (such as the Himalayan mountain range) might have enabled "Cannabis" gene pools to diverge before the onset of human intervention, resulting in speciation. It remains controversial whether sufficient morphological and genetic divergence occurs within the genus as a result of geographical or ecological isolation to justify recognition of more than one species.
History of cannabis.
"Cannabis sativa" appears naturally in many tropical and humid parts of the world. Its use as a mind-altering drug has been documented by archaeological finds in prehistoric societies in Euro-Asia and Africa.
The oldest written record of cannabis usage is the Greek historian Herodotus's reference to the central Eurasian Scythians taking cannabis steam baths. His (c. 440 BCE) "Histories" records, "The Scythians, as I said, take some of this hemp-seed [presumably, flowers], and, creeping under the felt coverings, throw it upon the red-hot stones; immediately it smokes, and gives out such a vapour as no Grecian vapour-bath can exceed; the Scyths, delighted, shout for joy." Classical Greeks and Romans were using cannabis, while in the Middle East, use spread throughout the Islamic empire to North Africa. In 1545 cannabis spread to the western hemisphere where Spaniards imported it to Chile for its use as fiber. In North America cannabis, in the form of hemp, was grown for use in rope, clothing and paper.
Early classifications.
The "Cannabis" genus was first classified using the "modern" system of taxonomic nomenclature by Carolus Linnaeus in 1753, who devised the system still in use for the naming of species. He considered the genus to be monotypic, having just a single species that he named "Cannabis sativa" L. (L. stands for Linnaeus, and indicates the authority who first named the species). Linnaeus was familiar with European hemp, which was widely cultivated at the time. In 1785, noted evolutionary biologist Jean-Baptiste de Lamarck published a description of a second species of "Cannabis", which he named "Cannabis indica" Lam. Lamarck based his description of the newly named species on plant specimens collected in India. He described "C. indica" as having poorer fiber quality than "C. sativa", but greater utility as an inebriant. Additional "Cannabis" species were proposed in the 19th century, including strains from China and Vietnam (Indo-China) assigned the names "Cannabis chinensis" Delile, and "Cannabis gigantea" Delile ex Vilmorin. However, many taxonomists found these putative species difficult to distinguish. In the early 20th century, the single-species concept was still widely accepted, except in the Soviet Union where "Cannabis" continued to be the subject of active taxonomic study. The name "Cannabis indica" was listed in various Pharmacopoeias, and was widely used to designate "Cannabis" suitable for the manufacture of medicinal preparations.
20th century.
In 1924, Russian botanist D.E. Janichevsky concluded that ruderal "Cannabis" in central Russia is either a variety of "C. sativa" or a separate species, and proposed "C. sativa" L. var. "ruderalis" Janisch. and "Cannabis ruderalis" Janisch. as alternative names. In 1929, renowned plant explorer Nikolai Vavilov assigned wild or feral populations of "Cannabis" in Afghanistan to "C. indica" Lam. var. "kafiristanica" Vav., and ruderal populations in Europe to "C. sativa" L. var. "spontanea" Vav. In 1940, Russian botanists Serebriakova and Sizov proposed a complex classification in which they also recognized "C. sativa" and "C. indica" as separate species. Within "C. sativa" they recognized two subspecies: "C. sativa" L. subsp. "culta" Serebr. (consisting of cultivated plants), and "C. sativa" L. subsp. "spontanea" (Vav.) Serebr. (consisting of wild or feral plants). Serebriakova and Sizov split the two "C. sativa" subspecies into 13 varieties, including four distinct groups within subspecies "culta". However, they did not divide "C. indica" into subspecies or varieties. This excessive splitting of "C. sativa" proved too unwieldy, and never gained many adherents.
In the 1970s, the taxonomic classification of "Cannabis" took on added significance in North America. Laws prohibiting "Cannabis" in the United States and Canada specifically named products of "C. sativa" as prohibited materials. Enterprising attorneys for the defense in a few drug busts argued that the seized "Cannabis" material may not have been "C. sativa", and was therefore not prohibited by law. Attorneys on both sides recruited botanists to provide expert testimony. Among those testifying for the prosecution was Dr. Ernest Small, while Dr. Richard E. Schultes and others testified for the defense. The botanists engaged in heated debate (outside of court), and both camps impugned the other's integrity. The defense attorneys were not often successful in winning their case, because the intent of the law was clear.
In 1976, Canadian botanist Ernest Small and American taxonomist Arthur Cronquist published a taxonomic revision that recognizes a single species of "Cannabis" with two subspecies: "C. sativa" L. subsp. "sativa", and "C. sativa" L. subsp. "indica" (Lam.) Small & Cronq. The authors hypothesized that the two subspecies diverged primarily as a result of human selection; "C. sativa" subsp. "sativa" was presumably selected for traits that enhance fiber or seed production, whereas "C. sativa" subsp. "indica" was primarily selected for drug production. Within these two subspecies, Small and Cronquist described "C. sativa" L. subsp. "sativa" var. "spontanea" Vav. as a wild or escaped variety of low-intoxicant "Cannabis", and "C. sativa" subsp. "indica" var. "kafiristanica" (Vav.) Small & Cronq. as a wild or escaped variety of the high-intoxicant type. This classification was based on several factors including interfertility, chromosome uniformity, chemotype, and numerical analysis of phenotypic characters.
Professors William Emboden, Loran Anderson, and Harvard botanist Richard E. Schultes and coworkers also conducted taxonomic studies of "Cannabis" in the 1970s, and concluded that stable morphological differences exist that support recognition of at least three species, "C. sativa", "C. indica", and "C. ruderalis." For Schultes, this was a reversal of his previous interpretation that "Cannabis" is monotypic, with only a single species. According to Schultes' and Anderson's descriptions, "C. sativa" is tall and laxly branched with relatively narrow leaflets, "C. indica" is shorter, conical in shape, and has relatively wide leaflets, and "C. ruderalis" is short, branchless, and grows wild in central Asia. This taxonomic interpretation was embraced by "Cannabis" aficionados who commonly distinguish narrow-leafed "sativa" drug strains from wide-leafed "indica" drug strains.
Continuing research.
Molecular analytical techniques developed in the late 20th century are being applied to questions of taxonomic classification. This has resulted in many reclassifications based on evolutionary systematics. Several studies of Random Amplified Polymorphic DNA (RAPD) and other types of genetic markers have been conducted on drug and fiber strains of "Cannabis", primarily for plant breeding and forensic purposes. Dutch "Cannabis" researcher E.P.M. de Meijer and coworkers described some of their RAPD studies as showing an "extremely high" degree of genetic polymorphism between and within populations, suggesting a high degree of potential variation for selection, even in heavily selected hemp cultivars. They also commented that these analyses confirm the continuity of the "Cannabis" gene pool throughout the studied accessions, and provide further confirmation that the genus comprises a single species, although theirs was not a systematic study "per se".
Karl W. Hillig, a graduate student in the laboratory of long-time "Cannabis" researcher Paul G. Mahlberg at Indiana University, conducted a systematic investigation of genetic, morphological, and chemotaxonomic variation among 157 "Cannabis" accessions of known geographic origin, including fiber, drug, and feral populations. In 2004, Hillig and Mahlberg published a chemotaxomic analysis of cannabinoid variation in their "Cannabis" germplasm collection. They used gas chromatography to determine cannabinoid content and to infer allele frequencies of the gene that controls CBD and THC production within the studied populations, and concluded that the patterns of cannabinoid variation support recognition of "C. sativa" and "C. indica" as separate species, but not "C. ruderalis." The authors assigned fiber/seed landraces and feral populations from Europe, central Asia, and Asia Minor to "C. sativa". Narrow-leaflet and wide-leaflet drug accessions, southern and eastern Asian hemp accessions, and feral Himalayan populations were assigned to "C. indica". In 2005, Hillig published a genetic analysis of the same set of accessions (this paper was the first in the series, but was delayed in publication), and proposed a three-species classification, recognizing "C. sativa", "C. indica", and (tentatively) "C. ruderalis". In his doctoral dissertation published the same year, Hillig stated that principal components analysis of phenotypic (morphological) traits failed to differentiate the putative species, but that canonical variates analysis resulted in a high degree of discrimination of the putative species and infraspecific taxa. Another paper in the series on chemotaxonomic variation in the terpenoid content of the essential oil of "Cannabis" revealed that several wide-leaflet drug strains in the collection had relatively high levels of certain sesquiterpene alcohols, including guaiol and isomers of eudesmol, that set them apart from the other putative taxa. Hillig concluded that the patterns of genetic, morphological, and chemotaxonomic variation support recognition of "C. sativa" and "C. indica" as separate species. He also concluded there is little support to treat "C. ruderalis" as a separate species from "C. sativa" at this time, but more research on wild and weedy populations is needed because they were underrepresented in their collection.
In September 2005, New Scientist reported that researchers at the Canberra Institute of Technology had identified a new type of "Cannabis" based on analysis of mitochondrial and chloroplast DNA. The New Scientist story, which was picked up by many news agencies and web sites, indicated that the research was to be published in the journal "Forensic Science International".
Popular usage.
The scientific debate regarding taxonomy has had little effect on the terminology in widespread use among cultivators and users of drug-type "Cannabis". "Cannabis" aficionados recognize three distinct types based on such factors as morphology, native range, aroma, and subjective psychoactive characteristics. "Sativa" is the most widespread variety, which is usually tall, laxly branched, and found in warm lowland regions. "Indica" designates shorter, bushier plants adapted to cooler climates and highland environments. "Ruderalis" is the informal name for the short plants that grow wild in Europe and central Asia.
Breeders, seed companies, and cultivators of drug type "Cannabis" often describe the ancestry or gross phenotypic characteristics of cultivars by categorizing them as "pure indica," "mostly indica," "indica/sativa," "mostly sativa", or "pure sativa."
Reproduction.
Breeding systems.
"Cannabis" is predominantly dioecious, although many monoecious varieties have been described. Subdioecy (the occurrence of monoecious individuals and dioecious individuals within the same population) is widespread. Many populations have been described as sexually labile.
As a result of intensive selection in cultivation, "Cannabis" exhibits many sexual phenotypes that can be described in terms of the ratio of female to male flowers occurring in the individual, or typical in the cultivar. Dioecious varieties are preferred for drug production, where typically the female flowers are used. Dioecious varieties are also preferred for textile fiber production, whereas monoecious varieties are preferred for pulp and paper production. It has been suggested that the presence of monoecy can be used to differentiate licit crops of monoecious hemp from illicit drug crops. However, "sativa" strains often produce monoecious individuals, probably as a result of inbreeding.
Mechanisms of sex determination.
"Cannabis" has been described as having one of the most complicated mechanisms of sex determination among the dioecious plants. Many models have been proposed to explain sex determination in "Cannabis".
Based on studies of sex reversal in hemp, it was first reported by K. Hirata in 1924 that an XY sex-determination system is present. At the time, the XY system was the only known system of sex determination. The X:A system was first described in Drosophila spp in 1925. Soon thereafter, Schaffner disputed Hirata's interpretation, and published results from his own studies of sex reversal in hemp, concluding that an X:A system was in use and that furthermore sex was strongly influenced by environmental conditions.
Since then, many different types of sex determination systems have been discovered, particularly in plants. Dioecy is relatively uncommon in the plant kingdom, and a very low percentage of dioecious plant species have been determined to use the XY system. In most cases where the XY system is found it is believed to have evolved recently and independently.
Since the 1920s, a number of sex determination models have been proposed for "Cannabis". Ainsworth describes sex determination in the genus as using "an X/autosome dosage type".
The question of whether heteromorphic sex chromosomes are indeed present is most conveniently answered if such chromosomes were clearly visible in a karyotype. "Cannabis" was one of the first plant species to be karyotyped; however, this was in a period when karyotype preparation was primitive by modern standards (see History of Cytogenetics). Heteromorphic sex chromosomes were reported to occur in staminate individuals of dioecious "Kentucky" hemp, but were not found in pistillate individuals of the same variety. Dioecious "Kentucky" hemp was assumed to use an XY mechanism. Heterosomes were not observed in analyzed individuals of monoecious "Kentucky" hemp, nor in an unidentified German cultivar. These varieties were assumed to have sex chromosome composition XX. According to other researchers, no modern karyotype of "Cannabis" had been published as of 1996. Proponents of the XY system state that Y chromosome is slightly larger than the X, but difficult to differentiate cytologically.
More recently, Sakamoto and various co-authors have used RAPD to isolate several genetic marker sequences that they name Male-Associated DNA in "Cannabis" (MADC), and which they interpret as indirect evidence of a male chromosome. Several other research groups have reported identification of male-associated markers using RAPD and AFLP. Ainsworth commented on these findings, stating,"It is not surprising that male-associated markers are relatively abundant. In dioecious plants where sex chromosomes have not been identified, markers for maleness indicate either the presence of sex chromosomes which have not been distinguished by cytological methods or that the marker is tightly linked to a gene involved in sex determination. "
Environmental sex determination is known to occur in a variety of species. Many researchers have suggested that sex in "Cannabis" is determined or strongly influenced by environmental factors. Ainsworth reviews that treatment with auxin and ethylene have feminizing effects, and that treatment with cytokinins and gibberellins have masculinizing effects. It has been reported that sex can be reversed in "Cannabis" using chemical treatment. A PCR-based method for the detection of female-associated DNA polymorphisms by genotyping has been developed.
Industrial and personal uses.
"Cannabis" is used for a wide variety of purposes.
Hemp.
The term "hemp" is used to name the durable soft fiber from the "Cannabis" plant stem (stalk). "Cannabis sativa" cultivars are used for fibers due to their long stems; Sativa varieties may grow more than six metres tall. However, "hemp" can refer to any industrial or foodstuff product that is not intended for use as a drug. Many countries regulate limits for psychoactive compound (THC) concentrations in products labeled as hemp.
Hemp is valuable in tens of thousands of commercial products, especially as fibre ranging from paper, cordage, construction material and textiles in general, to clothing. Hemp is stronger and longer-lasting than cotton. It also is a useful source of foodstuffs (hemp milk, hemp seed, hemp oil) and biofuels. Hemp has been used by many civilizations, from China to Europe (and later North America) during the last 12,000 years. In modern times novel applications and improvements have been explored with modest commercial success.
Psychoactive drug.
Cannabis is a popular recreational drug around the world, only behind alcohol, caffeine and tobacco. In the United States alone, it is believed that over 100 million Americans have tried cannabis, with 25 million Americans having used it within the past year.
The psychoactive effects of "Cannabis" are known to have a biphasic nature. Primary psychoactive effects include a state of relaxation, and to a lesser degree, euphoria from its main psychoactive compound, tetrahydrocannabinol. Secondary psychoactive effects, such as a facility for philosophical thinking, introspection and metacognition have been reported amongst cases of anxiety and paranoia. Finally, the tertiary psychoactive effects of the drug cannabis, can include an increase in heart rate and hunger, believed to be caused by 11-OH-THC, a psychoactive metabolite of THC produced in the liver.
Normal cognition is restored after approximately three hours for larger doses via a smoking pipe, bong or vaporizer. However, if a large amount is taken orally the effects may last much longer. After 24 hours to a few days, minuscule psychoactive effects may be felt, depending on dosage, frequency and tolerance to the drug.
Various forms of the drug cannabis exist, including extracts such as hashish and hash oil which, because of appearance, are more susceptible to adulterants when left unregulated.
Cannabidiol (CBD), which has no psychotropic effects by itself (although sometimes showing a small stimulant effect, similar to caffeine), attenuates, or reduces the higher anxiety levels caused by THC alone.
According to Delphic analysis by British researchers in 2007, cannabis has a lower risk factor for dependence compared to both nicotine and alcohol. However, everyday use of Cannabis can in some cases be correlated with psychological withdrawal symptoms such as irritability and insomnia, and evidence could suggest that if a user experiences stress, the likeliness of getting a panic attack increases because of an increase of THC metabolites. However, cannabis withdrawal symptoms are typically mild and are never life-threatening.
Medical use.
"Medical cannabis" (or "medical marijuana") refers to the use of cannabis and its constituent cannabinoids, such as tetrahydrocannabinol (THC), as medical therapy to treat disease or alleviate symptoms. The "Cannabis" plant has a history of medicinal use dating back thousands of years across many cultures.
Cannabis has been used to reduce nausea and vomiting in chemotherapy and people with AIDS, and to treat pain and muscle spasticity; its use for other medical applications has been studied but there is insufficient data for conclusions about safety and efficacy. Short-term use increases minor adverse effects, but does not appear to increase major adverse effects. Long-term effects are not clear, and there are safety concerns including memory and cognition problems, risk for dependence and the risk of children taking it by accident.
Ancient and religious uses.
The Yanghai Tombs, a vast ancient cemetery (54 000 m2) situated in the Turfan district of the Xinjiang Uyghur Autonomous Region of the People's Republic of China, have revealed the 2700-year-old grave of a shaman. He is thought to have belonged to the Jushi culture recorded in the area centuries later in the "Hanshu", Chap 96B. Near the head and foot of the shaman was a large leather basket and wooden bowl filled with 789g of cannabis, superbly preserved by climatic and burial conditions. An international team demonstrated that this material contained tetrahydrocannabinol, the psychoactive component of cannabis. The cannabis was presumably employed by this culture as a medicinal or psychoactive agent, or an aid to divination. This is the oldest documentation of cannabis as a pharmacologically active agent.
Settlements which date from "c". 2200–1700 BCE in the Bactria and Margiana contained elaborate ritual structures with rooms containing everything needed for making drinks containing extracts from poppy (opium), hemp (cannabis), and ephedra (which contains ephedrine)."While we have no evidence of the use of ephedra among the steppe tribes, we have already seen that they did share in the cultic use of hemp, a practice that ranged from Romania east to the Yenisei River from at least the 3rd millennium BC onwards where its use was later encountered in the apparatus for smoking hemp found at Pazyryk."
"Cannabis" is first referred to in Hindu Vedas between 2000 and 1400 BCE, in the "Atharvaveda". By the 10th century CE, it has been suggested that it was referred to by some in India as "food of the gods". Cannabis use eventually became a ritual part of the Hindu festival of Holi.
In Buddhism, cannabis is generally regarded as an intoxicant and may be a hindrance to development of meditation and clear awareness. In ancient Germanic culture, "Cannabis" was associated with the Norse love goddess, Freya. An anointing oil mentioned in Exodus is, by some translators, said to contain "Cannabis". Sufis have used "Cannabis" in a spiritual context since the 13th century CE.
In modern times the Rastafari movement has embraced "Cannabis" as a sacrament. Elders of the Ethiopian Zion Coptic Church, a religious movement founded in the United States in 1975 with no ties to either Ethiopia or the Coptic Church, consider "Cannabis" to be the Eucharist, claiming it as an oral tradition from Ethiopia dating back to the time of Christ. Like the Rastafari, some modern Gnostic Christian sects have asserted that "Cannabis" is the Tree of Life. Other organized religions founded in the 20th century that treat "Cannabis" as a sacrament are the THC Ministry, Cantheism, the Cannabis Assembly and the Church of Cognizance. Rastafarians tend to be among the biggest consumers of modern Cannabis use.
Further reading.
</dl>

</doc>
<doc id="38373" url="http://en.wikipedia.org/wiki?curid=38373" title="Risk (game)">
Risk (game)

Risk is a strategy board game produced by Parker Brothers (now a division of Hasbro). Winning Moves also makes a classic 1959 version. It was invented by French film director Albert Lamorisse and originally released in 1957 as La Conquête du Monde ("The Conquest of the World") in France. It was later bought by Parker Brothers and released in 1959 with some modifications to the rules as "Risk: The Continental Game", then as Risk: The Game of Global Domination.
"Risk" is a turn-based game for two to six players. The standard version is played on a board depicting a political map of the Earth, divided into forty-two territories, which are grouped into six continents. The object of the game is to occupy every territory on the board and in doing so, eliminate the other players. Players control armies with which they attempt to capture territories from other players, with results determined by dice rolls.
Equipment and design.
Each "Risk" game comes with a number of different colored tokens denoting troops (originally, one set each of black, blue, green, pink, red and yellow). In the first editions, the playing pieces were wooden cubes representing one troop each and a few rounded triangular prisms representing ten troops each, but in later versions of the game these pieces were molded of plastic to reduce costs. In the 1980s, these were changed to pieces shaped into the Roman numerals I, III, V, and X. The 1993 edition introduced plastic infantry tokens (representing a single unit), cavalry (representing five units), and artillery (representing ten units). The 40th Anniversary Collector's Edition contained the same troop pieces but made of metal rather than plastic. In the 2005 "bookcase" edition, playing pieces are once again wooden cubes. These token types are purely a convention for ease of representing a specific army size. If a player runs out of army pieces during the game, another color may be used to substitute, or another symbolic token to help keep track of armies. Standard equipment also includes five (originally six) dice in two colors: three red dice for the attacker, and two (originally three) white or blue dice for the defender.
Also included is a total of seventy-two "Risk" cards. Forty-two of these depict territories, in addition to a symbol of an infantry, cavalry, or artillery piece. One of these cards is awarded to a player at the end of each turn if the player has successfully conquered at least one territory during that turn. No more than one card may be awarded per turn. If a player collects either three cards with the same symbol, or one of each, these cards may be traded in for reinforcements at the beginning of a player's turn. These cards can also be used for game set-up (see below for details). Also included are two wild cards that depict an infantry, cavalry, and artillery piece, as opposed to one of the three and a territory. Because these cards have all three symbols, they can match with any two other cards to form a set. Twenty-eight Mission cards also come with the game to be used in the "Secret Mission Risk" rule variant.
In the 40th Anniversary Collector's Edition the movement route between the territories of East Africa and Middle East was removed; this was later confirmed to be a manufacturing error, an error repeated in Risk II. Subsequent editions restored the missing route. While the European versions of "Risk" had included the variation "Secret Mission Risk" for some time, the U.S. version did not have this added until 1993.
Setup.
Standard.
Each player first counts out a number of infantry for initial deployment. The number of starting armies depends on the number of players. If two are playing, then each player counts out 40 infantry, plus 40 more from a different color set. This third set is neutral and only defends if attacked (the player not attacking rolls for the neutral armies). If three are playing, each player counts out 35 infantry; four players, 30 infantry; five players, 25 infantry; six players, 20 infantry. Players then take turns claiming territories by placing an infantry on an unoccupied territory until all the territories are occupied. Players then take turns placing their remaining armies on their territories. Having done this, the actual game begins with another roll of a dice, which is used to determine the playing order.
Alternate.
An alternate and quicker method of setup from the original French rules is to deal out the deck of "Risk" cards minus the wild cards, assigning players to the territories on their cards. As in a standard game, players still count out the same number of starting infantry and take turns placing their armies. The original rules from 1959 state that the entire deck of "Risk" cards (minus the wild cards) is dealt out, assigning players to the territories on their cards. One and only one army is placed on each territory before the game commences.
Player turn.
There are three main phases to a player's turn: getting and placing new armies, attacking, and fortifying.
Getting and placing new armies.
Players draft new armies and then distribute these pieces to any of their territories at the beginning of their turn. The number of armies a player may draft hinges upon three factors: number of territories owned; continent bonus(es); and redeeming "Risk" cards. To calculate the number of armies drafted for number of territories owned, players divide their total number of territories by three and round down to the nearest integer. If this result is less than three, round up to three armies. Players also receive bonus armies for occupying an entire continent (see table to the right). Lastly, players may receive armies for turning in a set of three "Risk" cards. A set may consist of the three different army units (soldier, cavalry, artillery) or be three of a kind (e.g. all three cards have cavalry pictures). If the player has five cards, the player must trade in a set. The first set to be turned is worth 4 reinforcements; the second is worth 6; third, 8; fourth, 10; fifth, 12; sixth, 15 and for every additional set thereafter 5 more armies than the previous set turned in; the number of reinforcements received is shown by a Golden Cavalry which moves along a grid every time a set is traded in. The probability of having a tradeable set of cards when holding three cards is 33.3% (9/27), 74% holding four cards (60/81), and 100% holding 5 cards (243/243).
The player places these armies on any of his territories. If a player owns one or more of the territories depicted on the set of turned in cards, the player may choose one of these territories to be awarded two additional armies that must be placed in that territory.
Attacking.
When it is a player's turn to attack, the player can only attack territories that are adjacent to or connected by a sea-lane to a territory already held. A battle's outcome is decided by rolling dice. The attacking player attacks with an army, rolling up to three dice. At least one unit must remain behind in the attacking territory not involved in the attack, as a territory may never be left unoccupied. Before the attacker rolls, the defender must resist the attack with either one or two armies (using at most the number of armies currently occupying the defended territory:9) by rolling one or two dice. Each player's highest die is compared, as is their second-highest die (if both players roll more than one). In each comparison, the highest number wins. The defender wins in the event of a tie. With each dice comparison, the loser removes one army from his territory from the game board. Any extra dice are disregarded and do not affect the results.
If an attack eliminates the final defending army within a territory, the attacker then must occupy the newly conquered territory with at least the number of attacking armies used in the last round of attack. There is no limit to the total number of additional armies that may be sent in to occupy, providing at least one army remains behind in the original attacking territory. Players may attack any number of territories any number of times before yielding the turn to the next player. Attacking is optional; a player may decline to attack at all during the turn.
If an attacker occupies a defender's last territory, the defender is eliminated from the game and the attacker acquires all of the defender's "Risk" cards. If the conquering player then holds five or more cards, the player must trade in sets until the player has fewer than five. The gained armies are placed immediately.
If, at the end of attacking, at least one territory was conquered that turn, the player draws a "Risk" card from the deck.
Fortifying.
When finished attacking and before passing the turn over to the next player, a player has the option to maneuver any number of armies from a single territory occupied by the player into an adjacent territory occupied by the same player. This is sometimes referred to as a "free move". Under an alternate rule, the maneuvering armies may travel through as many territories to their final destination as desired, providing that all involved pass-through territories are connected and occupied by that same player. As always, at least one army must be left in the originating territory. However, the player can only distribute between two territories. Play then proceeds clockwise to the next player.
Strategy.
Basic strategy.
The official rulebook gives three basic strategy tips for the classic rules:
Holding continents is the most common way to increase reinforcements. Players often attempt to gain control of Australia early in the game, since Australia is the only continent that can be successfully defended by heavily fortifying one country (either Siam or Indonesia). Generally, continents with fewer borders are easier to defend as they possess fewer points that can be attacked by other players. South America has 2 access points, North America and Africa each have 3, Europe has 4, and Asia has 5.
Generally, it is thought advisable to hold "Risk" cards until they can be turned in for maximum reinforcements. This is especially true earlier on in gameplay, because extra armies make a greater difference in the beginning of the game. Eliminating a weak player who holds a large number of "Risk" cards is also a good strategy, since players who eliminate their opponents get possession of their opponents' "Risk" cards. In this case, trading in Risk cards earlier may help acquire the necessary troops. If the conquering player has six:10 or more "Risk" cards after taking the cards of another player, the cards must be immediately turned in for reinforcements until the player has fewer than five cards and then may continue attacking.
"Turtling" is a defensive strategy where a player who feels vulnerable tries to become too expensive to be removed while remaining a threat to harass other players. The objective of this strategy is to avoid defeat. A player using this strategy might remain in the game all the way to later stages and then mount an attack on the weakest player and start a chain elimination to remove one player after another to win the game. The player who uses this strategy is called a Turtle. The term was popularised in Real-time Strategy games where a player creates a defensive perimeter or a “Turtle Shell” around the base of operations. Solutions to counteract this strategy using cooperation have been proposed by Ehsan Honary.
Alliances.
The rules of "Risk" neither endorse nor prohibit alliances or truces. Thus players often form unofficial treaties for various reasons, such as safeguarding themselves from attacks on one border while they concentrate their forces elsewhere, or eliminating a player who has grown too strong. Because these agreements are not enforceable by the rules, these agreements are often broken. Alliance making/breaking can be one of the most important elements of the game, and it adds human interaction to a decidedly probabilistic game. Some players allow trading of "Risk" cards, but only during their turn. This optional rule makes alliances more powerful.
Dice probabilities.
Defenders always win ties when dice are rolled. This gives the defending player the advantage in "one-on-one" fights, but the attacker's ability to use more dice offsets this advantage, as indicated in the dice probability charts below. Actually capturing a territory depends on the number of attacking and defending armies and the associated probabilities can be expressed analytically using Markov chains, or studied numerically using stochastic simulation.
It is always advantageous to roll the maximum number of dice, unless an attacker wishes to avoid moving men into a 'dead-end' territory, in which case he may choose to roll fewer than three.
The table below states the probabilities of all possible outcomes of one attacker dice roll and one defender dice roll.  Green  indicates an advantage to the attacker and  red italic  an advantage to the defender.
Thus when rolling three dice against two dice (the most each player can roll), three against one, or two against one, the attacker has a slight advantage, otherwise the defender has an advantage. When large armies face off, a player will tend to gain a greater advantage over his opponent by attacking rather than defending. (Multiple opponents can change the prudence of such a strategy, however.)
The following table shows the probabilities that the attacker wins a whole battle between two countries (a sequence of dice rolls).  Green  indicates an advantage to the attacker (i.e. that the probability to win is larger than 50%), and  red italic  an advantage to the defender.
The number of attacking armies does not include the minimum one army that must be left behind in the territory (e.g. if the attacking territory has 10 armies total, it has maximum 9 attacking armies).
There are online tools available to compute the outcome of whole campaigns (i.e. the attacking of several territories in a row).
Risiko! variation odds.
Risiko! is a variant of the game released in Italy, in which the defender is allowed to roll up to three dice to defend. This variation dramatically shifts the balance of power towards defense. As shown in the following table, a defender in this variation has a 1-in-5 chance of holding a country with three defenders against eight armies in a whole battle (excluding the one that must remain behind):
For comparison, under the standard rules (in which the defender may roll up to only two dice at a time) three armies would only have 1-in-19 chance of holding a country against all-out attack by eight.
Rule variations.
Over the years, Parker Brothers and Hasbro have published many different editions of rules for the game.
Two-player Risk.
This 2-player version is played according to the traditional rules of Risk. Each player takes 40 armies and alternately places one army on an unoccupied territory until each has occupied 14 territories. The remaining armies are alternately distributed on the occupied territories. The remaining 14 territories are occupied by a force called the Allied Army. These armies are composed of playing pieces different in color from those used by the two players. Two Allied Armies will be placed on each unoccupied territory for a total of 28 armies.
Each player attacks according to the traditional rules. A player may attack the other player or the Allied Army. When a player attacks the Allied Army, the other player rolls the dice for the Allied Army.
The game ends when one player loses all his territories. If the Allied Army loses all its territories, game play is continued according to the traditional rules.:12
Capitol Risk.
Each player has a "capitol" in one of the initially occupied territories. The player to capture all capitols wins. Any armies and territories that belong to the losing nation are turned over to the victor. "Capitol Risk" often leads to much shorter games.
Secret Mission.
"Secret Mission Risk" was the standard game in European editions for some decades and was introduced to US editions in 1993. This form of play gives each player a specific mission short of complete world domination. Players do not reveal their missions to each other until the end of the game. The game ends when the first person to complete his mission reveals his "Secret Mission" card, thereby winning. In 2003, a different "Secret Mission" version of the game was released, in which each player received four (easier) secret missions to complete.
The original missions in the 1993 US edition are:
Note: In the UK edition, if a player's mission is to destroy all armies of a particular color, and another player kills off the last armies of that color, their mission changes to capturing 24 territories. In the US edition, no matter who eliminates the last army, the player with the mission wins automatically
Alternate card turn-in rules.
In some editions, the cards display either one or two stars. Cards may be exchanged to draft a number of armies depending on the sum of these stars (limited from 2 to 10 stars) according to the table below. Cards may be accumulated as long as the player wishes. The new armies are immediately deployed in any combination across the player's occupied territories.
If an Objective has been accomplished on the player's turn, that player is prohibited from also drawing a Risk card on that turn. The territory on the card is irrelevant when drafting troops.
An additional card exchange regime is to offer a fixed number of armies depending on the emblem on the card. Three infantry would receive four armies, three cavalry would receive six armies, three cannons would receive eight armies, and one of each emblem would receive 10 armies.
Yet another card exchange regime follows the escalating exchange rules, but after awarding 15 armies for the sixth exchanged set the number is reset to the original four armies before increasing again with each exchange.
Other rule variations.
The official rulebook suggests variations to the gameplay mechanics for ""Risk" experts," any or all of which can be used depending on player preference.:15 These suggestions include:
In addition to these official variations, many computer and Internet versions have different rules, and gaming clubs often use house rules or competition-adjusted rules. These may include structure such as forts, freeplay (players take turns simultaneously), or other rules.
There have been other variations of Risk, essentially "house rules," complete with titles like "Ultimate Risk" (which is played in the United States).
Territories.
The following is a typical layout of the "Risk" game board, with a table of the corresponding continent and territory names. Each territory on the typical "Risk" game board represents a real-life geographical or political region on Earth. As such, the territory borders are drawn to resemble the geography of those regions. This provides an interior space on which to place the army units, adds an element of realism to the game, and also adds complexity. The map is not accurate nor is it drawn to scale; New Zealand, for instance, is missing.
The numbers in parentheses represent the number of additional armies granted during the reinforcement stage of a player's turn who controls all of the territories in that continent.
Official licensed Risk games.
In addition to the original version of 1959, and a "40th Anniversary Edition" with metal pieces, a number of official variants of "Risk" have been released over the years. In recent years, Hasbro has predominantly based its "Risk" variants on popular films. The most recent example in this trend is the "Transformers" version, released in June 2007. In chronological order, the variations of "Risk" that have been released are:
Risk clones.
Many variants exist that are based on the original concept of the game of "Risk" and that contain much of the functionality of the original, but are not licensed by Hasbro, such as, for example, the video games "Global Domination" and "Lux". Known as Risk clones, such variants have names not containing the term "Risk" to avoid legal issues. Some of these clones are available commercially, of which many have been released through the iTunes App Store, especially for the iPad. Several other Risk clones are distributed freely over the Internet, such as Dice Wars and WarLight. Games such as Nintendo Wars can be seen as a complex evolution which still holds some elements from Risk. NarcoGuerra is a newsgame based on the basic Risk rules, played out over a map of Mexico with the intent of educating people on the Mexican Drug War.
In addition to Risk clones, third-party products have been created which slightly modify traditional gameplay. Among the most popular third-party editions are virtual dice-rolling simulators. These can act as virtual replacements to traditional dice or be used to automatically simulate the results of large battles between territories—significantly speeding up gameplay during battles between territories with many units.
Computer and video games.
Several computer and video game versions of "Risk" have been released as "", starting with the Commodore 64 edition in 1988 and the Macintosh edition in 1989. Since then, various other editions have been released for PC, Amiga, Sega Genesis, PlayStation, PlayStation 2, and Game Boy Advance. In 1996 Hasbro Interactive released a PC version of Risk that included a new variation on the game called "Ultimate Risk", which did not use dice but rather implemented the use of forts, generals, and complex battle strategies. "Risk II" for PC and Mac was released as a 2000 video game which includes classic Risk as well as board and gameplay variations. In 2010, Pogo.com added a licensed version of Risk to its library of online games. An Xbox Live Arcade version of Risk called was released on June 23, 2010. It includes classic Risk as well as a factions mode where players can play as Zombies, Robots, Cats, Soldiers, or Yetis.
As of August, 6th 2014, Hasbro and Ubisoft have announced a new Risk game to be released in Fall 2014, on PS4 and Xbox One, as well as Xbox 360 and PS3. "Risk will feature an online league play, modern armies, and 3D battlefields. The 2010 rules are the standard set, but players can choose rule modifiers and configurable win variants." 
An official licensed iOS app, "RISK : The Official Game", developed for the iPhone, iPod Touch, and iPad by Electronic Arts, was released on July 16, 2010. Although the iPad version (Risk HD) has to be bought separately from the iPhone version (Risk), local link up allows games to take place across versions. A maximum of 6 players can participate. If only one iOS device is available, the 'pass and play' mode allows several players to take part in a multi-player game.
A version for the Atari 8-bit computers was in development in 1983, to be published by Parker Brothers. An unfinished prototype exists in possession of its programmer Steve Kranish.
 is a multiplayer, strategy door game for bulletin board systems, adapted from Risk. The world is divided into six continents and 42 countries. Created by Joel Bergen in 1989, It was sold in 1997 to John Dailey Software.

</doc>
<doc id="38382" url="http://en.wikipedia.org/wiki?curid=38382" title="List of German monarchs">
List of German monarchs

This is a list of monarchs who ruled over the German territories of central Europe from the division of the Frankish Empire in 843 (by which a separate Eastern Frankish Kingdom was created), until the end of the Imperial Germany in 1918. It also includes the heads of the various German confederations after the collapse of the Holy Roman Empire of the German Nation in 1806.
Note on titles.
Emperors are listed in bold. Rival kings, anti-kings, and junior co-regents are "italicized".
Kings of Germany, 911–1806.
Germany within the Holy Roman Empire, 962–1806.
The title "King of the Romans", used under the Holy Roman Empire, is considered equivalent to King of Germany. A king was chosen by the German electors and would then proceed to Rome to be crowned emperor by the pope.

</doc>
<doc id="38390" url="http://en.wikipedia.org/wiki?curid=38390" title="Dementia">
Dementia

Dementia, also known as senility, is a broad category of brain diseases that cause a long term and often gradual decrease in the ability to think and remember such that a person's daily functioning is affected. Other common symptoms include emotional problems, problems with language, and a decrease in motivation. A person's consciousness is not affected. For the diagnosis to be present it must be a change from a person's usual mental functioning and a greater decline than one would expect due to aging. These diseases also have a significant effect on a person's caregivers.
The most common type of dementia is Alzheimer's disease which makes up 50% to 70% of cases. Other common types include vascular dementia (25%), Lewy body dementia (15%), and frontotemporal dementia. Less common causes include normal pressure hydrocephalus, Parkinson's disease, syphilis, and Creutzfeldt–Jakob disease among others. More than one type of dementia may exist in the same person. A small proportion of cases run in families. In the DSM-5, dementia was reclassified as a neurocognitive disorder, with various degrees of severity. Diagnosis is usually based on history of the illness and cognitive testing with medical imaging and blood work used to rule out other possible causes. The mini mental state examination is one commonly used cognitive test. Efforts to prevent dementia include trying to decrease risk factors such as high blood pressure, smoking, diabetes and obesity. Screening the general population for the disease is not recommended.
There is no cure for dementia. Cholinesterase inhibitors such as donepezil are often used and may be beneficial in mild to moderate disease. Overall benefit, however, may be minor. For people with dementia and those who care for them many measures can improve their lives. Cognitive and behavioral interventions may be appropriate. Educating and providing emotional support to the caregiver is important. Exercise programs are beneficial with respect to activities of daily living and potentially improve outcomes. Treatment of behavioral problems or psychosis due to dementia with antipsychotics is common but not usually recommended due to there often being little benefit and an increased risk of death.
Globally, dementia affects 36 million people. About 10% of people develop the disease at some point in their lives. It becomes more common with age. About 3% of people between the ages of 65–74 have dementia, 19% between 75 and 84 and nearly half of those over 85 years of age. In 2013 dementia resulted in about 1.7 million deaths up from 0.8 million in 1990. As more people are living longer, dementia is becoming more common in the population as a whole. For people of a specific age, however, it may be becoming less frequent, at least in the developed world, due to a decrease in risk factors. It is one of the most common causes of disability among the old. It is believed to result in economic costs of 604 billion USD a year. People with dementia are often physically or chemically restrained to a greater degree than necessary, raising issues of human rights. Social stigma against those affected is common.
Signs and symptoms.
Dementia affects the brain's ability to think, reason and remember clearly. The most common affected areas include memory, visual-spatial, language, attention, and executive function (problem solving). Most types of dementia are slow and progressive. By the time the person shows signs of the disease, the process in the brain has been happening for a long time. It is possible for a patient to have two types of dementia at the same time. About 10% of people with dementia have what is known as "mixed dementia", which is usually a combination of Alzheimer's disease and another type of dementia such as frontotemporal dementia or vascular dementia. Additional psychological and behavioral problems that often affect people who have dementia include: 
When people with dementia are put in circumstances beyond their abilities, there may be a sudden change to tears or anger (a "catastrophic reaction").
Depression affects 20–30% of people who have dementia, and about 20% have anxiety. Psychosis (often delusions of persecution) and agitation/aggression also often accompany dementia. Each of these must be assessed and treated independently of the underlying dementia.
Mild cognitive impairment.
In the first stages of dementia, the signs and symptoms of the disease may be subtle. Often, the early signs of dementia only become apparent when looking back in time. The earliest stage of dementia is called mild cognitive impairment (MCI). 70% of those diagnosed with MCI will progress to dementia at some point. In MCI, changes in the person's brain have been happening for a long time, but the symptoms of the disease are just beginning to show. These problems, however, are not yet severe enough to affect the person’s daily function. If they do, it is considered dementia. A person with MCI will score between 27 and 30 on the Mini-Mental State Examination (MMSE), which is a normal score. They may have some memory trouble and trouble finding words but they solve everyday problems and handle their own life affairs well.
Early stages.
In the early stage of dementia, the person will begin to show symptoms noticeable to the people around them. In addition, the symptoms begin to interfere with daily activities. The person will usually score between a 20 and 25 on the MMSE. The symptoms are dependent on the type of dementia a person has. The person may begin to have difficulty with more complicated chores and tasks around the house. The person can usually still take care of him or herself but may forget things like taking pills or doing laundry and may need prompting or reminders.
The symptoms of early dementia usually include memory difficulty, but can also include some word-finding problems (anomia) and problems with planning and organizational skills (executive function). One very good way of assessing a person's impairment is by asking if he or she is still able to handle his/her finances independently. This is often one of the first things to become problematic. Other signs might be getting lost in new places, repeating things, personality changes, social withdrawal and difficulties at work.
When evaluating a person for dementia, it is important to consider how the person was able to function five or ten years earlier. It is also important to consider a person's level of education when assessing for loss of function. For example, an accountant who can no longer balance a checkbook would be more concerning than a person who had not finished high school or had never taken care of his/her own finances.
In Alzheimer's dementia the most prominent early symptom is memory difficulty. Others include word-finding problems and getting lost. In other types of dementia, like dementia with Lewy bodies and fronto-temporal dementia, personality changes and difficulty with organization and planning may be the first signs.
Middle stages.
As dementia progresses, the symptoms first experienced in the early stages of the dementia generally worsen. The rate of decline is different for each person. A person with moderate dementia will score between 6-17 on the MMSE. For example, if the person has Alzheimer's dementia, in the moderate stages almost all new information will be lost very quickly. The person may be severely impaired in solving problems and their social judgment is usually also impaired. The person cannot usually function outside of his or her own home, and generally should not be left alone. He or she may be able to do simple chores around the house but not much else and begins to require assistance for personal care and hygiene other than simple reminders.
Late stages.
People with late-stage dementia typically turn increasingly inward and need assistance with most or all of their personal care. Persons with dementia in the late stages usually need 24-hour supervision to ensure personal safety, as well as to ensure that basic needs are being met. If left unsupervised, a person with late-stage dementia may wander and fall, may not recognize common dangers around them such as a hot stove, may not realize that they need to use the bathroom or become unable to control their bladder or bowels (incontinent). 
Changes in eating frequently occur, and those with late-stage dementia often need pureed diets, thickened liquids, and assistance in eating. Their appetite may decline to the point that the person does not want to eat at all. He or she may not want get out of bed, or may need complete assistance doing so. They may no longer recognize familiar people. He or she may have significant changes in sleeping habits or have trouble sleeping at all.
Causes.
Reversible causes.
There are four main causes of easily reversible dementia: hypothyroidism, vitamin B12 deficiency, Lyme disease, and neurosyphillis. All people with memory difficulty should be checked for hypothyroidism and B12 deficiency. For Lyme disease and neurosyphilis, testing should be done if there are risk factors for those diseases in the person.
Alzheimer's disease.
Alzheimer's disease is the most common form of dementia. Its most common symptoms are short-term memory loss and word-finding difficulties. People with Alzheimer's also have trouble with visual-spatial areas (for example they may begin to get lost often), reasoning, judgement, and insight. Insight refers to whether or not the person realizes he/she has memory problems.
Common early symptoms of Alzheimer's include repetition, getting lost, difficulties keeping track of bills, problems with cooking especially new or complicated meals, forgetting to take medication, and word-finding problems.
The part of the brain most affected by Alzheimer's is the hippocampus. Other parts of the brain that will show shrinking (atrophy) include the temporal and parietal lobes. Although this pattern suggests Alzheimer's, the brain shrinkage in Alzheimer's disease is very variable, and a scan of the brain cannot actually make the diagnosis.
Vascular dementia.
Vascular dementia is the cause of at least 20% of dementia cases, making it the second most common cause of dementia. It is caused by disease or injury to blood vessels that damage the brain, including strokes. The symptoms of this dementia depend on where in the brain the strokes have occurred and whether the vessels are large or small. Multiple injuries can cause progressive dementia over time, while a single injury located in an area critical for cognition (i.e., hippocampus, thalamus) can lead to sudden cognitive decline.
On scans of the brain, a person with vascular dementia may show evidence of multiple different strokes of different sizes. They tend to have risk factors for artery disease such as tobacco smoking, high blood pressure, atrial fibrillation, high cholesterol or diabetes, or other signs of blood vessel disease such as a previous heart attack or angina.
Dementia with Lewy bodies.
Dementia with Lewy bodies (DLB) is a dementia that has the primary symptoms of visual hallucinations and "Parkinsonism." Parkinsonism is a term that describes a person with features of Parkinson's disease. This includes tremor, rigid muscles, and a face without emotion. The visual hallucinations in DLB are generally very vivid hallucinations of people and/or animals and they often occur when someone is about to fall asleep or just waking up. Other prominent symptoms include problems with attention, organization, problem solving and planning (executive function) and difficulty with visual-spatial function.
Again, imaging studies cannot necessarily make the diagnosis of DLB, but some signs are particularly common. A person with DLB will often show occipital hypoperfusion on SPECT scan or occipital hypometabolism on a PET scan. Generally, a diagnosis of DLB is straightforward and unless it is complicated, a brain scan is not always necessary.
Frontotemporal dementia.
Frontotemporal dementia (FTD) is a dementia that is characterized by drastic personality changes and language difficulties. In all FTD the person will have a relatively early social withdrawal and early lack of insight into the disease. Memory problems are not a main feature of this disease.
There are three main types of FTD. The first has major symptoms in the area of personality and behavior. This is called behavioral variant FTD (bv-FTD) and is the most common. In bv-FTD, the person will have a change in personal hygiene, they will become rigid in their thinking, they rarely recognize that there is a problem, they will be socially withdrawn, and they will often have a drastic increase in appetite. The person may also be socially inappropriate. For example, the person may make inappropriate sexual comments, or may begin using pornography openly when they had not before. One of the most common signs is apathy, or not caring about anything. Apathy, however, is a common symptom in many different dementias.
The other two types of FTD feature language problems as the main symptom. The second type is called semantic dementia or temporal variant dementia (TV-FTD). The main feature of this is the loss of the meaning of words. It may begin with difficulty naming things. The person eventually may also lose the meaning of objects as well. For example, a drawing of a bird, dog, and an airplane in someone with FTD may all appear just about the same. In a classic test for this, a patient is shown a picture of a pyramid and below there is a picture of both a palm tree and a pine tree. The person is asked to say which one goes best with the pyramid. In TV-FTD the person would not be able to answer that question.
The last type of FTD is called progressive non-fluent aphasia (PNFA). This is mainly a problem with producing speech. They have trouble finding the right words, but mostly they have a difficulty coordinating the muscles they need to speak. Eventually, someone with PNFA will only use one-syllable words or may become totally mute.
With both TV-FTD and PNFA the symptoms of behavior may be present, but milder and later than in bv-FTD. On imaging studies, there will be shrinking of the frontal and temporal lobes of the brain.
Progressive supranuclear palsy.
Progressive supranuclear palsy (PSP) is a form of dementia that is characterized by problems with eye movements. Generally the problems begin with difficulty moving the eyes up and/or down (vertical gaze palsy). Since difficulty moving the eyes upward can sometimes happen in normal aging, problems with downward eye movements are the key in PSP. Other key symptoms of PSP include falls backwards, balance problems, slow movements, rigid muscles, irritability, apathy, social withdrawal and depression. The person may also have certain "frontal lobe signs" such as perseveration, a grasp reflex and utilization behavior (the need to use an object once you see it). People with PSP often have progressive difficulty eating and swallowing, and eventually with talking as well. Because of the rigidity and slow movements, PSP is sometimes misdiagnosed as Parkinson's disease.
On scans of the brain, the midbrain of people with PSP is generally shrunken (atrophied), but there are no other common brain abnormalities visible on images of the person's brain.
Corticobasal degeneration.
Corticobasal degeneration is a rare form of dementia that is characterized by many different types of neurological problems that get progressively worse over time. This is because the disease affects the brain in many different places, but at different rates. One common sign is difficulty with using only one limb. One symptom that is extremely rare in any condition other than corticobasal degeneration is the "alien limb." The alien limb is a limb of the person that seems to have a mind of its own, it moves without control of the person's brain. Other common symptoms include jerky movements of one or more limbs (myoclonus), symptoms that are different in different limbs (asymmetric), difficulty with speech that is due to not being able to move the mouth muscles in a coordinated way, numbness and tingling of the limbs and neglecting one side of the person's vision or senses. In neglect, a person will ignore the opposite side of the body than the one that has the problem. For example, a person may not feel pain on one side, or may only draw half of a picture when asked. In addition, the person's affected limbs may be rigid or have muscle contractions causing strange repetitive movements (dystonia).
The area of the brain most often affected in corticobasal degeneration is the posterior frontal lobe and parietal lobe. Still, many other part of the brain can be affected.
Rapidly progressive.
Creutzfeldt-Jakob disease typically causes a dementia that worsens over weeks to months, being caused by prions. The common causes of slowly progressive dementia also sometimes present with rapid progression: Alzheimer's disease, dementia with Lewy bodies, frontotemporal lobar degeneration (including corticobasal degeneration and progressive supranuclear palsy).
On the other hand, encephalopathy or delirium may develop relatively slowly and resemble dementia. Possible causes include brain infection (viral encephalitis, subacute sclerosing panencephalitis, Whipple's disease) or inflammation (limbic encephalitis, Hashimoto's encephalopathy, cerebral vasculitis); tumors such as lymphoma or glioma; drug toxicity (e.g., anticonvulsant drugs); metabolic causes such as liver failure or kidney failure; and chronic subdural hematoma.
Other conditions.
There are many other medical and neurological conditions in which dementia only occurs late in the illness. For example, a proportion of patients with Parkinson's disease develop dementia, though widely varying figures are quoted for this proportion. When dementia occurs in Parkinson's disease, the underlying cause may be dementia with Lewy bodies or Alzheimer's disease, or both. Cognitive impairment also occurs in the Parkinson-plus syndromes of progressive supranuclear palsy and corticobasal degeneration (and the same underlying pathology may cause the clinical syndromes of frontotemporal lobar degeneration). Chronic inflammatory conditions of the brain may affect cognition in the long term, including Behçet's disease, multiple sclerosis, sarcoidosis, Sjögren's syndrome and systemic lupus erythematosus. Although the acute porphyrias may cause episodes of confusion and psychiatric disturbance, dementia is a rare feature of these rare diseases.
Aside from those mentioned above, inherited conditions that can cause dementia (alongside other symptoms) include:
Mild cognitive impairment.
Mild cognitive impairment basically means that the person is exhibiting memory or thinking difficulties, but it is not severe enough yet to be given a diagnosis. He or she should score between 25-30 on the MMSE. Around 70% of people with MCI will go on to develop some form of dementia. MCI is generally divided into two categories. The first is one that is primarily memory loss (amnestic MCI). The second category is anything that is not primarily memory difficulties (non-amnestic MCI). People with primarily memory problems generally go on to develop Alzheimer's disease. People with the other type of MCI may go on to develop other types of dementia.
Diagnosis of MCI is often difficult, as cognitive testing may be normal. Often, more in-depth neuropsychological testing is necessary to make the diagnosis. the most commonly used criteria are called the Peterson criteria and include:
Fixed cognitive impairment.
Various types of brain injury may cause irreversible cognitive impairment that will not get worse over time. Traumatic brain injury may cause generalized damage to the white matter of the brain (diffuse axonal injury), or more localized damage (as also may neurosurgery). A temporary reduction in the brain's supply of blood or oxygen may lead to hypoxic-ischemic injury. Strokes (ischemic stroke, or intracerebral, subarachnoid, subdural or extradural hemorrhage) or infections (meningitis and/or encephalitis) affecting the brain, prolonged epileptic seizures and acute hydrocephalus may also have long-term effects on cognition. Excessive alcohol use may cause alcohol dementia, Wernicke's encephalopathy and/or Korsakoff's psychosis.
Slowly progressive.
Dementia that begins gradually and worsens progressively over several years is usually caused by neurodegenerative disease—that is, by conditions that affect only or primarily the neurons of the brain and cause gradual but irreversible loss of function of these cells. Less commonly, a non-degenerative condition may have secondary effects on brain cells, which may or may not be reversible if the condition is treated.
Causes of dementia depend on the age at which symptoms begin. In the elderly population (usually defined in this context as over 65 years of age), a large majority of dementia cases are caused by Alzheimer's disease, vascular dementia, or both. Dementia with Lewy bodies is another commonly exhibited form, which again may occur alongside either or both of the other causes. Hypothyroidism sometimes causes slowly progressive cognitive impairment as the main symptom, and this may be fully reversible with treatment. Normal pressure hydrocephalus, though relatively rare, is important to recognize since treatment may prevent progression and improve other symptoms of the condition. However, significant cognitive improvement is unusual.
Dementia is much less common under 65 years of age. Alzheimer's disease is still the most frequent cause, but inherited forms of the disease account for a higher proportion of cases in this age group. Frontotemporal lobar degeneration and Huntington's disease account for most of the remaining cases. Vascular dementia also occurs, but this in turn may be due to underlying conditions (including antiphospholipid syndrome, CADASIL, MELAS, homocystinuria, moyamoya and Binswanger's disease). People who receive frequent head trauma, such as boxers or football players, are at risk of chronic traumatic encephalopathy (also called dementia pugilistica in boxers).
In young adults (up to 40 years of age) who were previously of normal intelligence, it is very rare to develop dementia without other features of neurological disease, or without features of disease elsewhere in the body. Most cases of progressive cognitive disturbance in this age group are caused by psychiatric illness, alcohol or other drugs, or metabolic disturbance. However, certain genetic disorders can cause true neurodegenerative dementia at this age. These include familial Alzheimer's disease, SCA17 (dominant inheritance); adrenoleukodystrophy (X-linked); Gaucher's disease type 3, metachromatic leukodystrophy, Niemann-Pick disease type C, pantothenate kinase-associated neurodegeneration, Tay-Sachs disease and Wilson's disease (all recessive). Wilson's disease is particularly important since cognition can improve with treatment.
At all ages, a substantial proportion of patients who complain of memory difficulty or other cognitive symptoms have depression rather than a neurodegenerative disease. Vitamin deficiencies and chronic infections may also occur at any age; they usually cause other symptoms before dementia occurs, but occasionally mimic degenerative dementia. These include deficiencies of vitamin B12, folate or niacin, and infective causes including cryptococcal meningitis, HIV, Lyme disease, progressive multifocal leukoencephalopathy, subacute sclerosing panencephalitis, syphilis and Whipple's disease.
Diagnosis.
As seen above, there are many specific types and causes of dementia, often showing slightly different symptoms. However, the symptoms are very similar and it is usually difficult to diagnose the type of dementia by symptoms alone. Diagnosis may be aided by brain scanning techniques. In many cases, the diagnosis cannot be absolutely sure except with a brain biopsy, but this is very rarely recommended (though it can be performed at autopsy). In those who are getting older, general screening for cognitive impairment using cognitive testing or early diagnosis of dementia has not been shown to improve outcomes. However, it has been shown that screening exams are useful in those people over the age of 65 with memory complaints.
Normally, symptoms must be present for at least six months to support a diagnosis. Cognitive dysfunction of shorter duration is called "delirium". Delirium can be easily confused with dementia due to similar symptoms. Delirium is characterized by a sudden onset, fluctuating course, a short duration (often lasting from hours to weeks), and is primarily related to a somatic (or medical) disturbance. In comparison, dementia has typically a long, slow onset (except in the cases of a stroke or trauma), slow decline of mental functioning, as well as a longer duration (from months to years).
Some mental illnesses, including depression and psychosis, may produce symptoms that must be differentiated from both delirium and dementia. Therefore, any dementia evaluation should include a depression screening such as the Neuropsychiatric Inventory or the Geriatric Depression Scale. It used to be thought that anyone who came in with memory complaints had depression and not dementia (because it was thought that those with dementia are generally unaware of their memory problems). This is called pseudodementia. However, in recent years we have realized that many older people with memory complaints in fact have MCI, the earliest stage of dementia. Depression should always remain high on the list of possibilities, however, for an elderly person with memory trouble.
Cognitive testing.
There are some brief tests (5–15 minutes) that have reasonable reliability to screen for dementia.
While many tests have been studied, presently the mini mental state examination (MMSE) is the best studied and most commonly used, albeit some may emerge as better alternatives. Other examples include the abbreviated mental test score (AMTS), the, "Modified Mini-Mental State Examination" (3MS), the "Cognitive Abilities Screening Instrument" (CASI), the Trail-making test, and the clock drawing test. The MOCA (Montreal Cognitive Assessment) is a very reliable screening test and is available online for free in 35 different languages. The MOCA has also been shown to be somewhat better at detecting mild cognitive impairment than the MMSE.
Another approach to screening for dementia is to ask an informant (relative or other supporter) to fill out a questionnaire about the person's everyday cognitive functioning. Informant questionnaires provide complementary information to brief cognitive tests. Probably the best known questionnaire of this sort is the "Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE)". The Alzheimer's Disease Caregiver Questionnaire is another tool. It is about 90% accurate for Alzheimer's and can be completed online or in the office by a caregiver. On the other hand the "General Practitioner Assessment Of Cognition" combines both, a patient assessment and an informant interview. It was specifically designed for the use in the primary care setting.
Clinical neuropsychologists provide diagnostic consultation following administration of a full battery of cognitive testing, often lasting several hours, to determine functional patterns of decline associated with varying types of dementia. Tests of memory, executive function, processing speed, attention, and language skills are relevant, as well as tests of emotional and psychological adjustment. These tests assist with ruling out other etiologies and determining relative cognitive decline over time or from estimates of prior cognitive abilities.
Laboratory tests.
Routine blood tests are also usually performed to rule out treatable causes. These tests include vitamin B12, folic acid, thyroid-stimulating hormone (TSH), C-reactive protein, full blood count, electrolytes, calcium, renal function, and liver enzymes. Abnormalities may suggest vitamin deficiency, infection or other problems that commonly cause confusion or disorientation in the elderly. The problem is complicated by the fact that these cause confusion more often in persons who have early dementia, so that "reversal" of such problems may ultimately only be temporary.
Testing for alcohol and other known dementia-inducing drugs may be indicated.
Imaging.
A CT scan or magnetic resonance imaging (MRI scan) is commonly performed, although these tests do not pick up diffuse metabolic changes associated with dementia in a person that shows no gross neurological problems (such as paralysis or weakness) on neurological exam. CT or MRI may suggest normal pressure hydrocephalus, a potentially reversible cause of dementia, and can yield information relevant to other types of dementia, such as infarction (stroke) that would point at a vascular type of dementia.
The functional neuroimaging modalities of SPECT and PET are more useful in assessing long-standing cognitive dysfunction, since they have shown similar ability to diagnose dementia as a clinical exam and cognitive testing. The ability of SPECT to differentiate the vascular cause (i.e., multi-infarct dementia) from Alzheimer's disease dementias, appears superior to differentiation by clinical exam.
Recent research has established the value of PET imaging using carbon-11 Pittsburgh Compound B as a radiotracer (PIB-PET) in predictive diagnosis of various kinds of dementia, in particular Alzheimer's disease. Studies from Australia have found PIB-PET 86% accurate in predicting which patients with mild cognitive impairment would develop Alzheimer's disease within two years. In another study, carried out using 66 patients seen at the University of Michigan, PET studies using either PIB or another radiotracer, carbon-11 dihydrotetrabenazine (DTBZ), led to more accurate diagnosis for more than one-fourth of patients with mild cognitive impairment or mild dementia.
Prevention.
Many prevention measures have been proposed, including both lifestyle changes and medication although none has been reliably shown to be effective. Among old people who are otherwise healthy computerized cognitive training may improve memory; however it is not known if it prevents dementia.
Management.
Except for the treatable types listed above, there is no cure. Cholinesterase inhibitors are often used early in the disease course; however, benefit is generally small. Cognitive and behavioral interventions may be appropriate. Educating and providing emotional support to the caregiver is of importance as well. Exercise programs are beneficial with respect to activities of daily living and potentially improve dementia.
Psychological therapies.
Psychological therapies which are considered as a treatment for dementia include music therapy with unclear evidence, tentative evidence for reminiscence therapy, some benefit for cognitive reframing for caretakers, unclear evidence for validation therapy, and tentative evidence for mental exercise.
Adult daycare centers as well as special care units in nursing homes often provide specialized care for dementia patients. Adult daycare centers offer supervision, recreation, meals, and limited health care to participants, as well as providing respite for caregivers. In addition, home care can provide one-on-one support and care in the home allowing for more individualized attention that is needed as the disease progresses. Psychiatric nurses can make a distinctive contribution to people's mental health.
Since dementia impairs normal communication due to changes in receptive and expressive language, as well as the ability to plan and problem solve, agitated behaviour is often a form of communication for the person with dementia and actively searching for a potential cause, such as pain, physical illness, or overstimulation can be helpful in reducing agitation. Additionally, using an "ABC analysis of behaviour" can be a useful tool for understanding behavior in people with dementia. It involves looking at the antecedents (A), behavior (B), and consequences (C) associated with an event to help define the problem and prevent further incidents that may arise if the person's needs are misunderstood.
Medications.
Currently, no medications have been shown to prevent or cure dementia. Medications may be used to treat the behavioural and cognitive symptoms but have no effect on the underlying disease process.
Acetylcholinesterase inhibitors, such as donepezil, may be useful for Alzheimer disease and dementia in Parkinson's, DLB, or vascular dementia. The quality of the evidence however is poor and the benefit is small. No difference has been shown between the agents in this family.
In a minority of people side effects include bradycardia and syncope.
As assessment for an underlying cause of the behavior is a needed before prescribing antipsychotic medication for symptoms of dementia. Antipsychotic drugs should be used to treat dementia only if non-drug therapies have failed to be effective and the person's actions threaten themselves or others. Aggressive behavior changes are sometimes the result of other solvable problems, that could make treatment with antipsychotics unnecessary. Because people with dementia can be aggressive, resistant to their treatment, and otherwise disruptive, sometimes antipsychotic drugs are considered as a therapy in response. These drugs have risky adverse effects, including increasing the patient's chance of stroke and death. Generally stopping antipsychotics for people with dementia does not cause problems, even in those who have been on them a long time.
N-methyl-D-aspartate (NMDA) receptor blockers such as memantine may be of benefit but the evidence is less conclusive than for AChEIs. Due to their differing mechanisms of action memantine and acetylcholinesterase inhibitors can be used in combination however the benefit is slight.
Antidepressant drugs: Depression is frequently associated with dementia and generally worsens the degree of cognitive and behavioral impairment. Antidepressants effectively treat the cognitive and behavioral symptoms of depression in patients with Alzheimer's disease, but evidence for their use in other forms of dementia is weak.
It is recommended that benzodiazepines such as diazepam be avoided in dementia due to the risks of increased cognitive impairment and falls. There is little evidence for the effectiveness in this population.
There is no solid evidence that folate or vitamin B12 improves outcomes in those with cognitive problems.
Pain.
As people age, they experience more health problems, and most health problems associated with aging carry a substantial burden of pain; so, between 25% and 50% of older adults experience persistent pain. Seniors with dementia experience the same prevalence of conditions likely to cause pain as seniors without dementia. Pain is often overlooked in older adults and, when screened for, often poorly assessed, especially among those with dementia since they become incapable of informing others that they're in pain. Beyond the issue of humane care, unrelieved pain has functional implications. Persistent pain can lead to decreased ambulation, depressed mood, sleep disturbances, impaired appetite and exacerbation of cognitive impairment, and pain-related interference with activity is a factor contributing to falls in the elderly.
Although persistent pain in the person with dementia is difficult to communicate, diagnose and treat, failure to address persistent pain has profound functional, psychosocial and quality of life implications for this vulnerable population. Health professionals often lack the skills and usually lack the time needed to recognize, accurately assess and adequately monitor pain in people with dementia. Family members and friends can make a valuable contribution to the care of a person with dementia by learning to recognize and assess their pain. Educational resources (such as the tutorial) and observational assessment tools are available.
Eating difficulties.
Persons with dementia may have difficulty eating. Whenever it is available as an option, the recommended response to eating problems is having a caretaker do assisted feeding for the person. A secondary option for people who cannot swallow effectively is to consider gastrostomy feeding tube placement as a way to give nutrition. However, in bringing person comfort and keeping functional status while lowering risk of aspiration pneumonia and death, assistance with oral feeding is at least as good as tube feeding. Tube-feeding is associated with agitation, increased use of physical and chemical restraints and worsening pressure ulcers. Tube feedings may also cause fluid overload, diarrhea, abdominal pain, local complications, less human interaction, and may increase the risk of aspiration.
Benefits of this procedure in those with advanced dementia has not been shown. The risks of using tube feeding include agitation, the person pulling out the tube or otherwise being physically or chemically immobilized to prevent them from doing this, or getting pressure ulcers. There is about a 1% fatality rate directly related to the procedure with a 3% major complication rate.
Alternative medicine.
Other therapies that have been studied for effectiveness include aromatherapy with slight evidence, massage with unclear evidence.
Palliative care.
Given the progressive and terminal nature of dementia, palliative care can be helpful to patients and their caregivers by helping both people with the disease and their caregivers understand what to expect, deal with loss of physical and mental abilities, plan out a patient’s wishes and goals including surrogate decision making, and discuss wishes for or against CPR and life support. Because the decline can be rapid, and because most people prefer to allow the person with dementia make his or her own decisions, palliative care involvement before the late stages of dementia is recommended.
Epidemiology.
The number of cases of dementia worldwide in 2010 was estimated at 35.6 million. Rates increase significantly with age, with dementia affecting 5% of the population older than 65 and 20–40% of those older than 85. Around two thirds of individuals with dementia live in low and middle income countries, where the sharpest increases in numbers are predicted. Rates are slightly higher in women than men at ages 65 and greater.
In 2013 dementia resulted in about 1.7 million deaths up from 0.8 million in 1990.
History.
Until the end of the 19th century, dementia was a much broader clinical concept. It included mental illness and any type of psychosocial incapacity, including conditions that could be reversed. Dementia at this time simply referred to anyone who had lost the ability to reason, and was applied equally to psychosis of mental illness, "organic" diseases like syphilis that destroy the brain, and to the dementia associated with old age, which was attributed to "hardening of the arteries."
Dementia has been referred to in medical texts since antiquity. One of the earliest known accounts was by the 7th century BC physician and mathematician Pythagoras, who divided the human lifespan into six distinct phases, which were 0-6 (infancy), 7-21 (adolescence), 22-49 (young adulthood), 50-62 (middle age), 63-79 (old age), and 80- (advanced age). The last two he described as the "senium", a period of mental and physical decay, and of the final phase being where "the scene of mortal existence closes after a great length of time that very fortunately, few of the human species arrive at, where the mind is reduced to the imbecility of the first epoch of infancy". In 550 BC, the Athenian statesman and poet Solon argued that the terms of a man's will might be invalidated if he exhibited loss of judgement due to advanced age. Chinese medical texts made allusions to the condition as well, and the characters for "dementia" translate literally to "foolish old person".
Aristotle and Plato both spoke of the mental decay of advanced age, but apparently simply viewed it as an inevitable process that affected all old men and which nothing could be done to prevent. The latter stated that the elderly were unsuited for any position of responsibility because "There is not much acumen of the mind that once carried them in their youth, those characteristics one would call judgement, imagination, power of reasoning, and memory. They see them gradually blunted by deterioration and can hardly fulfill their function."
For comparison, the Roman statesman Cicero held a view much more in line with modern-day medical wisdom that loss of mental function was not inevitable in the elderly and "affected only those old men who were weak-willed". He spoke of how those who remained mentally active and eager to learn new things could stave off dementia. However, Cicero's views on aging, although progressive, were largely ignored in a world that would be dominated by Aristotle's medical writings for centuries. Subsequent physicians during the time of Roman Empire such as Galen and Celsus simply repeated the beliefs of Aristotle while adding few new contributions to medical knowledge.
Byzantine physicians sometimes wrote of dementia, and it is recorded that at least seven emperors whose lifespans exceeded the age of 70 displayed signs of cognitive decline. In Constantinople, there existed special hospitals to house those diagnosed with dementia or insanity, but these naturally did not apply to the emperors who were above the law and whose health conditions could not be publicly acknowledged.
Otherwise, little is recorded about senile dementia in Western medical texts for nearly 1700 years. One of the few references to it was the 13th century friar Roger Bacon, who viewed old age as divine punishment for original sin. Although he repeated existing Aristotelian beliefs that dementia was inevitable after a long enough lifespan, he did make the extremely progressive assertion that the brain was the center of memory and thought rather than the heart.
Poets, playwrights, and other writers however made frequent allusions to the loss of mental function in old age. Shakespeare notably mentions it in some of his plays including Hamlet and King Lear.
Dementia in the elderly was called "senile dementia" or "senility," and viewed as a normal and somewhat inevitable aspect of growing old, rather than as being caused by any specific diseases. At the same time, in 1907, a specific organic dementing process of early onset, called Alzheimer's disease, had been described. This was associated with particular microscopic changes in the brain, but was seen as a rare disease of middle age because the first patient diagnosed with it was a 50-year-old woman.
During the 19th century, doctors generally came to believe that dementia in the elderly was the result of cerebral atheroslerosis, although opinions fluctuated between the idea that it was due to blockage of the major arteries supplying the brain or small strokes within the vessels of the cerebral cortex. This viewpoint remained conventional medical wisdom through the first half of the 20th century, but by the 1960s was increasingly challenged as the link between neurodegenerative diseases and age-related cognitive decline was established. By the 1970s, the medical community maintained that vascular dementia was rarer than previously thought and Alzheimer's Disease caused the vast majority of mental impairments in old age. More recently however, it is believed that dementia is often a mixture of both conditions.
Much like other diseases associated with aging, dementia was comparatively rare before the 20th century, due to the fact that it is most common in people over 80, and such lifespans were uncommon in preindustrial times. Conversely, syphilitic dementia was widespread in the developed world until largely being eradicated by the use of penicillin after WWII. With significant increases in life expectancy following WWII, the number of people in developed countries over 65 started rapidly climbing. While elderly persons constituted an average of 3-5% of the population prior to 1945, by 2010 it was common in many countries to have 10-14% of people over 65 and in Germany and Japan, this figure exceeded 20%. Public awareness of Alzheimer's Disease was greatly increased in 1994 when former US president Ronald Reagan announced that he was suffering from the condition.
By the period of 1913–20, schizophrenia had been well-defined in a way similar to today, and also the term dementia praecox had been used to suggest the development of senile-type dementia at a younger age. Eventually the two terms fused, so that until 1952 physicians used the terms "dementia praecox" (precocious dementia) and "schizophrenia" interchangeably. The term "precocious dementia" for a mental illness suggested that a type of mental illness like schizophrenia (including paranoia and decreased cognitive capacity) could be expected to arrive normally in all persons with greater age (see paraphrenia). After about 1920, the beginning use of "dementia" for what we now understand as schizophrenia and senile dementia helped limit the word's meaning to "permanent, irreversible mental deterioration." This began the change to the more recognizable use of the term today.
In 1976, neurologist Robert Katzmann suggested a link between senile dementia and Alzheimer's disease. Katzmann suggested that much of the senile dementia occurring (by definition) after the age of 65, was pathologically identical with Alzheimer's disease occurring before age 65 and therefore should not be treated differently. He noted that the fact that "senile dementia" was not considered a disease, but rather part of aging, was keeping millions of aged patients experiencing what otherwise was identical with Alzheimer's disease from being diagnosed as having a disease process, rather than simply considered as aging normally. Katzmann thus suggested that Alzheimer's disease, if taken to occur over age 65, is actually common, not rare, and was the 4th or 5th leading cause of death, even though rarely reported on death certificates in 1976.
This suggestion opened the view that dementia is never normal, and must always be the result of a particular disease process, and is not part of the normal healthy aging process, "per se". The ensuing debate led for a time to the proposed disease diagnosis of "senile dementia of the Alzheimer's type" (SDAT) in persons over the age of 65, with "Alzheimer's disease" diagnosed in persons younger than 65 who had the same pathology. Eventually, however, it was agreed that the age limit was artificial, and that Alzheimer's disease was the appropriate term for persons with the particular brain pathology seen in this disease, regardless of the age of the person with the diagnosis. A helpful finding was that although the incidence of Alzheimer's disease increased with age (from 5–10% of 75-year-olds to as many as 40–50% of 90-year-olds), there was no age at which all persons developed it, so it was not an inevitable consequence of aging, no matter how great an age a person attained. Evidence of this is shown by numerous documented supercentenarians (people living to 110+) that experienced no serious cognitive impairment. There is some evidence that dementia is most likely to develop between the ages of 80-84 and individuals who pass that point without being affected have a lower chance of developing it. Women account for a larger percentage of dementia cases than men, although this can be attributed to their longer overall lifespan and greater odds of attaining an age where the condition is likely to occur.
Also, after 1952, mental illnesses like schizophrenia were removed from the category of "organic brain syndromes," and thus (by definition) removed from possible causes of "dementing illnesses" (dementias). At the same, however, the traditional cause of senile dementia– "hardening of the arteries" – now returned as a set of dementias of vascular cause (small strokes). These were now termed "multi-infarct dementias" or "vascular dementias".
In the 21st century, a number of other types of dementia have been differentiated from Alzheimer's disease and vascular dementias (these two being the most common types). This differentiation is on the basis of pathological examination of brain tissues, symptomatology, and by different patterns of brain metabolic activity in nuclear medical imaging tests such as SPECT and PETscans of the brain. The various forms of dementia have differing prognoses (expected outcome of illness), and also differing sets of epidemologic risk factors. The causal etiology of many of them, including Alzheimer's disease, remains unclear, although many theories exist such as accumulation of protein plaques as part of normal aging, inflammation (either from bacterial pathogens or exposure to toxic chemicals), inadequate blood sugar, and traumatic brain injury.
Society and culture.
Many countries consider the care of people living with dementia to be a national priority, and invest in resources and education to better inform health and social service workers, unpaid caregivers, relatives and members of the wider community. Several countries have national plans or strategies. In these national plans, there is recognition that people can live well with dementia for a number of years, as long as there is the right support and timely access to a diagnosis. David Cameron has described dementia as being a "national crisis", affecting 800,000 people in the United Kingdom.
In the United States, Florida's Baker Act allows law-enforcement authorities and the judiciary to force mental evaluation for those suspected of having developed dementia or other mental incapacities. In the United Kingdom, as with all mental disorders, where a person with dementia could potentially be a danger to themselves or others, they can be detained under the Mental Health Act 1983 for the purposes of assessment, care and treatment. This is a last resort, and usually avoided if the patient has family or friends who can ensure care.
Driving with dementia could lead to severe injury or even death to self and others. Doctors should advise appropriate testing on when to quit driving. The United Kingdom DVLA (Driver & Vehicle Licensing Agency) states that people with dementia who specifically have poor short term memory, disorientation, lack of insight or judgment are not fit to drive, and in these instances the DVLA must be informed so that the driving licence can be revoked. They do, however, acknowledge low-severity cases and those with an early diagnosis, and those drivers may be permitted to drive pending medical reports.
There are many support networks available to those who have a diagnosis of dementia, and their families and caregivers. There are also charitable organisations which aim to raise awareness and campaign for the rights of people living with dementia. There is also support and guidance on assessing testamentary capacity in people who have dementia.

</doc>
<doc id="38432" url="http://en.wikipedia.org/wiki?curid=38432" title="1103">
1103

Year 1103 (MCIII) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="38449" url="http://en.wikipedia.org/wiki?curid=38449" title="Affine transformation">
Affine transformation

In geometry, an affine transformation, affine map or an affinity (from the Latin, "affinis", "connected with") is a function between affine spaces which preserves points, straight lines and planes. Also, sets of parallel lines remain parallel after an affine transformation. An affine transformation does not necessarily preserve angles between lines or distances between points, though it does preserve ratios of distances between points lying on a straight line.
Examples of affine transformations include translation, scaling, homothety, similarity transformation, reflection, rotation, shear mapping, and compositions of them in any combination and sequence.
If formula_1 and formula_2 are affine spaces, then every affine transformation formula_3 is of the form formula_4, where formula_5 is a linear transformation on formula_1 and formula_7 is a vector in formula_2. Unlike a purely linear transformation, an affine map need not preserve the zero point in a linear space. Thus, every linear transformation is affine, but not every affine transformation is linear.
For many purposes an affine space can be thought of as Euclidean space, though the concept of affine space is far more general (i.e., all Euclidean spaces are affine, but there are affine spaces that are non-Euclidean). In affine coordinates, which include Cartesian coordinates in Euclidean spaces, each output coordinate of an affine map is a linear function (in the sense of calculus) of all input coordinates. Another way to deal with affine transformations systematically is to select a point as the origin; then, any affine transformation is equivalent to a linear transformation (of position vectors) followed by a translation.
Mathematical definition.
An affine map formula_9 between two affine spaces is a map on the points that acts linearly on the vectors (that is, the vectors between points of the space). In symbols, "formula_10" determines a linear transformation "formula_11" such that, for any pair of points formula_12:
or
We can interpret this definition in a few other ways, as follows.
If an origin formula_15 is chosen, and formula_16 denotes its image formula_17, then this means that for any vector formula_18:
If an origin formula_20 is also chosen, this can be decomposed as an affine transformation formula_21 that sends formula_22, namely
followed by the translation by a vector formula_24.
The conclusion is that, intuitively, formula_10 consists of a translation and a linear map.
Alternative definition.
Given two affine spaces formula_26 and formula_27, over the same field, a function formula_28 is an affine map if and only if for every family formula_29 of weighted points in formula_26 such that 
we have
In other words, formula_10 preserves barycenters.
Representation.
As shown above, an affine map is the composition of two functions: a translation and a linear map. Ordinary vector algebra uses matrix multiplication to represent linear maps, and vector addition to represent translations. Formally, in the finite-dimensional case, if the linear map is represented as a multiplication by a matrix "A" and the translation as the addition of a vector formula_34, an affine map formula_10 acting on a vector formula_18 can be represented as
Augmented matrix.
Using an augmented matrix and an augmented vector, it is possible to represent both the translation and the linear map using a single matrix multiplication. The technique requires that all vectors are augmented with a "1" at the end, and all matrices are augmented with an extra row of zeros at the bottom, an extra column—the translation vector—to the right, and a "1" in the lower right corner. If "A" is a matrix,
is equivalent to the following
The above-mentioned augmented matrix is called "affine transformation matrix", or "projective transformation matrix" (as it can also be used to perform projective transformations).
This representation exhibits the set of all invertible affine transformations as the semidirect product of "K""n" and GL("n", "k"). This is a group under the operation of composition of functions, called the affine group.
Ordinary matrix-vector multiplication always maps the origin to the origin, and could therefore never represent a translation, in which the origin must necessarily be mapped to some other point. By appending the additional coordinate "1" to every vector, one essentially considers the space to be mapped as a subset of a space with an additional dimension. In that space, the original space occupies the subset in which the additional coordinate is 1. Thus the origin of the original space can be found at (0,0, ... 0, 1). A translation within the original space by means of a linear transformation of the higher-dimensional space is then possible (specifically, a shear transformation). The coordinates in the higher-dimensional space are an example of homogeneous coordinates. If the original space is Euclidean, the higher dimensional space is a real projective space.
The advantage of using homogeneous coordinates is that one can combine any number of affine transformations into one by multiplying the respective matrices. This property is used extensively in computer graphics, computer vision and robotics.
Example augmented matrix.
If the vectors formula_40 are a basis of the domain's projective vector space and if formula_41 are the corresponding vectors in the codomain vector space then the augmented matrix "M" that achieves this affine transformation
is
This formulation works irrespective of whether any of the domain, codomain and image vector spaces have the same number of dimensions.
For example, the affine transformation of a vector plane is uniquely determined from the knowledge of where the three vertices of a non-degenerate triangle are mapped to.
Properties.
An affine transformation preserves:
An affine transformation is invertible if and only if "A" is invertible. In the matrix representation, the inverse is:
The invertible affine transformations (of an affine space onto itself) form the affine group, which has the general linear group of degree "n" as subgroup and is itself a subgroup of the general linear group of degree "n" + 1.
The similarity transformations form the subgroup where "A" is a scalar times an orthogonal matrix. For example, if the affine transformation acts on the plane and if the determinant of "A" is 1 or −1 then the transformation is an equi-areal mapping. Such transformations form a subgroup called the "equi-affine group" A transformation that is both equi-affine and a similarity is an isometry of the plane taken with Euclidean distance.
Each of these groups has a subgroup of transformations which preserve orientation: those where the determinant of "A" is positive. In the last case this is in 3D the group of rigid body motions (proper rotations and pure translations).
If there is a fixed point, we can take that as the origin, and the affine transformation reduces to a linear transformation. This may make it easier to classify and understand the transformation. For example, describing a transformation as a rotation by a certain angle with respect to a certain axis may give a clearer idea of the overall behavior of the transformation than describing it as a combination of a translation and a rotation. However, this depends on application and context.
Affine transformation of the plane.
Affine transformations in two real dimensions include:
To visualise the general affine transformation of the Euclidean plane, take labelled parallelograms "ABCD" and "A′B′C′D′". Whatever the choices of points, there is an affine transformation "T" of the plane taking "A" to "A′", and each vertex similarly. Supposing we exclude the degenerate case where "ABCD" has zero area, there is a unique such affine transformation "T". Drawing out a whole grid of parallelograms based on "ABCD", the image "T"("P") of any point "P" is determined by noting that "T"("A") = "A′", "T" applied to the line segment "AB" is "A′B′", "T" applied to the line segment "AC" is "A′C′", and "T" respects scalar multiples of vectors based at "A". [If "A", "E", "F" are collinear then the ratio length("AF")/length("AE") is equal to length("A"′"F"′)/length("A"′"E"′).] Geometrically "T" transforms the grid based on "ABCD" to that based in "A′B′C′D′".
Affine transformations don't respect lengths or angles; they multiply area by a constant factor
A given "T" may either be "direct" (respect orientation), or "indirect" (reverse orientation), and this may be determined by its effect on "signed" areas (as defined, for example, by the cross product of vectors).
Examples of affine transformations.
Affine transformations over the real numbers.
Functions "f" : R → R, "f"("x") = "mx" + "c" with "m" and "c" constant, are commonplace affine transformations.
Affine transformation over a finite field.
The following equation expresses an affine transformation in GF(28):
For instance, the affine transformation of the element {a} = "y"7 + "y"6 + "y"3 + "y" = {11001010} in big-endian binary notation = {CA} in big-endian hexadecimal notation, is calculated as follows:
Thus, {"a"′} = "y"7 + "y"6 + "y"5 + "y"3 + "y"2 + 1 = {11101101} = {ED}.
Affine transformation in plane geometry.
In ℝ2, the transformation shown at left is accomplished using the map given by:
Transforming the three corner points of the original triangle (in red) gives three new points which form the new triangle (in blue). This transformation skews and translates the original triangle.
In fact, all triangles are related to one another by affine transformations. This is also true for all parallelograms, but not for all quadrilaterals.

</doc>
<doc id="38454" url="http://en.wikipedia.org/wiki?curid=38454" title="Gravitational constant">
Gravitational constant

The gravitational constant, approximately and denoted by letter "G", is an empirical physical constant involved in the calculation(s) of gravitational force between two bodies. It usually appears in Sir Isaac Newton's law of universal gravitation, and in Albert Einstein's general theory of relativity. It is also known as the universal gravitational constant, Newton's constant, and colloquially as Big G. It should not be confused with "small g" ("g"), which is the local gravitational field (equivalent to the free-fall acceleration).
Laws and constants.
According to the law of universal gravitation, the attractive force ("F") between two bodies is directly proportional to the product of their masses ("m"1 and "m"2), and inversely proportional to the square of the distance, "r", (inverse-square law) between them:
The constant of proportionality, "G", is the gravitational constant.
The gravitational constant is a physical constant that is difficult to measure with high accuracy. In SI units, the 2010 CODATA-recommended value of the gravitational constant (with standard uncertainty in parentheses) is:
with relative standard uncertainty .
Dimensions, units, and magnitude.
The dimensions assigned to the gravitational constant in the equation above—length cubed, divided by mass, and by time squared (in SI units, meters cubed per kilogram per second squared)—are those needed to balance the units of measurements in gravitational equations. However, these dimensions have fundamental significance in terms of Planck units; when expressed in SI units, the gravitational constant is dimensionally and numerically equal to the cube of the Planck length divided by the product of the Planck mass and the square of Planck time.
In natural units, of which Planck units are a common example, "G" and other physical constants such as "c" (the speed of light) may be set equal to 1.
In many secondary school texts, the dimensions of "G" are derived from force in order to assist student comprehension:
In cgs, "G" can be written as:
"G" can also be given as:
Given the fact that the period "P" of an object in circular orbit around a spherical object obeys
where "V" is the volume inside the radius of the orbit, we see that
This way of expressing "G" shows the relationship between the average density of a planet and the period of a satellite orbiting just above its surface.
In some fields of astrophysics, where distances are measured in parsecs (pc), velocities in kilometers per second (km/s) and masses in solar units (formula_8), it is useful to express "G" as:
The gravitational force is extremely weak compared with other fundamental forces. For example, the gravitational force between an electron and proton one meter apart is approximately , whereas the electromagnetic force between the same two particles is approximately . Both these forces are weak when compared with the forces we are able to experience directly, but the electromagnetic force in this example is some 39 orders of magnitude (i.e. 1039) greater than the force of gravity—roughly the same ratio as the mass of the Sun compared to a microgram.
History of measurement.
The gravitational constant appears in Newton's law of universal gravitation, but it was not measured until seventy-one years after Newton's death by Henry Cavendish with his Cavendish experiment, performed in 1798 ("Philosophical Transactions" 1798). Cavendish measured "G" implicitly, using a torsion balance invented by the geologist Rev. John Michell. He used a horizontal torsion beam with lead balls whose inertia (in relation to the torsion constant) he could tell by timing the beam's oscillation. Their faint attraction to other balls placed alongside the beam was detectable by the deflection it caused. Cavendish's aim was not actually to measure the gravitational constant, but rather to measure the Earth's density relative to water, through the precise knowledge of the gravitational interaction. In retrospect, the density that Cavendish calculated implies a value for "G" of .
The accuracy of the measured value of "G" has increased only modestly since the original Cavendish experiment. "G" is quite difficult to measure, as gravity is much weaker than other fundamental forces, and an experimental apparatus cannot be separated from the gravitational influence of other bodies. Furthermore, gravity has no established relation to other fundamental forces, so it does not appear possible to calculate it indirectly from other constants that can be measured more accurately, as is done in some other areas of physics. Published values of "G" have varied rather broadly, and some recent measurements of high precision are, in fact, mutually exclusive. This led to the 2010 CODATA value by NIST having 20% increased uncertainty than in 2006.
In the January 2007 issue of "Science", Fixler et al. described a new measurement of the gravitational constant by atom interferometry, reporting a value of "G" = . An improved cold atom measurement by Rosi et al. was published in 2014 of "G"= .
A 2015 study of all previous measurements of "G" led to the discovery that most of the mutually exclusive values can be explained by a periodic variation. This variation has a period of 5.9 years, similar to one observed in length of day (LOD) measurements, perhaps hinting a common physical cause of these systematic variations. The observed behaviour can be confirmed by repeated measurements over the next half decade or so. Combined with new methods of measuring "G", such as quantum interferometry, this could help establish a more precise value of "G".
Under the assumption that the physics of type Ia supernovae are universal, analysis of observations of 580 type Ia supernovae has shown that the gravitational constant has varied by less than one part in ten billion per year over the last nine billion years.
The "GM" product.
The quantity "GM"—the product of the gravitational constant and the mass of a given astronomical body such as the Sun or the Earth—is known as the standard gravitational parameter and is denoted formula_10. Depending on the body concerned, it may also be called the geocentric or heliocentric gravitational constant, among other names.
This quantity gives a convenient simplification of various gravity-related formulas. Also, for celestial bodies such as the Earth and the Sun, the value of the product "GM" is known much more accurately than each factor independently. Indeed, the limited accuracy available for "G" often limits the accuracy of scientific determination of such masses in the first place.
For Earth, using formula_11 as the symbol for the mass of the Earth, we have
For Sun, we have
Calculations in celestial mechanics can also be carried out using the unit of solar mass rather than the standard SI unit kilogram. In this case we use the Gaussian gravitational constant "k", where
and
If instead of mean solar day we use the sidereal year as our time unit, the value of "ks" is very close to 2π ("k" = ).
The standard gravitational parameter "GM" appears as above in Newton's law of universal gravitation, as well as in formulas for the deflection of light caused by gravitational lensing, in Kepler's laws of planetary motion, and in the formula for escape velocity.
References.
</dl>

</doc>
<doc id="38476" url="http://en.wikipedia.org/wiki?curid=38476" title="Franche-Comté">
Franche-Comté

Franche-Comté (]; literally "Free County", Frainc-Comtou dialect: "Fraintche-Comtè"; Arpitan: "Franche-Comtât") is an administrative region and a traditional province of eastern France. It is composed of the modern departments of Doubs, Jura, Haute-Saône and Territoire de Belfort and has a population of 1,168,208 (2009).
The region is named after the "Franche Comté de Bourgogne" (Free County of Burgundy), definitively separated from what is now the region of Burgundy proper in the 15th century. In 2016, these two halves of the historic Kingdom of Burgundy will be reunited as the region of Bourgogne-Franche-Comté.
The principal cities are the capital Besançon, Belfort, and Montbéliard (latter two form the "aire urbaine" Belfort-Montbéliard-Héricourt-Delle). Other important cities are Dole (capital before the region was conquered by Louis XIV in the late 17th century), Vesoul (capital of Haute-Saône), Arbois (the "wine capital" of the Jura), and Lons-le-Saunier (capital of Jura).
History.
The region has been inhabited since the Paleolithic age and was occupied by the Gauls. Little touched by the Germanic migrations, it was part of the territory of the Alemanni in the 5th century, then the Kingdom of Burgundy from 457 to 534. It was Christianized through the influence of St. Columbanus, who founded several monasteries there. In 534, it became part of the Frankish kingdom. In 561 it was included in the Merovingian Kingdom of Burgundy under Guntram, the third son of Clotaire I. In 613, Clotaire II reunited the Frankish Kingdom under his rule, and the region remained a part of the Kingdom of Burgundy under the later Merovingians and Carolingians.
The name "Franche Comté de Bourgogne" (Free County of Burgundy) did not officially appear until 1366. It had been a territory of the County of Burgundy from 888, the province becoming subject to the Holy Roman Empire in 1034. It was definitively separated from the neighboring Duchy of Burgundy upon the latter's incorporation into the Kingdom of France in 1477. That year at the Battle of Nancy during the Burgundian Wars, the last duke, Charles the Bold, was killed in battle. It was incorporated into the territories of the Habsburg monarchy with the marriage of Mary of Burgundy with Maximilian I. The territory was inherited by Philip II of Spain, from his father the emperor Charles V. Franche-Comté was captured by France in 1668 but returned under the Treaty of Aix-la-Chapelle. It was conquered a second time in 1674, and was finally ceded to France in the Treaty of Nijmegen (1678). Enclaves such as Montbéliard remained outside French control.
Franche-Comté was one of the last parts of France to have serfdom. In 1784, half the population were serfs, accounting for 400,000 out of the 1 million French serfs. Landowners took one-twelfth of the sales price if a serf ("mainmortable") wanted to sell up. Serfs were not forced to stay on the land, but the lord could claim "droit de suite", whereby a peasant who died away from his holding left it to the lord, even if he had heirs. A runaway serf's land was forfeit after ten years. Louis XVI issued a decree banning these practices on 8 August 1779, but the Parlement of Besançon blocked this until 1787.
The region's population fell by a fifth from 1851 to 1946, reflecting low French natural growth and migration to more urbanized parts of the country. Most of the decline occurred in Haute-Saône and Jura, which remain among the country's more agriculture-dependent areas.
Environment.
This region borders Switzerland and shares much of its architecture, cuisine, and culture with its neighbour. Between the Vosges range of mountains to the north and the Jura range to the south, the landscape consists of rolling cultivated fields, dense pine forest and rampart-like mountains. Not as majestic as the Alps, the Jura mountains are more accessible and are France's first cross-country skiing area. It is also a superb place to hike, and there are some fine nature trails on the more gentle slopes. The Doubs and Loue valleys, with their timbered houses perched on stilts in the river, and the high valley of Ain, are popular visitor areas. The "Région des Lacs" is a land of gorges and waterfalls dotted with tiny villages, each with a domed belfry decorated with mosaic of tiles or slates or beaten from metal. The lakes are perfect for swimming in the warmer months. The summits of Haut Jura have wonderful views across Lac Léman (Lake Geneva) and towards the Alps.
Forty percent of the region's GDP is dependent on manufacturing activities, and most of its production is exported. Construction of automobiles and their parts is one of the most buoyant industries here. Forestry exploitation is steadily growing, and 38% of the agriculture is dairy and 17% cattle farming. The region has a large and lucrative cheese-making industry, with 40 million tonnes of cheese produced here each year, much of which is made by "fruitières" (traditional cheese dairies of Franche-Comté); for instance, Comté cheese comes from this region.
Language.
Among the regional languages of France, the term "Franc-comtois" refers to two dialects of two different languages. Franc-comtois is the name of both the dialect of Langue d'Oïl spoken by people in the northern part of the region and the dialect of Arpitan spoken in its southern part since as early as the 13th century (the southern two-thirds of Jura and the southern third of Doubs). Both are recognized as languages of France.

</doc>
<doc id="38498" url="http://en.wikipedia.org/wiki?curid=38498" title="Frog">
Frog

Frogs are a diverse and largely carnivorous group of short-bodied, tailless amphibians composing the order Anura (Ancient Greek "an-," without + "oura", tail). The oldest fossil "proto-frog" appeared in the early Triassic of Madagascar, but molecular clock dating suggests their origins may extend further back to the Permian, 265 million years ago. Frogs are widely distributed, ranging from the tropics to subarctic regions, but the greatest concentration of species diversity is found in tropical rainforests. There are approximately 4,800 recorded species, accounting for over 85% of extant amphibian species. They are also one of the five most diverse vertebrate orders.
The body plan of an adult frog is generally characterized by a stout body, protruding eyes, cleft tongue, limbs folded underneath, and the absence of a tail in adults. Besides living in fresh water and on dry land, the adults of some species are adapted for living underground or in trees. The skin of the frog is glandular, with secretions ranging from distasteful to toxic. Warty species of frog tend to be called toads but the distinction between frogs and toads is based on informal naming conventions concentrating on the warts rather than taxonomy or evolutionary history; some toads are more closely related to frogs than to other toads. Frogs' skins vary in colour from well-camouflaged dappled brown, grey and green to vivid patterns of bright red or yellow and black to advertise toxicity and warn off predators.
Frogs typically lay their eggs in water. The eggs hatch into aquatic larvae called tadpoles that have tails and internal gills. They have highly specialized rasping mouth parts suitable for herbivorous, omnivorous or planktivorous diets. The life cycle is completed when they metamorphose into adults. A few species deposit eggs on land or bypass the tadpole stage. Adult frogs generally have a carnivorous diet consisting of small invertebrates, but omnivorous species exist and a few feed on fruit. Frogs are extremely efficient at converting what they eat into body mass, which makes them an important food source for predators. Frogs are a keystone group in the food web dynamics of many of the world's ecosystems. The skin is semi-permeable, making them susceptible to dehydration, so they either live in moist places or have special adaptations to deal with dry habitats. Frogs produce a wide range of vocalizations, particularly in their breeding season, and exhibit many different kinds of complex behaviours to attract mates, to fend off predators and to generally survive.
Frog populations have declined significantly since the 1950s. More than one third of species are considered to be threatened with extinction and over one hundred and twenty are believed to have become extinct since the 1980s. The number of malformations among frogs is on the rise and an emerging fungal disease, chytridiomycosis, has spread around the world. Conservation biologists are working to understand the causes of these problems and to resolve them. Frogs are valued as food by humans and also have many cultural roles in literature, symbolism and religion.
Etymology and taxonomy.
The name frog derives from Old English "frogga", abbreviated to "frox", "forsc", and "frosc", probably deriving from Proto-Indo-European "preu" = "to jump". About 88% of amphibian species are classified in the order Anura. These include around 4,810 species in 33 families, of which the Leptodactylidae (1,100 spp.), Hylidae (800 spp.) and Ranidae (750 spp.) are the richest in species.
The use of the common names "frog" and "toad" has no taxonomic justification. From a classification perspective, all members of the order Anura are frogs, but only members of the family Bufonidae are considered "true toads". The use of the term "frog" in common names usually refers to species that are aquatic or semi-aquatic and have smooth, moist skins; the term "toad" generally refers to species that are terrestrial with dry, warty skins. There are numerous exceptions to this rule. The European fire-bellied toad ("Bombina bombina") has a slightly warty skin and prefers a watery habitat whereas the Panamanian golden frog ("Atelopus zeteki") is in the toad family Bufonidae and has a smooth skin.
The Anura include all modern frogs and any fossil species that fit within the anuran definition. The characteristics of anuran adults include: 9 or fewer presacral vertebrae, the presence of a urostyle formed of fused vertebrae, no tail, a long and forward-sloping ilium, shorter fore limbs than hind limbs, radius and ulna fused, tibia and fibula fused, elongated ankle bones, absence of a prefrontal bone, presence of a hyoid plate, a lower jaw without teeth (with the exception of "Gastrotheca guentheri") consisting of three pairs of bones (angulosplenial, dentary, and mentomeckelian, with the last pair being absent in Pipoidea), an unsupported tongue, lymph spaces underneath the skin, and a muscle, the protractor lentis, attached to the lens of the eye. The anuran larva or tadpole has a single central respiratory spiracle and mouthparts consisting of keratinous beaks and denticles.
Frogs and toads are broadly classified into three suborders: Archaeobatrachia, which includes four families of primitive frogs; Mesobatrachia, which includes five families of more evolutionary intermediate frogs; and Neobatrachia, by far the largest group, which contains the remaining 24 families of modern frogs, including most common species found throughout the world. The Neobatrachia suborder is further divided into the two superfamilies Hyloidea and Ranoidea. This classification is based on such morphological features as the number of vertebrae, the structure of the pectoral girdle, and the morphology of tadpoles. While this classification is largely accepted, relationships among families of frogs are still debated.
Some species of anurans hybridize readily. For instance, the edible frog ("Pelophylax esculentus") is a hybrid between the pool frog ("P. lessonae") and the marsh frog ("P. ridibundus"). The fire-bellied toads "Bombina bombina" and "B. variegata" are similar in forming hybrids. These are less fertile than their parents, giving rise to a hybrid zone where the hybrids are prevalent.
Evolution.
The origins and evolutionary relationships between the three main groups of amphibians are hotly debated. A molecular phylogeny based on rDNA analysis dating from 2005 suggests salamanders and caecilians are more closely related to each other than they are to frogs and the divergence of the three groups took place in the Paleozoic or early Mesozoic before the breakup of the supercontinent Pangaea and soon after their divergence from the lobe-finned fishes. This would help account for the relative scarcity of amphibian fossils from the period before the groups split. Another molecular phylogenetic analysis conducted about the same time concluded the lissamphibians first appeared about 330 million years ago and that the temnospondyl-origin hypothesis is more credible than other theories. The neobatrachians seemed to have originated in Africa/India, the salamanders in East Asia and the caecilians in tropical Pangaea. Other researchers, while agreeing with the main thrust of this study, questioned the choice of calibration points used to synchronise the data. They proposed that the date of lissamphibian diversification be put in the Permian, rather less than 300 million years ago, a date in better agreement with the palaeontological data. A further study in 2011 using both extinct and living taxa sampled for morphological, as well as molecular data, came to the conclusion that Lissamphibia is monophyletic and that it should be nested within Lepospondyli rather than within Temnospondyli. The study postulated the Lissamphibia originated no earlier than the late Carboniferous, some 290 to 305 million years ago. The split between Anura and Caudata was estimated as taking place 292 million years ago, rather later than most molecular studies suggest, with the caecilians splitting off 239 million years ago.
In 2008, "Gerobatrachus hottoni", a temnospondyl with many frog- and salamander-like characteristics, was discovered in Texas. It dated back 290 million years and was hailed as a missing link, a stem batrachian close to the common ancestor of frogs and salamanders, consistent with the widely accepted hypothesis that frogs and salamanders are more closely related to each other (forming a clade called Batrachia) than they are to caecilians. However, others have suggested that "Gerobatrachus hottoni" was only a dissorophoid temnospondyl unrelated to extant amphibians. The earliest known salientians (see below), closer to the extant frogs than to the extant salamanders, are "Triadobatrachus massinoti", from the Early Triassic of Madagascar (about 250 million years ago), and the fragmentary "Czatkobatrachus polonicus" from the Early Triassic of Poland (about the same age as "Triadobatrachus"). The skull of "Triadobatrachus" is frog-like, being broad with large eye sockets, but the fossil has features diverging from modern frogs. These include a longer body with more vertebrae. The tail has separate vertebrae unlike the fused urostyle or coccyx found in modern frogs. The tibia and fibula bones are also separate, making it probable that "Triadobatrachus" was not an efficient leaper.
Salientia (Latin "salere" ("salio"), "to jump") is a stem group including modern frogs in the order Anura and their close fossil relatives the "proto-frogs" (e.g., "Triadobatrachus" and "Czatkobatrachus"). The common features possessed by the "proto-frogs" in the Salientia group include 14 presacral vertebrae (modern frogs have eight or 9), a long and forward-sloping ilium in the pelvis, the presence of a frontoparietal bone, and a lower jaw without teeth. The earliest frog fossil that falls into the anuran lineage proper, "Prosalirus bitis", lived in the early Jurassic. It was discovered in 1995 in the Kayenta Formation of Arizona and dates back to the Early Jurassic epoch (199.6 to 175 million years ago), making "Prosalirus" somewhat more recent than "Triadobatrachus". Like the latter, "Prosalirus" did not have greatly enlarged legs, but had the typical three-pronged pelvic structure of modern frogs. Unlike "Triadobatrachus", "Prosalirus" had already lost nearly all of its tail and was well adapted for jumping.
The earliest known "true frog" is "Vieraella herbsti", from the Early Jurassic. It is known only from the dorsal and ventral impressions of a single animal and was estimated to be 33 mm from snout to vent. "Notobatrachus degiustoi" from the middle Jurassic is slightly younger, about 155–170 million years old. The main evolutionary changes in this species involved the shortening of the body and the loss of the tail. The evolution of modern Anura likely was complete by the Jurassic period. Since then, evolutionary changes in chromosome numbers have taken place about 20 times faster in mammals than in frogs, which means speciation is occurring more rapidly in mammals.
Frog fossils have been found on all continents except Antarctica, but biogeographic evidence suggests they also inhabited Antarctica in an earlier era when the climate was warmer.
A cladogram showing the relationships of the different families of frogs in the clade Anura can be seen in the table above. This diagram, in the form of a tree, shows how each frog family is related to other families, with each node representing a point of common ancestry. It is based on Frost "et al." (2006), Heinicke "et al." (2009) and Pyron and Wiens (2011).
Morphology and physiology.
Frogs have no tail, except as larvae, and most have long hind legs, elongated ankle bones, webbed toes, no claws, large eyes, and a smooth or warty skin. They have short vertebral columns, with no more than 10 free vertebrae and fused tailbones (urostyle or coccyx). Like other amphibians, oxygen can pass through their highly permeable skins. This unique feature allows them to remain in places without access to the air, respiring through their skins. The ribs are poorly developed, so the lungs are filled by buccal pumping and a frog deprived of its lungs can maintain its body functions without them. For the skin to serve as a respiratory organ, it must remain moist. This makes frogs susceptible to various substances they may encounter in the environment, some of which may be toxic and can dissolve in the water film and be passed into their bloodstream. This may be one of the causes of the worldwide decline in frog populations.
Frogs range in size from the recently discovered 7.7 mm "Paedophryne amauensis" of Papua New Guinea to the 300 mm goliath frog ("Conraua goliath") of Cameroon. The skin hangs loosely on the body because of the lack of loose connective tissue. Frogs have three eyelid membranes: one is transparent to protect the eyes underwater, and two vary from translucent to opaque. They have a tympanum on each side of their heads which is involved in hearing and, in some species, is covered by skin. True toads completely lack teeth, but most frogs have them, specifically pedicellate teeth in which the crown is separated from the root by fibrous tissue. These are on the edge of the upper jaw and vomerine teeth are also on the roof of their mouths. No teeth are in the lower jaw and frogs usually swallow their food whole. The teeth are mainly used to grip the prey and keep it in place till swallowed, a process assisted by retracting the eyes into the head. The African bullfrog ("Pyxicephalus"), which preys on relatively large animals such as mice and other frogs, has cone shaped bony projections called odontoid processes at the front of the lower jaw which function like teeth.
Feet and legs.
The structure of the feet and legs varies greatly among frog species, depending in part on whether they live primarily on the ground, in water, in trees or in burrows. Frogs must be able to move quickly through their environment to catch prey and escape predators, and numerous adaptations help them to do so. Most frogs are either proficient at jumping or are descended from ancestors that were, with much of the musculoskeletal morphology modified for this purpose. The tibia, fibula, and tarsals have been fused into a single, strong bone, as have the radius and ulna in the fore limbs (which must absorb the impact on landing). The metatarsals have become elongated to add to the leg length and allow the frog to push against the ground for a longer period on take-off. The illium has elongated and formed a mobile joint with the sacrum which, in specialist jumpers such as ranids and hylids, functions as an additional limb joint to further power the leaps. The tail vertebrae have fused into a urostyle which is retracted inside the pelvis. This enables the force to be transferred from the legs to the body during a leap.
The muscular system has been similarly modified. The hind limbs of ancestral frogs presumably contained pairs of muscles which would act in opposition (one muscle to flex the knee, a different muscle to extend it), as is seen in most other limbed animals. However, in modern frogs, almost all muscles have been modified to contribute to the action of jumping, with only a few small muscles remaining to bring the limb back to the starting position and maintain posture. The muscles have also been greatly enlarged, with the main leg muscles accounting for over 17% of the total mass of the frog.
Many frogs have webbed feet and the degree of webbing is directly proportional to the amount of time the species spends in the water. The completely aquatic African dwarf frog ("Hymenochirus" sp.) has fully webbed toes, whereas those of White's tree frog ("Litoria caerulea"), an arboreal species, are only a quarter or half webbed.
Arboreal frogs have pads located on the ends of their toes to help grip vertical surfaces. These are not suction pads, the surface consisting instead of columnar cells with flat tops with small gaps between them lubricated by mucous glands. When the frog applies pressure, the cells adhere to irregularities on the surface and the grip is maintained through surface tension. This allows the frog to climb on smooth surfaces, but the system does not function efficiently when the pads are excessively wet.
In many arboreal frogs, a small "intercalary structure" on each toe increases the surface area touching the substrate. Furthermore, since hopping through trees can be dangerous, many arboreal frogs have hip joints to allow both hopping and walking. Some frogs that live high in trees even possess an elaborate degree of webbing between their toes. This allows the frogs to "parachute" or make a controlled glide from one position in the canopy to another.
Ground-dwelling frogs generally lack the adaptations of aquatic and arboreal frogs. Most have smaller toe pads, if any, and little webbing. Some burrowing frogs such as Couch's spadefoot ("Scaphiopus couchii") have a flap-like toe extension on the hind feet, a keratinised tubercle often referred to as a spade, that helps them to burrow.
Sometimes during the tadpole stage, one of the developing rear legs is eaten by a predator such as a dragonfly nymph. In some cases, the full leg still grows, but in others it does not, although the frog may still live out its normal lifespan with only three limbs. Occasionally, a parasitic flatworm ("Ribeiroia ondatrae") digs into the rear of a tadpole, causing a rearrangement of the limb bud cells and the frog develops an extra leg or two.
Skin.
A frog's skin is protective, has a respiratory function, can absorb water and helps control body temperature. It has many glands, particularly on the head and back, which often exude distasteful and toxic substances. The secretion is often sticky and helps keep the skin moist, protects against the entry of moulds and bacteria, and make the animal slippery and more able to escape from predators. The skin is shed every few weeks. It usually splits down the middle of the back and across the belly, and the frog pulls its arms and legs free. The sloughed skin is then worked towards the head where it is quickly eaten.
Being cold-blooded, frogs have to adopt suitable behaviour patterns to regulate their temperature. To warm up, they can move into the sun or onto a warm surface; if they overheat, they can move into the shade or adopt a stance that exposes the minimum area of skin to the air. This posture is also used to prevent water loss and involves the frog squatting close to the substrate with its hands and feet tucked under its chin and body. The colour of a frog's skin is used for thermoregulation. In cool damp conditions, the colour will be darker than on a hot dry day. The grey foam-nest tree frog ("Chiromantis xerampelina") is even able to turn white to minimize the chance of overheating.
Many frogs are able to absorb water and oxygen directly through the skin, especially around the pelvic area, but the permeability of a frog's skin can also result in water loss. Glands located all over the body exude mucus which helps keep the skin moist and reduces evaporation. Some glands on the hands and chest of males are specialized to produce sticky secretions to aid in amplexus. Similar glands in tree frogs produce a glue-like substance on the adhesive discs of the feet. Some arboreal frogs reduce water loss by having a waterproof layer of skin, and several South American species coat their skin with a waxy secretion. Others frogs have adopted behaviours to conserve water, including becoming nocturnal and resting in a water-conserving position. Some frogs may also rest in large groups with each frog pressed against its neighbours. This reduces the amount of skin exposed to the air or a dry surface, and thus reduces water loss. Woodhouse's toad ("Bufo woodhousii"), if given access to water after confinement in a dry location, sits in the shallows to rehydrate. The male hairy frog ("Trichobatrachus robustus") has dermal papillae projecting from its lower back and thighs, giving it a bristly appearance. They contain blood vessels and are thought to increase the area of the skin available for respiration.
Some species have bony plates embedded in their skin, a trait that appears to have evolved independently several times. In certain other species, the skin at the top of the head is compacted and the connective tissue of the dermis is co-ossified with the bones of the skull (exostosis).
Camouflage is a common defensive mechanism in frogs. Most camouflaged frogs are nocturnal; during the day, they seek out a position where they can blend into the background and remain undetected. Some frogs have the , but this is usually restricted to a small range of colours. For example, White's tree frog ("Litoria caerulea") varies between pale green and dull brown according to the temperature, and the Pacific tree frog ("Pseudacris regilla") has green and brown morphs, plain or spotted, and changes colour depending on the time of year and general background colour. Features such as warts and skin folds are usually found on ground-dwelling frogs, where a smooth skin would not provide such effective camouflage. Certain frogs change colour between night and day, as light and moisture stimulate the pigment cells and cause them to expand or contract.
Pouched frog ("Assa darlingtoni") camouflaged against leaf litter
Respiration and circulation.
The skin of a frog is permeable to oxygen and carbon dioxide, as well as to water. There are blood vessels near the surface of the skin and when a frog is underwater, oxygen diffuses directly into the blood. When not submerged, a frog breathes by a process known as buccal pumping. Its lungs are similar to those of humans, but the chest muscles are not involved in respiration, and no ribs or diaphragm exist to help move air in and out. Instead, it puffs out its throat and draws air in through the nostrils, which in many species can then be closed by valves. When the floor of the mouth is compressed, air is forced into the lungs. The Borneo flat-headed frog ("Barbourula kalimantanensis") was first discovered in a remote part of Indonesia in 2007. It is entirely aquatic and is the first species of frog known to science that has no lungs.
Frogs have three-chambered hearts, a feature they share with lizards. Oxygenated blood from the lungs and de-oxygenated blood from the respiring tissues enter the heart through separate atria. When these chambers contract, the two blood streams pass into a common ventricle before being pumped via a spiral valve to the appropriate vessel, the aorta for oxygenated blood and pulmonary artery for deoxygenated blood. The ventricle is partially divided into narrow cavities which minimizes the mixing of the two types of blood. These features enable frogs to have a higher metabolic rate and be more active than would otherwise be possible.
Some species of frog have adaptations that allow them to survive in oxygen deficient water. The Lake Titicaca frog ("Telmatobius culeus") is one such species and has wrinkly skin that increases its surface area to enhance gas exchange. It normally makes no use of its rudimentary lungs but will sometimes raise and lower its body rhythmically while on the lake bed to increase the flow of water around it.
Digestion and excretion.
Frogs have maxillary teeth along their upper jaw which are used to hold food before it is swallowed. These teeth are very weak, and cannot be used to chew or catch and harm agile prey. Instead, the frog uses its sticky, cleft tongue to catch flies and other small moving prey. The tongue normally lies coiled in the mouth, free at the back and attached to the mandible at the front. It can be shot out and retracted at great speed. Some frogs have no tongue and just stuff food into their mouths with their hands. The eyes assist in the swallowing of food as they can be retracted through holes in the skull and help push food down the throat. The food then moves through the oesophagus into the stomach where digestive enzymes are added and it is churned up. It then proceeds to the small intestine (duodenum and ileum) where most digestion occurs. Pancreatic juice from the pancreas, and bile, produced by the liver and stored in the gallbladder, are secreted into the small intestine, where the fluids digest the food and the nutrients are absorbed. The food residue passes into the large intestine where excess water is removed and the wastes are passed out through the cloaca.
Although adapted to terrestrial life, frogs resemble freshwater fish in their inability to conserve body water effectively. When they are on land, much water is lost by evaporation from the skin. The excretory system is similar to that of mammals and there are two kidneys that remove nitrogenous products from the blood. Frogs produce large quantities of dilute urine in order to flush out toxic products from the kidney tubules. The nitrogen is excreted as ammonia by tadpoles and aquatic frogs but mainly as urea, a less toxic product, by most terrestrial adults. A few species of tree frog with little access to water excrete the even less toxic uric acid. The urine passes along paired ureters to the urinary bladder from which it is vented periodically into the cloaca. All bodily wastes exit the body through the cloaca which terminates in a cloacal vent.
Reproductive system.
In the male frog, the two testes are attached to the kidneys and semen passes into the kidneys through fine tubes called efferent ducts. It then travels on through the ureters, which are consequently known as urinogenital ducts. There is no penis, and sperm is ejected from the cloaca directly onto the eggs as the female lays them. The ovaries of the female frog are beside the kidneys and the eggs pass down a pair of oviducts and through the cloaca to the exterior.
When frogs mate, the male climbs on the back of the female and wraps his fore limbs round her body, either behind the front legs or just in front of the hind legs. This position is called amplexus and may be held for several days. The male frog has certain hormone-dependent secondary sexual characteristics. These include the development of special pads on his thumbs in the breeding season, to give him a firm hold. The grip of the male frog during amplexus stimulates the female to release eggs, usually wrapped in jelly, as spawn. In many species the male is smaller and slimmer than the female. Males have vocal cords and make a range of croaks, particularly in the breeding season, and in some species they also have vocal sacs to amplify the sound.
Nervous system.
The frog has a highly developed nervous system that consists of a brain, spinal cord and nerves. Many parts of the frog's brain correspond with those of humans. It consists of two olfactory lobes, two cerebral hemispheres, a pineal body, two optic lobes, a cerebellum and a medulla oblongata. Muscular coordination and posture are controlled by the cerebellum, and the medulla oblongata regulates respiration, digestion and other automatic functions. The relative size of the cerebrum in frogs is much smaller than it is in humans. Frogs have ten pairs of cranial nerves which pass information from the outside directly to the brain, and ten pairs of spinal nerves which pass information from the extremities to the brain through the spinal cord. By contrast, all amniotes (mammals, birds and reptiles) have twelve pairs of cranial nerves.
Sight.
The eyes of most frogs are located on either side of the head near the top and project outwards as hemispherical bulges. They provide binocular vision over a field of 100° to the front and a total visual field of almost 360°. They may be the only part of an otherwise submerged frog to protrude from the water. Each eye has closable upper and lower lids and a nictitating membrane which provides further protection, especially when the frog is swimming. Members of the aquatic family Pipidae have the eyes located at the top of the head, a position better suited for detecting prey in the water above. The irises come in a range of colours and the pupils in a range of shapes. The common toad ("Bufo bufo") has golden irises and horizontal slit-like pupils, the red-eyed tree frog ("Agalychnis callidryas") has vertical slit pupils, the poison dart frog has dark irises, the fire-bellied toad ("Bombina spp.") has triangular pupils and the tomato frog ("Dyscophus spp.") has circular ones. The irises of the southern toad ("Anaxyrus terrestris") are patterned so as to blend in with the surrounding camouflaged skin.
The distant vision of a frog is better than its near vision. Calling frogs will quickly become silent when they see an intruder or even a moving shadow but the closer an object is, the less well it is seen. When a frog shoots out its tongue to catch an insect it is reacting to a small moving object that it cannot see well and must line it up precisely beforehand because it shuts its eyes as the tongue is extended. Whether a frog sees in colour is debatable but it has been shown that it responds positively to blue light, perhaps because that colour is associated with bodies of water that can provide refuge when the frog feels threatened.
Hearing.
Frogs can hear both in the air and below water. They do not have external ears; the eardrums (tympanic membranes) are directly exposed or may be covered by a layer of skin and are visible as a circular area just behind the eye. The size and distance apart of the eardrums is related to the frequency and wavelength at which the frog calls. In some species such as the bullfrog, the size of the tympanum indicates the sex of the frog; males have tympani that are larger than their eyes while in females, the eyes and tympani are much the same size. A noise causes the tympanum to vibrate and the sound is transmitted to the middle and inner ear. The middle ear contains semicircular canals which help control balance and orientation. In the inner ear, the auditory hair cells are arranged in two areas of the cochlea, the basilar papilla and the amphibian papilla. The former detects high frequencies and the latter low frequencies. Because the cochlea is short, frogs use electrical tuning to extend their range of audible frequencies and help discriminate different sounds. This arrangement enables detection of the territorial and breeding calls of their conspecifics. In some species that inhabit arid regions, the sound of thunder or heavy rain may arouse them from a dormant state. A frog may be startled by an unexpected noise but it will not usually take any action until it has located the source of the sound by sight.
Call.
The call or croak of a frog is unique to its species. Frogs create this sound by passing air through the larynx in the throat. In most calling frogs, the sound is amplified by one or more vocal sacs, membranes of skin under the throat or on the corner of the mouth, that distend during the amplification of the call. Some frog calls are so loud that they can be heard up to a mile away.
Frogs in the genera "Heleioporus" and "Neobatrachus" lack vocal sacs but can still produce a loud call. Their buccal cavity is enlarged and dome-shaped, acting as a resonance chamber that amplifies the sound. Species of frog that lack vocal sacs and that do not have a loud call tend to inhabit areas close to constantly noisy, flowing water. They need to use an alternative means to communicate. The coastal tailed frog ("Ascaphus truei") lives in mountain streams in North America and does not vocalize.
The main reason for calling is to allow male frogs to attract a mate. Males may call individually or there may be a chorus of sound where numerous males have converged on breeding sites. Females of many frog species, such as the common tree frog ("Polypedates leucomystax"), reply to the male calls, which acts to reinforce reproductive activity in a breeding colony. Female frogs prefer males that produce sounds of greater intensity and lower frequency, attributes that stand out in a crowd. The rationale for this is thought to be that by demonstrating his prowess, the male shows his fitness to produce superior offspring.
A different call is emitted by a male frog or unreceptive female when mounted by another male. This is a distinct chirruping sound and is accompanied by a vibration of the body. Tree frogs and some non-aquatic species have a rain call that they make on the basis of humidity cues prior to a shower. Many species also have a territorial call that is used to drive away other males. All of these calls are emitted with the mouth of the frog closed. A distress call, emitted by some frogs when they are in danger, is produced with the mouth open resulting in a higher-pitched call. It is typically used when the frog has been grabbed by a predator and may serve to distract or disorientate the attacker so that it releases the frog.
Many species of frog have deep calls. The croak of the American bullfrog ("Rana catesbiana") is sometimes written as "jug o' rum". The Pacific tree frog ("Pseudacris regilla") produces the onomatopoeic "ribbit" often heard in films. Other renderings of frog calls into speech include "brekekekex koax koax", the call of the marsh frog ("Pelophylax ridibundus") in "The Frogs", an Ancient Greek comic drama by Aristophanes.
Torpor.
During extreme conditions, some frogs enter a state of torpor and remain inactive for months. In colder regions, many species of frog hibernate in winter. Those that live on land such as the American toad ("Bufo americanus") dig a burrow and make a hibernaculum in which to lie dormant. Others, less proficient at digging, find a crevice or bury themselves in dead leaves. Aquatic species such as the American bullfrog ("Rana catesbeiana") normally sink to the bottom of the pond where they lie, semi-immersed in mud but still able to access the oxygen dissolved in the water. Their metabolism slows down and they live on their energy reserves. Some frogs can even survive being frozen. Ice crystals form under the skin and in the body cavity but the essential organs are protected from freezing by a high concentration of glucose. An apparently lifeless, frozen frog can resume respiration and the heart beat can restart when conditions warm up.
At the other extreme, the striped burrowing frog ("Cyclorana alboguttata") regularly aestivates during the hot, dry season in Australia, surviving in a dormant state without access to food and water for nine or ten months of the year. It burrows underground and curls up inside a protective cocoon formed by its shed skin. Researchers at the University of Queensland have found that during aestivation, the metabolism of the frog is altered and the operational efficiency of the mitochondria is increased. This means that the limited amount of energy available to the comatose frog is used in a more efficient manner. This survival mechanism is only useful to animals that remain completely unconscious for an extended period of time and whose energy requirements are low because they are cold-blooded and have no need to generate heat. Other research showed that, to provide these energy requirements, muscles atrophy, but hind limb muscles are preferentially unaffected.
Locomotion.
Different species of frog use a number of methods of moving around including jumping, running, walking, swimming, burrowing, climbing and gliding.
Frogs are generally recognized as exceptional jumpers and, relative to their size, the best jumpers of all vertebrates. The Australian rocket frog, "Litoria nasuta", can leap over 2 m, a distance that is more than fifty times its body length of 5.5 cm. There are tremendous differences between species in jumping capability. Within a species, jump distance increases with increasing size, but relative jumping distance (body-lengths jumped) decreases. The Indian skipper frog ("Euphlyctis cyanophlyctis") has the ability to leap out of the water from a position floating on the surface. The tiny northern cricket frog ("Acris crepitans") can "skitter" across the surface of a pond with a series of short rapid jumps.
Slow-motion photography shows that the muscles have passive flexibility. They are first stretched while the frog is still in the crouched position, then they are contracted before being stretched again to launch the frog into the air. The fore legs are folded against the chest and the hind legs remain in the extended, streamlined position for the duration of the jump. In some extremely capable jumpers, such as the Cuban tree frog ("Osteopilus septentrionalis") and the northern leopard frog ("Rana pipiens"), the peak power exerted during a jump can exceed that which the muscle is theoretically capable of producing. When the muscles contract, the energy is first transferred into the stretched tendon which is wrapped around the ankle bone. Then the muscles stretch again at the same time as the tendon releases its energy like a catapult to produce a powerful acceleration beyond the limits of muscle-powered acceleration. A similar mechanism has been documented in locusts and grasshoppers.
Frogs in the families Bufonidae, Rhinophrynidae, and Microhylidae have short back legs and tend to walk rather than jump. When they try to move rapidly, they speed up the rate of movement of their limbs or resort to an ungainly hopping gait. The western narrow-mouthed toad ("Gastrophryne olivacea") has been described as having a gait that is "a combination of running and short hops that are usually only an inch or two in length". In an experiment, Fowler's toad ("Bufo fowleri") was placed on a treadmill which was turned at varying speeds. By measuring the toad's uptake of oxygen it was found that hopping was an inefficient use of resources during sustained locomotion but was a useful strategy during short bursts of high-intensity activity.
The red-legged running frog ("Kassina maculata") has short, slim hind limbs unsuited to jumping. It can move fast by using a running gait in which the two hind legs are used alternately. Slow-motion photography shows, unlike a horse that can trot or gallop, the frog's gait remained similar at slow, medium, and fast speeds. This species can also climb trees and shrubs, and does so at night to catch insects. The Indian skipper frog ("Euphlyctis cyanophlyctis") has broad feet and can run across the surface of the water for several metres (yards).
Frogs that live in or visit water have adaptations that improve their swimming abilities. The hind limbs are heavily muscled and strong. The webbing between the toes of the hind feet increases the area of the foot and helps propel the frog powerfully through the water. Members of the family Pipidae are wholly aquatic and show the most marked specialization. They have inflexible vertebral columns, flattened, streamlined bodies, lateral line systems, and powerful hind limbs with large webbed feet. Tadpoles mostly have large tail fins which provide thrust when the tail is moved from side to side.
Some frogs have become adapted for burrowing and a life underground. They tend to have rounded bodies, short limbs, small heads with bulging eyes, and hind feet adapted for excavation. An extreme example of this is the purple frog ("Nasikabatrachus sahyadrensis") from southern India which feeds on termites and spends almost its whole life underground. It emerges briefly during the monsoon to mate and breed in temporary pools. It has a tiny head with a pointed snout and a plump, rounded body. Because of this fossorial existence, it was first described in 2003, being new to the scientific community at that time, although previously known to local people.
The spadefoot toads of North America are also adapted to underground life. The plains spadefoot toad ("Spea bombifrons") is typical and has a flap of keratinised bone attached to one of the metatarsals of the hind feet which it uses to dig itself backwards into the ground. As it digs, the toad wriggles its hips from side to side to sink into the loose soil. It has a shallow burrow in the summer from which it emerges at night to forage. In winter, it digs much deeper and has been recorded at a depth of 4.5 m. The tunnel is filled with soil and the toad hibernates in a small chamber at the end. During this time, urea accumulates in its tissues and water is drawn in from the surrounding damp soil by osmosis to supply the toad's needs. Spadefoot toads are "explosive breeders", all emerging from their burrows at the same time and converging on temporary pools, attracted to one of these by the calling of the first male to find a suitable breeding location.
The burrowing frogs of Australia have a rather different lifestyle. The western spotted frog ("Heleioporus albopunctatus") digs a burrow beside a river or in the bed of an ephemeral stream and regularly emerges to forage. Mating takes place and eggs are laid in a foam nest inside the burrow. The eggs partially develop there, but do not hatch until they are submerged following heavy rainfall. The tadpoles then swim out into the open water and rapidly complete their development. Madagascan burrowing frogs are less fossorial and mostly bury themselves in leaf litter. One of these, the green burrowing frog ("Scaphiophryne marmorata"), has a flattened head with a short snout and well-developed metatarsal tubercles on its hind feet to help with excavation. It also has greatly enlarged terminal discs on its fore feet that help it to clamber around in bushes. It breeds in temporary pools that form after rains.
Tree frogs are found high in the canopy, where they scramble around on the branches, twigs, and leaves, sometimes never coming down to earth. The "true" tree frogs belong to the family Hylidae, but members of other frog families have independently adopted an arboreal habit, a case of convergent evolution. These include the glass frogs (Centrolenidae), the bush frogs (Hyperoliidae), some of the narrow-mouthed frogs (Microhylidae), and the shrub frogs (Rhacophoridae). Most tree frogs are under 10 cm in length, with long legs and long toes with adhesive pads on the tips. The surface of the toe pads is formed from a closely packed layer of flat-topped, hexagonal epidermal cells separated by grooves into which glands secrete mucus. These toe pads, moistened by the mucus, provide the grip on any wet or dry surface, including glass. The forces involved include boundary friction of the toe pad epidermis on the surface and also surface tension and viscosity. Tree frogs are very acrobatic and can catch insects while hanging by one toe from a twig or clutching onto the blade of a windswept reed. Some members of the subfamily Phyllomedusinae have opposable toes on their feet. The reticulated leaf frog ("Phyllomedusa ayeaye") has a single opposed digit on each fore foot and two opposed digits on its hind feet. This allows it to grasp the stems of bushes as it clambers around in its riverside habitat.
During the evolutionary history of the frog, several different groups have independently taken to the air. Some frogs in the tropical rainforest are specially adapted for gliding from tree to tree or parachuting to the forest floor. Typical of them is Wallace's flying frog ("Rhacophorus nigropalmatus") from Malaysia and Borneo. It has large feet with the fingertips expanded into flat adhesive discs and the digits fully webbed. Flaps of skin occur on the lateral margins of the limbs and across the tail region. With the digits splayed, the limbs outstretched, and these flaps spread, it can glide considerable distances, but is unable to undertake powered flight. It can alter its direction of travel and navigate distances of up to 15 m between trees.
Life history.
Like other amphibians, the life cycle of a frog normally starts in water with an egg that hatches into a limbless larva with gills, commonly known as a tadpole. After further growth, during which it develops limbs and lungs, the tadpole undergoes metamorphosis in which its appearance and internal organs are rearranged. After this it is able to leave the water as a miniature, air-breathing frog.
Reproduction.
Two main types of reproduction occur in frogs, prolonged breeding and explosive breeding. In the former, adopted by the majority of species, adult frogs at certain times of year assemble at a pond, lake or stream to breed. Many frogs return to the bodies of water in which they developed as larvae. This often results in annual migrations involving thousands of individuals. In explosive breeders, mature adult frogs arrive at breeding sites in response to certain trigger factors such as rainfall occurring in an arid area. In these frogs, mating and spawning take place promptly and the speed of larval growth is rapid in order to make use of the ephemeral pools before they dry up.
Among prolonged breeders, males usually arrive at the breeding site first and remain there for some time whereas females tend to arrive later and depart soon after they have spawned. This means that males outnumber females at the water's edge and defend territories from which they expel other males. They advertise their presence by calling, often alternating their croaks with neighbouring frogs. Larger, stronger males tend to have deeper calls and maintain higher quality territories. Females select their mates at least partly on the basis of the depth of their voice. In some species there are satellite males who have no territory and do not call. They may intercept females that are approaching a calling male or take over a vacated territory. Calling is an energy-sapping activity. Sometimes the two roles are reversed and a calling male gives up its territory and becomes a satellite.
In explosive breeders, the first male that finds a suitable breeding location, such as a temporary pool, calls loudly and other frogs of both sexes converge on the pool. Explosive breeders tend to call in unison creating a chorus that can be heard from far away. The spadefoot toads ("Scaphiopus spp.") of North America fall into this category. Mate selection and courtship is not as important as speed in reproduction. In some years, suitable conditions may not occur and the frogs may go for two or more years without breeding. Some female New Mexico spadefoot toads ("Spea multiplicata") only spawn half of the available eggs at a time, perhaps retaining some in case a better reproductive opportunity arises later.
At the breeding site, the male mounts the female and grips her tightly round the body. Typically, amplexus takes place in the water, the female releases her eggs and the male covers them with sperm; fertilization is external. In many species such as the Great Plains toad ("Bufo cognatus"), the male restrains the eggs with his back feet, holding them in place for about three minutes. Members of the West African genus "Nimbaphrynoides" are unique among frogs in that they are viviparous; "Limnonectes larvaepartus", "Eleutherodactylus jasperi" and members of the Tanzanian genus "Nectophrynoides" are the only frogs known to be ovoviviparous. In these species, fertilization is internal and females give birth to fully developed juvenile frogs, except "L. larvaepartus", which give birth to tadpoles.
Life cycle.
Eggs / frogspawn.
Frogs' embryos are typically surrounded by several layers of gelatinous material. When several eggs are clumped together, they are collectively known as frogspawn. The jelly provides support and protection while allowing the passage of oxygen, carbon dioxide and ammonia. It absorbs moisture and swells on contact with water. After fertilization, the innermost portion liquifies to allow free movement of the developing embryo. In certain species, such as the Northern red-legged frog ("Rana aurora") and the wood frog ("Rana sylvatica"), symbiotic unicellular green algae are present in the gelatinous material. It is thought that these may benefit the developing larvae by providing them with extra oxygen through photosynthesis. Most eggs are black or dark brown and this has the advantage of absorbing warmth from the sun which the insulating capsule retains. The interior of globular egg clusters of the wood frog ("Rana sylvatica") has been found to be up to 6 °C (11 °F) warmer than the surrounding water and this speeds up the development of the larvae.
The shape and size of the egg mass is characteristic of the species. Ranids tend to produce globular clusters containing large numbers of eggs whereas bufonids produce long, cylindrical strings. The tiny yellow-striped pygmy eleuth ("Eleutherodactylus limbatus") lays eggs singly, burying them in moist soil. The smoky jungle frog ("Leptodactylus pentadactylus") makes a nest of foam in a hollow. The eggs hatch when the nest is flooded, or the tadpoles may complete their development in the foam if flooding does not occur. The red-eyed treefrog ("Agalychnis callidryas") deposits its eggs on a leaf above a pool and when they hatch, the larvae fall into the water below. The larvae developing in the eggs can detect vibrations caused by nearby predatory wasps or snakes, and will hatch early to avoid being eaten. In general, the length of the egg stage depends on the species and the environmental conditions. Aquatic eggs normally hatch within one week when the capsule splits as a result of enzymes released by the developing larvae.
Tadpoles.
The larvae that emerge from the eggs, known as tadpoles (or occasionally polliwogs), typically have oval bodies and long, vertically flattened tails. As a general rule, free-living larvae are fully aquatic, but at least one species ("Nannophrys ceylonensis") has semiterrestrial tadpoles which live among wet rocks. Tadpoles lack eyelids and have cartilaginous skeletons, lateral line systems, gills for respiration (external gills at first, internal gills later), and vertically flattened tails they use for swimming.
From early in its development, a gill pouch covers the tadpole's gills and front legs. The lungs soon start to develop and are used as an accessory breathing organ. Some species go through metamorphosis while still inside the egg and hatch directly into small frogs. Tadpoles lack true teeth, but the jaws in most species have two elongated, parallel rows of small, keratinized structures called keradonts in their upper jaws. Their lower jaws usually have three rows of keradonts surrounded by a horny beak, but the number of rows can vary and the exact arrangements of mouth parts provide a means for species identification. In the Pipidae, with the exception of "Hymenochirus", the tadpoles have paired anterior barbels, which make them resemble small catfish. Their tails are stiffened by a notochord, but does not contain any bony or cartilaginous elements except for a few vertebrae at the base which forms the urostyle during metamorphosis. This has been suggested as an adaptation to their lifestyles; because the transformation into frogs happens very fast, the tail is made of soft tissue only, as bone and cartilage take a much longer time to be broken down and absorbed. The tail fin and tip is fragile and will easily tear, which is seen as an adaptation to escape from predators which tries to grasp them by the tail.
Tadpoles are typically herbivorous, feeding mostly on algae, including diatoms filtered from the water through the gills. Some species are carnivorous at the tadpole stage, eating insects, smaller tadpoles, and fish. The Cuban tree frog ("Osteopilus septentrionalis") is one of a number of species in which the tadpoles can be cannibalistic. Tadpoles that develop legs early may be eaten by the others, so late developers may have better long-term survival prospects.
Tadpoles are highly vulnerable to being eaten by fish, newts, predatory diving beetles, and birds, such as kingfishers. Some tadpoles, including those of the cane toad ("Bufo marinus"), are poisonous. The tadpole stage may be as short as a week in explosive breeders or it may last through one or more winters followed by metamorphosis in the spring.
Metamorphosis.
At the end of the tadpole stage, a frog undergoes metamorphosis in which its body makes a sudden transition into the adult form. This metamorphosis typically lasts only 24 hours, and is initiated by production of the hormone thyroxine. This causes different tissues to develop in different ways. The principal changes that take place include the development of the lungs and the disappearance of the gills and gill pouch, making the front legs visible. The lower jaw transforms into the big mandible of the carnivorous adult, and the long, spiral gut of the herbivorous tadpole is replaced by the typical short gut of a predator. The nervous system becomes adapted for hearing and stereoscopic vision, and for new methods of locomotion and feeding. The eyes are repositioned higher up on the head and the eyelids and associated glands are formed. The eardrum, middle ear, and inner ear are developed. The skin becomes thicker and tougher, the lateral line system is lost, and skin glands are developed. The final stage is the disappearance of the tail, but this takes place rather later, the tissue being used to produce a spurt of growth in the limbs. Frogs are at their most vulnerable to predators when they are undergoing metamorphosis. At this time, the tail is being lost and locomotion by means of limbs is only just becoming established.
Adults.
After metamorphosis, young adults may disperse into terrestrial habitats or continue to live in water. Almost all species of frogs are carnivorous as adults, preying on invertebrates, including arthropods, worms, snails, and slugs. A few of the larger ones may eat other frogs, small mammals, and fish. Some frogs use their sticky tongues to catch fast-moving prey, while others push food into their mouths with their hands. A few species also eat plant matter; the tree frog "Xenohyla truncata" is partly herbivorous, its diet including a large proportion of fruit, "Leptodactylus mystaceus" has been found to eat plants, and folivory occurs in "Euphlyctis hexadactylus", with plants constituting 79.5% of its diet by volume. Adult frogs are themselves attacked by many predators. The northern leopard frog ("Rana pipiens") is eaten by herons, hawks, fish, large salamanders, snakes, raccoons, skunks, mink, bullfrogs, and other animals.
Frogs are primary predators and an important part of the food web. Being cold-blooded, they make efficient use of the food they eat with little energy being used for metabolic processes, while the rest is transformed into biomass. They are themselves eaten by secondary predators and are the primary terrestrial consumers of invertebrates, most of which feed on plants. By reducing herbivory, they play a part in increasing the growth of plants and are thus part of a delicately balanced ecosystem.
Little is known about the longevity of frogs and toads in the wild, but some can live for many years. Skeletochronology is a method of examining bones to determine age. Using this method, the ages of mountain yellow-legged frogs ("Rana muscosa") were studied, the phalanges of the toes showing seasonal lines where growth slows in winter. The oldest frogs had ten bands, so their age was believed to be 14 years, including the four-year tadpole stage. Captive frogs and toads have been recorded as living for up to 40 years, an age achieved by a European common toad ("Bufo bufo"). The cane toad ("Bufo marinus") has been known to survive 24 years in captivity, and the American bullfrog ("Rana catesbeiana") 14 years. Frogs from temperate climates hibernate during the winter, and four species are known to be able to withstand freezing during this time, including the wood frog ("Rana sylvatica").
Parental care.
Although care of offspring is poorly understood in frogs, up to an estimated 20% of amphibian species may care for their young in some way. The evolution of parental care in frogs is driven primarily by the size of the water body in which they breed. Those that breed in smaller water bodies tend to have greater and more complex parental care behaviour. Because predation of eggs and larvae is high in large water bodies, some frog species started to lay their eggs on land. Once this happened, the desiccating terrestrial environment demands that one or both parents keep them moist to ensure their survival. The subsequent need to transport hatched tadpoles to a water body required an even more intense form of parental care.
In small pools, predators are mostly absent and competition between tadpoles becomes the variable that constrains their survival. Certain frog species avoid this competition by making use of smaller phytotelmata (water-filled leaf axils or small woody cavities) as sites for depositing a few tadpoles. While these smaller rearing sites are free from competition, they also lack sufficient nutrients to support a tadpole without parental assistance. Frog species that changed from the use of larger to smaller phytotelmata have evolved a strategy of providing their offspring with nutritive but unfertilized eggs. The female strawberry poison-dart frog ("Oophaga pumilio") lays her eggs on the forest floor. The male frog guards them from predation and carries water in his cloaca to keep them moist. When they hatch, the female moves the tadpoles on her back to a water-holding bromeliad or other similar water body, depositing just one in each location. She visits them regularly and feeds them by laying one or two unfertilized eggs in the phytotelma, continuing to do this until the young are large enough to undergo metamorphosis. The granular poison frog ("Oophaga granulifera") looks after its tadpoles in a similar way.
Many other diverse forms of parental care are seen in frogs. The tiny male "Colostethus subpunctatus" stands guard over his egg cluster, laid under a stone or log. When the eggs hatch, he transports the tadpoles on his back to a temporary pool, where he partially immerses himself in the water and one or more tadpoles drop off. He then moves on to another pool. The male common midwife toad ("Alytes obstetricans") carries the eggs around with him attached to his hind legs. He keeps them damp in dry weather by immersing himself in a pond, and prevents them from getting too wet in soggy vegetation by raising his hindquarters. After three to six weeks, he travels to a pond and the eggs hatch into tadpoles. The tungara frog ("Physalaemus pustulosus") builds a floating nest from foam to protect its eggs from predation. The foam is made from proteins and lectins, and seems to have antimicrobial properties. Several pairs of frogs may form a colonial nest on a previously built raft. The eggs are laid in the centre, followed by alternate layers of foam and eggs, finishing with a foam capping.
Some frogs protect their offspring inside their own bodies. Both male and female pouched frogs ("Assa darlingtoni") guard their eggs, which are laid on the ground. When the eggs hatch, the male lubricates his body with the jelly surrounding them and immerses himself in the egg mass. The tadpoles wriggle into skin pouches on his side, where they develop until they metamorphose into juvenile frogs. The female gastric-brooding frog ("Rheobatrachus" sp.) from Australia, now probably extinct, swallows her fertilized eggs, which then develop inside her stomach. She ceases to feed and stops secreting stomach acid. The tadpoles rely on the yolks of the eggs for nourishment. After six or seven weeks, they are ready for metamorphosis. The mother regurgitates the tiny frogs, which hop away from her mouth. The female Darwin's frog ("Rhinoderma darwinii") from Chile lays up to 40 eggs on the ground, where they are guarded by the male. When the tadpoles are about to hatch, they are engulfed by the male, which carries them around inside his much-enlarged vocal sac. Here they are immersed in a frothy, viscous liquid that contains some nourishment to supplement what they obtain from the yolks of the eggs. They remain in the sac for seven to ten weeks before undergoing metamorphosis, after which they move into the male's mouth and emerge.
Defence.
At first sight, frogs seem rather defenceless because of their small size, slow movement, thin skin, and lack of defensive structures, such as spines, claws or teeth. Many use camouflage to avoid detection, the skin often being spotted or streaked in neutral colours that allow a stationary frog to merge into its surroundings. Some can make prodigious leaps, often into water, that help them to evade potential attackers, while many have other defensive adaptations and strategies.
The skin of many frogs contains mild toxic substances called bufotoxins to make them unpalatable to potential predators. Most toads and some frogs have large poison glands, the parotoid glands, located on the sides of their heads behind the eyes and other glands elsewhere on their bodies. These glands secrete mucus and a range of toxins that make frogs slippery to hold and distasteful or poisonous. If the noxious effect is immediate, the predator may cease its action and the frog may escape. If the effect develops more slowly, the predator may learn to avoid that species in future. Poisonous frogs tend to advertise their toxicity with bright colours, an adaptive strategy known as aposematism. The poison dart frogs in the family Dendrobatidae do this. They are typically red, orange, or yellow, often with contrasting black markings on their bodies. "Allobates zaparo" is not poisonous, but mimics the appearance of two different toxic species with which it shares a common range in an effort to deceive predators. Other species, such as the European fire-bellied toad ("Bombina bombina"), have their warning colour underneath. They "flash" this when attacked, adopting a pose that exposes the vivid colouring on their bellies.
Some frogs, such as the poison dart frogs, are especially toxic. The native people of South America extract poison from these frogs to apply to their weapons for hunting, although few species are toxic enough to be used for this purpose. At least two non-poisonous species of frogs in tropical America ("Eleutherodactylus gaigei" and "Lithodytes lineatus") mimic the colouration of dart poison frogs for self-protection. Some frogs obtain poisons from the ants and other arthropods they eat. Others, such as the Australian corroboree frogs ("Pseudophryne corroboree" and "Pseudophryne pengilleyi"), can synthesize the alkaloids themselves. The chemicals involved may be irritants, hallucinogens, convulsants, nerve poisons or vasoconstrictors. Many predators of frogs have become adapted to tolerate high levels of these poisons, but other creatures, including humans who handle the frogs, may be severely affected.
Some frogs use bluff or deception. The European common toad ("Bufo bufo") adopts a characteristic stance when attacked, inflating its body and standing with its hindquarters raised and its head lowered. The bullfrog ("Rana catesbeiana") crouches down with eyes closed and head tipped forward when threatened. This places the parotoid glands in the most effective position, the other glands on its back begin to ooze noxious secretions and the most vulnerable parts of its body are protected. Another tactic used by some frogs is to "scream", the sudden loud noise tending to startle the predator. The gray tree frog ("Hyla versicolor") makes an explosive sound that sometimes repels the shrew "Blarina brevicauda". Although toads are avoided by many predators, the common garter snake ("Thamnophis sirtalis") regularly feeds on them. The strategy employed by juvenile American toads ("Bufo americanus") on being approached by a snake is to crouch down and remain immobile. This is usually successful, with the snake passing by and the toad remaining undetected. If it is encountered by the snake's head, however, the toad hops away before crouching defensively.
Distribution and conservation status.
Frogs are found on all the continents except Antarctica, but they are not present on certain islands, especially those far away from continental land masses. Many species are isolated in restricted ranges by changes of climate or inhospitable territory, such as stretches of sea, mountain ridges, deserts, forest clearance, road construction, or other man-made barriers. Usually, a greater diversity of frogs occurs in tropical areas than in temperate regions, such as Europe. Some frogs inhabit arid areas, such as deserts, and rely on specific adaptations to survive. Members of the Australian genus "Cyclorana" bury themselves underground where they create a water-impervious cocoon in which to aestivate during dry periods. Once it rains, they emerge, find a temporary pool, and breed. Egg and tadpole development is very fast in comparison to those of most other frogs, so breeding can be completed before the pond dries up. Some frog species are adapted to a cold environment. The wood frog ("Rana sylvatica"), whose habitat extends into the Arctic Circle, buries itself in the ground during winter. Although much of its body freezes during this time, it maintains a high concentration of glucose in its vital organs, which protects them from damage.
In 2006, of 4,035 species of amphibians that depend on water during some lifecycle stage, 1,356 (33.6%) were considered to be threatened. This is likely to be an underestimate because it excludes 1,427 species for which evidence was insufficient to assess their status. Frog populations have declined dramatically since the 1950s. More than one-third of frog species are considered to be threatened with extinction, and more than 120 species are believed to have become extinct since the 1980s. Among these species are the gastric-brooding frogs of Australia and the golden toad of Costa Rica. The latter is of particular concern to scientists because it inhabited the pristine Monteverde Cloud Forest Reserve and suffered a population crash in 1987, along with about 20 other frog species found in the area. This could not be linked directly to human activities, such as deforestation, and was outside the range of normal fluctuations in population size. Elsewhere, habitat loss is a significant cause of frog population decline, as are pollutants, climate change, increased UVB radiation, and the introduction of non-native predators and competitors. A Canadian study conducted in 2006 suggested heavy traffic in their environment was a larger threat to frog populations than was habitat loss. Emerging infectious diseases, including chytridiomycosis and ranavirus, are also devastating populations.
Many environmental scientists believe amphibians, including frogs, are good biological indicators of broader ecosystem health because of their intermediate positions in food chains, their permeable skins, and typically biphasic lives (aquatic larvae and terrestrial adults). It appears that species with both aquatic eggs and larvae are most affected by the decline, while those with direct development are the most resistant.
Frog mutations and genetic defects have increased since the 1990s. These often include missing legs or extra legs. Various causes have been identified or hypothesized, including an increase in ultraviolet radiation affecting the spawn on the surface of ponds, chemical contamination from pesticides and fertilizers, and parasites such as the trematode "Ribeiroia ondatrae". Probably all these are involved in a complex way as stressors, environmental factors contributing to rates of disease, and vulnerability to attack by parasites. Malformations impair mobility and the individuals may not survive to adulthood. An increase in the number of frogs eaten by birds may actually increase the likelihood of parasitism of other frogs, because the trematode's complex lifecycle includes the ramshorn snail and several intermediate hosts such as birds.
In a few cases, captive breeding programs have been established and have largely been successful. In 2007, the application of certain probiotic bacteria was reported to protect amphibians from chytridiomycosis. One current project, the Panama Amphibian Rescue and Conservation Project, has subsequently been developed to rescue species at risk of this disease in eastern Panama, and to develop field applications for probiotic therapy. The World Association of Zoos and Aquariums named 2008 as the "Year of the Frog" in order to draw attention to the conservation issues faced by them.
The cane toad ("Bufo marinus") is a very adaptable species native to South and Central America. In the 1930s, it was introduced into Puerto Rico, and later various other islands in the Pacific and Caribbean region, as a biological pest control agent. In 1935, 3000 toads were liberated in the sugar cane fields of Queensland, Australia, in an attempt to control cane beetles such as "Dermolepida albohirtum", the larvae of which damage and kill the canes. Initial results in many of these countries were positive, but it later became apparent that the toads upset the ecological balance in their new environments. They bred freely, competed with native species of frogs, ate bees and other harmless native invertebrates, had few predators in their adopted habitats, and poisoned pets, carnivorous birds, and mammals. In many of these countries, they are now regarded both as pests and invasive species, and scientists are looking for a biological method to control them.
Uses.
Frog legs are eaten by humans in many parts of the world. Originally, they were supplied from local wild populations, but overexploitation led to a diminution in the supply. This resulted in the development of frog farming and a global trade in frogs. The main importing countries are France, Belgium, Luxembourg, and the United States, while the chief exporting nations are Indonesia and China. The annual global trade in the American bullfrog ("Rana catesbeiana"), mostly farmed in China, varies between 1200 and 2400 tonnes.
Frogs are sometimes used for dissections in high school and university anatomy classes, often first being injected with coloured substances to enhance the contrast between the biological systems. This practice is declining with increasing concerns about animal welfare, and "digital frogs" are now available for virtual dissection.
Frogs have served as experimental animals throughout the history of science. Eighteenth-century biologist Luigi Galvani discovered the link between electricity and the nervous system through studying frogs. In 1852, H. F. Stannius used a frog's heart in a procedure called a Stannius ligature to demonstrate the ventricle and atria beat independently of each other and at different rates. The African clawed frog or platanna ("Xenopus laevis") was first widely used in laboratories in pregnancy assays in the first half of the 20th century. A sample of urine from a pregnant woman injected into a female frog induces it to lay eggs, a discovery made by English zoologist Lancelot Hogben. This is because a hormone, human chorionic gonadotropin, is present in substantial quantities in the urine of women during pregnancy. In 1952, Robert Briggs and Thomas J. King cloned a frog by somatic cell nuclear transfer. This same technique was later used to create Dolly the sheep, and their experiment was the first time a successful nuclear transplantation had been accomplished in higher animals.
Frogs are used in cloning research and other branches of embryology. Although alternative pregnancy assays have been developed, biologists continue to use "Xenopus" as a model organism in developmental biology because their embryos are large and easy to manipulate, they are readily obtainable, and can easily be kept in the laboratory. "Xenopus laevis" is increasingly being displaced by its smaller relative, "Xenopus tropicalis", which reaches its reproductive age in five months rather than the one to two years for "X. laevis", thus facilitating faster studies across generations. The genome of "X. tropicalis" is being sequenced.
Because frog toxins are extraordinarily diverse, they have raised the interest of biochemists as a "natural pharmacy". The alkaloid epibatidine, a painkiller 200 times more potent than morphine, is found in some species of poison dart frogs. Other chemicals isolated from the skins of frogs may offer resistance to HIV infection. Dart poisons are under active investigation for their potential as therapeutic drugs.
It has long been suspected that pre-Columbian Mesoamericans used a toxic secretion produced by the cane toad as a hallucinogen, but more likely they used substances secreted by the Colorado River toad ("Bufo alvarius"). These contain bufotenin (5-MeO-DMT), a psychoactive compound that has been used in modern times as a recreational drug. Typically, the skin secretions are dried and then smoked. Illicit drug use by licking the skin of a toad has been reported in the media, but this may be an urban myth.
Exudations from the skin of the golden poison frog ("Phyllobates terribilis") are traditionally used by native Colombians to poison the darts they use for hunting. The tip of the projectile is rubbed over the back of the frog and the dart is launched from a blowgun. The combination of the two alkaloid toxins batrachotoxin and homobatrachotoxin is so powerful, one frog contains enough poison to kill an estimated 22,000 mice. Two other species, the Kokoe poison dart frog ("Phyllobates aurotaenia") and the black-legged dart frog ("Phyllobates bicolor") are also used for this purpose. These are less toxic and less abundant than the golden poison frog. They are impaled on pointed sticks and may be heated over a fire to maximise the quantity of poison that can be transferred to the dart.
Cultural beliefs.
Frogs feature prominently in folklore, fairy tales, and popular culture. They tend to be portrayed as benign, ugly, and clumsy, but with hidden talents. Examples include Michigan J. Frog, "The Frog Prince", and Kermit the Frog. The Warner Brothers cartoon "One Froggy Evening" features Michigan J. Frog, that will only dance and sing for the demolition worker who opens his time capsule, but will not perform in public. "The Frog Prince" is a fairy tale about a frog that turns into a handsome prince after he has rescued a princess's golden ball and she has taken him into her palace. Kermit the Frog is a conscientious and disciplined character from "The Muppet Show" and "Sesame Street"; while openly friendly and greatly talented, he is often portrayed as cringing at the fanciful behavior of more flamboyant characters.
Toads have a more sinister reputation. It was believed in European folklore that they were associated with witches as their familiar spirits and had magical powers. The toxic secretions from their skin was used in brewing evil potions, but was also put to use to create magical cures for human and livestock ailments. They were associated with the devil; in John Milton's "Paradise Lost", Satan was depicted as a toad pouring poison into Eve's ear.
The Moche people of ancient Peru worshipped animals, and often depicted frogs in their art. In Panama, local legend held that good fortune would come to anyone who spotted a Panamanian golden frog. Some believed when one of these frogs died, it would turn into a golden talisman known as a "huaca". Today, despite being extinct in the wild, Panamanian golden frogs remain an important cultural symbol and can be found on decorative cloth "molas" made by the Kuna people. They also appear as part of the inlaid design on a new overpass in Panama City, on T-shirts, and even on lottery tickets.
References.
Bibliography.
</dl>
External links.
Media

</doc>
<doc id="38660" url="http://en.wikipedia.org/wiki?curid=38660" title="1672">
1672

Year 1672 (MDCLXXII) was a leap year starting on Friday (link will display the full calendar) of the Gregorian calendar and a leap year starting on Monday of the 10-day slower Julian calendar.
Events.
<onlyinclude>
Undated.
</onlyinclude>

</doc>
<doc id="38672" url="http://en.wikipedia.org/wiki?curid=38672" title="1548">
1548

Year 1548 (MDXLVIII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="38683" url="http://en.wikipedia.org/wiki?curid=38683" title="1530">
1530

Year 1530 (MDXXX) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="38693" url="http://en.wikipedia.org/wiki?curid=38693" title="1515">
1515

Year 1515 (MDXV) was a common year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="38705" url="http://en.wikipedia.org/wiki?curid=38705" title="1502">
1502

Year 1502 (MDII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="38721" url="http://en.wikipedia.org/wiki?curid=38721" title="Bofors">
Bofors

Bofors AB is a Swedish arms manufacturing company. The name has been associated with the iron industry and artillery manufacturing for more than 350 years. 
History.
Located in Karlskoga, Sweden, the company originates from the hammering trip hammer mill "Boofors" founded as a royal state-owned company in 1646. The modern corporate structure was created in 1873 with the foundation of Aktiebolaget (AB) Bofors-Gullspång as a totally government-owned public company. A leading Swedish steel producer by the early 1870s, Bofors expanded into weapons manufacture when steel produced by the Siemens-Martin process began to be used for gun manufacture. Bofors was divested by the late 1880s, with the government retaining a small share and two members on the highest body. The company's first cannon workshop was opened in 1884. Bofors' most famous owner was Alfred Nobel, who owned the company from 1894 until his death in December 1896. Nobel played the key role in reshaping the former iron and steel producer to a modern cannon manufacturer and chemical industry participant. The powder manufacturer AB Bofors Nobelkrut, later an explosives and general organic-chemical producer, was created in 1898 as a wholly owned subsidiary. By 1911 AB Bofors-Gullspång had outcompeted, bought and closed down its Finspång Swedish competitor in cannon manufacture. The company's name was shortened to AB Bofors in 1919.
Bofors gun scandal.
In 1986, a $285 million contract between the Government of India and Swedish arms company Bofors was signed for supply of 410 155mm howitzer field guns. In 1987, Swedish Radio alleged that Bofors paid illegal commissions to top Indian politicians and key defence officials to seal the deal. The scandal contributed to the defeat of Rajiv Gandhi in elections three years later.
Present ownership.
In 1999 Saab AB purchased the Celsius Group, then the parent company for Bofors. In September 2000 United Defense Industries (UDI) of the United States acquired Bofors Weapons Systems (the heavy weapons division), while Saab retained the missile interests.
BAE Systems acquired United Defense and its Bofors subsidiary in 2005, and BAE Systems Bofors is now a business unit of BAE Systems AB, Saab Bofors Dynamics is a unit of Saab AB 
Products.
The name Bofors is strongly associated with the 40 mm anti-aircraft gun used by both sides during World War II. This automatic cannon is often simply called the "Bofors gun" and saw service on both land and sea. It became so widely known that anti-aircraft guns in general were often referred to as Bofors guns. Another well-known gun made by the company was the Bofors 37 mm anti-tank gun, a standard anti-tank weapon used by a variety of armies early in the war. It was built under licence in Poland and the USA and was also used in a variety of tanks, including the 7TP and M3A3 Stuart. 

</doc>
<doc id="38725" url="http://en.wikipedia.org/wiki?curid=38725" title="Cross of Gold speech">
Cross of Gold speech

The Cross of Gold speech was delivered by William Jennings Bryan, a former United States Representative from Nebraska, at the Democratic National Convention in Chicago on July 9, 1896. In the address, Bryan supported bimetallism or "free silver", which he believed would bring the nation prosperity. He decried the gold standard, concluding the speech, "you shall not crucify mankind upon a cross of gold". Bryan's address helped catapult him to the Democratic Party's presidential nomination; it is considered one of the greatest political speeches in American history.
For twenty years, Americans had been bitterly divided over the nation's monetary standard. The gold standard, which the United States had effectively been on since 1873, limited the money supply but eased trade with other nations, such as the United Kingdom, whose currency was also based on gold. Many Americans, however, believed that bimetallism (making both gold and silver legal tender) was necessary for the nation's economic health. The financial Panic of 1893 intensified the debates, and when Democratic President Grover Cleveland continued to support the gold standard against the will of much of his party, activists became determined to take over the Democratic Party organization and nominate a silver-supporting candidate in 1896.
Bryan had been a dark horse candidate with little support in the convention. His speech, delivered at the close of the debate on the party platform, electrified the convention and is generally credited with getting him the nomination for president. However, he lost the general election to William McKinley and the United States formally adopted the gold standard in 1900.
Background.
Monetary standards and the United States.
In January 1791, at the request of Congress, Secretary of the Treasury Alexander Hamilton issued a report on the currency. At the time, there was no mint in the United States; foreign coins were used. Hamilton proposed a monetary system based on bimetallism, in which the new currency would be equal to a given amount of gold, or a larger amount of silver; at the time a given weight of gold was worth about 15 times as much as the same amount of silver. Although Hamilton understood that adjustment might be needed from time to time as precious metal prices fluctuated, he believed that if the nation's unit of value were defined only by one of the two precious metals used for coins, the other would descend to the status of mere merchandise. He also proposed the establishment of a mint, at which citizens could present gold or silver, and receive it back, struck into money. On April 2, 1792, Congress passed the Mint Act of 1792. This legislation defined a unit of value for the new nation, to be known as a dollar. The new unit of currency was defined to be equal to 24.75 gr of gold, or alternatively, 371.25 gr of silver, establishing a ratio of value between gold and silver of 15:1. The legislation also established the Mint of the United States.
In the early 19th century, the economic disruption caused by the Napoleonic Wars caused United States gold coins to be worth more as bullion than as money, and they vanished from circulation. Governmental response to this shortage was hampered by the fact that officials did not clearly understand what had happened. In 1830, Treasury Secretary Samuel D. Ingham proposed adjusting the ratio between gold and silver in US currency to 15.8:1, which had for some time been the ratio in Europe. It was not until 1834 that Congress acted, changing the gold/silver ratio to 16.002:1. This was close enough to the market value to make it uneconomic to export either US gold or silver coins. When silver prices rose relative to gold as a reaction to the California Gold Rush, silver coinage was worth more than face value, and rapidly flowed overseas for melting. Despite vocal opposition led by Tennessee Representative (and future president) Andrew Johnson, the precious metal content of smaller silver coins was reduced in 1853. Silver was now undervalued at the Mint; accordingly little was presented for striking into money.
The Coinage Act of 1873 eliminated the standard silver dollar. It also repealed the statutory provisions allowing silver bullion to be presented to the Mint and returned in the form of circulating money. In passing the Coinage Act, Congress eliminated bimetallism. This was not the actual intent of the legislation; Congress noted in the act that silver was not being coined, and eliminated a practice thought obsolete. However, during the economic chaos of the Panic of 1873, the price of silver dropped significantly, but the Mint would accept none for striking into legal tender. Silver producers complained, and many Americans came to believe that only through bimetallism could the nation achieve and maintain prosperity. They called for the return to pre-1873 laws, which would require the Mint to take all the silver offered it and return it, struck into silver dollars. This would inflate the money supply, and, adherents argued, increase the nation's prosperity. Critics contended that the inflation which would follow the introduction of such a policy would harm workers, whose wages would not rise as fast as prices would, and the operation of Gresham's law would drive gold from circulation, effectively placing the United States on a silver standard.
Early attempts towards free silver.
To advocates of what became known as free silver, the 1873 act became known as the "Crime of '73". Pro-silver forces, with congressional leaders such as Missouri Representative Richard P. Bland, sought the passage of bills to allow depositors of silver bullion to receive it back in the form of coin. Such bills, sponsored by Bland, passed the House of Representatives in 1876 and 1877, but both times failed in the Senate. A third attempt in early 1878 again passed the House, and eventually both houses after being amended in the Senate. The bill, as modified by amendments sponsored by Iowa Senator William B. Allison, did not reverse the 1873 provisions, but required the Treasury to purchase a minimum of $2 million of silver bullion per month; the profit, or seignorage from monetizing the silver was to be used to purchase more silver bullion. The silver would be struck into dollar coins, to be circulated or else stored and used as backing for silver certificates. The Bland-Allison Act was vetoed by President Rutherford B. Hayes, but was enacted by Congress over his veto on February 28, 1878.
Implementation of the Bland-Allison Act did not end calls for free silver. The 1880s saw a steep decline in the prices of grain and other agricultural commodities. Silver advocates argued that this dropoff, which caused the price of grain to fall below its cost of production, was caused by the failure of the government to adequately increase the money supply, which had remained steady on a per capita basis. Advocates of the gold standard attributed the decline to advances in production and transportation. The late 19th century saw divergent views in economics as the "laissez faire" orthodoxy was questioned by younger economists, and both sides found ample support for their views from theorists.
In 1890, the Sherman Silver Purchase Act greatly increased government purchases of silver. The government pledged to stand behind the silver dollars and treasury notes issued under the act by redeeming them in gold. Pursuant to this promise, government gold reserves dwindled over the following three years. Although the economic Panic of 1893 had a number of causes, President Grover Cleveland believed the inflation caused by Sherman's act to be a major factor, and called a special session of Congress to repeal it. Congress did so, but the debates showed bitter divides in both major parties between silver and gold factions. Cleveland tried to replenish the Treasury through issuance of bonds which could only be purchased with gold, with little effect but to increase the public debt, as the gold continued to be withdrawn in redemption for paper and silver currency. Many in the public saw the bonds as benefiting bankers, not the nation. The bankers' feeling was that they did not want loans repaid in an inflated currency—the gold standard was deflationary, and as creditors, they preferred to be paid in such a currency, whereas debtors preferred to repay in inflated currency. 
The effects of the recession which began in 1893, and which continued through 1896, ruined many Americans. Contemporary estimates were an unemployment rate as high as 25%. The task of relieving the jobless fell to churches and other charities, as well as to labor unions. Farmers went bankrupt; their farms were sold to pay their debts. Some of the impoverished died of disease or starvation; others killed themselves.
Bryan seeks the nomination.
Among those who spoke against the repeal of the Sherman Silver Purchase Act was Nebraska Congressman William Jennings Bryan. Known as an orator even then, Bryan had not always favored free silver out of conviction, stating in 1892 that he was for it because the people of Nebraska were for it. By 1893, his views on silver had evolved, and on the floor of the House of Representatives, he delivered a riveting three-hour address against repeal of the Silver Purchase Act. In his conclusion, Bryan reached back in history:
When a crisis like the present arose and the national bank of his day sought to control the politics of the nation, God raised up an Andrew Jackson, who had the courage to grapple with that great enemy, and by overthrowing it, he made himself the idol of the people and reinstated the Democratic party in public confidence. What will the decision be today? The Democratic party has won the greatest success in its history. Standing upon this victory-crowned summit, will it turn its face to the rising or the setting sun? Will it choose blessings or cursings—life or death—which? Which?
Despite the repeal of the act, economic conditions failed to improve. The year 1894 saw considerable labor unrest. President Cleveland sent federal troops to Illinois to end the Pullman strike—workers at the Pullman Palace Car Company, which made railroad cars, had struck after wages were cut. Railway employees had refused to handle Pullman cars in sympathy with the strikers; this action threatened to paralyze the nation's rail lines. The President's move was opposed by the Democratic Governor of Illinois, John Altgeld. Angered by Cleveland's actions in the labor dispute, and by his uncompromising stand against silver, Altgeld began to organize Democrats against Cleveland's renomination in 1896. Although Altgeld and his adherents urged voters to distinguish between Cleveland and his party, the Democrats lost 113 seats in the House in the 1894 midterm elections, the greatest loss by a majority party in congressional history. The Republicans gained control of the House, as well as the Senate, which until 1913 was elected by the state legislatures rather than by the popular vote. Among those defeated for Senate was Bryan in Nebraska.
Bryan had long planned to run for president. Although he would only be 36 years old in 1896—one year above the constitutional minimum—he believed the silver question could carry him not only to the nomination, but to the presidency. He traveled widely, speaking to audiences across the nation. His speeches impressed many; even some of his opponents later conceded that Bryan was the most compelling speaker they had ever heard. Bryan's speeches evolved over time; in December 1894, in a speech in Congress, he first used a phrase from which would come the conclusion to his most famous address: as originally stated, it was "I will not help to crucify mankind upon a cross of gold."
A myth has arisen that Bryan was an unknown prior to 1896. This was not the case; Bryan was well known as an orator on the tariff and silver questions. Albert Shaw, editor of "The Review of Reviews", stated that after Bryan's nomination, many easterners professed not to have heard of him but: "If, indeed, they had not heard of Mr. Bryan before, they had failed to follow closely the course of American politics in the past eight years. As a Democratic member of the Ways and Means Committee through two Congresses, Mr. Bryan was by all odds the ablest and strongest orator on the Democratic side of the House. His subsequent canvass [campaign] for the United States senatorship in Nebraska was noteworthy and conspicuous on many accounts."
In the aftermath of the 1894 election, the silver forces, led by Altgeld and others, began an attempt to take over the machinery of the Democratic Party. Historian Stanley Jones, in his study of the 1896 election, suggests that western Democrats would have opposed Cleveland even if the party had held its congressional majority in 1894; with the disastrous defeat, they believed the party would be wiped out in the West if it did not support silver. Bryan biographer Paulo E. Coletta wrote, "during this year [July 1894–June 1895] of calamities, disintegration and revolution, each crisis aided Bryan because it caused division within his party and permitted him to contest for its mastery as it slipped from Cleveland's fingers."
In early 1896, with the economy still poor, there was widespread discontent with the two existing major political parties. Some people, for the most part Democrats, joined the far-left Populist Party. Many Republicans in the western states, dismayed by the strong allegiance of eastern Republicans to the gold standard, considered forming their own party. When the Republicans in June 1896 nominated former Ohio Governor William McKinley for president and passed at his request a platform strongly supporting "sound money (the gold standard unless modified by international agreement), a number of "Silver Republicans" walked out of the convention. The leader of those who left was Colorado Senator Henry M. Teller; he was immediately spoken of as a possible candidate for the Democratic nomination.
Bryan believed that he could, if nominated, unite the disaffected behind a strong silver campaign. However, part of his strategy was to remain inconspicuous until the last possible moment at the convention. He sent letters to national convention delegates, urging them to support silver, and enclosing copies of his photograph, writings, and speeches. Jones points out that though Bryan's speaking engagements were not deemed political by the standards of 1896, by modern measurements he was far more active in campaigning for the nomination than most of the better-known candidates.
Historian James A. Barnes, in his historical journal article pointing out myths that have arisen about Bryan's candidacy and campaign, stated that Bryan's efforts bore fruit even before the convention:
By April, 1896, many individuals were quietly working for Bryan's nomination. Circulars were being distributed in Illinois, and admirers in Nebraska, North Carolina, Mississippi, Louisiana, Texas, Arkansas, and other states were urging his selection among their friends. It was not in any concerted or open action, however, that Bryan had his strength; it was in the friendly predisposition of the mass of the delegates that he had hopes.
Selection of delegates.
The 1896 Democratic National Convention followed events unique in post-Civil War American history. One after another, state conventions to elect delegates to the national convention in Chicago repudiated an incumbent elected president of their party, who had not declared whether he would be a candidate for renomination. According to Barnes:
The people of the South and the West had for years been convinced of the enormity of the "crime of 1873", and they had long since come to regard silver as the sword that would cut the Gordian knot of privilege. Consciousness of grievances of years and not of months was reflected in the decisive action of the state Democratic conventions in the spring and early summer of 1896.
Many state conventions elected delegates pledged to support bimetallism in the party platform. Gold Democrats were successful in a few states in the Northeast, but had little luck elsewhere. Speakers in some states cursed Cleveland; the South Carolina convention denounced him. Cleveland issued a statement urging Democratic voters to support gold—the next convention to be held, in Illinois, unanimously supported silver; the keynote speaker prayed for divine forgiveness for Cleveland's 1892 nomination. Gold and silver factions in some states, such as Bryan's Nebraska, sent rival delegations to the convention.
1896 convention.
The 1896 Democratic convention opened at the Chicago Coliseum on July 7, 1896. Much activity took place in advance of the formal opening as the silver and (vastly outnumbered) gold forces prepared their strategies. Silver forces were supported by the Democratic National Bimetallic Committee, the umbrella group formed in 1895 to support silver Democrats in their insurgency against Cleveland. Gold Democrats looked to the President for leadership, but Cleveland, trusting few in his party, did not involve himself further in the gold efforts, but spent the week of the convention fishing off the New Jersey coast.
The Bimetallic Committee carefully planned to take control of every aspect of the convention, eliminating any threat that the minority gold faction could take power. It made no secret of these preparations. This takeover was considered far more important than was the choice of presidential candidate, and the committee decided to take no position on who should win the race for the nomination, reasoning that the victor, no matter who he was, would be a silver man. Well aware of the overwhelming forces against them, many gold delegates were inclined to concede the platform battle.
Bryan arrived quietly and took rooms at a modest hotel; the Nebraskan later calculated that he spent less than $100 while in Chicago. He arrived convinced that he would win the nomination. He had already begun work on a speech. On the evening of July 5, Bryan was visited by a delegation of Coloradans, seeking his support for Senator Teller. They went away apologetically, not having known Bryan sought the nomination.
Candidates for the nomination.
Despite the desire of silver delegates to nominate a candidate who shared their beliefs, and although several states instructed their delegates to vote for a specific candidate, there was no overwhelming favorite for the nomination going into the convention. With a two-thirds vote of the delegates needed to nominate, almost every silver delegate would have to vote for the same candidate to assure success, though any organized support from gold delegates would greatly damage a silver candidate's chances.
The only gold man who put together any sort of campaign for the Democratic nomination was Treasury Secretary John G. Carlisle, but he withdrew in April, stating that he was more concerned about the platform of the party than who would lead it. However, as late as June, the gold forces, which still controlled the Democratic National Committee (DNC), continued to believe that the nominee could be pro-gold. Cleveland friend and former Postmaster General Donald M. Dickinson wrote to the President in June 1896 hoping that the delegates would recognize "common sense" and be frightened at the thought of nominating a radical.
One of the leaders of the silver movement was Illinois Governor Altgeld; a native of Germany, he was constitutionally barred from the presidency by his foreign birth. Going into the convention, the two leading candidates for the nomination were former Congressman Bland, who had originated the Bland-Allison Act, and former Iowa Governor Horace Boies, with Bland considered the frontrunner. These were the only two candidates to put together organizations to try to secure delegate votes, though both efforts were cash-starved. Both men had electoral problems: Bland at age 61 was seen by some as a man whose time had passed; Boies was a former Republican who had once decried bimetallism. There were a large number of potential candidates seen as having less support; these included Vice President Adlai Stevenson of Illinois, Senator Joseph C. Blackburn of Kentucky, Senator Teller, and Bryan.
Silver advocates take control.
Although Bryan had decided on a strategy to gain the nomination—to give a speech which would make him the logical candidate in the eyes of delegates—he faced obstacles along the way. For one thing, he began the 1896 convention without any official status—the Democratic National Committee, which made the initial determination of which delegations would be seated, had chosen the pro-gold Nebraskans to represent their state. Bryan had been waiting outside the committee room when his rivals were seated by a 27–23 vote; contemporary accounts state he was "somewhat surprised" at the result. The DNC's action could be reversed, but not until the convention's credentials committee reported. However, Barnes deemed the actions by the committee immaterial to the outcome due to the silver strength in the convention:
Anyone who doubts the power the silverites were ready to unleash in a disciplined and irresistible attack needs only to read the results of the election of temporary chairman. The gold men, though they possessed the machinery of the party, had neither the power nor the strength to challenge their opponents. They could only beg them to spare the party the humiliation of broken traditions and the overthrowing of established control. Nevertheless, Senator John W. Daniel of Virginia was by an overwhelming vote elected temporary chairman, and a Committee on Credentials was appointed that seated Bryan and his contesting Nebraska delegation.
We demand the free and unlimited coinage of both silver and gold at the present legal ratio of 16 to 1 without waiting for the aid or consent of any other nation. We demand that the standard silver dollar shall be a full legal tender, equally with gold, for all debts, public and private, and we favor such legislation as will prevent for the future the demonitization of any kind of legal tender by private contract.
”
From the money plank of the Democratic platform
Good luck favored Bryan—he was considered for various convention roles by the silverites, but each time was not selected. The temporary chairmanship, for example, would have permitted him to deliver the keynote address. However, Bryan, lacking a seat at the start of the convention, could not be elected temporary chairman. Bryan considered this no loss at all; the focus of the convention was on the party platform and the debate which would precede its adoption. The platform would symbolize the repudiation of Cleveland and his policies after the insurgents' long struggle, and Bryan was determined to close the debate on the platform. Bryan, once seated, was Nebraska's representative to the Committee on Resolutions (generally called the "platform committee"), which allocated 80 minutes to each side in the debate and selected Bryan as one of the speakers. South Carolina Senator Benjamin Tillman was to be the other pro-silver speaker, and originally wished to close the debate. However, the senator wanted 50 minutes to speak, too long for a closing address, and at Bryan's request agreed to open the debate instead. Accordingly, Bryan became the final speaker on the platform.
Delegates, as they waited for the committees to complete their work, spent much of the first two days listening to various orators. Of these, only Senator Blackburn, a silver supporter, sparked much reaction, and that only momentary. Delegates called for better-known speakers, such as Altgeld or Bryan, but were granted neither then; the Illinois governor declined, and the Nebraskan, once seated, spent much of his time away from the convention floor at the platform committee meeting at the Palmer House.
The debate on the platform opened at the start of the third day of the convention, July 9, 1896. The session was supposed to begin at 10:00 a.m., but as delegates, slowed by the long commute from the hotels to the Coliseum and fatigue from the first two days, did not arrive on time, proceedings did not begin until 10:45. Nevertheless, large crowds gathered outside the public entrances; the galleries were quickly packed. Once the convention came to order, Arkansas Senator James K. Jones, chair of the Committee on Resolutions, read the proposed platform to cheers by many delegates; the reading of the pro-gold minority report attracted less applause.
"Pitchfork Ben" Tillman lived up to his nickname with an incendiary address which began with a reference to his home state's role in beginning the Civil War. Although Tillman endorsed silver, his address was so laced with sectionalism that most silver delegates remained silent for fear of being seen as supporting him. Tillman's speech, scheduled to be the only one in support of silver except Bryan's, was so badly received that Senator Jones, who had not planned to speak, gave a brief address asserting that silver was a national issue.
Senator David B. Hill of New York, a gold supporter, was next. As Hill moved to the podium, a reporter friend passed Bryan a note urging him to make a patriotic speech without hint of sectionalism; Bryan responded, "You will not be disappointed." Hill gave a calm speech defending the gold position, and swayed few delegates. He was followed by two other gold men, Senator William Vilas of Wisconsin and former Massachusetts Governor William E. Russell. Vilas gave a lengthy defense of the Cleveland administration's policies, so long that Russell, fearing that Vilas' speech would cut into his time, asked that the time given to the gold proponents be extended by ten minutes. Bryan consented, on condition that his own time was extended by the same amount; this was agreed to. "And I needed it for the speech I was to make." Bryan later wrote, "This was another unexpected bit of good fortune. I had never had such an opportunity before in my life and never expect to have again."
Vilas quickly lost his audience, which did not want to hear Cleveland defended. Russell's address was inaudible to most of the Coliseum; he was ill and died just over a week later. As the gold men spoke, Bryan ate a sandwich to settle his stomach; he was often nervous before major speeches. Another reporter approached him and asked him who he thought would win the nomination. "Strictly confidential, not to be quoted for publication: I will be."
Bryan addresses the convention.
As Russell concluded, to strong applause from gold delegates, there was a buzz of anticipation as Bryan ascended to the podium. There was loud cheering as Bryan stood there, waiting for his audience to calm. Bryan's lecture tours had left him a well-known spokesman for silver. As yet, no one at the convention had effectively spoken for that cause, which was paramount to the delegates. According to political scientist Richard F. Bensel in his study of the 1896 Democratic convention, "Although the silver men knew they would win this fight, they nonetheless needed someone to tell them—and the gold men—why they must enshrine silver at the heart of the platform." Bensel noted, "The pump was more than primed, it was ready to explode." Bryan would say little that he had not said before—the text is similar to that of a speech he had given at Crete, Nebraska the previous week—but he would give the convention its voice.
Bryan began softly,
I would be presumptuous, indeed, to present myself against the distinguished gentlemen to whom you have listened if this were a mere measuring of abilities; but this is not a contest between persons. The humblest citizen in all the land, when clad in the armor of a righteous cause, is stronger than all the hosts of error. I come to speak to you in defense of a cause as holy as the cause of liberty—the cause of humanity.
Bryan's opening claimed no personal prestige for himself—but nevertheless placed him as the spokesman for silver. According to Bensel, the self-deprecation helped disarm the delegates. As Bryan was not deemed a major contender for the nomination, even delegates committed to a candidate could cheer him without seeming to betray their allegiance. Bryan then recounted the history of the silver movement; the audience, which had loudly demonstrated its approval of his opening statements, quieted. Throughout the speech, Bryan had the delegates in the palm of his hand; they cheered on cue. The Nebraskan later described the audience as like a trained choir. As he concluded his historical recitation, he reminded the silver delegates that they had come to crown their victory, "not to discuss, not to debate, but to enter up the judgment already rendered by the plain people of this country".
Bryan continued with language evoking the Civil War, telling his audience that "in this contest brother has been arrayed against brother, father against son." By then, as he spoke in a sincere tone, his voice sounded clearly and loudly through the hall. He denied, however that the contest was personal; he bore no ill-will towards those who supported the gold standard. However, he stated, facing towards the gold delegates, "when you come before us and tell us that we are about to disturb your business interests, we reply that you have disturbed our business interests by your course." The gold men, during the address, paid close attention and showed their appreciation for Bryan's oratory. Bryan then defended the right of silver supporters to make their argument against opposition from gold men, who were associated with financial interests, especially in the East. Although his statements nominally responded to a point made by Russell, Bryan had thought of the argument the previous evening, and had not used it in earlier speeches. He always regarded it as the best point he made during the speech, and only the ending caused more reaction from his listeners:
We say to you that you have made the definition of a business man too limited in its application. The man who is employed for wages is as much a business man as his employer; the attorney in a country town is as much a business man as the corporation counsel in a great metropolis; the merchant at the cross-roads store is as much a business man as the merchant of New York; the farmer who goes forth in the morning and toils all day, who begins in spring and toils all summer, and who by the application of brain and muscle to the natural resources of the country creates wealth, is as much a business man as the man who goes upon the Board of Trade and bets upon the price of grain; the miners who go down a thousand feet into the earth, or climb two thousand feet upon the cliffs, and bring forth from their hiding places the precious metals to be poured into the channels of trade are as much business men as the few financial magnates who, in a back room, corner the money of the world. We come to speak of this broader class of business men.
Through this passage, Bryan maintained the contrast between the common man and the city-dwelling elite. It was clear to listeners as he worked his way through the comparisons that he would refer to the farmer, and when he did, the hall exploded with sound. His sympathetic comparison contrasted the hardworking farmer with the city businessman, whom Bryan cast as a gambler. The galleries were filled with white as spectators waved handkerchiefs, and it was several minutes before he could continue. The police in the convention hall, not sharing the enthusiasm for silver, were described by the press (some of whose members were caught up in the frenzy) as standing as if they thought the audience was about to turn on them. When Bryan resumed, his comparison of miner with miser again electrified the audience; the uproar prevented him from continuing for several minutes. One farmer in the gallery had been about to leave rather than listen to Bryan, whom he deemed a Populist; he had been persuaded to stay. At Bryan's words, he threw his hat into the air, slapped the empty seat in front of him with his coat, and shouted, "My God! My God! My God!"
Bryan, having established the right of silver supporters to petition, explained why that petition was not to be denied:
It is for these that we speak. We do not come as aggressors. Our war is not a war of conquest; we are fighting in the defense of our homes, our families, and posterity. We have petitioned, and our petitions have been scorned; we have entreated, and our entreaties have been disregarded; we have begged, and they have mocked when our calamity came. We beg no longer; we entreat no more; we petition no more. We defy them!
With this call to action, Bryan abandoned any hint at compromise, and adopted the techniques of the radical, polarizing orator, finding no common ground between silver and gold forces. He then defended the remainder of the platform, though only speaking in general terms. He mocked McKinley, said by some to resemble Napoleon, noting that he was nominated on the anniversary of the Battle of Waterloo. The lengthy passage as he discussed the platform and the Republicans helped calm the audience, ensuring he would be heard as he reached his peroration. But Bryan first wished to tie the silver question to a greater cause:
Upon which side will the Democratic Party fight; upon the side of "the idle holders of idle capital" or upon the side of "the struggling masses"? That is the question which the party must answer first, and then it must be answered by each individual hereafter. The sympathies of the Democratic Party, as shown by the platform, are on the side of the struggling masses, who have ever been the foundation of the Democratic Party.
He faced in the direction of the gold-dominated state delegations:
There are two ideas of government. There are those who believe that, if you will only legislate to make the well-to-do prosperous, their prosperity will leak through on those below. The Democratic idea, however, has been that if you legislate to make the masses prosperous, their prosperity will find its way up through every class which rests upon them. You come to us and tell us that the great cities are in favor of the gold standard; we reply that the great cities rest upon our broad and fertile prairies. Burn down your cities and leave our farms, and your cities will spring up again as if by magic; but destroy our farms and the grass will grow in the streets of every city in the country.
This statement attracted great cheering, and Bryan turned to rhetorically demolish the compromise position on bimetallism—that it should only be accomplished through international agreement:
It is the issue of 1776 over again. Our ancestors, when but three millions in number, had the courage to declare their political independence of every other nation; shall we, their descendants, when we have grown to seventy millions, declare that we are less independent than our forefathers? No, my friends, that will never be the verdict of our people. Therefore, we care not upon what lines the battle is fought. If they say bimetallism is good, but that we cannot have it until other nations help us, we reply that, instead of having a gold standard because England has, we will restore bimetallism, and then let England have bimetallism because the United States has it. If they dare to come out in the open field and defend the gold standard as a good thing, we will fight them to the uttermost.
Now, Bryan was ready to conclude the speech, and according to his biographer, Michael Kazin, step "into the headlines of American history".
Having behind us the producing masses of this nation and the world, supported by the commercial interests, the laboring interests, and the toilers everywhere, we will answer their demand for a gold standard by saying to them: "You shall not press down upon the brow of labor this crown of thorns; you shall not crucify mankind upon a cross of gold."
As Bryan spoke his final sentence, recalling the Crucifixion of Jesus, he placed his hands to his temples, fingers extended; with the final words, he extended his arms to his sides straight out to his body and held that pose for about five seconds as if offering himself as sacrifice for the cause, as the audience watched in dead silence. He then lowered them, descended from the podium, and began to head back to his seat as the stillness held.
Reception and nomination.
Convention events.
Bryan later described the silence as "really painful" and momentarily thought he had failed. As he moved towards his seat, the Coliseum burst into pandemonium. Delegates threw hats, coats, and handkerchiefs into the air. Others took up the standards with the state names on them with each delegation, and planted them by Nebraska's. Two alert police officers had joined Bryan as he left the podium, anticipating the crush. The policemen were swept away by the flood of delegates, who raised Bryan to their shoulders and carried him around the floor. "The Washington Post" newspaper recorded, "bedlam broke loose, delirium reigned supreme."
It took about 25 minutes to restore order, and according to Bensel, "somewhere in the mass demonstration that was convulsing the convention hall, the transfer of sentiment from silver as a policy to Bryan as a presidential candidate took place". Newspaper accounts of the convention leave little doubt but that, had a vote been taken at that moment (as many were shouting to do), Bryan would have been nominated. Bryan was urged by Senator Jones to allow it, but refused, stating that if his boom would not last overnight, it would never last until November. He soon retired from the convention, returning to his hotel to await the outcome. The convention passed the platform in Bryan's absence and recessed.
The balloting began the following morning, July 10, with a two-thirds vote necessary to nominate. Bryan, who remained at his hotel, sent word to the Nebraska delegation to make no deals on his behalf. He stood second out of fourteen candidates in the first ballot, behind Bland. On the second ballot, Bryan still stood second, but had gained as other candidates had fallen away. The third ballot saw Bland still in the lead, but Bryan took the lead on the fourth ballot. According to Jones, it was clear that Bland could not win, and that Bryan could not be stopped. On the fifth ballot, the Illinois delegation, led by Governor Altgeld, switched its votes from Bland to Bryan. Other delegations, seeing that Bryan would be nominated, also switched, securing the victory. Nevertheless, he won the nomination without the votes of the gold delegates, most of whom either left the convention or refused to vote.
Press reaction.
Most contemporary press accounts attributed Bryan's nomination to his eloquence, though in the case of Republican and other gold-favoring newspapers, they considered it his demagoguery. The pro-silver "Cleveland Plain Dealer" called Bryan's speech "an eloquent, stirring, and manly appeal". The "Chicago Tribune" reported that Bryan had lit the spark "which touched off the trail of gun-powder". The "St. Louis Post-Dispatch" opined that with the speech, Bryan "just about immortalized himself".
According to the "New York World", "Lunacy having dictated the platform, it was perhaps natural that hysteria should evolve the candidate." "The New York Times" disparaged Bryan as "the gifted blatherskite from Nebraska". The only paper to predict, after Bryan gave his speech, that he would not be nominated was "The Wall Street Journal", which stated, "Bryan has had his day". The "Akron Journal and Republican", no friend to Bryan, opined that "never probably has a national convention been swayed or influenced by a single speech as was the national Democratic convention".
Campaign and aftermath.
The Pullman Company offered Bryan a private car for his trip home; he declined, not wishing to accept corporate favors. As he traveled by rail to Lincoln, he saw farmers and others standing by the tracks, hoping for a glimpse of the new Democratic nominee. He received many letters from supporters, expressing their faith in him in stark terms. One Indiana voter wrote, "God has sent you amongst our people to save the poor from starvation, and we no ["sic"] you will save us." A farmer in Iowa, in a letter to Bryan, stated, "You are the first big man that i ["sic"] ever wrote to."
When McKinley heard that Bryan was likely to be the nominee, he called the report "rot" and hung up the phone. The Republican nominee was slow to realize the surge of support for Bryan after the nomination, stating his view that the silver sentiment would be gone in a month. When McKinley and his advisers, such as industrialist and future senator Mark Hanna, realized that the views were more than transitory, they began intensive fundraising from corporations and the wealthy. The money went for speakers, pamphlets, and other means of conveying their "sound money" campaign to the voter. With far less money than McKinley, Bryan embarked on a nationwide campaign tour by train on a then-unprecedented scale. McKinley on the other hand, opted for a front porch campaign. Both men spoke to hundreds of thousands of people from their chosen venues.
Bryan's nomination divided the party. The dissidents nominated their own ticket; the split in the vote would contribute to Bryan's defeat. However, Bryan did gain the support of the Populists, as well as a convention of Silver Republicans. Bryan spoke on silver throughout the campaign; he rarely addressed other issues. Bryan won the South and most of the West, but McKinley's victories in the more populous Northeast and Midwest carried him to the presidency. The Democratic candidate failed to gain a majority of the labor vote; McKinley won in working-class areas as well as wealthy precincts. Although McKinley outpolled him by 600,000 votes, Bryan received more votes than any previous presidential candidate.
After McKinley's inauguration, increases in gold availability from new discoveries and improved refining methods led to a considerable increase in the money supply. Even so, in 1900, Congress passed the Gold Standard Act, formally placing the United States on that standard. Although Bryan ran again on a silver platform in the 1900 presidential election, the issue failed to produce the same resonance with the voters. McKinley won more easily than in 1896, making inroads in the silver West.
Legacy.
Bryan's speech is considered one of the most powerful political addresses in American history. Stanley Jones, however, suggested that even if Bryan had never made it, he would still have been nominated. Jones deemed the Democrats likely to nominate a candidate who would appeal to the Populist Party, and Bryan had been elected to Congress with Populist support. According to rhetorical historian William Harpine in his study of the rhetoric of the 1896 campaign, "Bryan's speech cast a net for the true believers, but only for the true believers." Harpine suggested that, "by appealing in such an uncompromising way to the agrarian elements and to the West, Bryan neglected the national audience who would vote in the November election". Bryan's emphasis on agrarian issues, both in his speech and in his candidacy, may have helped cement voting patterns which kept the Democrats largely out of power until the 1930s.
Writer Edgar Lee Masters called the speech, "the beginning of a changed America." Bryan's words gave rise to later economic and political philosophies, including Huey Long's 1930s Share Our Wealth program, with its trigger phrase, "Every Man a King" inspired by Bryan's speech. Author and political commentator William Safire, in his political dictionary, traced the term "trickle-down economics" (common in the Reagan era) to Bryan's statement that some believe that government should legislate for the wealthy, and allow prosperity to "leak through" on those below. Historian R. Hal Williams suggested that the opposite philosophy, of legislation for the masses leading to prosperity for all, advocated by Bryan in his speech, informed the domestic policies of later Democratic presidents, including Franklin Roosevelt with his New Deal.
Bensel ties the delegates' response to Bryan's address to their uncertainty in their own beliefs:
In a very real sense, adoption of the silver plank in the platform was akin to a millennial expectation that the "laws of economics" would henceforth be suspended and that the silver men could simply "will" that silver and gold would, in fact, trade on financial markets at a ratio of sixteen to one. The silver men were thus in the hunt for a charismatic leader who would underpin what they already desperately wanted to believe. They manufactured that leader in the convention, a fabrication in which Bryan was only too happy to assist.

</doc>
<doc id="38743" url="http://en.wikipedia.org/wiki?curid=38743" title="Cape Cod">
Cape Cod

Cape Cod is a geographic cape and peninsula that juts out into the Atlantic Ocean in the easternmost part of the state of Massachusetts, in the Northeastern United States. Its historic, maritime character and ample beaches attract heavy tourism during the summer months.
Cape Cod stretches from Provincetown in the northeast to Woods Hole in the southwest. Cape Cod, as defined by the Cape Cod Commission's enabling legislation, comprises Barnstable County. The Cape Cod Canal cuts 17.5 mi roughly across the base of the peninsula, though the western boundary of the cape extends to include small portions of the towns of Bourne and Sandwich which lie on the mainland side of the canal. 
Two road bridges cross the Cape Cod Canal: the Sagamore Bridge and the Bourne Bridge. In addition, the Cape Cod Canal Railroad Bridge carries railway freight and limited passenger services onto the Cape. Cape territory is divided into fifteen towns with many villages.
Region of Cape Cod and the Islands.
Like Cape Cod itself, the islands south of the Cape have evolved from whaling and trading areas to become resort destinations, attracting wealthy families, celebrities, and other tourists. These include the large nearby islands of Martha's Vineyard and Nantucket, which are themselves famous summer tourist destinations, commonly accessed by ferry from Cape Cod. The phrases Cape Cod and the Islands and the Cape and Islands are often used to describe the whole region of Barnstable County, Dukes County (including the Vineyard and the smaller Elizabeth Islands), and Nantucket County.
Several small islands right off Cape Cod, including Monomoy Island, Monomoscoy Island, Popponesset Island, and Seconsett Island, are also in Barnstable County.
The Forbes family-owned Naushon Island was first purchased by John Murray Forbes. Naushon is one of the Elizabeth Islands, many of which are privately owned. One of the publicly accessible Elizabeths is the southernmost island in the chain, Cuttyhunk, with a year-round population of 52 people. Several prominent families have established compounds or estates on the larger islands, making these islands some of the wealthiest resorts in the Northeast, yet they retain much of the early merchant trading and whaling culture.
Cape Cod in particular is a popular retirement area, and the average age of residents is the highest of any area in New England. By voter registration numbers, Democrats outnumber Republicans by less in the three counties than in the whole of Massachusetts, to varying degrees.
The bulk of the land in the area is glacial terminal moraine and represents the southernmost extent of glacial coverage in southeast New England; similar glacial formations make up Long Island in New York and Block Island in Rhode Island.
Geography and political divisions.
Physical geography and boundaries.
The name "Cape Cod", as it was first used in 1602, applied only to the very tip of the peninsula. It remained that way for 125 years, until the "Precinct of Cape Cod" was incorporated as the Town of Provincetown. No longer in "official" use over the ensuing decades, the name came to mean all of the land east of the Manomet and Scusset rivers – essentially along the line that became the Cape Cod Canal. The creation of the canal separated the majority of the peninsula from the mainland, effectively turning it into an island. Indeed, most agencies, including the Cape Cod Commission and the Federal Emergency Management Agency (FEMA), treat the Cape as an island with regards to disaster preparedness, groundwater management, and the like. Most "Cape Codders" – residents of "The Cape" – refer to all land on the mainland side of the canal as "off-Cape." However, the legal delineation of Cape Cod, coincident to the boundaries of Barnstable County, extends to the northwest boundaries of the towns of Bourne and Sandwich – and small portions of each are located west of the canal.
Cape Cod Bay lies in between Cape Cod and the mainland – bounded on the north by a horizontal line between Provincetown and Marshfield. North of Cape Cod Bay (and Provincetown) is Massachusetts Bay, which contains the Stellwagen Bank National Marine Sanctuary, located 5 mi north of Provincetown. The Atlantic Ocean is to the east of Cape Cod, and to the southwest of the Cape is Buzzards Bay. The Cape Cod Canal, completed in 1916, connects Buzzards Bay to Cape Cod Bay; its creation shortened the trade route between New York and Boston by 62 mi.
Cape Cod extends 65 mi into the Atlantic Ocean, with a breadth of between 1 –, and covers more than 400 miles of shoreline. Its elevation ranges from 306 ft at its highest point, at the top of Pine Hill, in the Bourne portion of Joint Base Cape Cod, down to sea level.
One of the biggest barrier islands in the world, Cape Cod shields much of the Massachusetts coastline from North Atlantic storm waves. This protection erodes the Cape's shoreline at the expense of its cliffs, while protecting towns from Fairhaven to Marshfield.
Cape Cod and the Islands are part of a continuous archipelagic region consisting of a thin line of islands stretching west to include Long Island, in New York. This region is historically and collectively known by naturalists as the Outer Lands.
Towns and villages.
Cape Cod incorporates all of Barnstable County, which comprises 15 towns: Bourne, Sandwich, Falmouth, Mashpee, Barnstable, Yarmouth, Harwich, Dennis, Brewster, Chatham, Orleans, Eastham, Wellfleet, Truro, and Provincetown. Each of these towns include a number of villages; see Barnstable County for a complete list.
Barnstable, the most populated municipality on Cape Cod, is the only one to have adopted a city form of government, whose legislative body is an elected 13-member council. However, Like other smaller Massachusetts cities, Barnstable retained its "Town of Barnstable" moniker. All of the other towns elect a 5-member Board of Selectmen as the executive policy-setting board, and utilize Town Meetings as their legislative body.
Cape Cod and the Islands.
To the south of Cape Cod lie Nantucket Sound; Nantucket and Martha's Vineyard, both large islands; and the mostly privately owned Elizabeth Islands.
Sections.
For most of the 18th, 19th, and 20th centuries, Cape Cod was considered to consist of three sections (see map):
"Upper" and "Lower".
The terms "Upper Cape" and "Lower Cape", and references to traveling "up Cape" or "down Cape" have long been a source of confusion for the uninitiated Cape Cod visitor, who, mistakenly associating "up" with "north", might get turned around by passages such as these from 1920: 
There are many theories to explain the apparent paradox. One is that the terms derive from early nautical navigation. When one traveled to the east, one went down the longitudinal scale (toward zero at Greenwich, England). Additionally, prevailing fair weather winds (generally out of the southwest) have been used as the basis for directional descriptions by European settlers and their descendants in eastern North America. That is, one would be traveling "down [wind]" to the east with a westerly wind at one's back. To this day, on nearby Martha's Vineyard, "Up Island" is the western section and "Down Island" is to the east.
The arrival of the railroad during the nineteenth century reinforced the "up/down" concept, as train schedules between Boston and Cape Cod always showed Boston at the top – the timetable for trains headed onto the Cape would be read from the top down, and those of returning trains would be read from the bottom up. Provincetown, therefore, despite being the Cape's northernmost town, was the furthest "down" that one could travel. (The Cape's unique shape brought a new paradox along with the automobile and highway system: when driving "down Cape" on US Route 6 "eastbound", the final 30 miles from Orleans to Provincetown takes one in nearly every direction "except" east.)
The best known colloquial explanation, however, is that the shape of the peninsula as it appears on maps and charts resembles that of a human arm. In that analogy, the southern portion of the Cape represents the "upper arm", Chatham the elbow, and the north-south portion is the "lower arm", or forearm. Going further, some say Provincetown is the curled hand, or fist, with Race Point and Wood End at its knuckles, and Long Point at the fingertips.
In the late twentieth century, as the Cape began drawing more vacationers and artists on retreat, the nautical nomenclature and potential confusion over directions have gradually been giving way to the simpler "Outer Cape", although the older terms are still used by some local residents.
Geology.
"East of America, there stands in the open Atlantic the last fragment of an ancient and vanished land. Worn by the breakers and the rains, and disintegrated by the wind, it still stands bold."
 
The bulk of the land on Cape Cod consists of glacial landforms, formed by terminal moraine and outwash plains. This represents the southernmost extent of glacial coverage in southeast New England; similar glacial formations make up Long Island in New York and Block Island in Rhode Island. Together, these formations are known as the Outer Lands, or more obscurely as the "Isles of Stirling". Geologically speaking, Cape Cod is quite young, having been laid down some 16,000 to 20,000 years ago.
Most of Cape Cod's geological history involves the advance and retreat of the Laurentide ice sheet in the late Pleistocene geological era and the subsequent changes in sea level. Using radiocarbon dating techniques, researchers have determined that around 23,000 years ago, the ice sheet reached its maximum southward advance over North America, and then started to retreat. Many kettle ponds – clear, cold lakes – were formed and remain on Cape Cod as a result of the receding glacier. By about 18,000 years ago, the ice sheet had retreated past Cape Cod. By roughly 15,000 years ago, it had retreated past southern New England. When so much of Earth's water was locked up in massive ice sheets, the sea level was lower. Truro's bayside beaches used to be a petrified forest, before it became a beach.
As the ice began to melt, the sea began to rise. Initially, sea level rose quickly, about 15 m per 1,000 years, but then the rate declined. On Cape Cod, sea level rose roughly 3 m per millennium between 6,000 and 2,000 years ago. After that, it continued to rise at about 1 m per millennium. By 6,000 years ago, the sea level was high enough to start eroding the glacial deposits that the vanished continental ice sheet had left on Cape Cod. The water transported the eroded deposits north and south along the outer Cape's shoreline through a process known as longshore drift. Those reworked sediments that moved north went to the tip of Cape Cod. The entire town of Provincetown, at the extreme tip of the Cape, is a spit consisting largely of deposited marine sediment that was eroded and transported from farther south along the shore. Those sediments that instead moved south created the islands and shoals of Monomoy. So while other parts of the Cape have dwindled from the action of the waves, these parts of the Cape have grown through the deposition of sediment in just the last 6,000 years.
This process continues today. Due to their position jutting out into the Atlantic Ocean, the Cape and islands are subject to massive coastal erosion. Geologists say that, due to erosion, the Cape will be completely submerged by the sea in thousands of years. This erosion causes the washout of beaches and the destruction of the barrier islands; for example, the ocean broke through the barrier island at Chatham during Hurricane Bob in 1991, allowing waves and storm surges to hit the coast with no obstruction. Consequently, the sediment and sand from the beaches is being washed away and deposited elsewhere. While this destroys land in some places, it creates land elsewhere, most noticeably in marshes where sediment is deposited by waters running through them.
Climate.
Although Cape Cod's weather is typically more moderate than inland locations, there have been occasions where Cape Cod has dealt with the brunt of extreme weather situations (such as the Blizzard of 2005 and Hurricane Bob). Because of the influence of the Atlantic Ocean, temperatures are typically a few degrees cooler in the summer and a few degrees warmer in the winter. A common misconception is that the climate is influenced largely by the warm Gulf Stream current; however, that current turns eastward off the coast of Virginia, and the waters off the Cape are more influenced by the cold Canadian Labrador Current. As a result, the ocean temperature rarely gets above 65 F, except along the shallow west coast of the Upper Cape and along the southern coast (Nantucket Sound), where water temperatures can sometimes reach 70 F or higher.
Cape Cod's climate is also notorious for a delayed spring season, being surrounded by an ocean which is still cold from the winter; however, it is also known for an exceptionally mild fall season (Indian summer), thanks to the ocean remaining warm from the summer. The highest temperature ever recorded on Cape Cod was 104 F in Provincetown, and the lowest temperature ever was -12 F in Barnstable.
The water surrounding Cape Cod moderates winter temperatures nearly enough to extend the humid subtropical climate zone to what could be its northernmost limit in eastern North America, as the majority of Cape Cod is in USDA hardiness zone 7a. Consequently, many subtropical indicator plant species typically found in more southerly latitudes grow there, including "Camellias", "Ilex opaca", "Magnolia grandiflora" and "Albizia julibrissin". However, Cape Cod falls below the 72 F threshold, as the warmest month, July, averages around 68.25 F. Therefore, the climate may be better characterized as either a maritime climate or a humid continental climate (particularly on the northern coast of the upper and mid cape, which is somewhat sheltered from the cooler onshore wind to the south).
Precipitation on Cape Cod and the islands of Martha's Vineyard and Nantucket is the lowest in the New England region, averaging slightly less than 40 in a year (most parts of New England average 42 to). This is due to storm systems which move across western areas, building up in mountainous regions, and dissipating before reaching the coast where the land has leveled out. The region does not experience a greater number of sunny days, however, as the number of cloudy days is the same as inland locales, in addition to increased fog. On average, roughly 27 in of snow, which is about 17 in less than Boston, falls in an average winter.
Once every five or six years, a tropical storm, accompanied by very high and potentially damaging winds and heavy rain, will strike the region. About once every 11 or 12 years a hurricane brings damaging winds and storm surges to the region. Several Category 3 storms have struck Cape Cod since record-keeping began, such as the Long Island Express of 1938 and the 1944 Great Atlantic hurricane. Strong Category 2 storms, such as Hurricane Carol of 1954 and Hurricane Bob of 1991 also caused considerable damage. Notable Category 1 storms include Hurricane Edna of 1954 (shortly following Hurricane Carol) and Hurricane Donna of 1960, also Hurricane Irene in 2011, and Hurricane Sandy in 2012. Other notable storms include the Gale of 1815, which would likely have been rated a strong hurricane on the Saffir-Simpson scale, and the so-called "Perfect Storm" of October 31, 1991. The February 2013 nor'easter produced winds in excess of 80 mph and dropped over 24 in of snow on some parts of Cape Cod. The storm knocked out power to tens of thousands of Cape Cod residents, some for up to two weeks.
Native population.
Cape Cod has been the home of the Wampanoag tribe of Native American people for many centuries. They survived off the sea and were accomplished farmers. They understood the principles of sustainable forest management, and were known to light controlled fires to keep the underbrush in check. They helped the Pilgrims, who arrived in the fall of 1620, survive at their new Plymouth Colony.
The Indians lost their lands through continued purchase and expropriation by the English colonists. The documentary "Natives of the Narrowland" (1993), narrated by actress Julie Harris, shows the history of the Wampanoag people through Cape Cod archaeological sites.
In 1974, the Mashpee Wampanoag Tribal Council was formed to articulate the concerns of those with Native American ancestry. They petitioned the federal government in 1975 and again in 1990 for official recognition of the Mashpee Wampanoag as a tribe. In May 2007, the Wampanoag tribe was federally recognized as a tribe.
History.
Cape Cod was a landmark for early explorers. It may have been the "Promontory of Vinland" mentioned by the Norse voyagers (985–1025). The Manomet River area (taken up by the western end of the Cape Cod Canal in the early 20th century) is claimed by some to have been visited by Leif Eiriksson, and a stone wall discovered in Provincetown in 1805 is also claimed to have been built by his younger brother Thorvald Eiriksson around 1007 AD, when according to Norse sagas, the keel of his ship was repaired in the harbor. He was killed later in the same journey, and is said to have been returned to this spot for burial. However, there is no tangible support of the presence of Norse voyagers in Cape Cod, and the view is not generally accepted by archaeologists or historians.
Giovanni da Verrazzano in 1524 approached it from the south. He named Martha's Vineyard Claudia, after Claude of France, the wife of Francis I of France. In 1525, Portuguese explorer Estêvão Gomes, sailing under the Spanish crown, called it Cabo de la Arenas.
In 1602 Bartholomew Gosnold named the tip Cape Cod, the surviving term and the ninth oldest English place-name in the U.S. Samuel de Champlain charted its sand-silted harbors in 1606, and Henry Hudson landed there in 1609. Captain John Smith noted it on his map of 1614, and at last the Pilgrims entered the "Cape Harbor" and – contrary to the popular myth of Plymouth Rock – made their first landing near present-day Provincetown on November 11, 1620. Nearby, in what is now Eastham, they had their first encounter with Native Americans.
Cape Cod was among the first places settled by the English in North America. Aside from Barnstable (1639), Sandwich (1637) and Yarmouth (1639), the Cape's fifteen towns developed slowly. The final town to be established on the Cape was Bourne in 1884, breaking off from Sandwich. Provincetown was a group of huts until the 18th century. A channel from Massachusetts Bay to Buzzards Bay is shown on Southack's map of 1717. The present Cape Cod Canal was slowly developed from 1870 to 1914. The federal government purchased it in 1928.
Because of early colonial settlement and intensive land use, by the time Henry Thoreau saw Cape Cod during his four visits over 1849 to 1857, its vegetation was depauperate and trees were scarce. As the settlers heated by fires, and it took 10 to 20 cords (40 to 80 m³) of wood to heat a home, they cleared most of Cape Cod of timber early on. They planted familiar crops, but these were unsuited to Cape Cod's thin, glacially derived soils. For instance, much of Eastham was planted to wheat. The settlers practiced burning of woodlands to release nutrients into the soil. Improper and intensive farming led to erosion and the loss of topsoil. Farmers grazed their cattle on the grassy dunes of coastal Massachusetts, only to watch "in horror as the denuded sands 'walked' over richer lands, burying cultivated fields and fences." Dunes on the outer Cape became more common, and many harbors filled in with eroded soils.
By 1800, much of Cape Cod's firewood had to be transported by boat from Maine. The paucity of vegetation was worsened by the raising of merino sheep that reached its peak in New England around 1840. The early industrial revolution, which occurred through much of Massachusetts and Rhode Island, mostly bypassed Cape Cod due to a lack of significant water power in the area. As a result, and also because of its geographic position, the Cape developed as a large fishing and whaling center. After 1860 and the opening of the American West, farmers abandoned agriculture on the Cape. By 1950 forests had recovered to an extent not seen since the 18th century.
Cape Cod became a summer haven for city dwellers beginning at the end of the 19th century. Improved rail transportation made the towns of the Upper Cape, such as Bourne and Falmouth, accessible to Bostonians. At the beginning of the twentieth century, the Northeastern mercantile elite built many large, shingled "cottages" along Buzzards Bay. The relaxed summer environment offered by Cape Cod was highlighted by writers including Joseph C. Lincoln, who published novels and countless short stories about Cape Cod folks in popular magazines such as the "Saturday Evening Post" and the "Delineator".
Guglielmo Marconi made the first transatlantic wireless transmission originating in the United States from Cape Cod, at Wellfleet. The beach below the bluffs where his station was located is now called Marconi Beach. In 1914 he began construction of a new transatlantic wireless receiver station in Chatham and a companion transmitter station in Marion. In 1920 the stations were acquired by RCA, and in 1921 Chatham began operations as a maritime radio station communicating to ships at sea using the callsign WCC. WCC supported the communications of Amelia Earhart, Howard Hughes, Admiral Byrd, and the "Hindenburg". Marconi chose Chatham due to its vantage point on the Atlantic Ocean, surrounded on three sides by water. Walter Cronkite narrated a 17-minute documentary in 2005 about the history of the Chatham Station.
Much of the east-facing Atlantic seacoast of Cape Cod consists of wide, sandy beaches. In 1961, a significant portion of this coastline, already slated for housing subdivisions, was made a part of the Cape Cod National Seashore by President John F. Kennedy. It was protected from private development and preserved for public use. Large portions are open to the public, including the Marconi Site in Wellfleet. This is a park encompassing the site of the first two-way transoceanic radio transmission from the United States. (Theodore Roosevelt used Marconi's equipment for this transmission.)
The Kennedy Compound in Hyannis Port was President Kennedy's summer White House during his presidency, and the Kennedy family continues to maintain residences on the compound. President Grover Cleveland maintained a summer home in the Gray Gables section of Bourne. Other notable residents of Cape Cod have included actress Julie Harris, US Supreme Court justice Louis Brandeis, figure skater Todd Eldredge, composer and radio personality Canary Burton, and novelists Norman Mailer and Kurt Vonnegut. Influential natives included the patriot James Otis, historian and writer Mercy Otis Warren, jurist Lemuel Shaw, and naval officer John Percival.
Lighthouses of Cape Cod.
"Lighthouses, from ancient times, have fascinated members of the human race. There is something about a lighted beacon that suggests hope and trust and appeals to the better instincts of mankind."
 
Due to its dangerously hidden and constantly moving shoals located just off-shore, Cape Cod's coastline from Chatham to Provincetown – a mere fifty-mile stretch of sea – has been called an "ocean graveyard", containing over 3,000 shipwrecks. Indeed, between Truro and Wellfleet alone, there have been more than 1,000 wrecks, including that of the Whydah Gally, the famed pirate ship of Samuel Bellamy that went down with over 4.5 ST of treasure.
Beginning in 1857, lighthouses were erected to serve as beacons to warn ships of the danger. Highland Light (or Cape Cod Light) is the oldest and tallest of these, and remains as one of a number of working lighthouses on Cape Cod and the Islands. Many of Cape Cod's earliest lighthouses featured a light tower that was attached directly to – and centered on the roof of – the keeper's dwelling. A stairway to the lantern room was accessible only from the top floor of the house. This came to be known as a Cape Cod style lighthouse, yet today, the only fully intact specimens are on the West Coast.
Most of Cape Cod's lighthouses are operated by the U.S. Coast Guard with some exceptions such as the Nauset Light, which was decommissioned in 1996 and is now maintained by the Nauset Light Preservation Society under the auspices of Cape Cod National Seashore. These lighthouses are frequently photographed symbols of Cape Cod.
In 1996, both the Highland Light and Nauset Light were moved, because they were each at risk of being lost to the encroaching seas. The Highland Light, then 110 ft from the ocean, was moved 450 ft to the west, and the Nauset Light, a mere 37 ft from the ocean, was moved 300 ft west.
The lighthouses of Cape Cod include:
Transportation.
Road.
Cape Cod is connected to the mainland by a pair of canal-spanning highway bridges, the Bourne and Sagamore that were constructed in the 1930s (replacing a 1912 drawbridge). The two parallel road bridges are four miles apart, with the Bourne Bridge to the west, and the Sagamore to the east. The bridges form a bottleneck, resulting in traffic backups of several miles during the tourist season - especially going on-cape at the beginning of the weekend and off-cape at the end of the weekend.
The entire Cape is roughly bisected lengthwise by U.S. Route 6, locally known as the Mid-Cape Highway and officially as the Grand Army of the Republic Highway.
Air and water.
Commercial air service to Cape Cod operates out of Barnstable Municipal Airport and Provincetown Municipal Airport. General aviation airports are:
There is one military airport at Otis Air National Guard Base.
There are ferry connections from Boston to Provincetown, as well as from Hyannis and Woods Hole to the islands.
Bus.
Cape Cod Regional Transit Authority operates a year-round public bus system comprising three long distance routes and a local bus in Hyannis and Barnstable Village. From mid June until October, additional local routes are added in Falmouth and Provincetown. CCRTA also operates Barnstable County's ADA-required paratransit (dial-a-ride) service, under the name "B-Bus."
Long distance bus service is available through Plymouth and Brockton Street Railway, with regular service to downtown Boston and Logan Airport, as well as less frequent service to Provincetown. Peter Pan Bus Lines also runs long distance service to T.F. Green Airport in Providence, Rhode Island; New York City; and service between Logan Airport, Boston South Station, and Woods Hole.
Rail service.
The third bridge over the Cape Cod Canal is a vertical-lift railroad bridge, providing an alternative land transport option.
CapeFLYER is a seasonal passenger rail service between Boston and Hyannis that operates on summer weekends from Memorial Day through Columbus Day, beginning in 2013.
Regular passenger rail service through Cape Cod ended in June 1959. In 1978, the tracks east of South Dennis were abandoned and replaced with the Cape Cod Rail Trail. Another bike path, the Shining Sea Bikeway, was built over abandoned tracks between Woods Hole and Falmouth in 1975, and in 2008 the 7.4 mi rail line between Falmouth and North Falmouth was removed and the right-of-way converted into an extension of the Shining Sea Bikeway.
Active freight service remains in the Upper Cape area in Sandwich and in Bourne, largely due to a trash transfer station located at Joint Base Cape Cod along the Bourne-Falmouth rail line. In 1986, Amtrak operated a seasonal service in the summer from New York City to Hyannis called the "Cape Codder". From 1988, Amtrak and the Massachusetts Department of Transportation increased service to a daily frequency, until service ended in 1996.
The Cape Cod Central Railroad is a heritage railroad on Cape Cod. The service is primarily tourist-oriented and includes a dinner train over a scenic route between downtown Hyannis and the Cape Cod Canal lasting about 2½ hours round trip. The Massachusetts Coastal Railroad is planning to return passenger railroad services to the Bourne-Falmouth rail line in the future.
Bicycle.
Bicycle and pedestrian access to the Cape is possible via a sidewalk on the southbound side of the Bourne Bridge. There are a number of dedicated bike trails and paths around the Cape, including:
For long-distance biking, the mostly on-road Claire Saltonstall Bikeway connects Cape Cod to the Charles River Bike Path in Boston.
Tourism.
Although Cape Cod has a year-round population of about 220,000, it experiences a tourist season each summer, the beginning and end of which can be roughly approximated as Memorial Day and Labor Day, respectively. Many businesses specifically target summer visitors, although the "on season" has been expanding somewhat in recent years due to Indian Summer, reduced lodging rates, and the number of people visiting the Cape after Labor Day who either have no school-age children, and the elderly, reducing the true "off season" to six or seven months. In the late 20th century, tourists and owners of second homes began visiting the Cape more and more in the spring and fall, softening the definition of the high season and expanding it somewhat (see above). Some particularly well-known Cape products and industries include cranberries, shellfish (particularly oysters and clams) and lobstering.
Provincetown, at the tip of Cape Cod, also berths the original East Coast whale watching fleet (Dolphin Fleet) who patrol the Stellwagen Bank National Marine Sanctuary. The fleet guarantee a whale sighting (mostly humpback whale, fin whale, minke whale, sei whale, and the critically endangered North Atlantic right whale), and is the only federally certified operation qualified to rescue whales. Provincetown has also long been known as an art colony, attracting writers and artists. The town is home to the Cape's most attended art museum, the Provincetown Art Association and Museum.
Cape Cod is a popular destination for beachgoers from all over. With 559.6 mi of coastline, beaches, both public and private, are easily accessible. The Cape has upwards of sixty public beaches, many of which offer parking for non-residents for a daily fee (in summer). The Cape Cod National Seashore has 40 mi of sandy beach and many walking paths.
Cape Cod is also popular for its outdoor activities like beach walking, biking, boating, fishing, go-karts, golfing, kayaking, miniature golf, and unique shopping. There are 27 public, daily-fee golf courses and 15 private courses on Cape Cod. Bed and breakfasts or vacation houses are often used for lodging.
Each summer the Naukabout Music Festival is held at the Barnstable County Fair Grounds located in East Falmouth, typically during the first weekend of August. The festival features local, regional and national talent, along with food, arts and family-friendly activities.
Sport fishing.
Cape Cod is known around the world as a spring-to-fall destination for sport anglers. Among the species most widely pursued are striped bass, bluefish, bluefin tuna, false albacore (little tunny), bonito, tautog, flounder and fluke. The Cape Cod Bay side of the Cape, from Sandwich to Provincetown, has numerous harbors, saltwater creeks, and shoals that hold bait fish and attract the larger game fish, such as striped bass, bluefish and bluefin tuna.
The outer edge of the Cape, from Provincetown to Falmouth, faces the open Atlantic from Provincetown to Chatham, and then the more protected water of Nantucket and Vineyard Sounds, from Chatham to Falmouth. The bays, harbors and shoals along this coastline also provide a robust habitat for game species, and during the late summer months warm-water species such as mahi-mahi and marlin will also appear on the southern edge of Cape Cod's waters. Nearly every harbor on Cape Cod hosts sport fishing charter boats, which run from May through October.
One of the most popular fishing spots on the East Coast is the Cape Cod Canal. Striped bass, especially, in season attract anglers from far and wide. A large part of the attraction involves ease of access. Ample free parking exists all along the waterway, and the banks are a short walk from one's vehicle. This reduces fishing to the basics – a pole and a few lures.
Sports.
The Cape has nine amateur baseball franchises playing within Barnstable County in the Cape Cod Baseball League. The Wareham Gatemen also play in the Cape Cod Baseball League in nearby Wareham in Plymouth County. The league originated 1923, although intertown competition traces to 1866. The current teams in the league are the Bourne Braves, Brewster Whitecaps, Chatham Anglers (formerly the Chatham Athletics), Cotuit Kettleers, Falmouth Commodores, Harwich Mariners, Hyannis Harbor Hawks (formerly the Hyannis Mets), Orleans Firebirds (formerly the Orleans Cardinals), Wareham Gatemen and the Yarmouth–Dennis Red Sox. MLB scouts frequent the games in the summer, looking for stars of the future.
Along with the Cape Cod Baseball League and the new Junior Hockey League team, the Cape Cod Cubs, many high school players are being seriously recruited as well. Barnstable and Harwich have each sent multiple players to Division 1 colleges for baseball. Harwich has also won three state titles since 1996 (1996, 2006, 2007). Bourne and Sandwich, rivals in hockey, have each won state championships recently, Bourne in 2004 and Sandwich in 2007. Nauset, Barnstable, and Martha's Vineyard are also state hockey powerhouses. Barnstable and Falmouth hold the title of having one of the longest Thanksgiving football rivalries in the country. The teams have played each other every year on Thanksgiving since 1895. High school football teams on the Cape have also recently become successful and the region has also become a hot-spot for college recruiting. In 2011, four high school football teams from the Cape won state championships in their respective divisions; Dennis-Yarmouth (Division 2A), Bourne (Division 3A), Mashpee (Division 4), and Nantucket (Division 5). Also, numerous other Cape schools have made appearances in the football state championship game recently, including Barnstable in 2012, Martha's Vineyard in 2008, Cape Cod Tech in 2006, and Dennis-Yarmouth in 2013. The Bourne and Barnstable girl's volleyball teams are two of the best teams in the state and Barnstable is considered one of the best programs in the country. Bourne won the state title in 2003 and 2007, and Barnstable has won 12 Division 1 state titles in the past 13 years and has won the state title the three years in a row (2011–2013). In the 2010 cross country season, Sturgis Charter Public School's Division 4 cross country team remained virtually unbeaten throughout their running season.
The Cape is home to the Cape Cod Frenzy, a team in the American Basketball Association.
Soccer on Cape Cod is represented by the Cape Cod Crusaders, playing in the USL Premier Development League (PDL) based in Hyannis. In addition, a summer Cape Cod Adult Soccer League (CCASL) is active in several towns on the Cape.
Cape Cod is the home of the Cape Cod Cubs, a new junior league hockey team that is based out of Hyannis at the new community center being built on Bearses Way.
The end of each summer is marked with the running of the Falmouth Road Race, held on the third Saturday in August. It draws about 10,000 runners to the Cape and showcases the finest runners in the world (mainly for the large purse that the race is able to offer). The race is 7.2 mi long, which is a non-standard distance. The reason for the unusual distance is that the man who thought the race up (Tommy Leonard) was a bartender who wanted a race along the coast from one bar (The Cap'n Kidd in Woods Hole) to another (The Brothers Four in Falmouth Heights). While the bar in Falmouth Heights is now the British Beer Company, the race still starts at the front door of the Cap'n Kidd in Woods Hole and now finishes at the beach in Falmouth Heights. Prior to the Falmouth race is an annual 5 mi race through Brewster called the Brew Run, held early in August.
Education.
Each town usually consists of a few elementary schools, one or two middle schools and one large public high school that serves the entire town. Exceptions to this include Dennis-Yarmouth Regional High School located in Yarmouth, which serves the two towns in its name, Monomoy Regional High School, located in Harwich and serving that town as well as Chatham, and Nauset Regional High School in Eastham, which serves the towns of Brewster, Orleans, Eastham, Wellfleet, Truro, and Provincetown. Bourne High School is the public school for students residing in the town of Bourne, which is gathered from villages in Bourne, including Sagamore, Sagamore Beach, and Buzzards Bay. Barnstable High School is the largest high school and is known for its girls' volleyball team which have been state champions a total of 12 times. Barnstable High School also boasts one of the country's best high school drama clubs which were awarded a contract by Warner Bros. to create a documentary in webisode format based on their production of "The Wizard of Oz". Sturgis Charter Public School is a public school in Hyannis which was featured in "Newsweek" magazine's "Best High Schools" ranking. It ranked 28th in the country and 1st in the state of Massachusetts in the 2009 edition and ranked 43rd and 55th in the 2008 and 2007 edition, respectively. Sturgis offers the International Baccalaureate Diploma Programme in their junior and senior year and is open to students from as far as Plymouth. The Cape also contains two vocational high schools. One is the Cape Cod Regional Technical High School in Harwich, and the other is Upper Cape Cod Regional Technical High School in Bourne.
In 1976 the Cape schools and districts petitioned the Massachusetts Legislature to create an educational collaborative, the , to facilitate cooperation and efficiency in providing gifted and talented, and special needs programs. With locations in Osterville and Bourne the Cape Cod Collaborative provides transportation services, professional development, autism support, developmental training, itinerant services and an alternative education program. Each summer, in cooperation with the Massachusetts Maritime Academy, it operates a gifted and talented science based program for students from around the Cape.
Mashpee High School is home to the Mashpee Chapter of SMPTE, the Society of Motion Picture and Television Engineers. This chapter is the first and only high school chapter in the world to be a part of this organization and has received much recognition within the Los Angeles broadcasting industry as a result.
In addition to public schools, Cape Cod has a wide range of private schools. The town of Barnstable has Trinity Christian Academy, Cape Cod Academy, St. Francis Xavier Preparatory School, and Pope John Paul II High School. Bourne offers the Waldorf School of Cape Cod, Orleans offers the Lighthouse Charter School for elementary and middle school students, and Falmouth offers Falmouth Academy. Riverview School is located in East Sandwich and is a special co-ed boarding school which serves students as old as 22 who have learning disabilities. Another specialized school is the Penikese Island School, located in the Elizabeth Islands off southwestern Cape Cod, which serves struggling and troubled teenage boys.
Cape Cod contains three institutions of higher education. One is the Cape Cod Community College located in West Barnstable. The second is Massachusetts Maritime Academy in the village of Buzzards Bay. Massachusetts Maritime Academy is the oldest continuously operating maritime college in the United States. The third is Bridgewater State University, which recently opened a satellite campus in South Yarmouth in January 2015. The school will provide 40 undergraduate and graduate courses leading to the completion of Bachelor's degree and Master's degree programs in Early Childhood Education, Educational Leadership, Secondary Education, Reading, and Special Education. The campus will also offer Certificate Programs in Business and Social Work. Beginning in the Summer 2015, the campus will begin to offer undergraduate credit courses in History. 
Cultural influences.
The Cape Codder cocktail is named after the peninsula; both are notable for cranberry. Cape Cod also generated a distinctive Cape style house and Cape lighthouse.
The virtues of Cape Cod are extolled in the song Old Cape Cod.
The indie rock band Vampire Weekend references Cape Cod in a number their songs, namely in "Cape Cod Kwassa Kwassa" and "Walcott"
According to Cole Porter, in his song "Let's Do It," "cold Cape Cod clams 'gainst their wish do it."
References.
Sources.
, by the United States Geological Survey (USGS)

</doc>
<doc id="38822" url="http://en.wikipedia.org/wiki?curid=38822" title="Power transmission">
Power transmission

Power transmission is the movement of energy from its place of generation to a location where it is applied to performing useful work.
Power is defined formally as units of energy per unit time. In SI units:
Since the development of technology, transmission and storage systems have been of immense interest to technologists and technology users.
Electrical Power
With the widespread establishment of electrical grids, power transmission is usually associated most with electric power transmission. Alternating current is normally preferred as its voltage may be easily stepped up by a transformer in order to minimize resistive loss in the conductors used to transmit power over great distances; another set of transformers is required to step it back down to safer or more usable voltage levels at destination.
Power transmission is usually performed with overhead lines as this is the most economical way to do so. Underground transmission by high-voltage cables is chosen in crowded urban areas and in high-voltage direct-current (HVDC) submarine connections.
Wireless Transmission.
Power might also be transmitted by changing electromagnetic fields or by radio waves; microwave energy may be carried efficiently over short distances by a waveguide.
Mechanical Power.
Electrical power transmission has replaced mechanical power transmission in all but the very shortest distances. From the 16th century through the industrial revolution to the end of the 19th century mechanical power transmission was the norm. The oldest long-distance power transmission technology involved systems of push-rods ("stängenkunst" or "feldstängen") connecting waterwheels to distant mine-drainage and brine-well pumps. A surviving example from 1780 exists at Bad Kösen that transmits power approximately 200 meters from a waterwheel to a salt well, and from there, an additional 150 meters to a brine evaporator. This technology survived into the 21st century in a handful of oilfields in the US, transmitting power from a central pumping engine to the numerous pump-jacks in the oil field.
Factories were fitted with overhead line shafts providing rotary power. Short line-shaft systems were described by Agricola, connecting a waterwheel to numerous ore-processing machines. While the machines described by Agricola used geared connections from the shafts to the machinery, by the 19th century, drivebelts would become the norm for linking individual machines to the line shafts. One mid 19th century factory had 1,948 feet of line shafting with 541 pulleys.
Mechanical power may be transmitted directly using a solid structure such as a driveshaft; transmission gears can adjust the amount of torque or force vs. speed in much the same way an electrical transformer adjusts voltage vs current.
Hydraulic systems use liquid under pressure to transmit power; canals and hydroelectric power generation facilities harness natural water power to lift ships or generate electricity. Pumping water or pushing mass uphill with (windmill pumps) is one possible means of energy storage. London had a hydraulic network powered by five pumping stations operated by the London Hydraulic Power Company, with a total effect of 5 MW.
Pneumatic systems use gasses under pressure to transmit power; compressed air is commonly used to operate pneumatic tools in factories and repair garages. A pneumatic wrench (for instance) is used to remove and install automotive tires far more quickly than could be done with standard manual hand tools.
A pneumatic system was proposed by proponents of Edison's direct current as the basis of the power grid. Compressed air generated at Niagara Falls would drive far away generators of DC power. The War of Currents ended with alternating current (AC) as the only means of long distance power transmission.
Chemicals and fuels.
Power (and energy) may be transmitted by physically transporting chemical or nuclear fuels. Possible artificial fuels include radioactive isotopes, wood alcohol, grain alcohol, methane, synthetic gas, hydrogen gas (H2), cryogenic gas, and liquefied natural gas (LNG).

</doc>
<doc id="38824" url="http://en.wikipedia.org/wiki?curid=38824" title="Electric power transmission">
Electric power transmission

Electric-power transmission is the bulk transfer of electrical energy, from generating power plants to electrical substations located near demand centers. This is distinct from the local wiring between high-voltage substations and customers, which is typically referred to as electric power distribution. Transmission lines, when interconnected with each other, become transmission networks. The combined transmission and distribution network is known as the "power grid" in North America, or just "the grid". In the United Kingdom, the network is known as the "National Grid".
A wide area synchronous grid, also known as an "interconnection" in North America, directly connects a large number of generators delivering AC power with the same relative "frequency", to a large number of consumers. For example, there are four major interconnections in North America (the Western Interconnection, the Eastern Interconnection, the Quebec Interconnection and the Electric Reliability Council of Texas (ERCOT) grid), and one large grid for most of continental Europe.
The same relative "frequency", but almost never the same relative "phase" as ac power interchange is a function of the phase difference between any two nodes in the network, and zero degrees difference means no power is interchanged; any phase difference up to 90 degrees is stable by the "equal area criteria"; any phase difference above 90 degrees is absolutely unstable; the interchange partners are responsible for maintaining frequency as close to 60.0000 Hz as is practical, and the phase differences between any two nodes significantly less than 90 degrees; should 90 degrees be exceeded, a system separation is executed, and remains separated until the trouble has been corrected.
Historically, transmission and distribution lines were owned by the same company, but starting in the 1990s, many countries have liberalized the regulation of the electricity market in ways that have led to the separation of the electricity transmission business from the distribution business.
System.
Most transmission lines are high-voltage three-phase alternating current (AC), although single phase AC is sometimes used in railway electrification systems. High-voltage direct-current (HVDC) technology is used for greater efficiency at very long distances (typically hundreds of miles (kilometers)), or in submarine power cables (typically longer than 30 miles (50 km)). HVDC links are also used to stabilize and control problems in large power distribution networks where sudden new loads or blackouts in one part of a network can otherwise result in synchronization problems and cascading failures.
Electricity is transmitted at high voltages (120 kV or above) to reduce the energy losses in long-distance transmission. Power is usually transmitted through overhead power lines. Underground power transmission has a significantly higher cost and greater operational limitations but is sometimes used in urban areas or sensitive locations.
A key limitation of electric power is that, with minor exceptions, electrical energy cannot be stored, and therefore must be generated as needed. A sophisticated control system is required to ensure electric generation very closely matches the demand. If the demand for power exceeds the supply, generation plant and transmission equipment can shut down, which in the worst case may lead to a major regional blackout, such as occurred in the US Northeast blackout of 1965, 1977, 2003, and other regional blackouts in 1996 and 2011. It is to reduce the risk of such a failure that electric transmission networks are interconnected into regional, national or continent wide networks thereby providing multiple redundant alternative routes for power to flow should such equipment failures occur. Much analysis is done by transmission companies to determine the maximum reliable capacity of each line (ordinarily less than its physical or thermal limit) to ensure spare capacity is available should there be any such failure in another part of the network.
Overhead transmission.
High-voltage overhead conductors are not covered by insulation. The conductor material is nearly always an aluminum alloy, made into several strands and possibly reinforced with steel strands. Copper was sometimes used for overhead transmission, but aluminum is lighter, yields only marginally reduced performance and costs much less. Overhead conductors are a commodity supplied by several companies worldwide. Improved conductor material and shapes are regularly used to allow increased capacity and modernize transmission circuits. Conductor sizes range from 12 mm2 (#6 American wire gauge) to 750 mm2 (1,590,000 circular mils area), with varying resistance and current-carrying capacity. Thicker wires would lead to a relatively small increase in capacity due to the skin effect, that causes most of the current to flow close to the surface of the wire. Because of this current limitation, multiple parallel cables (called bundle conductors) are used when higher capacity is needed. Bundle conductors are also used at high voltages to reduce energy loss caused by corona discharge.
Today, transmission-level voltages are usually considered to be 110 kV and above. Lower voltages, such as 66 kV and 33 kV, are usually considered subtransmission voltages, but are occasionally used on long lines with light loads. Voltages less than 33 kV are usually used for distribution. Voltages above 765 kV are considered extra high voltage and require different designs compared to equipment used at lower voltages.
Since overhead transmission wires depend on air for insulation, the design of these lines requires minimum clearances to be observed to maintain safety. Adverse weather conditions, such as high wind and low temperatures, can lead to power outages. Wind speeds as low as 23 kn can permit conductors to encroach operating clearances, resulting in a flashover and loss of supply.
Oscillatory motion of the physical line can be termed gallop or flutter depending on the frequency and amplitude of oscillation.
Underground transmission.
Electric power can also be transmitted by underground power cables instead of overhead power lines. Underground cables take up less right-of-way than overhead lines, have lower visibility, and are less affected by bad weather. However, costs of insulated cable and excavation are much higher than overhead construction. Faults in buried transmission lines take longer to locate and repair. Underground lines are strictly limited by their thermal capacity, which permits less overload or re-rating than overhead lines. Long underground AC cables have significant capacitance, which may reduce their ability to provide useful power to loads beyond 50 miles. Long underground DC cables have no such issue and can run for thousands of miles.
History.
In the early days of commercial electric power, transmission of electric power at the same voltage as used by lighting and mechanical loads restricted the distance between generating plant and consumers. In 1882, generation was with direct current (DC), which could not easily be increased in voltage for long-distance transmission. Different classes of loads (for example, lighting, fixed motors, and traction/railway systems) required different voltages, and so used different generators and circuits.
Due to this specialization of lines and because transmission was inefficient for low-voltage high-current circuits, generators needed to be near their loads. It seemed, at the time, that the industry would develop into what is now known as a distributed generation system with large numbers of small generators located near their loads.
In 1886, in Great Barrington, Massachusetts, a 1 kV alternating current (AC) distribution system was installed. That same year, AC power at 2 kV, transmitted 30 km, was installed at Cerchi, Italy. At an AIEE meeting on May 16, 1888, Nikola Tesla delivered a lecture entitled "", describing the equipment which allowed efficient generation and use of polyphase alternating currents. The transformer, and Tesla's polyphase and single-phase induction motors, were essential for a combined AC distribution system for both lighting and machinery. Ownership of the rights to the Tesla patents was a key advantage to the Westinghouse Company in offering a complete alternating current power system for both lighting and power.
Regarded as one of the most influential electrical innovations, the "universal system" used transformers to step-up voltage from generators to high-voltage transmission lines, and then to step-down voltage to local distribution circuits or industrial customers. By a suitable choice of utility frequency, both lighting and motor loads could be served. Rotary converters and later mercury-arc valves and other rectifier equipment allowed DC to be provided where needed. Generating stations and loads using different frequencies could be interconnected using rotary converters. By using common generating plants for every type of load, important economies of scale were achieved, lower overall capital investment was required, load factor on each plant was increased allowing for higher efficiency, a lower cost for the consumer and increased overall use of electric power.
By allowing multiple generating plants to be interconnected over a wide area, electricity production cost was reduced. The most efficient available plants could be used to supply the varying loads during the day. Reliability was improved and capital investment cost was reduced, since stand-by generating capacity could be shared over many more customers and a wider geographic area. Remote and low-cost sources of energy, such as hydroelectric power or mine-mouth coal, could be exploited to lower energy production cost.
The first transmission of three-phase alternating current using high voltage took place in 1891 during the international electricity exhibition in Frankfurt. A 25 kV transmission line, approximately 175 km long, connected Lauffen on the Neckar and Frankfurt.
Voltages used for electric power transmission increased throughout the 20th century. By 1914, fifty-five transmission systems each operating at more than 70 kV were in service. The highest voltage then used was 150 kV.
The rapid industrialization in the 20th century made electrical transmission lines and grids a critical infrastructure item in most industrialized nations. The interconnection of local generation plants and small distribution networks was greatly spurred by the requirements of World War I, with large electrical generating plants built by governments to provide power to munitions factories. Later these generating plants were connected to supply civil loads through long-distance transmission.
Bulk power transmission.
Engineers design transmission networks to transport the energy as efficiently as feasible, while at the same time taking into account economic factors, network safety and redundancy. These networks use components such as power lines, cables, circuit breakers, switches and transformers. The transmission network is usually administered on a regional basis by an entity such as a regional transmission organization or transmission system operator.
Transmission efficiency is greatly improved by devices that increase the voltage, (and thereby proportionately reduce the current) in the line conductors, thus allowing power to be transmitted with acceptable losses. The reduced current flowing through the line reduces the heating losses in the conductors. According to Joule's Law, energy losses are directly proportional to the square of the current. Thus, reducing the current by a factor of two will lower the energy lost to conductor resistance by a factor of four for any given size of conductor.
The optimum size of a conductor for a given voltage and current can be estimated by Kelvin's law for conductor size, which states that the size is at its optimum when the annual cost of energy wasted in the resistance is equal to the annual capital charges of providing the conductor. At times of lower interest rates, Kelvin's law indicates that thicker wires are optimal; while, when metals are expensive, thinner conductors are indicated: however, power lines are designed for long-term use, so Kelvin's law has to be used in conjunction with long-term estimates of the price of copper and aluminum as well as interest rates for capital.
The increase in voltage is achieved in AC circuits by using a "step-up transformer". HVDC systems require relatively costly conversion equipment which may be economically justified for particular projects such as submarine cables and longer distance high capacity point to point transmission. HVDC is necessary for the import and export of energy between grid systems that are not synchronized with each other.
A transmission grid is a network of power stations, transmission lines, and substations. Energy is usually transmitted within a grid with three-phase AC. Single-phase AC is used only for distribution to end users since it is not usable for large polyphase induction motors. In the 19th century, two-phase transmission was used but required either four wires or three wires with unequal currents. Higher order phase systems require more than three wires, but deliver little or no benefit.
The price of electric power station capacity is high, and electric demand is variable, so it is often cheaper to import some portion of the needed power than to generate it locally. Because loads are often regionally correlated (hot weather in the Southwest portion of the US might cause many people to use air conditioners), electric power often comes from distant sources. Because of the economic benefits of load sharing between regions, wide area transmission grids now span countries and even continents. The web of interconnections between power producers and consumers should enable power to flow, even if some links are inoperative.
The unvarying (or slowly varying over many hours) portion of the electric demand is known as the "base load" and is generally served by large facilities (which are more efficient due to economies of scale) with fixed costs for fuel and operation. Such facilities are nuclear, coal-fired or hydroelectric, while other energy sources such as concentrated solar thermal and geothermal power have the potential to provide base load power. Renewable energy sources, such as solar photovoltaics, wind, wave, and tidal, are, due to their intermittency, not considered as supplying "base load" but will still add power to the grid. The remaining or 'peak' power demand, is supplied by peaking power plants, which are typically smaller, faster-responding, and higher cost sources, such as combined cycle or combustion turbine plants fueled by natural gas.
Long-distance transmission of electricity (thousands of kilometers) is cheap and efficient, with costs of US$0.005–0.02/kWh (compared to annual averaged large producer costs of US$0.01–0.025/kWh, retail rates upwards of US$0.10/kWh, and multiples of retail for instantaneous suppliers at unpredicted highest demand moments). Thus distant suppliers can be cheaper than local sources (e.g., New York often buys over 1000 MW of electricity from Canada). Multiple local sources (even if more expensive and infrequently used) can make the transmission grid more fault tolerant to weather and other disasters that can disconnect distant suppliers.
Long-distance transmission allows remote renewable energy resources to be used to displace fossil fuel consumption. Hydro and wind sources cannot be moved closer to populous cities, and solar costs are lowest in remote areas where local power needs are minimal. Connection costs alone can determine whether any particular renewable alternative is economically sensible. Costs can be prohibitive for transmission lines, but various proposals for massive infrastructure investment in high capacity, very long distance super grid transmission networks could be recovered with modest usage fees.
Grid input.
At the power stations, the power is produced at a relatively low voltage between about 2.3 kV and 30 kV, depending on the size of the unit. The generator terminal voltage is then stepped up by the power station transformer to a higher voltage (115 kV to 765 kV AC, varying by the transmission system and by country) for transmission over long distances.
Losses.
Transmitting electricity at high voltage reduces the fraction of energy lost to resistance, which varies depending on the specific conductors, the current flowing, and the length of the transmission line. For example, a 100 mile 765 kV line carrying 1000 MW of power can have losses of 1.1% to 0.5%. A 345 kV line carrying the same load across the same distance has losses of 4.2%. For a given amount of power, a higher voltage reduces the current and thus the resistive losses in the conductor. For example, raising the voltage by a factor of 10 reduces the current by a corresponding factor of 10 and therefore the "I"2"R" losses by a factor of 100, provided the same sized conductors are used in both cases. Even if the conductor size (cross-sectional area) is reduced 10-fold to match the lower current, the "I"2"R" losses are still reduced 10-fold. Long-distance transmission is typically done with overhead lines at voltages of 115 to 1,200 kV. At extremely high voltages, more than 2,000 kV exists between conductor and ground, corona discharge losses are so large that they can offset the lower resistive losses in the line conductors. Measures to reduce corona losses include conductors having larger diameters; often hollow to save weight, or bundles of two or more conductors.
Transmission and distribution losses in the USA were estimated at 6.6% in 1997 and 6.5% in 2007. By using underground DC transmission, these losses can be cut in half. Underground cables can be larger diameter because they do not have the constraint of light weight that overhead conductors have. In general, losses are estimated from the discrepancy between power produced (as reported by power plants) and power sold to the end customers; the difference between what is produced and what is consumed constitute transmission and distribution losses, assuming no theft of utility occurs.
As of 1980, the longest cost-effective distance for direct-current transmission was determined to be 7000 km. For alternating current it was 4000 km, though all transmission lines in use today are substantially shorter than this.
In any alternating current transmission line, the inductance and capacitance of the conductors can be significant. Currents that flow solely in 'reaction' to these properties of the circuit, (which together with the resistance define the impedance) constitute reactive power flow, which transmits no 'real' power to the load. These reactive currents, however, are very real and cause extra heating losses in the transmission circuit. The ratio of 'real' power (transmitted to the load) to 'apparent' power (sum of 'real' and 'reactive') is the power factor. As reactive current increases, the reactive power increases and the power factor decreases. For transmission systems with low power factor, losses are higher than for systems with high power factor. Utilities add capacitor banks, reactors and other components (such as phase-shifting transformers; static VAR compensators; physical transposition of the phase conductors; and flexible AC transmission systems, FACTS) throughout the system to compensate for the reactive power flow and reduce the losses in power transmission and stabilize system voltages. These measures are collectively called 'reactive support'.
Subtransmission.
Subtransmission is part of an electric power transmission system that runs at relatively lower voltages. It is uneconomical to connect all distribution substations to the high main transmission voltage, because the equipment is larger and more expensive. Typically, only larger substations connect with this high voltage. It is stepped down and sent to smaller substations in towns and neighborhoods. Subtransmission circuits are usually arranged in loops so that a single line failure does not cut off service to a large number of customers for more than a short time. Loops can be "normally closed", where loss of one circuit should result in no interruption, or "normally open" where substations can switch to a backup supply. While subtransmission circuits are usually carried on overhead lines, in urban areas buried cable may be used. The lower-voltage subtransmission lines use less right-of-way and simpler structures; it is much more feasible to put them underground where needed. Higher-voltage lines require more space and are usually above-ground since putting them underground is very expensive.
There is no fixed cutoff between subtransmission and transmission, or subtransmission and distribution. The voltage ranges overlap somewhat. Voltages of 69 kV, 115 kV and 138 kV are often used for subtransmission in North America. As power systems evolved, voltages formerly used for transmission were used for subtransmission, and subtransmission voltages became distribution voltages. Like transmission, subtransmission moves relatively large amounts of power, and like distribution, subtransmission covers an area instead of just point to point.
Transmission grid exit.
At the substations, transformers reduce the voltage to a lower level for distribution to commercial and residential users. This distribution is accomplished with a combination of sub-transmission (33 kV to 132 kV) and distribution (3.3 to 25 kV). Finally, at the point of use, the energy is transformed to low voltage (varying by country and customer requirements—see Mains electricity by country).
Modeling: The Transmission Matrix.
Oftentimes, we are only interested in the terminal characteristics of the transmission line, which are the voltage and current at the sending and receiving ends. The transmission line itself is then modeled as a "black box" and a 2 by 2 transmission matrix is used to to model its behavior, as follows:
The line is assumed to be a reciprocal, symmetrical network, meaning that the receiving and sending labels can be switched with no consequence. The transmission matrix T also has the following properties:
The parameters A, B, C, and D differ depending on how the desired model handles the line's resistance (R), inductance (L), capacitance (C), and shunt (parallel) admittance Y. The four main models are the short line approximation, the medium line approximation, the long line approximation (with distributed parameters), and the lossless line. In all models described, a capital letter such as R refers to the total quantity summed over the line and a lowercase letter such as c refers to the per-unit-length quantity. 
The lossless line approximation, which is the least accurate model, is often used on short lines when the inductance of the line is much greater than its resistance. For this approximation, the voltage and current are identical at the sending and receiving ends. 
The short line approximation is normally used for lines less than 50 miles long. For a short line, only a series impedance Z is considered, while C and Y are ignored. The final result is that A = D = 1 per unit, B = Z ohms, and C = 0. 
The medium line approximation is used for lines between 50 and 150 miles long. In this model, the series impedance and the shunt admittance are considered, with half of the shunt admittance being placed at each end of the line. This circuit is often referred to as a "nominal pi" circuit because of the shape that is taken on when admittance is placed on both sides of the circuit diagram. The analysis of the medium line brings one to the following result:
The long line model is used when a higher degree of accuracy is needed or when the line under consideration is more than 150 miles long. Series resistance and shunt admittance are considered as distributed parameters, meaning each differential length of the line has a corresponding differential resistance and shunt admittance. The following result can be applied at any point along the transmission line, where gamma is defined as the propagation constant.
To find the voltage and current at the end of the long line, x should be replaced with L (the line length) in all parameters of the transmission matrix.
High-voltage direct current.
High-voltage direct current (HVDC) is used to transmit large amounts of power over long distances or for interconnections between asynchronous grids. When electrical energy is to be transmitted over very long distances, the power lost in AC transmission becomes appreciable and it is less expensive to use direct current instead of alternating current. For a very long transmission line, these lower losses (and reduced construction cost of a DC line) can offset the additional cost of the required converter stations at each end.
HVDC is also used for submarine cables because over about 30 km lengths AC cannot be supplied . In these cases special high-voltage cables for DC are used. Submarine HVDC systems are often used to connect the electricity grids of islands, for example, between Great Britain and mainland Europe, between Great Britain and Ireland, between Tasmania and the Australian mainland, and between the North and South Islands of New Zealand. Submarine connections up to 600 km in length are presently in use.
HVDC links can be used to control problems in the grid with AC electricity flow. The power transmitted by an AC line increases as the phase angle between source end voltage and destination ends increases, but too large a phase angle will allow the systems at either end of the line to fall out of step. Since the power flow in a DC link is controlled independently of the phases of the AC networks at either end of the link, this phase angle limit does not exist, and a DC link is always able to transfer its full rated power. A DC link therefore stabilizes the AC grid at either end, since power flow and phase angle can then be controlled independently.
As an example, to adjust the flow of AC power on a hypothetical line between Seattle and Boston would require adjustment of the relative phase of the two regional electrical grids. This is an everyday occurrence in AC systems, but one that can become disrupted when AC system components fail and place unexpected loads on the remaining working grid system. With an HVDC line instead, such an interconnection would: (1) Convert AC in Seattle into HVDC; (2) Use HVDC for the 3,000 miles of cross-country transmission; and (3) Convert the HVDC to locally synchronized AC in Boston, (and possibly in other cooperating cities along the transmission route). Such a system could be less prone to failure if parts of it were suddenly shut down. One example of a long DC transmission line is the Pacific DC Intertie located in the Western United States.
Capacity.
The amount of power that can be sent over a transmission line is limited. The origins of the limits vary depending on the length of the line. For a short line, the heating of conductors due to line losses sets a thermal limit. If too much current is drawn, conductors may sag too close to the ground, or conductors and equipment may be damaged by overheating. For intermediate-length lines on the order of 100 km, the limit is set by the voltage drop in the line. For longer AC lines, system stability sets the limit to the power that can be transferred. Approximately, the power flowing over an AC line is proportional to the cosine of the phase angle of the voltage and current at the receiving and transmitting ends. Since this angle varies depending on system loading and generation, it is undesirable for the angle to approach 90 degrees. Very approximately, the allowable product of line length and maximum load is proportional to the square of the system voltage. Series capacitors or phase-shifting transformers are used on long lines to improve stability. High-voltage direct current lines are restricted only by thermal and voltage drop limits, since the phase angle is not material to their operation.
Up to now, it has been almost impossible to foresee the temperature distribution along the cable route, so that the maximum applicable current load was usually set as a compromise between understanding of operation conditions and risk minimization. The availability of industrial distributed temperature sensing (DTS) systems that measure in real time temperatures all along the cable is a first step in monitoring the transmission system capacity. This monitoring solution is based on using passive optical fibers as temperature sensors, either integrated directly inside a high voltage cable or mounted externally on the cable insulation. A solution for overhead lines is also available. In this case the optical fiber is integrated into the core of a phase wire of overhead transmission lines (OPPC). The integrated Dynamic Cable Rating (DCR) or also called Real Time Thermal Rating (RTTR) solution enables not only to continuously monitor the temperature of a high voltage cable circuit in real time, but to safely utilize the existing network capacity to its maximum. Furthermore it provides the ability to the operator to predict the behavior of the transmission system upon major changes made to its initial operating conditions.
Control.
To ensure safe and predictable operation the components of the transmission system are controlled with generators, switches, circuit breakers and loads. The voltage, power, frequency, load factor, and reliability capabilities of the transmission system are designed to provide cost effective performance for the customers.
Load balancing.
The transmission system provides for base load and peak load capability, with safety and fault tolerance margins. The peak load times vary by region largely due to the industry mix. In very hot and very cold climates home air conditioning and heating loads have an effect on the overall load. They are typically highest in the late afternoon in the hottest part of the year and in mid-mornings and mid-evenings in the coldest part of the year. This makes the power requirements vary by the season and the time of day. Distribution system designs always take the base load and the peak load into consideration.
The transmission system usually does not have a large buffering capability to match the loads with the generation. Thus generation has to be kept matched to the load, to prevent overloading failures of the generation equipment.
Multiple sources and loads can be connected to the transmission system and they must be controlled to provide orderly transfer of power. In centralized power generation, only local control of generation is necessary, and it involves synchronization of the generation units, to prevent large transients and overload conditions.
In distributed power generation the generators are geographically distributed and the process to bring them online and offline must be carefully controlled. The load control signals can either be sent on separate lines or on the power lines themselves. Voltage and frequency can be used as signalling mechanisms to balance the loads.
In voltage signaling, the variation of voltage is used to increase generation. The power added by any system increases as the line voltage decreases. This arrangement is stable in principle. Voltage-based regulation is complex to use in mesh networks, since the individual components and setpoints would need to be reconfigured every time a new generator is added to the mesh.
In frequency signaling, the generating units match the frequency of the power transmission system. In droop speed control, if the frequency decreases, the power is increased. (The drop in line frequency is an indication that the increased load is causing the generators to slow down.)
Wind turbines, vehicle-to-grid and other distributed storage and generation systems can be connected to the power grid, and interact with it to improve system operation.
Failure protection.
Under excess load conditions, the system can be designed to fail gracefully rather than all at once. Brownouts occur when the supply power drops below the demand. Blackouts occur when the supply fails completely.
Rolling blackouts (also called load shedding) are intentionally engineered electrical power outages, used to distribute insufficient power when the demand for electricity exceeds the supply.
Communications.
Operators of long transmission lines require reliable communications for control of the power grid and, often, associated generation and distribution facilities. Fault-sensing protective relays at each end of the line must communicate to monitor the flow of power into and out of the protected line section so that faulted conductors or equipment can be quickly de-energized and the balance of the system restored. Protection of the transmission line from short circuits and other faults is usually so critical that common carrier telecommunications are insufficiently reliable, and in remote areas a common carrier may not be available. Communication systems associated with a transmission project may use:
Rarely, and for short distances, a utility will use pilot-wires strung along the transmission line path. Leased circuits from common carriers are not preferred since availability is not under control of the electric power transmission organization.
Transmission lines can also be used to carry data: this is called power-line carrier, or PLC. PLC signals can be easily received with a radio for the long wave range.
Optical fibers can be included in the stranded conductors of a transmission line, in the overhead shield wires. These cables are known as optical ground wire ("OPGW"). Sometimes a standalone cable is used, all-dielectric self-supporting ("ADSS") cable, attached to the transmission line cross arms.
Some jurisdictions, such as Minnesota, prohibit energy transmission companies from selling surplus communication bandwidth or acting as a telecommunications common carrier. Where the regulatory structure permits, the utility can sell capacity in extra dark fibers to a common carrier, providing another revenue stream.
Electricity market reform.
Some regulators regard electric transmission to be a natural monopoly and there are moves in many countries to separately regulate transmission (see electricity market).
Spain was the first country to establish a regional transmission organization. In that country, transmission operations and market operations are controlled by separate companies. The transmission system operator is Red Eléctrica de España (REE) and the wholesale electricity market operator is Operador del Mercado Ibérico de Energía – Polo Español, S.A. (OMEL) . Spain's transmission system is interconnected with those of France, Portugal, and Morocco.
In the United States and parts of Canada, electrical transmission companies operate independently of generation and distribution companies.
Cost of electric power transmission.
The cost of high voltage electricity transmission (as opposed to the costs of electric power distribution) is comparatively low, compared to all other costs arising in a consumer's electricity bill. In the UK, transmission costs are about 0.2p/kWh compared to a delivered domestic price of around 10p/kWh.
Research evaluates the level of capital expenditure in the electric power T&D equipment market will be worth $128.9bn in 2011.
Merchant transmission.
Merchant transmission is an arrangement where a third party constructs and operates electric transmission lines through the franchise area of an unrelated utility.
Operating merchant transmission projects in the United States include the Cross Sound Cable from Shoreham, New York to New Haven, Connecticut, Neptune RTS Transmission Line from Sayreville, N.J., to Newbridge, N.Y, and Path 15 in California. Additional projects are in development or have been proposed throughout the United States, including the Lake Erie Connector, an underwater transmission line proposed by ITC Holdings Corp., connecting Ontario to load serving entities in the PJM Interconnection region. 
There is only one unregulated or market interconnector in Australia: Basslink between Tasmania and Victoria. Two DC links originally implemented as market interconnectors, Directlink and Murraylink, have been converted to regulated interconnectors. 
A major barrier to wider adoption of merchant transmission is the difficulty in identifying who benefits from the facility so that the beneficiaries will pay the toll. Also, it is difficult for a merchant transmission line to compete when the alternative transmission lines are subsidized by other utility businesses.
Health concerns.
Some large studies, including a large United States study, have failed to find any link between living near power lines and developing any sickness or diseases, such as cancer. A 1997 study found that it did not matter how close one was to a power line or a sub-station, there was no increased risk of cancer or illness.
The mainstream scientific evidence suggests that low-power, low-frequency, electromagnetic radiation associated with household currents and high transmission power lines does not constitute a short or long term health hazard. Some studies, however, have found statistical correlations between various diseases and living or working near power lines. No adverse health effects have been substantiated for people not living close to powerlines.
There are established biological effects for acute "high" level exposure to magnetic fields well above 100 µT (1 G). In a residential setting, there is "limited evidence of carcinogenicity in humans and less than sufficient evidence for carcinogenicity in experimental animals", in particular, childhood leukemia, "associated with" average exposure to residential power-frequency magnetic field above 0.3 µT (3 mG) to 0.4 µT (4 mG). These levels exceed average residential power-frequency magnetic fields in homes, which are about 0.07 µT (0.7 mG) in Europe and 0.11 µT (1.1 mG) in North America.
The Earths natural geomagnetic field strength varies over the surface of the planet between 0.035 mT - 0.07 mT (35 µT - 70 µT or 0.35 G - 0.7 G) while the International Standard for the continuous exposure limit is set at 40 mT (40,000 µT or 400 G) for the general public.
Tree Growth Regulator and Herbicide Control Methods may be used in transmission line right of ways which may have health effects.
United States government policy.
Historically, local governments have exercised authority over the grid and have significant disincentives to encourage actions that would benefit states other than their own. Localities with cheap electricity have a disincentive to encourage making interstate commerce in electricity trading easier, since other regions will be able to compete for local energy and drive up rates. For example, some regulators in Maine do not wish to address congestion problems because the congestion serves to keep Maine rates low. Further, vocal local constituencies can block or slow permitting by pointing to visual impact, environmental, and perceived health concerns. In the US, generation is growing four times faster than transmission, but big transmission upgrades require the coordination of multiple states, a multitude of interlocking permits, and cooperation between a significant portion of the 500 companies that own the grid. From a policy perspective, the control of the grid is balkanized, and even former energy secretary Bill Richardson refers to it as a "third world grid". There have been efforts in the EU and US to confront the problem. The US national security interest in significantly growing transmission capacity drove passage of the 2005 energy act giving the Department of Energy the authority to approve transmission if states refuse to act. However, soon after the Department of Energy used its power to designate two National Interest Electric Transmission Corridors, 14 senators signed a letter stating the DOE was being too aggressive.
Special transmission.
Grids for railways.
In some countries where electric locomotives or electric multiple units run on low frequency AC power, there are separate single phase traction power networks operated by the railways. Prime example are the countries of Europe, which utilize the older AC technology based on 16 2/3 Hz.
Superconducting cables.
High-temperature superconductors (HTS) promise to revolutionize power distribution by providing lossless transmission of electrical power. The development of superconductors with transition temperatures higher than the boiling point of liquid nitrogen has made the concept of superconducting power lines commercially feasible, at least for high-load applications. It has been estimated that the waste would be halved using this method, since the necessary refrigeration equipment would consume about half the power saved by the elimination of the majority of resistive losses. Some companies such as Consolidated Edison and American Superconductor have already begun commercial production of such systems. In one hypothetical future system called a SuperGrid, the cost of cooling would be eliminated by coupling the transmission line with a liquid hydrogen pipeline.
Superconducting cables are particularly suited to high load density areas such as the business district of large cities, where purchase of an easement for cables would be very costly.
Single wire earth return.
Single-wire earth return (SWER) or single wire ground return is a single-wire transmission line for supplying single-phase electrical power for an electrical grid to remote areas at low cost. It is principally used for rural electrification, but also finds use for larger isolated loads such as water pumps. Single wire earth return is also used for HVDC over submarine power cables.
Wireless power transmission.
Both Nikola Tesla and Hidetsugu Yagi attempted to devise systems for large scale wireless power transmission in the late 1800s and early 1900s, with no commercial success.
In November 2009, LaserMotive won the NASA 2009 Power Beaming Challenge by powering a cable climber 1 km vertically using a ground-based laser transmitter. The system produced up to 1 kW of power at the receiver end. In August 2010, NASA contracted with private companies to pursue the design of laser power beaming systems to power low earth orbit satellites and to launch rockets using laser power beams.
Wireless power transmission has been studied for transmission of power from solar power satellites to the earth. A high power array of microwave or laser transmitters would beam power to a rectenna. Major engineering and economic challenges face any solar power satellite project.
Security of control systems.
The Federal government of the United States admits that the power grid is susceptible to cyber-warfare. The United States Department of Homeland Security works with industry to identify vulnerabilities and to help industry enhance the security of control system networks, the federal government is also working to ensure that security is built in as the U.S. develops the next generation of 'smart grid' networks.
References.
Notes
Further reading
External links.
Maps

</doc>
<doc id="38833" url="http://en.wikipedia.org/wiki?curid=38833" title="Sheepshank">
Sheepshank

The sheepshank is a type of knot that is used to shorten a rope or take up slack. This knot is not stable. It will fall apart under too much load or too little load.
The knot has several features which allow a rope to be shortened:
Construction methods.
A sheepshank knot may be constructed as follows: ...
An alternative method for quickly constructing a sheepshank is as follows:
The result is a flattened loop which is held at each end by a half hitch. If the sides of the flattened loop are pulled away from each other, the flattened loop ends pull out of the half hitches and the knot falls apart, but if the free ends
are pulled taut then the knot remains secure.
Usage.
Sheepshank knots are typically used for securing loads to trucks or trailers, and in sailing applications.
Disadvantages.
The sheepshank was developed before the use of modern "slippery" synthetic ropes. Constructed from such ropes, under load, it can fail. It is strongly advised that an alternative knot be used.
Variants.
Man-o'war sheepshank.
The man-o'war sheepshank is a sheepshank knot with a Handcuff knot in the middle. This configuration with the half-hitches formed close to the central knot is used in rope rescue and is called a Fireman's chair knot.
Sheepshank with Marlingspike hitches.
This version of the sheepshank is tied by using slipknots instead of half-hitches. It is one of the safest sheepshank variations. 
Kamikaze knot.
The kamikaze knot is a slight variant of the sheepshank. To perform a kamikaze knot, a sheepshank is first constructed. Whilst holding sufficient tension on the sheepshank so it will not slip out, the middle rope is sliced. This allows climbers rappelling down cliff faces to keep most of the rope used for the rappel, by tying the knot at the top, and shaking the rope when they reach the bottom. The shaking disconnects the knot at the top, allowing the longer section of rope to fall, meaning only a small amount of rope is retained by the anchor at the top of the cliff. Thin or slippery rope is unsuitable for such a knot, as it can easily slip, and the knot should not be performed unless desperately needed.
Use.
Although certainly not invented by him this variant of the sheepshank knot appeared in an episode of the TV show "Man Vs Wild". Bear Grylls uses a modification of this knot by cutting one of the lengths of rope in the knot, while rappelling down an edge during the Ireland episode of "Man vs. Wild" in order to retrieve his rope at the bottom by severing the middle leg of the sheepshank knot before his descent. He refers to it as a "Kamikaze" knot.

</doc>
<doc id="38846" url="http://en.wikipedia.org/wiki?curid=38846" title="Electoral Palatinate">
Electoral Palatinate

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1085–1095
The County Palatine of the Rhine (German: "Pfalzgrafschaft bei Rhein"), later the Electoral Palatinate (German: "Kurpfalz"), was a historical territory of the Holy Roman Empire, originally a palatinate administered by a count palatine. Its rulers served as prince-electors ("Kurfürsten") from "time immemorial", were noted as such in a papal letter of 1261, and were confirmed as electors by the Golden Bull of 1356.
The fragmented territory stretched from the left bank of the Upper Rhine, from the Hunsrück mountain range in what is today the Palatinate region in the German federal state of Rhineland-Palatinate and the adjacent parts of the French region of Alsace (bailiwick of Seltz from 1418 to 1766) to the opposite territory on the east bank of the Rhine in present-day Hesse and Baden-Württemberg up to the Odenwald range and the southern Kraichgau region, containing the capital cities of Heidelberg and Mannheim.
The Counts Palatine of the Rhine held the office of Imperial vicars in the territories under Frankish law (in Franconia, Swabia and the Rhineland) and ranked among the most significant secular Princes of the Holy Roman Empire. Their climax and decline is marked by the rule of Elector Palatine Frederick V, whose coronation as King of Bohemia in 1619 sparked the Thirty Years' War. After the 1648 Peace of Westphalia, the ravaged lands were further afflicted by the "Reunion" campaigns launched by King Louis XIV of France, culminating in the Nine Years' War (1688–97). Ruled in personal union with the Electorate of Bavaria from 1777, the Electoral Palatinate was finally disestablished with the German mediatization in 1803.
History.
The office of a Count palatine at the Frankish court of King Childebert I was already mentioned about 535. Up to the 10th century, the rule of the Merovingian and Carolingian dynasties was centered at the royal palace ("Pfalz") in Aachen, in what was to become the Frankish kingdom of Lotharingia.
Counts Palatine of Lotharingia.
In 985, Herman I, a scion of the Ezzonids, is mentioned as count palatine of Lotharingia. His territories were centered in the Rhineland proper around Cologne and Bonn, but extended south to the Moselle and Nahe Rivers in (Upper) Lorraine. In continual conflicts with the rivalling Archbishops of Cologne, he changed the emphasis of his rule to the southern Eifel region and further to the Upper Rhine, where the Ezzonian dynasty governed several counties on both banks of the river. The southernmost point was near Alzey.
Counts Palatine of the Rhine.
From about 1085/86, after the death of the last Ezzonian count palatine Herman II, the Palatinate lost its military importance in Lotharingia. The territorial authority of his successor Henry of Laach was reduced to the counties along the Upper Rhine, from then on called County Palatine of the Rhine. Various noble dynasties competed to be enfeoffed with the Palatinate by the Holy Roman Emperor, among them the House of Ascania, the House of Salm (Count Otto I of Salm in 1040) and the House of Babenberg (Henry Jasomirgott in 1140/41).
The first hereditary Count Palatine of the Rhine was Conrad, a member of the House of Hohenstaufen and younger half-brother of Emperor Frederick Barbarossa. The territories attached to this hereditary office in 1156 started from those held by the Hohenstaufens in the Donnersberg, Nahegau, Haardt, Bergstraße and Kraichgau regions (other branches of the Hohenstaufens received lands in the Duchy of Swabia, Franche-Comté, and so forth). Much of this was from their imperial ancestors, the Salian emperors, and apart from Conrad's maternal ancestry, the Counts of Saarbrücken. These backgrounds explain the composition of Upper and Rhenish Palatinate in the inheritance centuries onwards. About 1182, Conrad moved his residence from Stahleck Castle near Bacharach up the Rhine River to Heidelberg.
Upon Conrad's death in 1195, the Palatinate passed to the House of Welf through the—secret—marriage of his daughter Agnes with Henry of Brunswick. When Henry's son Henry the Younger died without heirs in 1214, the Hohenstaufen king Frederick II enfeoffed the Wittelsbach duke Louis I of Bavaria. The Bavarian House of Wittelsbach eventually held the Palatinate territories until 1918.
During a later division of territory among the heirs of Duke Louis II, Duke of Upper Bavaria, in 1294, the elder branch of the Wittelsbachs came into possession of both the Rhenish Palatinate and the territories in the Bavarian "Nordgau" (Bavaria north of the Danube river) with the centre around the town of Amberg. As this region was politically connected to the Rhenish Palatinate, the name Upper Palatinate (German: "Oberpfalz") became common from the early 16th century in contrast to the Lower Palatinate along the Rhine.
With the Treaty of Pavia in 1329, the Wittelsbach emperor Louis IV, a son of Louis II, returned the Palatinate to his nephews Rudolf and Rupert.
Electorate.
In the Golden Bull of 1356, the Palatinate was recognized as one of the secular electorates, and given the hereditary offices of archsteward (German: "Erztruchseß", Latin: "Archidapifer") of the Empire and imperial vicar ("Reichsverweser") of Franconia, Swabia, the Rhine, and southern Germany. From that time forth, the Count Palatine of the Rhine was usually known as the Elector Palatine (German: "Kurfürst von der Pfalz", Latin: "Palatinus elector").
Due to the practice of dividing territories among different branches of the family, by the early 16th century junior lines of the Palatine Wittelsbachs came to rule in Simmern, Kaiserslautern and Zweibrücken in the Lower Palatinate, and in Neuburg and Sulzbach in the Upper Palatinate. The Elector Palatine, now based in Heidelberg, adopted Lutheranism in the 1530s and Calvinism in the 1550s.
When the senior branch of the family died out in 1559, the Electorate passed to Frederick III of Simmern, a staunch Calvinist, and the Palatinate became one of the major centers of Calvinism in Europe, supporting Calvinist rebellions in both the Netherlands and France.
Thirty Years' War.
In 1619, Frederick V accepted the throne of Bohemia from the Bohemian estates. He was soon defeated by the forces of Emperor Ferdinand II at the Battle of White Mountain in 1620, and Spanish and Bavarian troops soon occupied the Palatinate itself. Called "the Winter King" because his reign in Bohemia only lasted one winter, Frederick was put under the ban of the Empire in 1623. Frederick V's territories and his position as Elector were transferred to the Duke of Bavaria, Maximilian I, of a distantly related branch of the House of Wittelsbach. Although technically Elector Palatine, he was known as the Elector of Bavaria. From 1648 he ruled in Bavaria and the Upper Palatinate alone, but retained all his Electoral dignities and the seniority of the Palatinate Electorate.
By the Peace of Westphalia in 1648, Frederick V's son, Charles Louis was restored to the Lower Palatinate, and given a "new" electoral title, also called "Elector Palatine", but lower in precedence than the other electorates.
Later history.
In 1685, the Simmern line died out, and the Palatinate was inherited by Philip William, Count Palatine of Neuburg (also Duke of Jülich and Berg), a Catholic. During the reign of Johann Wilhelm (1690-1716) the Electoral residence was moved to Düsseldorf, before being moved back to Heidelberg in 1718 and then to Mannheim in 1720. Johann Wilhelm's mistress, Dorothea von Velen, encouraged him to introduce liberal reforms, most notably the abolition of coverture and the declaration of religious toleration.
In 1742, the Palatinate was inherited by Charles Theodore, Duke of Sulzbach. Charles Theodore also inherited the Electorate of Bavaria when its ruling line became extinct in 1777. The title and authority of Elector Palatine were subsumed into the Electorate of Bavaria, Charles Theodore and his heirs retaining only the single vote and precedence of the Bavarian elector. They continued to use the title "Count Palatine of the Rhine" (German: "Pfalzgraf bei Rhein", Latin: "Comes Palatinus Rheni").
Charles Theodore's heir, Maximilian Joseph, Duke of Zweibrücken (on the French border), brought all the Wittelsbach territories under a single rule in 1799. The Palatinate was dissolved in the Wars of the French Revolution. First, its left bank territories were occupied, and then annexed, by France starting in 1795; then, in 1803, its right bank territories were taken by the Margrave of Baden. The Rhenish Palatinate, as a distinct territory, disappeared. In 1806, the Holy Roman Empire was abolished, and all the rights and responsibilities of the electors with it.
After the Empire.
In 1806, Baden was raised to a grand duchy and parts of the former Palatinate including Mannheim became part of the new grand duchy. At the Congress of Vienna in 1814 and 1815, the left-bank Palatinate — enlarged by other territories such as the former Bishopric of Speyer and the free imperial city of Speyer — was returned to the Wittelsbachs and became a formal part of the Kingdom of Bavaria in 1816 in exchange for Tirol, which Bavaria ceded to Austria. After this time, it was this region that was principally known as the Palatinate. The area remained a part of Bavaria until after the Second World War, when it was separated and became a part of the new state of Rhineland-Palatinate, along with former left bank territories of Prussia and Hesse-Darmstadt.
Coat of arms.
In 1156 Conrad of Hohenstaufen, brother of emperor Frederick Barbarossa became count palatinate. The old coat of arms of the House of Hohenstaufen, the single lion, became coat of arms of the palatinate. By marriage, the Palatinate's arms also became quartered with those of Welf and later Wittelsbach. The arms of Bavaria were also used with reference to the elector's holdings in Bavaria. This was extended to quartering of the lion and the Bavarian Arms upon the ascension of Maximilian I to the position of elector of the Palatinate in 1623, used concurrently with the arms shown. The orb represented their position as Arch-Steward of the Holy Roman Empire.

</doc>
<doc id="38872" url="http://en.wikipedia.org/wiki?curid=38872" title="Bessarabia">
Bessarabia

Bessarabia (Romanian: "Basarabia"; Russian: Бессарабия "Bessarabiya", Ukrainian: Бессарабія "Bessarabiya") is a historical region in Eastern Europe, bounded by the Dniester river on the east and the Prut river on the west. The bulk of Bessarabia is currently part of Moldova, whereas the northernmost regions, as well as the southern regions bordering the Black Sea (Budjak), are part of Ukraine.
In the aftermath of the Russo-Turkish War, 1806-1812, and the ensuing Peace of Bucharest, the eastern parts of the Principality of Moldavia, an Ottoman vassal, along with some areas formerly under direct Ottoman rule, were ceded to Imperial Russia. The acquisition was among the Empire's last territorial increments to take place in Europe (the last being Congress Poland, integrated into the Empire between 1815 and 1867). The newly acquired territories were organised as the Governorate of Bessarabia, adopting a name previously used for the southern plains, between the Dniester and the Danube rivers. Following the Crimean War, in 1856, the southern areas of Bessarabia were returned to Moldavian rule; Russian rule was restored over the whole of the region in 1878, when Romania, the result of Moldavia's union with Wallachia, was pressured into exchanging those territories for the Dobruja.
In 1917, in the wake of the Russian Revolution, the area constituted itself as the Moldavian Democratic Republic, an autonomous republic part of a proposed federative Russian state. Bolshevik agitation in late 1917 and early 1918 resulted in the intervention of the Romanian Army, ostensibly to pacify the region. Soon after, the parliamentary assembly declared independence, and then union with the Kingdom of Romania. The legality of these acts was however disputed, most prominently by the Soviet Union, which regarded the area as a territory occupied by Romania.
In 1940, after securing the assent of Nazi Germany through the Molotov-Ribbentrop Pact, the Soviet Union pressured Romania into withdrawing from Bessarabia, allowing the Red Army to occupy the region. The area was formally integrated into the Soviet Union: the core joined parts of the Moldavian ASSR to form the Moldavian SSR, while territories inhabited by Slavic majorities in the north and the south of Bessarabia were transferred to the Ukrainian SSR. Axis-aligned Romania briefly recaptured the region in 1941, during the Nazi invasion of the Soviet Union, but lost it in 1944, as the tide of war changed. In 1947, the Soviet-Romanian border along the Prut was internationally recognised by the Paris Treaty that ended World War II.
During the process of the dissolution of the Soviet Union, the Moldavian and Ukrainian SSRs proclaimed their independence in 1991, becoming the modern states of Moldova and Ukraine, while preserving the existing partition of Bessarabia. Following a short war in the early 1990s, Transnistria proclaimed itself the Pridnestrovian Moldavian Republic, separate from the government of the Republic of Moldova, extending its authority also over the municipality of Bender in Bessarabia. Part of the Gagauz-inhabited areas in the southern Bessarabia was organised in 1994 as an autonomous region within Moldova.
Etymology.
According to the traditional interpretation, the name Bessarabia ("Basarabia" in Romanian) derives from the Wallachian Basarab dynasty, who allegedly ruled over the southern part of the area in the 14th century. Recent research has however cast doubt on this view, as the name was first applied to the territory by Western cartographers, showing up in local sources only in the second half of the 17th century. Furthermore, the use of the term to refer to the Moldavian lands near the Black Sea was explicitly rejected as a cartographic confusion by the early Moldavian chronicler Miron Costin. The confusion may have been caused by Polish references to Wallachia as "Bessarbia", wrongly interpreted by medieval Western cartographers as a separate land between that country and Moldavia. According to Dimitrie Cantemir, the name originally applied only to the part of the territory south of the Upper Trajanic Wall, somewhat bigger than current Budjak.
Geography.
The region is bounded by the Dniester to the north and east, the Prut to the west and the lower River Danube and the Black Sea to the south. It is approximately 17,600 sqmi. The area is mostly hilly plains with flat steppes. It is very fertile, and has lignite deposits and stone quarries. People living in the area grow sugar beet, sunflowers, wheat, maize, tobacco, wine grapes and fruit. They also raise sheep and cattle. Currently, the main industry in the region is agricultural processing.
The region's main cities are Chișinău the capital of Moldova, Izmail, Bilhorod-Dnistrovs'kyi (historically called Cetatea Albă / Akkerman). Other towns of administrative or historical importance include: Khotyn, Lipcani, Briceni, Soroca, Bălți, Orhei, Ungheni, Bender/Tighina, Cahul, Reni and Kilia.
History.
In the late 14th century, the newly established Principality of Moldavia encompassed what later became known as Bessarabia. Afterwards, this territory was directly or indirectly, partly or wholly controlled by: the Ottoman Empire (as suzerain of Moldavia, with direct rule only in Budjak and Khotin), Russian Empire, Romania, the USSR. Since 1991, most of the territory forms the core of Moldova, with smaller parts in Ukraine.
Prehistory.
The territory of Bessarabia has been inhabited by people for thousands of years. Cucuteni-Trypillian culture flourished between the 6th and 3rd millennium BC. The Indo-European culture spread in the region around 2000 BC.
Ancient times.
In Antiquity the region was inhabited by Thracians, as well as for various shorter periods Cimmerians, Scythians, Sarmatians, and Celts, specifically by tribes such as Costoboci, Carpi, Britogali, Tyragetae, and Bastarnae. In the 6th century BC, Greek settlers established the colony of Tyras, along the Black Sea coast and traded with the locals. Also, Celts settled in the southern parts of Bessarabia, their main city being Aliobrix.
The first polity that is believed to have included the whole of Bessarabia was the Dacian polity of Burebista in the 1st century BC. After his death, the polity was divided into smaller pieces, and the central parts were unified in the Dacian kingdom of Decebalus in the 1st century AD. This kingdom was defeated by the Roman Empire in 106. Southern Bessarabia was included in the empire even before that, in 57 AD, as part of the Roman province Moesia Inferior, but it was secured only when the Dacian Kingdom was defeated in 106. The Romans built defensive earthen walls in Southern Bessarabia (e.g. Lower Trajan Wall) to defend the Scythia Minor province against invasions. Except for the Black Sea shore in the south, Bessarabia remained outside direct Roman control; the myriad of tribes there are called by modern historians Free Dacians. The 2nd to the 5th centuries also saw the development of the Chernyakhov culture.
In 270, the Roman authorities began to withdraw their forces south of the Danube, especially from the Roman Dacia, due to the invading Goths and Carpi. The Goths, a Germanic tribe, poured into the Roman Empire from the lower Dniepr River, through the southern part of Bessarabia (Budjak steppe), which due to its geographic position and characteristics (mainly steppe), was swept by various nomadic tribes for many centuries. In 378, the area was overrun by the Huns.
Early Middle Ages.
From the 3rd century until the 11th century, the region was invaded numerous times in turn by different tribes: Goths, Huns, Avars, Bulgars, Slavs (South, i.e. Bulgarian, and Eastern), Magyars, Pechenegs, Cumans and Mongols. The territory of Bessarabia was encompassed in dozens of ephemeral kingdoms which were disbanded when another wave of migrants arrived. Those centuries were characterized by a terrible state of insecurity and mass movement of these tribes. The period was later known as the "Dark Ages" of Europe, or Age of migrations. The Byzantine Empire allegedly maintained partial control of several cities and forts in southern Bessarabia until the 7th century. In particular, the fortress city of Tyras was plundered by the Huns in 375, but was rebuilt by the Byzantines in 545 as "Turris". It served as a trading post with Daco-Romans to the north-west, and Antes and Jassic people to the north-east.
In 561, the Avars captured Bessarabia and executed the local ruler Mesamer. Following Avars, Slavs started to arrive in the region and establish settlements. Then, in 582, Onogur Bulgars settled in southeastern Bessarabia and northern Dobruja, from which they moved to Moesia Inferior (allegedly under pressure from the Khazars), and formed the nascent region of "Bulgaria". With the rise of the Khazars' state in the east, the invasions began to diminish and it was possible to create larger states. According to some opinions, the southern part of Bessarabia remained under the influence of the First Bulgarian Empire until to the end of the 9th century.
Between the 8th and 10th centuries, the southern part of Bessarabia was inhabited by people from Balkan-Dunabian culture (the culture of the First Bulgarian Empire). Between the 9th and 13th centuries, Bessarabia is mentioned in Slav chronicles as part of "Bolohoveni" (north) and "Brodnici" (south) voivodeships, believed to be Vlach principalities of the early Middle Ages.
The last large scale invasions were those of the Mongols of 1241, 1290, and 1343. Sehr al-Jedid (near Orhei), an important settlement of the Golden Horde, dates from this period. They led to a retreat of a big part of the population to the mountainous areas in Eastern Carpathians and to Transylvania. Especially low became the population east of Prut at the time of the Tatar invasions.
In the Late Middle Age, chronicles mention a Tigheci "republic", predating the establishment of the Principality of Moldavia, situated near the modern town of Cahul in the southwest of Bessarabia, preserving its autonomy even during the later Principality even into the 18th century. Genovese merchants rebuilt or established a number of forts along Dniester (Moncastro, at Tighina, at the Old Orhei, at Soroca/"Olhionia") and Danube (including Kyliya/Chilia-Licostomo).
Principality of Moldavia.
After the 1360s the region was gradually included in the principality of Moldavia, which by 1392 established control over the fortresses of Cetatea Albă and Chilia, its eastern border becoming the River Dniester. In the latter part of the 14th century, the southern part of the region was for several decades part of Wallachia. The main dynasty of Wallachia was called Basarab, from which the current name of the region originated. In the 15th century, the entire region was a part of the principality of Moldavia. Stephen the Great ruled between 1457 and 1504, a period of nearly 50 years during which he won 32 battles defending his country against virtually all his neighbours (mainly the Ottomans and the Tatars, but also the Hungarians and the Poles), while losing only two. During this period, after each victory, he raised a monastery or a church close to the battlefield honoring Christianity. Many of these battlefields and churches, as well as old fortresses, are situated in Bessarabia (mainly along Dniester).
In 1484, the Turks invaded and captured Chilia and Cetatea Albă (Akkerman in Turkish), and annexed the shoreline southern part of Bessarabia, which was then divided into two sanjaks (districts) of the Ottoman Empire. In 1538, the Ottomans annexed more Bessarabian land in the south as far as Tighina, while the central and northern Bessarabia remained part of the Principality of Moldavia (which became a vassal of the Ottoman Empire). Between 1711 and 1812, the Russian Empire occupied the region five times during its wars against Ottoman and Austrian Empires.
Annexation by the Russian Empire.
By the Treaty of Bucharest of May 28, 1812—concluding the Russo-Turkish War, 1806-1812—the Ottoman Empire ceded the eastern half of the Principality of Moldavia to the Russian Empire. That entire region was then called "Bessarabia".
In 1814, the first German settlers arrived and mainly settled in the southern parts and Bessarabian Bulgarians began settling in the region too, founding towns such as Bolhrad. Between 1812 and 1846, the Bulgarian and Gagauz population migrated to the Russian Empire via the River Danube, after living many years under oppressive Ottoman rule, and settled in southern Bessarabia. Turkic-speaking tribes of the Nogai horde also inhabited the Budjak Region (in Turkish Bucak) of southern Bessarabia from the 16th to 18th centuries, but were totally driven out prior to 1812.
Administratively, Bessarabia became an "oblast" of the Russian Empire in 1818, and a "guberniya" in 1873.
By the Treaty of Adrianople that concluded the Russo-Turkish War of 1828-1829 the entire Danube Delta was added to the Bessarabian oblast. In 1834, Romanian language was banned from schools and government facilities, despite 80% of the population speaking the language. This would eventually lead to the banning of Romanian in churches, media and books. Those who protested the banning of Romanian could be sent to Siberia.
At the end of the Crimean War, in 1856, by the Treaty of Paris, three districts of southern Bessarabia (Cahul, Bolgrad, and Ismail) were returned to Moldavia, causing the Russian Empire to lose access to the Danube river.
In 1859, Moldavia and Wallachia united to form the Romanian United Principalities (Romania), which included the southern part of Bessarabia.
The railway Chișinău-Iași was opened on June 1, 1875 in preparation for the Russo-Turkish War (1877–1878) and the Eiffel Bridge was opened on April 21 [O.S. April 9] 1877, just three days before the outbreak of the war. The Romanian War of Independence was fought in 1877–78, with the help of the Russian Empire as an ally. Northern Dobruja was awarded to Romania for its role in the 1877-78 Russo-Turkish War, and as compensation for the transfer of the Southern Bessarabia.
The Kishinev pogrom took place in the capital of Bessarabia on April 6, 1903 after local newspapers published articles inciting the public to act against Jews; 47 or 49 Jews were killed, 92 severely wounded and 700 houses destroyed. The anti-Semitic newspaper Бессарабец (Bessarabetz, meaning "Bessarabian"), published by Pavel Krushevan, insinuated that a Russian boy was killed by local Jews. Another newspaper, Свет (Lat. Svet, meaning "World"), used the age-old blood libel against the Jews (alleging that the boy had been killed to use his blood in preparation of matzos).
After the 1905 Russian Revolution, a Romanian nationalist movement started to develop in Bessarabia. In the chaos brought by the Russian revolution of October 1917, a National Council (Sfatul Țării) was established in Bessarabia, with 120 members elected from Bessarabia by some political and professional organizations and 10 elected from Transnistria (the left bank of Dniester where Moldovans and Romanians accounted for less than a third and the majority of the population was Ukrainian. See Demographics of Transdniestria).
On January 14, 1918, during the disorderly retreat of two Russian divisions from the Romanian front, Chişinău was sacked. The "Rumcherod" Committee (Central Executive Committee of the Soviets of Romanian Front, Black Sea Fleet and Odessa Military District) proclaimed itself the supreme power in Bessarabia. The Russian commander of the region, General Dmitry Shcherbachev, unable to control Bessarabia due to the Bolshevik revolution, allegedly requested the Romanian Army for help. Russian historians dispute this request was made. On 16 January a Romanian division entered Chişinău, and on the following day Tighina on the shore of the river Dniester. The three-day Soviet rule in Bessarabia ended.
Ten days later, on January 24, 1918, Sfatul Țării declared Bessarabia's independence as the Moldavian Democratic Republic.
Unification with Romania.
The county councils of Bălți, Soroca and Orhei were the earliest to ask for unification with the Kingdom of Romania, and on April 9 [O.S. March 27] 1918, in the presence of the Romanian Army, Sfatul Ţării voted in favour of the union, with the following conditions:
86 deputies voted in support, 3 voted against and 36 abstained.
The first condition, the agrarian reform, was debated and approved in November 1918. Sfatul Țării also decided to remove the other conditions and made unification with Romania unconditional. This vote has been judged illegitimate, since there was no quorum: only 44 of the 125 members took part in it (all voted "for"). As of mid 1919, the population of Bessarabia was estimated at around 2 million.
In the autumn of 1919, elections for the Romanian Constituent Assembly were held in Bessarabia; 90 deputies and 35 senators were chosen. On December 20, 1919, these men voted, along with the representatives of Romania's other regions, to ratify the unification acts that had been approved by Sfatul Țării and the National Congresses in Transylvania and Bukovina.
The union was recognized by France, United Kingdom, Italy, and Japan in the Treaty of Paris of 1920, which however never came into force, because Japan did not ratify it. The United States refused to sign the treaty on the grounds that Russia was not represented at the Conference. Soviet Russia (and later, the USSR) did not recognize the union, and by 1924, after its demands for a regional plebiscite were declined by Romania for the second time, declared Bessarabia to be Soviet territory under foreign occupation.
The US also considered Bessarabia a territory under Romanian occupation, rather than Romanian territory, despite existing political and economic relations between the US and Romania.
Part of Romania.
A Provisional Workers' & Peasants' Government of Bessarabia was founded on May 5, 1919, in exile at Odessa, by the Bolsheviks.
On May 11, 1919, the Bessarabian Soviet Socialist Republic was proclaimed as an autonomous part of Russian SFSR, but was abolished by the military forces of Poland and France in September 1919 (see Polish-Soviet War). After the victory of Bolshevist Russia in the Russian Civil War, the Ukrainian SSR was created in 1922, and in 1924 the Moldovan Autonomous Soviet Socialist Republic was established on a strip of Ukrainian land on the left bank of Dniester where Moldovans and Romanians accounted for less than a third and the relative majority of population was Ukrainian. (See Demographics of Moldovan Autonomous Soviet Socialist Republic).
World War II.
The Soviet Union did not recognize incorporation of Bessarabia into Romania and throughout the entire interwar period engaged in attempts to undermine Romania and diplomatic disputes with the government in Bucharest over this territory. The Molotov-Ribbentrop Pact was signed on August 23, 1939. By Article 4 of the secret Annex to the Treaty, Bessarabia fell within the Soviet interest zone.
In spring of 1940, Western Europe was overrun by Nazi Germany. With world attention focused on those events, on June 26, 1940, the USSR issued an ultimatum to Romania, demanding immediate cession of Bessarabia and Northern Bukovina. Romania was given four days to evacuate its troops and officials. The two provinces had an area of 51,000 km2, and were inhabited by about 3.75 million people, half of them Romanians, according to official Romanian sources. Two days later, Romania yielded and began evacuation. During the evacuation, from June 28 to July 3, groups of local Communists and Soviet sympathizers attacked the retreating forces, and civilians who chose to leave. Many members of the minorities (Jews, ethnic Ukrainians and others) joined in these attacks. The Romanian Army was also attacked by the Soviet Army, which entered Bessarabia before the Romanian administration finished retreating. The casualties reported by the Romanian Army during those seven days consisted of 356 officers and 42,876 soldiers dead or missing.
On August 2, the Moldavian Soviet Socialist Republic was established on most of the territory of Bessarabia, merged with the western parts of the former Moldavian ASSR. Bessarabia was divided between the Moldavian SSR (70% of the territory and 80% of the population) and the Ukrainian SSR. Bessarabia's northern and southern districts (now Budjak and parts of the Chernivtsi oblast) were allotted to Ukraine, while some territories (4,000 km2) on the left (eastern) bank of Dniester (present Transnistria), previously part of Ukraine, were allotted to Moldavia. Following the Soviet takeover, many Bessarabians, who were accused of supporting the deposed Romanian administration, were executed or deported to Siberia and Kazakhstan.
Between September and November 1940, the ethnic Germans of Bessarabia were offered resettlement to Germany, following a German-Soviet agreement. Fearing Soviet oppression, almost all Germans (93,000) agreed. Most of them were resettled to the newly annexed Polish territories.
On June 22, 1941 the Axis invasion of the Soviet Union commenced with Operation Barbarossa. Between June 22 and July 26, 1941, Romanian troops with the help of Wehrmacht recovered Bessarabia and northern Bukovina. The Soviets employed scorched earth tactics during their forced retreat from Bessarabia, destroying the infrastructure and transporting movable goods to Russia by railway. At the end of July, after a year of Soviet rule, the region was once again under Romanian control.
As the military operation was still in progress, there were cases of Romanian troops "taking revenge" on Jews in Bessarabia, in the form of pogroms on civilians and murder of Jewish POWs, resulting in several thousand dead. The supposed cause for murdering Jews was that in 1940 some Jews welcomed the Soviet takeover as liberation. At the same time the notorious SS Einsatzgruppe D, operating in the area of the German 11th Army, committed summary executions of Jews under the pretext that they were spies, saboteurs, Communists, or under no pretext whatsoever.
The political solution of the "Jewish Question" was apparently seen by the Romanian dictator Marshal Ion Antonescu more in expulsion rather than extermination. That portion of the Jewish population of Bessarabia and Bukovina which did not flee before the retreat of the Soviet troops (147,000) was initially gathered into ghettos or Nazi concentration camps, and then deported during 1941-1942 in death marches into Romanian-occupied Transnistria, where the "Final Solution" was applied.
After three years of relative peace, the German-Soviet front returned in 1944 to the land border on the Dniester. On August 20, 1944, a c. 3,400,000-strong Red Army began a major summer offensive codenamed Jassy-Kishinev Operation. The Soviet armies overran Bessarabia in a two-pronged offensive within five days. In pocket battles at Chişinău and Sărata the German 6th Army of c. 650,000 men, newly reformed after the Battle of Stalingrad, was obliterated. Simultaneously with the success of the Russian attack, Romania broke the military alliance with the Axis and changed sides. On August 23, 1944, Marshal Ion Antonescu was arrested by King Michael, and later handed over to the Soviets.
Part of the Soviet Union.
The Soviet Union regained the region in 1944, and the Red Army occupied Romania. By 1947, the Soviets had imposed a communist government in Bucharest, which was friendly and obedient towards Moscow. The Soviet occupation of Romania lasted until 1958. The Romanian communist regime did not openly raise the matter of Bessarabia or Northern Bukovina in its diplomatic relations with the Soviet Union.
Between 1969 and 1971, a clandestine National Patriotic Front was established by several young intellectuals in Chişinău, totaling over 100 members, vowing to fight for the establishment of a Moldavian Democratic Republic, its secession from the Soviet Union and union with Romania.
In December 1971, following an informative note from Ion Stănescu, the President of the Council of State Security of the Romanian Socialist Republic, to Yuri Andropov, the chief of KGB, three of the leaders of the National Patriotic Front, Alexandru Usatiuc-Bulgar, Gheorghe Ghimpu and Valeriu Graur, as well as a fourth person, Alexandru Soltoianu, the leader of a similar clandestine movement in northern Bukovina (Bucovina), were arrested and later sentenced to long prison terms.
Rise of independent Moldova.
With the weakening of the Soviet Union, in February 1988, the first non-sanctioned demonstrations were held in Chișinău. At first pro-Perestroika, they soon turned anti-government and demanded official status for the Romanian (Moldavian) language instead of the Russian language. On August 31, 1989, following a 600,000-strong demonstration in Chișinău four days earlier, Romanian (Moldavian) became the official language of the Moldavian Soviet Socialist Republic. However, this was not implemented for many years. In 1990, the first free elections were held for Parliament, with the opposition Popular Front winning them. A government led by Mircea Druc, one of the leaders of the Popular Front, was formed. The Moldavian SSR became SSR Moldova, and later the Republic of Moldova. The Republic of Moldova became independent on August 31, 1991; it took over unchanged the boundaries of the Moldavian SSR.
Population.
The population before World War II consisted of Romanians (including Moldovans), Ukrainians (including Ruthenians), Russians, Bulgarians, Gagauz, Germans, and Jews. According to Romanian historians, during the 19th century the ethnic Romanians decreased from 86% (1817) to 47.6% (1897) (although other sources cite different data for the same period of time: 52% or 75% (Krusevan) for 1900, 53.9% (1907), 70% (1912, Laskov), or 65-67% (1918, J. Kaba)).
The Russian Census of 1817, which recorded 96,526 families and 482,630 inhabitants, did not register ethnic data except for recent refugees (primarily Bulgarians) and certain ethno-social categories (Jews, Armenians and Greeks). In the 20th century, Romanian historian Ion Nistor extrapolated the numbers for the other ethnic groups, providing the following estimates: 83,848 Romanian families (86%), 6,000 Ruthenian families (6.5%), 3,826 Jewish families (1.5%), 1,200 Lipovan families (1.5%), 640 Greek families (0.7%), 530 Armenian families (0.6%), 482 Bulgarian and Gagauz families (0.5%). An 1818 statistic of three counties in southern Bessarabia (Akkerman, Izmail and Bender) that had witnessed strong emigration of the Muslim population and immigration from other regions, including Ottoman lands south of the Danube, recorded the following percentages: 48.64% Moldovans, 7.07% Russians, 15.65% Ukrainians, 17.02% Bulgarians and 11.62% others, amounting to a total population of 113,835.
According to an administrative census made in 1843-1844 at the request of the Russian Academy of Sciences, the following proportions were recorded, in a total of 692,777 inhabitants: 59.4% Moldovans, 17.2% Ukrainians, 9.3% Bulgarians, 7.1% Jews and 2.2% Russians. It should be noted that, in the case of some urban centres, figures were not reported for all ethnic groups. Furthermore, the size of the total populations differs from other official reports of the same period, which put the population of Bessarabia at 774,492 or 793,103.
Church records gathered around 1850-1855 put the total population at 841,523, with the following composition: 51.4% Moldovans, 4.2% Russians, 21.3% Ukrainians, 10% Bulgarians, 7.2% Jews and 5.7% others. On the other hand, official data for 1855 record a total population of 980,031, excluding the population on the territory under the authority of the Special Administration of the town of Izmail.
According to Ion Nistor, the population of Bessarabia in 1856 was composed of 736,000 Romanians (74%), 119,000 Ukrainians (12%), 79,000 Jews (8%), 47,000 Bulgarians and Gagauz (5%), 24,000 Germans (2.4%), 11,000 Romani (1.1%), 6,000 Russians (0.6%), adding to a total of 990,274 inhabitants.
Russian data, 1889 (Total: 1,628,867 inhabitants)
Russian Census, 1897 (Total 1,935,412 inhabitants). By language:
Some scholars, however, believed in regard to the 1897 census that "[...] the census enumerator generally has instructions to count everyone who understands the state language as being of that nationality, no matter what his everyday speech may be.", thus a number of Moldavians (Romanians) might have been registered as Russians.
According to N. Durnovo, the population of Bessarabia in 1900 was (Total: 1,935,000 inhabitants):
Romanian Census, 1930 (Total: 2,864,662 inhabitants)
When?: Total: 2,995,821
Data of the Romanian census 1939 was not completely processed before the Soviet occupation. Estimates of the total population at 3.2 million.
Soviet census, 1979: 69% of Moldavian SSR's population were Moldovan, and 98% of them declared Moldovan language (Romanian language) as their native language.
Soviet census, 1989: There were 88,419 Bessarabian Bulgarians according to official data from Republic of Moldova
Estimate, 1992: 4,305 immigrants to Israel from the Republic of Moldova constituted 7.1 percent of all the immigrants to Israel from the former U.S.S.R. in this year.
Moldovan census, 2004: There were 65,072 Bessarabian Bulgarians according to the census not including Bulgarians in Transnistria.

</doc>
<doc id="38889" url="http://en.wikipedia.org/wiki?curid=38889" title="Sigismund, Holy Roman Emperor">
Sigismund, Holy Roman Emperor

Sigismund of Luxemburg (14 February 1368 – 9 December 1437) was Prince-elector of Brandenburg from 1378 until 1388 and from 1411 until 1415, King of Hungary and Croatia from 1387, King of Germany from 1411, King of Bohemia from 1419, King of Italy from 1431, and Holy Roman Emperor for four years from 1433 until 1437, the last male member of the House of Luxemburg. He was regarded as highly educated, spoke several languages (among them French, German, Hungarian, Italian, and Latin) and was – unlike his father Charles – an outgoing person who also took pleasure in the tournament.
Sigismund was one of the driving forces behind the Council of Constance that ended the Papal Schism, but which in the end also led to the Hussite Wars that dominated the later period of Sigismund's life. He was buried in Nagyvárad, Kingdom of Hungary (now Oradea, Romania), next to the tomb of the King Saint Ladislaus I of Hungary.
Biography.
Early life.
Born in Nuremberg, Sigismund was the son of the Holy Roman Emperor, Charles IV, and of his fourth wife, Elizabeth of Pomerania, the granddaughter of King Casimir III of Poland, and the great-granddaughter of the Grand Duke of Lithuania, Gediminas. He was named after Saint Sigismund of Burgundy, the favourite saint of Sigismund's father. From Sigismund's childhood he was nicknamed the "ginger fox" ("liška ryšavá") in the Crown of Bohemia, on account of his hair colour. King Louis the Great of Hungary and Poland always had a good and close relationship with Emperor Charles IV, and Sigismund was betrothed to Louis' eldest daughter, Mary, in 1374, when he was six years old. Upon his father's death in 1378, young Sigismund became Margrave of Brandenburg and was sent to the Hungarian court, where he soon learnt the Hungarian language and way of life, and became entirely devoted to his adopted country. King Louis named him as his heir and appointed him his successor as King of Hungary.
In 1381, the then 13-year-old Sigismund was sent to Kraków by his eldest half-brother and guardian Wenceslaus, King of Germany and Bohemia, to learn Polish and to become acquainted with the land and its people. King Wenceslaus also gave him Neumark to facilitate communication between Brandenburg and Poland.
The disagreement between Polish landlords of Lesser Poland on one side and landlords of Greater Poland on the other, regarding the choice of the future King of Poland, finally ended in choosing the Lithuanian side; the support of the lords of Greater Poland was not enough to give Prince Sigismund the Polish crown. Instead, the landlords of Lesser Poland gave it to Mary's younger sister Jadwiga I of Poland, who married Jogaila of Lithuania.
King of Hungary.
On the death of her father in 1382, his betrothed, Mary, became queen of Hungary and Sigismund married her in 1385 in Zólyom (today Zvolen). The next year, he was accepted as Mary's future co-ruler by the Treaty of Győr. However, Mary was captured, together with her mother, Elizabeth of Bosnia, who had acted as regent, in 1387 by the rebellious House of Horvat, Bishop Paul Horvat of Mačva, his brother John Horvat and younger brother Ladislav. Sigismund's mother-in-law was strangled, while Mary was liberated.
Having secured the support of the nobility, Sigismund was crowned King of Hungary at Székesfehérvár on 31 March 1387. Having raised money by pledging Brandenburg to his cousin Jobst, margrave of Moravia (1388), he was engaged for the next nine years in a ceaseless struggle for the possession of this unstable throne. The central power was finally weakened to such an extent that only Sigismund's alliance with the powerful Czillei-Garai League could ensure his position on the throne. It was not for entirely selfless reasons that one of the leagues of barons helped him to power: Sigismund had to pay for the support of the lords by transferring a sizable part of the royal properties. (For some years, the baron's council governed the country in the name of the Holy Crown). The restoration of the authority of the central administration took decades of work. The bulk of the nation headed by the House of Garai was with him; but in the southern provinces between the Sava and the Drava, the Horvathys with the support of King Tvrtko I of Bosnia, Mary's maternal uncle, proclaimed as their king Ladislaus, king of Naples, son of the murdered Charles II of Hungary. Not until 1395 did Nicholas II Garay succeed in suppressing them. Mary died heavily pregnant in 1395.
To ease the pressure from Hungarian nobles, Sigismud tried to employ foreign advisors which was not popular and he had to promise to not give land and nominations to other than Hungarian nobles. However, this was not applied to Stibor of Stiboricz who was Sigismund's closest friend and advisor. On a number of occasions, Sigismund was imprisoned by nobles, but with help of the armies of Garai and Stibor of Stiboricz, he would regain power.
In 1396 Sigismund led the combined armies of Christendom against the Turks, who had taken advantage of the temporary helplessness of Hungary to extend their dominion to the banks of the Danube. This crusade, preached by Pope Boniface IX, was very popular in Hungary. The nobles flocked in the thousands to the royal standard, and were reinforced by volunteers from nearly every part of Europe, the most important contingent being that of the French led by John the Fearless, son of Philip II, Duke of Burgundy. Sigismund set out with 90,000 men and a flotilla of 70 galleys. After capturing Vidin, he camped with his Hungarian armies before the fortress of Nicopolis. Sultan Bayezid I raised the siege of Constantinople and, at the head of 140,000 men, completely defeated the Christian forces in the Battle of Nicopolis fought between the 25 and 28 September 1396. He returned by sea and through the realm of Zeta, where he ordained a local Montenegrin lord Đurađ II with the islands of Hvar and Korčula for resistance against the Turks, which were returned to Sigismund after his death in April 1403.
The disaster in Nicopolis angered several Hungarian lords, leading to instability in the kingdom. Deprived of his authority in Hungary, Sigismund then turned his attention to securing the succession in Germany and Bohemia, and was recognized by his childless half-brother Wenceslaus IV as Vicar-General of the whole Empire. However, he was unable to support Wenceslaus when he was deposed in 1400 and Rupert of Germany, Elector Palatine, was elected German king in his stead.
On his return to Hungary in 1401, Sigismund was imprisoned once and deposed twice. In 1401 Sigismund helped an uprising against Wenceslaus, during the course of which the Bohemian king was taken prisoner, and Sigismund ruled Bohemia for nineteen months. He released Wenceslaus in 1403. In the meantime, a group of Hungarian noblemen swore loyalty to the last Anjou monarch, Ladislaus of Naples, putting their hands on the relic of Saint Ladislas of Hungary in Nagyvárad. Ladislaus was the son of the murdered Charles II of Hungary, and thus a distant relative of the long dead King Louis I of Hungary. Ladislaus captured Zadar in 1403, but soon stopped any military advance. This struggle in turn led to a war with the Republic of Venice, as Ladislaus had sold the Dalmatian cities to the Venetians for 100,000 ducats before leaving for his own land. In the following years Sigismund acted indirectly to thwart Ladislaus' attempts to conquer central Italy, by allying with the Italian cities resisting him and by applying diplomatic pressure on him.
In 1404 he introduced the "Placetum Regium". According to this decree, Papal bulls could not be pronounced in Hungary without the consent of the king.
In about 1406 Sigismund married Mary's cousin Barbara of Celje (Barbara Celjska, nicknamed the "Messalina of Germany"), daughter of Count Hermann II of Celje. Hermann's mother Katarina Kotromanić (of the House of Kotromanic) and Mary's mother Queen Elizabeta (Elisabeth of Bosnia) were sisters, or cousins who were adoptive sisters. Tvrtko I was their first cousin and adopted brother, and perhaps even became heir apparent to Queen Mary. Tvrtko may have been murdered in 1391 on Sigismund's order.
Sigismund founded his personal order of knights, the Order of the Dragon, after this victory. Members of the order were mostly his political allies and supporters. The most important European monarchs became members of the order. He encouraged international trade by abolishing internal duties, regulating tariffs on foreign goods and standardizing weights and measures throughout the country. Due to his frequent absences attending to business in the other countries over which he ruled, he was obliged to consult Diets in Hungary with more frequency than his predecessors and institute the office of Palatine as chief administrator while he was away. During his long reign royal castle of Buda became probably the largest Gothic palace of the late Middle Ages.
King of Croatia.
In Slavonia he managed to establish control. He did not hesitate to use violent methods (see Bloody Sabor of Križevci) but from the river Sava to the south his control was weak. Sigismund personally led an army of almost 50,000 "crusaders" against the Croats and Bosnians, which culminated in 1408 with the Battle of Dobor, and a massacre of about 200 noble families, many of them victors of numerous battles against the Ottomans.
King of the Romans.
After the death of King Rupert of Germany in 1410, Sigismund – ignoring the claims of his half-brother Wenceslaus – was elected as successor by three of the electors on 10 September 1410, but he was opposed by his cousin Jobst of Moravia, who had been elected by four electors in a different election on 1 October. Jobst's death 18 January 1411 removed this conflict and Sigismund was again elected King on 21 July 1411. His coronation was deferred until 8 November 1414, when it took place at Aachen.
Anti-Polish alliances.
On a number of occasions, and in 1410 in particular, Sigismund allied himself with the Teutonic Knights against Władysław II of Poland. In return for 300.000 ducats he would attack Poland from the south after the truce on St. John's Day, 24 June expired. Sigismund gave his most loyal friend Stibor of Stiboricz order to set up the attack on Poland. Stibor of Stiboricz was of Polish origin and from the main line of the powerful Clan of Ostoja that also was against of choosing Jagiello as King of Poland in the beginning. With the support of Sigismund, Stibor become one of the most influential men in late medieval Europe, holding titles as Duke of Transylvania and owning about 25% of Slovakia of today including 31 castles of which 15 were situated around the 406 km long Váh river with surrounding land that was given to him by Sigismund. In the diplomatic struggle to prevent war between Poland-Lithuania, which was supported by the Muscovites, and the Teutonic Knights, Sigismund used Stibors fine diplomacy to gain financially. The Polish side appointed several negotiators and most of them were also from the Clan of Ostoja, being distantly related to the Stibors. However, those "family meetings" could not prevent the war and an alliance of twenty-two western states formed an army against Poland in the Battle of Grunwald in July 1410. Stibor attacked then Nowy Sącz and burned it down to the ground but after that, he returned with his army back home at the Beckov Castle. After Polish-Lithuanian victory in the Battle of Grunwald, Teutonic knights had to pay huge sum of silver to Poland as reparation and again, through diplomacy of his friend Stibor, Sigismund could borrow all this silver from King Władysław II of Poland on good conditions. In the light of facts about the diplomatic work of Stibor and the Clan of Ostoja that was following the politics of King Sigismund, one can question if Sigismund really joined the anti-Polish alliance in reality.
Council of Constance.
From 1412 to 1423 he campaigned against the Venetians in Italy. The king took advantage of the difficulties of Antipope John XXIII to obtain a promise that a council should be called in Constance in 1414 to settle the Western Schism. He took a leading part in the deliberations of this assembly, and during the sittings made a journey to France, England and Burgundy in a vain attempt to secure the abdication of the three rival popes. The council ended in 1418, solving the Schism and — of great consequence to Sigismund's future career — having the Czech religious reformer, Jan Hus, burned at the stake for heresy in July 1415. The complicity of Sigismund in the death of Hus is a matter of controversy. He had granted him a safe-conduct and protested against his imprisonment; and the reformer was burned during his absence.
It was also at this Council that a cardinal ventured to correct Sigismund's Latin (he had construed the word "schisma" as feminine rather than neuter). To this Sigismund replied:
I am king of the Romans and above grammar.
An alliance with England against France, and a failed attempt, owing to the hostility of the princes, to secure peace in Germany by a league of the towns, were his main acts during these years. Also, Sigismund granted control of the Margraviate of Brandenburg (which he had received back after Jobst's death) to Frederick I of Hohenzollern, burgrave of Nuremberg (1415). This step made the Hohenzollern family one of the most important in Germany.
Sigismund began to shift his alliance from France to England after the French defeat at the Battle of Agincourt. The Treaty of Canterbury (August 15, 1416) culminated diplomatic efforts between Henry V of England and Sigismund and resulted in a defensive and offensive alliance against France. This, in turn, led the way to the resolution of the papal schism.
Hussite Wars.
In 1419, the death of Wenceslaus IV left Sigismund titular King of Bohemia, but he had to wait for seventeen years before the Czechs would acknowledge him. Although the two dignities of King of the Romans and King of Bohemia added considerably to his importance, and indeed made him the nominal temporal head of Christendom, they conferred no increase of power and financially embarrassed him. It was only as King of Hungary that he had succeeded in establishing his authority and in doing anything for the order and good government of the land. Entrusting the government of Bohemia to Sofia of Bavaria, the widow of Wenceslaus, he hastened into Hungary.
The Bohemians, who distrusted him as the betrayer of Hus, were soon in arms; and the flame was fanned when Sigismund declared his intention of prosecuting the war against heretics. Three campaigns against the Hussites ended in disaster although the army of his most loyal ally Stibor of Stiboricz and later his son Stibor of Beckov could hold the hussite side away from the borders of the Kingdom. The Turks were again attacking Hungary. The king, unable to obtain support from the German princes, was powerless in Bohemia. His attempts at the diet of Nuremberg in 1422 to raise a mercenary army were foiled by the resistance of the towns; and in 1424 the electors, among whom was Sigismund's former ally, Frederick I of Hohenzollern, sought to strengthen their own authority at the expense of the king. Although the scheme failed, the danger to Germany from the Hussites led to the Union of Bingen, which virtually deprived Sigismund of the leadership of the war and the headship of Germany.
In 1428 he led another campaign against the Turks, but again with few results. In 1431 he went to Milan where on 25 November he received the Iron Crown; after which he remained for some time at Siena, negotiating for his coronation as emperor and for the recognition of the Council of Basel by Pope Eugenius IV. He was crowned emperor in Rome on 31 May 1433, and after obtaining his demands from the Pope returned to Bohemia, where he was recognized as king in 1436, though his power was little more than nominal. 
Shortly, after being crowned, Pope Eugenius begun attempts to create a new anti-Ottoman alliance. This was sparked by an Albanian revolt against the Ottomans, which had begun in 1432. In 1435, Sigismund sent Fruzhin, a Bulgarian nobleman, to negotiate an alliance with the Albanians. He also sent Daud, a pretender to the Ottoman throne, in early 1436. However, following the defeat of the rebels in 1436, plans for an anti-Ottoman alliance ended.
He died in 9 December 1437 at Znojmo (German: "Znaim"), Moravia (now Czech Republic), and as ordered in life, he was buried at Nagyvárad, Hungary (today Oradea, Romania), next to the tomb of the king Saint Ladislaus I of Hungary, who was the ideal of the perfect monarch, warrior and Christian for that time and was deeply venerated by Sigismund. By his second wife, Barbara of Celje, he left an only daughter, Elisabeth of Luxembourg, who was married to Albert V, duke of Austria (later German king as Albert II) whom Sigismund named as his successor. As he left no sons his line of the House of Luxembourg became extinct on his death.
Family and issue.
Sigismund married twice but had little luck in securing the succession to his crowns. Each of his two marriages resulted in the birth of one child. His first-born child, probably a son, was born prematurely as a result of a horse riding accident suffered by Queen Mary of Hungary when she was well advanced in pregnancy. Mother and child both died shortly after the birth in the hills of Buda on 17 May 1395. This caused a deep succession crisis because Sigismund ruled over Hungary by right of his wife, and although he managed to keep his power, the crisis lasted until his second marriage to Barbara of Celje. Barbara's only child, born in the purple on 7 October 1409, probably in the castle of Visegrád, was Elisabeth of Luxembourg, the future queen consort of Hungary, Germany and Bohemia. Queen Barbara was unable to give birth to any further issue. Elisabeth of Bohemia was thus the only surviving legitimate offspring of Sigismund.
Hungarian affiliations.
Sigismund was known to speak fluent Hungarian, wore Hungarian style royal clothes and even grew his beard as the Hungarians.
He also spent huge amounts of money during his reign to rebuild the Gothic castles of Buda and Visegrád in the Kingdom of Hungary, ordering the transportation of materials from Austria and Bohemia.
His many affairs with women led to the birth of several legends, as the one that existed decades later during the reign of the King Matthias Corvinus of Hungary. According to this, John Hunyadi was Sigismund's illegitimate son. Sigismund gave a ring to the boy's mother when he was born, but one day in the forest a raven stole it from her, and the ring was only recovered after the bird was hunted down. It is said that this incident inspired the coat of arms of the Hunyadis, and later also appeared in the coat of arms of Matthias "Corvinus".
Sigismund adopted the Hungarian reverence for Saint Ladislaus I of Hungary, who was considered to be an ideal Christian knight at that time. He went on pilgrimage several times to his tomb in Nagyvárad. Before Sigismund died, he ordered to be buried next to the king saint.
Reformatio Sigismundi.
The Reformatio Sigismundi appeared in connection with efforts to reform the Holy Roman Empire during the reign of Emperor Sigismund (1410–1437). It was presented in 1439 at the Council of Basle, published by an anonymous author, and referred to the injustice of the German rulers. It included a vision of Sigismund's about the appearance of a priest-king, Frederick, as well as plans for a wide reform of the monarchy (and emperorship) and the (German) empire.
Titles.
"Sigismund, by God's grace, Holy Roman Emperor; King of the Romans, of Hungary, Bohemia, Italy, Dalmatia, Croatia, Rama, Serbia, Galicia, Lodomeria, Cumania and Bulgaria; Prince of Silesia and Luxembourg; Margrave of Moravia, Lusatia and Brandenburg."
In popular culture.
Sigismund has been portrayed in several films by different actors.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="38922" url="http://en.wikipedia.org/wiki?curid=38922" title="Takakura">
Takakura

Takakura may refer to:

</doc>
<doc id="38924" url="http://en.wikipedia.org/wiki?curid=38924" title="1161">
1161

Year 1161 (MCLXI) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By area.
Europe.
</onlyinclude>

</doc>
<doc id="38939" url="http://en.wikipedia.org/wiki?curid=38939" title="Michelangelo Antonioni">
Michelangelo Antonioni

Michelangelo Antonioni, Cavaliere di Gran Croce OMRI (29 September 1912 – 30 July 2007), was an Italian film director, screenwriter, editor, and short story writer. Best known for his "trilogy on modernity and its discontents"—"L'Avventura" (1960), "La Notte" (1961), and "L'Eclisse" (1962)—Antonioni "redefined the concept of narrative cinema" and challenged traditional approaches to storytelling, realism, drama, and the world at large. He produced "enigmatic and intricate mood pieces" and rejected action in favor of contemplation, focusing on image and design over character and story. His films defined a "cinema of possibilities".
Antonioni received numerous awards and nominations throughout his career, including the Cannes Film Festival Jury Prize (1960, 1962), Palme d'Or (1966), and 35th Anniversary Prize (1982); the Venice Film Festival Silver Lion (1955), Golden Lion (1964), FIPRESCI Prize (1964, 1995), and Pietro Bianchi Award (1998); the Italian National Syndicate of Film Journalists Silver Ribbon eight times; and an honorary Academy Award in 1995.
Personal life.
Antonioni was born into a prosperous family of landowners in Ferrara, Emilia Romagna, in northern Italy. He was the son of Elisabetta (née Roncagli) and Ismaele Antonioni. The director explained to Italian film critic Aldo Tassone:
My childhood was a happy one. My mother ... was a warm and intelligent woman who had been a labourer in her youth. My father also was a good man. Born into a working-class family, he succeeded in obtaining a comfortable position through evening courses and hard work. My parents gave me free rein to do what I wanted: with my brother, we spent most of our time playing outside with friends. Curiously enough, our friends were invariably proletarian, and poor. The poor still existed at that time, you recognized them by their clothes. But even in the way they wore their clothes, there was a fantasy, a frankness that made me prefer them to boys of bourgeois families. I always had sympathy for young women of working-class families, even later when I attended university: they were more authentic and spontaneous.
While still a child, Antonioni was fond of drawing and music. A precocious violinist, he gave his first concert at the age of nine. Although he abandoned the violin with the discovery of cinema in his teens, drawing would remain a lifelong passion. "I have never drawn, even as a child, either puppets or silhouettes but rather facades of houses and gates. One of my favourite games consisted of organising towns. Ignorant in architecture, I constructed buildings and streets crammed with little figures. I invented stories for them. These childhood "happenings" - I was eleven years old - were like little films."
Upon graduation from the University of Bologna with a degree in economics, he started writing for the local Ferrara newspaper "Il Corriere Padano" in 1935 as a film journalist.
In 1940, Antonioni moved to Rome, where he worked for "Cinema", the official Fascist film magazine edited by Vittorio Mussolini. However, Antonioni was fired a few months afterward. Later that year he enrolled at the Centro Sperimentale di Cinematografia to study film technique, but left it after three months. He was drafted into the army afterwards. During the war Antonioni survived being condemned to death for his membership in the resistance. For a comprehensive account of Antonioni's war time biography see "Michelangelo Antonioni's 'L'eclisse.'" Endnote No. 43.
Antonioni died aged 94 on 30 July 2007 in Rome, the same day that another renowned film director, Ingmar Bergman, also died. Antonioni lay in state at City Hall in Rome where a large screen showed black-and-white footage of him among his film sets and behind-the-scenes. He was buried in his home town of Ferrara on 2 August 2007.
He was an atheist.
Career.
Early film work.
In 1942, Antonioni co-wrote "A Pilot Returns" with Roberto Rossellini and worked as assistant director on Enrico Fulchignoni's "I due Foscari". In 1943, he travelled to France to assist Marcel Carné on "Les visiteurs du soir" and then began a series of short films with "Gente del Po" (1943), a story of poor fishermen of the Po valley. After the Liberation, the film stock was stored in the East-Italian Fascist "Republic of Salò" and could not be recovered and edited until 1947 (the complete footage was never retrieved). These films were neorealist in style, being semi-documentary studies of the lives of ordinary people.
However, Antonioni's first full-length feature film "Cronaca di un amore" (1950) broke away from neorealism by depicting the middle classes. He continued to do so in a series of other films: "I vinti" ("The Vanquished", 1952), a trio of stories, each set in a different country (France, Italy and England), about juvenile delinquency; "La signora senza camelie" ("The Lady Without Camellias", 1953) about a young film star and her fall from grace; and "Le amiche" ("The Girlfriends", 1955) about middle class women in Turin. "Il grido" ("The Outcry", 1957) was a return to working class stories, depicting a factory worker and his daughter. Each of these stories is about social alienation.
International recognition.
In "Le Amiche" (1955), Antonioni experimented with a radical new style: instead of a conventional narrative, he presented a series of apparently disconnected events, and he used long takes as part of his film making style. Antonioni returned to their use in "L'avventura" (1960), which became his first international success. At the Cannes Film Festival it received a mixture of cheers and boos, but the film was popular in art house cinemas around the world. "La notte" (1961), starring Jeanne Moreau and Marcello Mastroianni, and "L'Eclisse" (1962), starring Alain Delon, followed "L'avventura". These three films are commonly referred to as a trilogy because they are stylistically similar and all concerned with the alienation of man in the modern world. "La notte" won the Golden Bear award at the 11th Berlin International Film Festival, His first color film, "Il deserto rosso" ("The Red Desert", 1964), deals with similar themes, and is sometimes considered the fourth film of the "trilogy". All of these films star Monica Vitti, his lover during that period.
Antonioni then signed a deal with producer Carlo Ponti that would allow artistic freedom on three films in English to be released by MGM. The first, "Blowup" (1966), set in Swinging London, was a major international success. The script was loosely based on the short story "The Devil's Drool" (otherwise known as "Blow Up") by Argentinian writer Julio Cortázar. Although it dealt with the challenging theme of the impossibility of objective standards and the ever-doubtable truth of memory, it was a successful and popular hit with audiences, no doubt helped by its sex scenes, which were explicit for the time. It starred David Hemmings and Vanessa Redgrave. The second film was "Zabriskie Point" (1970), his first set in America and with a counterculture theme. The soundtrack carried popular artists such as Pink Floyd (who wrote new music specifically for the film), the Grateful Dead and the Rolling Stones. However, its release was a critical and commercial disaster. The third, "The Passenger" (1975), starring Jack Nicholson and Maria Schneider, received critical praise, but also did poorly at the box office. It was out of circulation for many years, but was re-released for a limited theatrical run in October 2005 and has subsequently been released on DVD.
In 1972, in between "Zabriskie Point" and "The Passenger", Antonioni was invited by the Mao government of the People's Republic of China to visit the country. He made the documentary "Chung Kuo, Cina", but it was severely denounced by the Chinese authorities as "anti-Chinese" and "anti-communist". The documentary had its first showing in China on 25 November 2004 in Beijing with a film festival hosted by the Beijing Film Academy to honor the works of Michelangelo Antonioni.
Later career.
In 1980, Antonioni made "Il mistero di Oberwald" ("The Mystery of Oberwald"), an experiment in the electronic treatment of color, recorded in video then transferred to film, featuring Monica Vitti once more. It is based on Jean Cocteau's play "L'Aigle à deux têtes" ("The Eagle With Two Heads"). "Identificazione di una donna" ("Identification of a Woman", 1982), filmed in Italy, deals one more time with the recursive subjects of his Italian trilogy. In 1985, Antonioni suffered a stroke, which left him partly paralyzed and unable to speak. However, he continued to make films, including "Beyond the Clouds" (1995), for which Wim Wenders filmed some scenes. As Wenders has explained, Antonioni rejected almost all the material filmed by Wenders during the editing, except for a few short interludes. They shared the FIPRESCI Prize at the Venice Film Festival with "Cyclo".
In 1994 he was given the Honorary Academy Award "in recognition of his place as one of the cinema's master visual stylists." It was presented to him by Jack Nicholson. Months later, the statuette was stolen by burglars and had to be replaced. Previously, he had been nominated for Academy Awards for Best Director and Best Screenplay for "Blowup". Antonioni's final film, made when he was in his 90s, was a segment of the anthology film "Eros" (2004), entitled "Il filo pericoloso delle cose" ("The Dangerous Thread of Things"). The short film's episodes are framed by dreamy paintings and the song "Michelangelo Antonioni", composed and sung by Caetano Veloso. However, it was not well-received internationally; in America, for example, Roger Ebert claimed that it was neither erotic nor about eroticism. The U.S. DVD release of the film includes another 2004 short film by Antonioni, "Lo sguardo di Michelangelo" ("The Gaze of Michelangelo").
Reception.
Film historian Virginia Wright Wexman describes Antonioni's perspective on the world as that of a "postreligious Marxist and existentialist intellectual." In a speech at Cannes about "L'Avventura", Antonioni said that in the modern age of reason and science, mankind still lives by "a rigid and stereotyped morality which all of us recognize as such and yet sustain out of cowardice and sheer laziness". He said his films explore the paradox that "we have examined those moral attitudes very carefully, we have dissected them and analyzed them to the point of exhaustion. We have been capable of all this, but we have not been capable of finding new ones." Nine years later he expressed a similar attitude in an interview, saying that he loathed the word 'morality': "When man becomes reconciled to nature, when space becomes his true background, these words and concepts will have lost their meaning, and we will no longer have to use them."
One of the recurring themes in Antonioni's films is characters who suffer from ennui and whose lives are empty and purposeless aside from the gratification of pleasure or the pursuit of material wealth. Film historian David Bordwell writes that in his films, "Vacations, parties and artistic pursuits are vain efforts to conceal the characters' lack of purpose and emotion. Sexuality is reduced to casual seduction, enterprise to the pursuit of wealth at any cost." Antonioni's films tend to have spare plots and dialogue, and much of the screen time is spent lingering on certain settings, such as the seven-minute continuous take at the end of "The Passenger" and the beautiful long-take near the beginning that "mixes time", or the scene in "L'Eclisse" in which Monica Vitti stares curiously at electrical posts accompanied by ambient sounds of wires clanking. Virginia Wright Wexman summarizes his style in the following terms: "The camera is placed at a medium distance more often than close in, frequently moving slowly; the shots are permitted to extend uninterrupted by cutting. Thus each image is more complex, containing more information than it would in a style in which a smaller area is framed ... In Antonioni's work we must regard his images at length; he forces our full attention by continuing the shot long after others would cut away." Antonioni is also noted for exploiting colour as a significant expressive element of his cinematic style, especially in "Il deserto rosso", his first colour film.
Bordwell explains that Antonioni's films were extremely influential on subsequent art films: "More than any other director, he encouraged filmmakers to explore elliptical and open-ended narrative". Film director Akira Kurosawa considered Antonioni one of the most interesting filmmakers. Stanley Kubrick listed "La Notte" as one of his ten favorite films in a 1963 Poll. Andrei Tarkovsky also listed Antonioni as one of his favorite filmmakers. Miklós Jancsó considers Antonioni as his master.
Antonioni's spare style and purposeless characters, however, have not received universal acclaim. Ingmar Bergman stated in 2002 that he considered some of Antonioni's films, including "Blowup" and "La notte", masterpieces for their detached and dreamlike quality, but found the other films boring and noted that he had never understood why Antonioni was held in such esteem. Orson Welles regretted the Italian director's use of the long take: "I don't like to dwell on things. It's one of the reasons I'm so bored with Antonioni - the belief that, because a shot is good, it's going to get better if you keep looking at it. He gives you a full shot of somebody walking down a road. And you think, 'Well, he's not going to carry that woman all the way up that road.' But he "does". And then she leaves and you go on looking at the road after she's gone."
American actor Peter Weller, whom Antonioni directed in "Beyond the Clouds", explained in a 1996 interview: "There is no director living except maybe Kurosawa, Bergman, or Antonioni that I would fall down and do anything for. I met Antonioni three years ago in Taormina at a film festival. I introduced myself and told him that I adored his movies, his contributions to film, because he was the first guy who really started making films about the reality of the vacuity between people, the difficulty in traversing this space between lovers in modern day ... and he never gives you an answer, Antonioni – that's the beautiful thing."
References.
Bibliography.
</dl>
Further reading.
</dl>
Documentaries.
</dl>

</doc>
<doc id="38957" url="http://en.wikipedia.org/wiki?curid=38957" title="Snowboarding">
Snowboarding

Snowboarding is a recreational activity that involves descending a slope that is covered with snow while standing on a snowboard attached to a rider's feet. The snowboard is attached using a special boot set into a mounted binding.
The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing. It was developed in the United States in the 1960s by Gordon Kosteroski and became a Winter Olympic Sport in 1998. Its popularity (as measured by equipment sales) peaked in 2007 and has been in a decline since.
History.
The first evidence of snowboarding dates back as early as the 1910s, when people would tie plywood or wooden planks from logs to their feet using fishing string and horse reins in order to steer themselves down hills.
Modern snowboarding began in 1965 when Sherman Poppen, an engineer in Muskegon, Michigan, invented a toy for his daughters by fastening two skis together and attaching a rope to one end so they would have some control as she stood on the board and glided downhill. Dubbed the "snurfer" (combining snow and surfer) by his wife Nancy, the toy proved so popular among his daughters' friends that Poppen licensed the idea to a manufacturer, Brunswick Corporation, that sold about a million snurfers over the next decade. And, in 1966 alone over half a million snurfers were sold.
In the early 1970s, Poppen organized snurfing competitions at a Michigan ski resort that attracted enthusiasts from all over the country. One of those early pioneers was Tom Sims, a devotee of skateboarding (a sport born in the 1950s when kids attached roller skate wheels to small boards that they steered by shifting their weight). As an eighth grader in Haddonfield, New Jersey, in the 1960s, Sims crafted a snowboard in his school shop class by gluing carpet to the top of a piece of wood and attaching aluminum sheeting to the bottom. He produced commercial snowboards in the mid 70s and it became an olympic sport in 1998. During this same time, Dimitrije Milovich—an American surfing enthusiast who had also enjoyed sliding down snowy hills on cafeteria trays during his college years in upstate New York—constructed a snowboard called "Winterstick," inspired by the design and feel of a surfboard. Articles about his invention in such mainstream magazines as "Newsweek" helped publicize the young sport.
Also during this same period, in 1977, Jake Burton Carpenter, a Vermont native who had enjoyed snurfing since the age of 14, impressed the crowd at a Michigan snurfing competition with bindings he had designed to secure his feet to the board. That same year, he founded Burton Snowboards in Londonderry, Vermont. The "snowboards" were made of wooden planks that were flexible and had water ski foot traps. Very few people picked up snowboarding because the price of the board was considered too high at $38, but eventually Burton would become the biggest snowboarding company in the business. In the spring of 1976 Welsh skateboarders Jon Roberts and Pete Matthews developed a Plywood deck with foot bindings for use on the Dry Ski Slope at the school camp, Ogmore-by-Sea, Wales. UK. Further development of the board was limited as Matthews suffered serious injury while boarding at Ogmore and access for the boarders declined following the incident. The 'deck' was much shorter than current snow boards. Bevelled edges and a convex, polyurethane varnished bottom to the board, allowed quick downhill movement, but limited turning ability.
The first competitions to offer prize money were the National Snurfing Championship, held at Muskegon State Park in Muskegon Michigan. In 1979, Jake Burton Carpenter, came from Vermont to compete with a snowboard of his own design. There were protests about Jake entering with a non-snurfer board. Paul Graves, and others, advocated that Jake be allowed to race. A "modified" "Open" division was created and won by Jake as the sole entrant. That race was considered the first competition for snowboards and is the start of what has now become competitive snowboarding. It was also the first competition to offer prize money. Ken Kampenga, John Asmussen and Jim Trim placed 1st, 2nd and 3rd respectively in the Standard competition with best 2 combined times of 24.71, 25.02 and 25.41 and Jake Carpenter won prize money as the sole entrant in the "open" division with a time of 26.35. In 1980 the event moved to Pando Winter Sports Park near Grand Rapids, Michigan because of a lack of snow that year at the original venue.
As snowboarding became more popular in the 1970s and 1980s, pioneers such as Dimitrije Milovich, Jake Burton Carpenter (founder of Burton Snowboards from Londonderry, Vermont), Tom Sims (founder of Sims Snowboards), Chuck Barfoot (founder of Barfoot Snowboards) and Mike Olson (founder of Gnu Snowboards) came up with new designs for boards and mechanisms that slowly developed into the snowboards and other related equipment that we know today.
In 1982, the first USA National Snowboard race was held near Woodstock, Vermont, at Suicide Six. The race, organized by Graves, was won by Burton's first team rider Doug Bouton.
In 1983, the first World Championship halfpipe competition was held at Soda Springs, California. Tom Sims, founder of Sims Snowboards, organized the event with the help of Mike Chantry, a snowboard instructor at Soda Springs.
In 1985, the first World Cup was held in Zürs, Austria, further cementing snowboarding's recognition as an official international competitive sport.
In 1990, the International Snowboard Federation (ISF) was founded to provide universal contest regulations. In addition, the United States of America Snowboard Association (USASA) provides instructing guidelines and runs snowboard competitions in the U.S. today, high-profile snowboarding events like the Winter X Games, Air & Style, US Open, Olympic Games and other events are broadcast worldwide. Many alpine resorts have terrain parks.
At the 1998 Winter Olympic Games in Nagano, Japan, Snowboarding became an official Olympic event. Canadian Ross Rebagliati was the first ever to win an Olympic gold medal for Men's Snowboarding at the 1998 Winter Olympics. After winning the gold, he was found to have Tetrahydrocannabinol (THC) in his circulatory system following a blood test and he was automatically disqualified. This decision was eventually overturned, largely on the basis that marijuana was not on the list of banned substances, and Rebagliati was given back the medal. THC has since been listed by the World Anti-Doping Agency (WADA) as a banned substance.
Initially, ski areas adopted the sport at a much slower pace than the winter sports public. Indeed, for many years, there was animosity between skiers and snowboarders, which led to an ongoing skier vs snowboarder feud. Early snowboards were banned from the slopes by park officials. For several years snowboarders would have to take a small skills assessment prior to being allowed to ride the chairlifts. It was thought that an unskilled snowboarder would wipe the snow off the mountain. In 1985, only seven percent of U.S. ski areas allowed snowboarding, with a similar proportion in Europe. As equipment and skills improved, gradually snowboarding became more accepted. In 1990, most major ski areas had separate slopes for snowboarders. Now, approximately 97% of all ski areas in North America and Europe allow snowboarding, and more than half have jumps, rails and half pipes.
An excellent year for snowboarding was 2004, with 6.6 million participants. An industry spokesman said that "twelve year-olds are out-riding adults." The same article said that most snowboarders are 18–24 years old and that females constitute 25% of participants. Now, entering the second decade of the 2000s, snowboarding continues to increase in popularity among all demographic regimes regardless of age, sex, or ability levels.
There were 8.2 million snowboarders in the USA and Canada for the 2009-2010 season. There was a 10% increase over the previous season, accounting for more than 30% of all snow sports participants.
On 2 May 2012, the International Paralympic Committee announced that adaptive snowboarding (dubbed "para-snowboarding") would debut as a men's and women's medal event in the 2014 Paralympic Winter Games taking place in Sochi, Russia.
Styles.
Since snowboarding's inception as an established winter sport, it has developed various styles, each with its own specialized equipment and technique. The most common styles today are: freeride, freestyle, and freecarve/race. These styles are used for both recreational and professional snowboarding. While each style is unique, there is overlap between them. "See also List of snowboard tricks."
Jibbing.
"Jibbing" is technical riding on non-standard surfaces, which usually includes performing tricks. The word "jib" is both a noun and a verb, depending on the usage of the word. As a noun: a jib includes metal rails, boxes, benches, concrete ledges, walls, vehicles, rocks and logs. As a verb: to jib is referring to the action of jumping, sliding or riding on top of objects other than snow. It is directly influenced by grinding a skateboard. Jibbing is a freestyle snowboarding technique of riding. Typically jibbing occurs in a snowboard resort park but can also be done in urban environments.
Freeride snowboarders also commonly find incidental jibs, such as a downed tree, that prove suitable to ride over in the course of their line or run.
Freeriding.
Freeriding communicates the concept of dynamically altering various snowboarding styles in a fluid motion, such that the combination becomes a style unto itself.
Freeriding incorporates various aspects of snowboarding into a style that adapts to the variations and challenges of natural terrain, and eschews man-made features such as jumps, rails, half-pipes, or groomed snow. To master freeriding is to seamlessly merge these aspects of other snowboarding disciplines such as freestyle and alpine snowboarding into an all-around style - giving you the freedom to make the most of whatever terrain comes your way. Whereas freestyle snowboarding relies on the use of man-made terrain such as jumps, rails and half-pipes, and alpine snowboarding is done on groomed snow - the focus of freeriding is on utilising the random flow of natural terrain. Although similar tricks may be performed in freeride competition as in freestyle competition, the major defining difference is that utilises natural terrain, not man-made features such as the terrain parks used in slopestyle competition.
Freeride equipment generally comprises a stiffer boot/binding combination and a stiffer, directional snowboard. Since the freeride style may encounter many different types of snow conditions, such as ice and deep powder, a stiffer setup is recommended to maintain stability in deeper snow and at higher speeds.
Freestyle.
Freestyle snowboarding is any riding that includes performing tricks. In freestyle, the rider utilizes natural and man-made features such as rails, jumps, boxes, and innumerable others to perform tricks. It is a popular all-inclusive concept that distinguishes the creative aspects of snowboarding, in contrast to a style like alpine snowboarding.
Man made features are sometimes made to resemble street riding conditions such as a metal handrail and concrete staircase. The term "box" refers to an object with a slick top, usually of polyethylene(HDPE) plastic, that the rider can slide on with the base of their board. Like big freestyle features, boxes come in a variety of shapes, sizes, and difficulty levels. The intent of freestyle is to use these features to perform a number of aerial or jib tricks. This most commonly refers to tricks done on boxes, rails, or even trees.
The equipment used in freestyle is usually a soft boot with a twin tipped board for better balance while riding regular or switch, though free-ride equipment is often used successfully. The most common binding stance used in freestyle is called "duck foot", in which the trailing foot has a negative degree of arc setup while the leading foot is in the positive range i.e. +12°/-9°. Freestyle riders who specialize in jibbing often use boards that are shorter than usual, with softer flex and filed down edges. Shorter length enables the board to be rotated faster, and a softer flex requires less energy for a rider to press a feature. Reverse camber boards, or better known as rocker boards, are most often used as freestyle boards due to their softer flex and inverted 'camber' design. Pressing refers to a type of jib where the rider leans heavily toward the nose or tail of their board- causing the opposite end of their board to lift off of the feature they are sliding on. This trick is typically done for added style.
Freestyle also includes halfpipe tricks. A halfpipe (or "pipe") is a trench-like half-tube made of snow. Tricks performed may be rotations such as a 360° (a full turn) in the air, or an off-axis spin like a "McTwist". Tricks can be modified while hitting different features.
Alpine Snowboarding.
Sometimes called freecarving, this takes place on hard packed snow or groomed runs and focuses on carving linked turns, much like surfing or longboarding. Little or no jumping takes place in this discipline. Alpine Snowboarding consists of a small portion of the general snowboard population, that has a well connected social community and its own specific board manufacturers. Alpine Snowboard equipment is a ski-like hardshell boot and plate binding system with a true directional snowboard that is stiffer and narrower to manage linking turns with greater forces and speed. Shaped skis can thank these "freecarve" snowboards for the cutting-edge technology leading to their creation. Highlights of alpine snowboarding includes a unique sensation felt through each carved turn. A skilled alpine snowboarder can link numerous turns into a run placing their body very close to the ground each turn, similar to a motogp turn or waterski carve. Depending on factors including stiffness, turning radius and personality this can be done slowly or fast.
Carvers make perfect half-circles out of each turn, changing edges when the snowboard is perpendicular to the fall line and starting every turn on the downhill edge. Carving on a snowboard is like riding a roller coaster, because the board will lock into a turn radius and provide what feels like multiple Gs of acceleration.
Slopestyle.
Competitors perform tricks while descending a course, moving around, over, across, up, or down terrain features. The course is full of obstacles including boxes, rails, jumps, jibs (includes anything the board or rider can slide across).
Slope-style contests consists of choosing your own line in a terrain park using a variety of boxes, jibs and jumps. To win a slope-style contest one must pick the best and most difficult line in the terrain park and have a smooth flowing line of tricks performed on the obstacles. Overall impression is also a huge factor in winning a slope-style contest. The rider who lands the hardest tricks will not always win over the rider who lands easier tricks.
Big air.
Big air competitions are contests where riders perform tricks after launching off a man made jump built specifically for the event. Competitors perform tricks in the air, aiming to attain sizable height and distance, all while securing a clean landing. Many competitions also require the rider to do a trick to win the prize. Not all competitions call for a trick to win the gold; some intermittent competitions are based solely on height and distance of the launch of the snowboarder. One of the first snowboard competitions where Travis Rice attempted and landed a "double back flip backside 180" took place at the 2006 Red Bull Gap Session.
Half-pipe.
The half-pipe is a semi-circular ditch dug into the mountain or purpose built ramp made up of snow, with walls between 8 and 23 ft. Competitors perform tricks while going from one side to the other and while in the air above the sides of the pipe.
Boardercross.
Boardercross, also known as "Boarder X" and "Snowboard X", is a very popular but relatively recent winter sport, starting in the 1980s and earning its place as an official Winter Olympic sport in the 2006 Turin games. In Boardercross, several riders (usually 4 to 6) race down a course similar to a motorcycle motocross track (with jumps, berms and other obstacles constructed out of snow on a downhill course). Unlike traditional head-to-head races, competitors use the same terrain, sometimes resulting in accidental collisions.
Competitions involve a series of heats, traditionally with the first 2 riders in each heat advancing to the next round. The overall winner is the rider that finishes first in the final round. Due to the head-to-head nature of the sport and the constant possibility of collisions, it is touted as one of the most volatile and unpredictable of the Olympic snow sports.
Big mountain and freeride.
A big mountain contest is one that takes place in open terrain, and challenges riders to find their way down the mountain with the most style and difficulty. Big mountain events usually take place in powder snow conditions in closed off areas of resorts or in the backcountry. There are a number of big mountain events in Europe, the United States and in New Zealand and this aspect of snowboarding competition is quickly rising in popularity. Snowboarders consider Alaska the pinnacle of this style of riding, being featured in some of the most popular snowboarding videos and has given rise to one of the sport's most popular events, Tailgate Alaska, a yearly gathering of riders on Alaska's Thompson Pass.
Rail jam.
A rail jam is a jib contest. Riders perform tricks on rails, boxes, pipes, wall rides, and several other creative features. Rail jams are done in a small area, usually with two or three choices of features for the rider to hit on a run. They are sometimes done in an urban setting, due to the relatively small amount of snow required. Scoring is done in the "jam" format, where every rider can take as many runs as time allows, usually around an hour; prizes are typically awarded for best overall and best trick in the male and female category.
Snowboard Racing.
In Snowboarding Racing, riders must complete a downhill course constructed of a series of turning indicators (gates) placed in the snow at prescribed distances apart. A gate consists of a tall pole, and a short pole, connected by a triangular panel. The racer must pass around the short side of the gate. There are 3 main formats used in snowboard racing including; single person, parallel courses or multiple people on the course at the same time (SBX).
Olympic Snowboard Racing Disciplines include Parallel Giant Slalom (PGS) and Parallel Slalom (PSL). Additional Snowboard Race include; Giant Slalom, Slalom, Triple Slalom, Super G and Banked Slalom.
Parallel slalom, boarders race downhill through sets of gates that force extremely tight and quick turns requiring plenty of technical skill while racing against an opponent in the other course.
Parallel Giant slalom uses a much longer course with gates set further apart resulting in even higher speeds, while racing against an opponent on a similar course place parallel to the other course.
Super G is the fastest of all, with speeds of up to 45 mi/h.
Competitions.
Some of the larger snowboarding contests include: the Air & Style, the X-Trail Jam, Burton Global Open Series, Shakedown, FIS world championships, FIS World Cup, the Winter X Games and the Winter Dew Tour.
The TTR world snowboard tour is the largest culmination of independent freestyle events acting under one common Tour Flag. Officially recognized as the World Snowboard Tour, this culmination of Independent Freestyle Snowboard events has grown substantially over the last four years. Now in its seventh year, the TTR has a 10-month competition season including snowboarding events over four geographical zones. The Tour includes events like the TTR 6Star Air & Style, The Arctic Challenge and the US Open of Snowboarding. In 2012, ESPN's X-Games joined the World Snowboard Tour.
Snowboarder Magazine's Superpark event was created in 1996. Over 150 of the World's top pros are invited to advance freestyle snowboarding on the most progressive terrain parks.
Tailgate Alaska is one of the sports most recognized and popular events. It is a backcounty gathering in Valdez, Alaska where riders challenge themselves in the worlds best mountains and snow conditions. It is a ten-day festival held every March–April. Competition in Alaska was also brought back by Tailgate Alaska founder Mark Sullivan with the World Freeride Championships and is considered the top freeriding event in the world. More recently a new competition has hit the snowboarding scene. popular snowboarder Travis rice coordinated with RedBull to create Redbull Supernatural, an original contest held in backcountry in which competitors test there overall snowboarding skills by throwing tricks off of man made obstacles. What makes supernatural different is the fact that competitors get no practice runs and the line a rider takes is completely up to the rider. 
Part of the snowboarding approach is to ensure maximum fun, friendship and event quality. Reflecting this perspective of snowboarding, you can find "Anti Contests" including are an important part of its identity including The Holy Oly Revival at The Summit at Snoqualmie, The Nate Chute Hawaiian Classic at Whitefish, the original anti-contst, the World Quarterpipe Championships and the Grenade Games.
One of the more unique and legendary contests is the Mt. Baker Legendary Banked Slalom. Since 1985, it has been won by some of the biggest names in the history of the sport and continues to be an event that attracts the top riders from around the world. Terje Håkonsen and Karleen Jeffery are the riders that have won the most in the race with six wins each.
The United States of America Snowboarding Association (USASA) features three different divisions which include alpine, freestyle, and boardercross. Alpine consists of giant slalom and slalom which is a competition in which the agility and ability to make sharp turns of the snowboarders are tested. Freestyle consists of slopestyle and halfpipe. In boardercross, the idea is to be the first snowboarder down the mountain where everyone is racing each other through an obstacle course of harsh turns and wipeout potential is very likely. The USASA has 36 regional snowboard series in which anyone can compete.
Subculture.
The snowboarding way of life came about as a natural response to the culture from which it emerged. Early on, there was a rebellion against skiing culture and the view that snowboarders were inferior. Skiers did not easily accept this new culture on their slopes. The two cultures contrasted each other in several ways including how they spoke, acted, and their entire style of clothing. Snowboarders first embraced the punk and later the hip-hop look into their style. Words such as "dude", "gnarly", and "Shred the Gnar" are some examples of words used in the snowboarding culture. Snowboarding subculture became a crossover between the urban and suburban styles on snow, which made an easy transition from surfing and skateboarding culture over to snowboarding culture.
The early stereotypes of snowboarding included "lazy", "grungy", "punk", "stoners", "troublemakers", and numerous others, many of which are associated with skateboarding and surfing as well. However, these stereotypes may be considered "out of style". Snowboarding has become a sport that encompasses a very diverse international based crowd and fanbase of many millions, so much so that it is no longer possible to stereotype such a large community. Reasons for these dying stereotypes include how mainstream and popular the sport has become, with the shock factor of snowboarding's quick take off on the slopes wearing off. Skiers and snowboarders are becoming used to each other, showing more respect to each other on the mountain. "The typical stereotype of the sport is changing as the demographics change".
Countries with strong snowboarding subcultures and many local riders include Australia, Austria, Canada, Poland, Chile, Finland, France, Germany, Greece, Japan, the Netherlands, New Zealand, Norway, Russia, Slovenia, Sweden, Switzerland, the United Kingdom, and the United States. In the US, snowboarding culture thrives in the communities of Alaska, the Pacific Northwest, parts of New England, Colorado, Utah, and California. In Canada, snowboarding is popular in the provinces of British Columbia, Alberta, Quebec, Nova Scotia, Ontario, and New Brunswick.
Safety and precautions.
Like some other winter sports, snowboarding comes with a certain level of risk.
The injury rate for snowboarding is about four to six per thousand persons per day, which is around double the injury rate for alpine skiing. Injuries are more likely amongst beginners, especially those who do not take lessons with professional instructors. A quarter of all injuries occur to first-time riders and half of all injuries occur to those with less than a year of experience. Experienced riders are less likely to suffer injury, but the injuries that do occur tend to be more severe.
Two thirds of injuries occur to the upper body and one third to the lower body. This contrasts with alpine skiing where two thirds of injuries are to the lower body. The most common types of injuries are sprains, which account for around 40% of injuries. The most common point of injury is the wrists – 40% of all snowboard injuries are to the wrists and 24% of all snowboard injuries are wrist fractures. There are around 100,000 wrist fractures worldwide among snowboarders each year. For this reason the use of wrist guards, either separate or built into gloves, is very strongly recommended. They are often compulsory in beginner's classes and their use reduces the likelihood of wrist injury by half. In addition it is important for snow boarders to learn how to fall without stopping the fall with their hand by trying to "push" the slope away, as landing a wrist which is bent at a 90 degree angle increase the chance of it breaking. Rather, landing with the arms stretched out (like a wing) and slapping the slope with the entire arm is an effective way to break a fall. This is the method used by practitioners of Judo and other martial arts to break a fall when they are thrown against the floor, by a training partner.
The risk of head injury is two to six times greater for snowboarders than for skiers and injuries follow the pattern of being rarer, but more severe, with experienced riders. Head injuries can occur both as a consequence of a collision and when failing to carry out a heel-side turn. The latter can result in the rider landing on his or her back and slamming the back of his or her head onto the ground, resulting in an occipital head injury. For this reason, helmets are widely recommended. Protective eyewear is also recommended as eye injury can be caused by impact and snow blindness can be a result of exposure to strong ultra-violet light in snow-covered areas. The wearing of ultra-violet-absorbing goggles is recommended even on hazy or cloudy days as ultra-violet light can penetrate clouds.
Unlike ski bindings, snowboard bindings are not designed to release automatically in a fall. The mechanical support provided by the feet being locked to the board has the effect of reducing the likelihood of knee injury – 15% of snowboard injuries are to the knee, compared with 45% of all skiing injuries. Such injuries are typically to the knee ligaments, bone fractures are rare. Fractures to the lower leg are also rare but 20% of injuries are to the foot and ankle. Fractures of the talus bone are rare in other sports but account for 2% of snowboard injuries – a lateral process talus fracture is sometimes called "snowboarder's ankle" by medical staff. This particular injury results in persistent lateral pain in the affected ankle yet is difficult to spot in a plain X-ray image. It may be misdiagnosed as just a sprain, with possibly serious consequences as not treating the fracture can result in serious long-term damage to the ankle. The use of portable ultrasound for mountainside diagnostics has been reviewed and appears to be a plausible tool for diagnosing some of the common injuries associated with the sport.
Four to eight percent of snowboarding injuries take place while the person is waiting in ski-lift lines or entering and exiting ski lifts. Snowboarders push themselves forward with a free foot while in the ski-lift line, leaving the other foot (usually that of the lead leg) locked on the board at a 9–27 degree angle, placing a large torque force on this leg and predisposing the person to knee injury if a fall occurs. Snowboard binding rotating devices are designed to minimize the torque force, Quick Stance being the first developed in 1995. They allow snowboarders to turn the locked foot straight into the direction of the tip of the snowboard without removing the boot from the boot binding.
Avalanches are a clear danger when on snowy mountain slopes.
It is best to learn the different kinds of avalanches, how to prevent causing one and how to react when one is going to happen. Also when going out onto the snow, all who practice an activity with increased chances of injury should have a basic First Aid knowledge and know how to deal with injuries that may occur.
Snowboarding boots should be well-fitted, with toes snug in the end of the boot to minimize movement. Padding or "armor" is recommended on other body parts such as hips, knees, spine, and shoulders. To further help avoid injury to body parts, especially knees, it is recommended to use the right technique. To acquire the right technique, one should be taught by a qualified instructor. Also, when snowboarding alone, precaution should be taken to avoid tree wells, a particularly dangerous area of loose snow that may form at the base of trees.
Some care is also required when waxing a board as fluorocarbon waxes emit toxic fumes when overheated. Waxing is best performed in a ventilated area with care being taken to use the wax at the correct temperature – the wax should be melted but not smoking or smoldering.
In a study conducted to examine the types of snowboarding injuries and changes in injury patterns over time, data was collected on injured snowboarders and skiers in a base-lodge clinic of a ski resort in Vermont over 18 seasons (1988–2006) and included extensive information about injury patterns, demographics, and experience. In conclusion of the study, the highest rate of injury was among young, inexperienced, female snowboarders. Injury rates in snowboarders have fluctuated over time but still remain higher than skiers. No evidence was found that those who spend more time in terrain parks are over represented in the injury population.
Media.
Films.
Snowboarding films have become a main part of progression in the sport. Each season, many films are released, usually in Autumn. These are made by many snowboard specific video production companies as well as manufacturing companies that use these films as a form of advertisement. Snowboarding videos usually contain video footage of professional riders sponsored by companies. An example of commercial use of snowboarding films would be "The White Album", a film by snowboarding legend and filmmaker Dave Seoane about Shaun White, that includes cameos by Tony Hawk and was sponsored by PlayStation, Mountain Dew and Burton Snowboards. Snowboarding films are also used as documentation of snowboarding and showcasing of current trends and styles of the sport.
However, sometimes the snowboarding industry is not supportive of all snowboarding-themed films. In 2013, "The Crash Reel", a feature-length documentary by filmmaker Lucy Walker about former Shaun White rival Kevin Pearce, premiered on the film festival circuit to critical acclaim and was subsequently broadcast on HBO. Using Pearce's career-ending traumatic brain injury and subsequent recovery as a backdrop, the film examines the physical dangers inherent to pro snowboarders and other extreme sports professional athletes under pressure by sponsors and the media to perform increasingly spectacular feats. Although there are significant references to various brands in the film, Walker is "adamant" that the snowboarding industry did not sponsor the film in any way and in fact has been unsupportive, despite the film's mainstream media success.
Magazines.
Snowboard magazines are integral in promoting the sport, although less so with the advent of the internet age. Photo incentives are written into many professional riders' sponsorship contracts giving professionals not only a publicity but a financial incentive to have a photo published in a magazine. Snowboard magazine staff travel with professional riders throughout the winter season and cover travel, contests, lifestyle, rider and company profiles, and product reviews. Snowboard magazines have recently made a push to expand their brands to the online market, and there has also been a growth in online-only publications. Popular magazines include Kronicle (USA), Transworld Snowboarding (USA), Snowboarder Magazine (USA), Snowboard Magazine (USA), snowboarderMBM (Germany), Yobeat, (USA) Whitelines (UK), Pleasure (Germany), Method (Europe), Onboard (Europe), Whiteroom Magazine (BG), Snowboard Canada (Canada), NZ Snowboarder, (New Zealand) and Snowboard Colorado, (USA).
Video games.
Snowboarding video games provide interactive entertainment on and off season. Most games for this genre have been made for consoles, such as the Xbox and PlayStation. A plethora of online casual snowboarding games also exist along with games for mobile phone. most recently Snowboard simulators have been implemented. as a way to practice on the off season or a way to learn all together.

</doc>
<doc id="38985" url="http://en.wikipedia.org/wiki?curid=38985" title="Waterskiing">
Waterskiing

Water skiing is a surface water sport in which an individual is pulled behind a boat or a cable ski installation over a body of water, skimming the surface on two skis or one (slalom) ski. The sport requires sufficient area on a smooth stretch of water, one or two skis, a tow boat with tow rope, three people, and a personal flotation device. In addition, the skier must have adequate upper and lower body strength, muscular endurance, and good balance. Skiing is a fun pastime that allows people of all skill levels and ages to enjoy. There is no minimum age necessary to waterski.
There are water ski participants around the world, in Asia and Australia, Europe, Africa, and the Americas. In the United States alone, there are approximately 11 million water skiers and over 900 sanctioned waterski competitions every year. Australia boasts 1.3 million water skiers.
There are many options for recreational or competitive waterskiers. These include speed skiing, trick skiing, show skiing, slaloming, jumping, and barefoot skiing. Similar, related sports are wakeboarding, kneeboarding, discing, tubing, and sit-down hydrofoil.
Basic technique.
Water skiers can start their ski round in one of two ways: wet is the most common, but dry is possible. Water skiing typically begins with a deep water start. The skier enters the water with their skis on or they jump in without the skis on their feet, have the skis floated to them, and put them on while in the water. Most times it can be easier to put the skis on when they are wet. Once the skier has their skis on they will be thrown a tow rope from the boat, which they position between their skis. In the deep water start, the skier crouches down in the water while holding onto the ski rope; they are leaning back with their legs tucked into their chests, with skis pointing towards the sky and approximately 30 cm of the ski out of the water. The skier can also perform a "dry start" by standing on the shore or a pier; however, this type of entry is recommended for professionals only. When the skier is ready (usually acknowledged by them yelling "go"), the driver accelerates the boat. As the boat accelerates and takes up the slack on the rope, the skier allows the boat to pull him/her out of the water by applying some muscle strength to get him/her into an upright body position.
By leaning back and keeping the legs slightly bent, the skis will eventually plane out and the skier will start to glide over the water. The skier turns by shifting weight left or right. The skier's body weight must be balanced behind the feet or else the boat's force will pull the skier forward out of the skis. While being towed, the skier's arms should be slightly bent but still fully extended so as to reduce stress on the arms. The handle can be held vertically or horizontally, depending on whichever position is more comfortable for the skier.
In addition to the driver and the skier, a third person known as the "spotter" or the "observer" should be present. The spotter's job is to watch the skier and inform the driver if the skier falls. The spotter usually sits in a chair on the boat facing backwards to see the skier. The skier and the boat's occupants communicate using hand signals (see the Safety section below).
Equipment.
Water.
Water skiing can take place on any type of water – such as a river, lake, or ocean – but calmer waters are ideal for recreational skiing.
There should be a 200 ft skiing space and the water should be at least 5 to deep. There must be enough space for the waterskier to safely "get up," or successfully be in the upright skiing position. Skiers and their boat drivers must also have sufficient room to avoid hazards.
Skis.
Younger skiers generally start out on children's skis, which consist of two skis tied together at their back and front. These connections mean that less strength is necessary for the child to keep the skis together. Sometimes these skis can come with a handle to help balance the skier as well. Children's skis are short – usually 45 - long – reflecting the skier's smaller size. Once a person is strong enough to hold the skis together themselves there are various options depending upon their skill level and weight.
Water skiers can use two skis (one on each foot, also called "combo skiing") or one ski (dominant foot in front of the other foot, also called "slalom skiing"). Generally the heavier the person, the bigger the skis will be. Length will also vary based on the type of waterskiing being performed; slalom skis, for example, are longer than skis used in regular straight-line recreational skiing.
Boat.
Competition skiing uses specifically designed towboats. Most towboats have a very small hull and a flat bottom to minimize wake. A true tournament ski boat will have a direct drive motor shaft that centers the weight in the boat for an optimal wake shape. However, some recreational ski boats will have the motor placed in the back of the boat (v-drive), which creates a bigger wake. Permitted towboats used for tournament water skiing are the Mastercraft ProStar 197, Mastercraft ProStar 190, Nautique 200, Malibu Response TXi, and Centurion Carbon Pro. These boats have ability to pull skiers for trick skiing, jumping, and slalom.
Recreational boats can serve as water skiing platforms as well as other purposes such as cruising and fishing. Popular boat types include bowriders, deckboats, cuddy cabins, and jetboats.
The towboat must be capable of maintaining the proper speed. Speeds vary with the skier's weight, experience level, comfort level, and type of skiing. For example, a child on two skis would require speeds of 13 -, whereas an adult on one ski might require as high as 36 mph. Barefoot skiing requires speeds of approximately 72 km/h. Competition speeds have a wide range: as slow as 22 km/h up to 58 km/h for slalom water skiing, and approaching 190 km/h in water ski racing.
The boat must be equipped with a ski rope and handle. The tow rope must be sufficiently long for maneuvering, with a recommended length of 75 ft (within tolerance) although length varies widely depending on the type of water skiing and the skier's skill level. Competition requirements on rope construction have changed over the years, from "quarter-inch polypropylene rope" in 1992 to the 2003 flexibility as long as the same specification is used "for the entire event." The rope and handle are anchored to the boat and played out at the stern. This anchor point on a recreation boat is commonly a tow ring or cleat, mounted on the boat's stern. For more dedicated skiers, a metal ski pylon is placed in the center of the boat in front of the engine to connect the skier. This pylon is mounted securely, since a skilled slalom skier can put a considerable amount of tension in the ski rope and the pylon.
Safety measures.
As water skiing is a potentially dangerous sport, safety is important.
There should be a 200 ft wide skiing space and the water should be at least 5 to deep. The towboat should stay at least 100 ft from docks, swim areas, and the shore, and other boats should steer clear of skiers by at least 100 ft. Without proper space and visibility skiing can be extremely dangerous.
The skier should know how to swim, but he or she should wear a life jacket regardless of swimming ability. Specially-designed life jackets or ski vests allow movement needed for the sport while still providing floation for a downed or injured skier. The most common water ski injuries involve the lower legs, such as the knee, because a fall at high speed can create irregular angles of collision between the skier's body and the water surface. Another common cause of injury is colliding with objects on or near the water, like docks.
The tow boat must contain at least two people. The driver maintains a steady course, free of obstacles to the skier. The observer continually observes the skier, relays the condition of the skier to the boat driver, and if necessary flags a downed skier to warn other vessels. The skier and observer should agree on a set of standard hand-signals for easy communication: stop, speed up, turn, I'm OK, skier in the water, etc.
History.
Water skiing was invented in 1922 when Ralph Samuelson used a pair of boards as skis and a clothesline as a towrope on Lake Pepin in Lake City, Minnesota. Samuelson experimented with different positions on the skis for several days until 2 July 1922. Samuelson discovered that leaning backwards in the water with ski tips up and poking out of the water at the tip was the optimal method. His brother Ben towed him and they reached a speed of 20 mph. Samuelson spent 15 years performing shows and teaching water skiing to people in the United States.
Samuelson went through several iterations of equipment in his quest to ski on water. His first equipment consisted of barrel staves for skis. He later tried snow skis, but finally fabricated his own design out of lumber with bindings made of strips of leather. The ski rope was made from a long window sash. Samuelson never patented any of his ski equipment. The first patent for water skis was issued to Fred Waller, of Huntington, NY, on 27 October 1925, for skis he developed independently and marketed as "Dolphin Akwa-Skees." Waller's skis were constructed of kiln-dried mahogany, as were some boats at that time. Jack Andresen patented the first trick ski, a shorter, fin-less water ski, in 1940.
The sport of water skiing remained an obscure activity for several years after 1922, until Samuelson performed water ski shows from Michigan to Florida. The American Water Ski Association formally acknowledged Samuelson in 1966 as the first recorded water skier in history. Samuelson was also the first ski racer, slalom skier, and the first organizer of a water ski show.
Water skiing gained international attention in the hands of famed promoter, Dick Pope, Sr., often referred to as the "Father of American Water Skiing" and founder of Cypress Gardens in Winter Haven, Florida. Pope cultivated a distinct image for his theme-park, which included countless photographs of the water skiers featured at the park. These photographs began appearing in magazines worldwide in the 1940s and 1950s, helping to bring international attention to the sport for the first time. He was also the first person to complete a jump on water skis, jumping over a wooden ramp in 1928, for a distance of 25 feet. His son, Dick Pope, Jr., is the inventor of bare-foot skiing. Both men are in the Water Ski Hall of Fame. Today, Winter Haven, Florida, with its famous Chain of Lakes, remains an important city for water skiing, with several major ski schools operating there.
Water skiing has developed over time. Water skiing tournaments and water skiing competitions have been organized. As an exhibition sport, water skiing was included in the 1972 Olympics. The first National Show Ski Tournament was held in 1974, and the first ever National Intercollegiate Water Ski Championships was held in 1979. The Home CARE US National Water Ski Challenge, the first competition for people with disabilities, was organized ten years later.
The first patented design of a water ski which included carbon fiber was that of Hani Audah at SPORT labs in 2001. Its first inclusion in tournament slalom skiing was in 2003.
Disciplines.
3-Event Tournament Waterskiing.
Each summer in the USA alone there are over 900 sanctioned waterski competitions. Competitive waterskiing consists of three events: slalom, jump, and trick.
Slalom.
In an attempt to become as agile as possible, slalom waterskiers use only one ski with feet oriented forward, one in front of the other. Slalom skis are narrow and long, at 57 - depending on the height and weight of the skier. The two forward-facing bindings vary: they can be made of rubber or thick plastic, and they can be designed more like a snow ski binding or more like a roller blade boot.
Slalom skiing involves a multi-buoy course that the skier must go around in order to complete the pass. A complete slalom waterski course consists of 25 buoys. There are entrance gates at the beginning and end of the course that the skier must go between, and there are 6 turn buoys that the skier must navigate around in a zigzag pattern. The remainder of the buoys are for the driver to ensure the boat goes straight down the center of the course. For a tournament to be sanctioned as 'record capable' by the International Waterski & Wakeboard Federation (IWWF), the entire course must be surveyed prior to competition by a land surveyor to ensure its accuracy. The drivers boat path must be verified as well to ensure that all skiers are getting a fair pull.
Every consecutive pass is harder than the pass before it. When a pass is completed, the boat is sped up by 3 km/h until the maximum speed has been reached for the division, based on the skier's gender and age (55 km/h for women and 58 km/h for men). After the skier has run their maximum speed pass, the rope is shortened at specific increments to make it more difficult to reach the buoy width. In a tournament, the boat speeds up or the rope shortens until the skier fails to complete the slalom course by falling or missing a buoy.
A skier's score is based upon the number of successful buoys cleared, the speed of the boat, and the length of the rope. In a tournament, skiers choose the starting boat speed and rope length (with a maximum length of 18.25 m). Professional waterskiers will typically start at the max speed of 58 km/h with a rope that has already been shortened to 13 m. The skier with the most buoys wins the competition.
The turn buoys are positioned 11.5 m away from the center of the slalom course so as the rope is shortened beyond that the skiers are required to use the momentum generated through their turns to swing up on the side of the boat and reach out in order to get their ski around the next buoy. At this rope length the skier's body is experiencing intense isometric contractions and extreme upper body torque with loads of up to 600 kg as they begin accelerating after rounding a turn buoy. Their top speeds will generally be more than double the boat's speed, which means that the Pro men can reach speeds in excess of 116 km/h and each turn will generally generate around 4 g of force. Essentially, slalom waterskiers are using their body as a lever, which allows them to withstand loads that would otherwise not be possible for the human body.
Jump.
Waterski jumpers use two long skis to ride over a waterski jump in an attempt to travel the longest distance. In a tournament, skiers are given three attempts to hit the ramp. The winner is the skier who travels the farthest calculated distance and successfully rides away for at least 100 ft. There are no style points, simply distance.
Waterski jumps have specific dimensions and the ramp height is adjustable. Skiers may choose their boat speed and ramp height, although there are maximums based the skier's gender and age. Professional ski jumpers have a maximum boat speed of 58 km/h. The ramp height must be between 5 and. As a professional jumper approaches the ramp they will zigzag behind the boat in a series of cuts to generate speed and angle. When the jumper hits the ramp they will generally be going over 112 km/h and the load they have generated on the rope can be over 600 kg.
Trick.
The Trick competition has been described as the most technical of the three classic water skiing events.
Trick skiing uses small, oval-shaped waterskis. Beginners generally use two skis while more advanced skiers use one. The shorter, wider Trick ski has a front binding facing forward and a back binding facing at a 45°. It has a smooth bottom that allows it to turn over the surface of the water. According to official 2013 Tournament Rules for 3-event competition in the United States and the Pan-Am Games, skis used in the Tricks event must be a single ski without fins, although molded rails/grooves less than 1/4 in are allowed, as are a foot pad cemented to the ski as a place for the rear foot; in addition, the ski must float with all bindings, fins, etc., installed. The ski's configuration allows the skier to perform both surface and air tricks in quick succession.
In a tournament, skiers are given two 20-second runs during which they perform a series of their chosen tricks. One pass is for hand tricks, which includes surface turns, rotations over the wake, and flips. The second pass is for toe tricks, which are done by doing wake turns and rotations with only a foot attaching them to the handle; the foot is either in the toehold part of the handle or, professionally, attached to the rope. A trick cannot be repeated. Each trick has a point value. A panel of five judges accesses which tricks were competed correctly and assigns that predetermined point value to each successfully competed trick. The skier with the most points wins.
Barefoot waterskiing.
A barefoot water skier should use a wetsuit instead of a life jacket because the wetsuit covers more of the body in case of a fall at high speed. The wetsuit also allows the skier to do starts in the water where they lie on their back. Unlike a normal life jacket, the "barefoot wetsuit" allows the skier to glide on their back on top of the water once they reach a high enough speed. The barefoot wetsuit is generally thicker in the back, rear, and chest for flotation and impact absorption.
Barefoot skiing requires a higher speed because the skier's feet are smaller than skis, providing less lift. A rule of thumb for barefoot water skiing speed in miles per hour is (M/10)+18=S, where M equals the skier's weight in pounds. In other words, a 175 lb person would have to divide 175/10, which is 17.5; then simply add 17.5+18 which equals 35.5 mph.
Another tool used in barefoot water skiing is the barefoot boom. It provides a stable aluminum bar on the side of the boat where a short rope can be attached or the skier can grip the bar itself. The skier is within earshot of the people in the boat, providing a good platform for teaching. Once the bare footer is good enough, he/she will go behind the boat with a long rope.
A beginner can wear shoes to decrease the necessary speed, lessen foot injury from choppy water, learn better technique, and master the sport.
Show skiing.
Show skiing is a type of water skiing where skiers perform tricks somewhat similar to those of gymnasts while being pulled by the boat. Traditional ski show acts include pyramids, ski doubles, freestyle jumping, and swivel skiing. Show skiing is normally performed in water ski shows, with elaborate costumes, choreography, music, and an announcer. Show teams may also compete regionally or nationally. In the USA, each team member must be a member of USA Water Ski to compete.
The first organized show occurred in 1928. The bi-annual World Show Ski Championship was inaugurated in September 2012 in Janesville, Wisconsin. Past competition included teams from Australia, Belgium, Canada, China, and the United States.
Freestyle jumping.
Freestyle jumping is often related to show skiing. The goal is to go off the jump, perform one of many stunts, and successfully land back on the water. The most common freestyle stunts – in order of usual progression – would be a heli (360°), a flip (forwards), a gainer (a back flip), and a möbius (back flip with 360°).
Ski racing.
Water ski racing consists of a number of water skiers who race around a set course.
A team consists of a boat driver, an observer, and one to two skiers depending on the race. The driver tows the skier behind a powerboat, varying the speed based on conditions, his or her knowledge of the skier, the observer's ability to read the skier, and the skier's signals.
World records.
The most skiers towed behind one boat is 145, set by the Horsehead Water Ski Club in Strahan, Tasmania, Australia, on 27 January 2012. Previously the record had stood at 100 skiers for 24 years before a huge team lead by Horsehead broke the record in 2010 with 114 skiers. The skiers used 8 km of ski rope and were towed by the 36 m 3000 hp World Heritage Cruises' catamaran "Eagle".
On 31 August 1974, David Scott Munro of the Ross-shire Caberfeidh Water Ski Club became the first person to water ski (mono ski) the length of Loch Ness, Scotland. From Lochend to Fort Augustus and back, he covered the 48 mi in 77 minutes at an average speed of 37 mph.
The world record (approved) in slalom is 2.5 buoys at 9.75 m or 75 ft (43 ft off) which was set by Nate Smith in September 2013.
World records in ski jump have been tracked since the first World Championships, in 1949. The current world record is held by Freddy Krueger; it was set in 2008 at a distance of 75.3 m.

</doc>
<doc id="38993" url="http://en.wikipedia.org/wiki?curid=38993" title="Optical depth">
Optical depth

In physics, optical depth or optical thickness or Napierian absorbance, is the "natural logarithm" of the ratio of incident to "transmitted" radiant power through a material, and spectral optical depth or spectral optical thickness or spectral Napierian absorbance is the natural logarithm of the ratio of incident to "transmitted" spectral radiant power through a material. Optical depth is dimensionless, and in particular is not a length, though it is a monotonically increasing function of path length, and approaches zero as the path length approaches zero. The use of the term "optical density" for optical depth is discouraged.
In chemistry, a closely related quantity called "absorbance" or "decadic absorbance" is used instead of optical depth: the "common logarithm" of the ratio of incident to "transmitted" radiant power through a material, that is the optical depth divided by ln 10.
Mathematical definitions.
Optical depth.
Optical depth of a material, denoted "τ", is given by:
where
Absorbance is related to optical depth by:
where "A" is the absorbance.
Spectral optical depth.
Spectral absorbance in frequency and spectral absorbance in wavelength of a material, denoted "τ"ν and "τ"λ respectively, are given by:
where
Spectral absorbance is related to spectral optical depth by:
where
Relationship with attenuation.
Attenuance.
Optical depth measures the attenuation of the transmitted radiant power in a material. Attenuation can be caused by absorption, but also reflection, scattering, and other physical processes. Optical depth of a material is approximately equal to its "attenuance" when both the absorbance is much less than 1 and the emittance of that material (not to be confused with radiant exitance or emissivity) is much less than the optical depth:
where
and according to Beer–Lambert law,
so:
Attenuation coefficient.
Optical depth of a material is also related to its "attenuation coefficient" by:
where
and if "α"("z") is uniform along the path, the attenuation is said to be a "linear attenuation" and the relation becomes:
Sometimes the relation is given using the "attenuation cross section" of the material, that is its attenuation coefficient divided by its number density:
where
and if "N"("z") is uniform along the path, the relation becomes:
Applications.
Atomic physics.
In atomic physics, the spectral optical depth of a cloud of atoms can be calculated from the quantum-mechanical properties of the atoms. It is given by
where
Atmospheric sciences.
In atmospheric sciences, one often refers to the optical depth of the atmosphere as corresponding to the vertical path from Earth's surface to outer space; at other times the optical path is from the observer's altitude to outer space. The optical depth for a slant path is "τ" = "mτ"′, where "τ′" refers to a vertical path, "m" is called the relative airmass, and for a plane-parallel atmosphere it is determined as "m" = sec "θ" where "θ" is the zenith angle corresponding to the given path. Therefore
The optical depth of the atmosphere can be divided into several components, ascribed to Rayleigh scattering, aerosols, and gaseous absorption. The optical depth of the atmosphere can be measured with a sun photometer.
Astronomy.
In astronomy, the photosphere of a star is defined as the surface where its optical depth is 2/3. This means that each photon emitted at the photosphere suffers an average of less than one scattering before it reaches the observer. At the temperature at optical depth 2/3, the energy emitted by the star (the original derivation is for the Sun) matches the observed total energy emitted.
Note that the optical depth of a given medium will be different for different colors (wavelengths) of light.
For planetary rings, the optical depth is the (negative logarithm of the) proportion of light blocked by the ring when it lies between the source and the observer. This is usually obtained by observation of stellar occultations.

</doc>
<doc id="38999" url="http://en.wikipedia.org/wiki?curid=38999" title="Bolesław I Chrobry">
Bolesław I Chrobry

Bolesław I Chrobry (Bolesław I the Valiant, or the Brave; Czech: "Boleslav Chrabrý",   ; 967 – 17 June 1025; previously also known as Bolesław I the Great, "Wielki"), was a Duke of Poland from 992 to 1025 and the first crowned King of Poland from 18 April 1025 until his death two months later. He was also Duke of Bohemia as Boleslav IV from 1002 to 1003.
He was the first-born son of Mieszko I by his first wife Dobrawa, daughter of Boleslav I the Cruel, Duke of Bohemia. Bolesław I the Brave was named after his maternal grandfather. He assumed the control over the country in 992 after having expelled his step-mother Oda of Haldensleben and his half-brothers.
He supported the missionary views of Adalbert, Bishop of Prague and Bruno of Querfurt. The martyrdom of the first (in 997) and his imminent canonization was used for political purposes, leading the called Congress of Gniezno (11 March 1000), where was established a Polish church structure with a Metropolitan See at Gniezno -independent of the German Archbishopric of Magdeburg, which had tried to lay claim to Polish church jurisdictions- and the Bishoprics of Kraków, Wrocław and Kołobrzeg; in addition, Bolesław formally renounced tribute payments to the Holy Roman Empire. A vassal of Otto III, Holy Roman Emperor, who may have crowned him "rex" (King), following his death (1002), Bolesław carried out a series of successful wars against the Holy Roman Empire and Otto's cousin and heir, Henry II, ending in the Peace of Bautzen (1018).
In the summer of 1018, in one of his expeditions, Bolesław I captured Kiev, where he installed as ruler his son-in-law Sviatopolk I. According to legend, he chipped his sword when striking Kiev's Golden Gate. Later, a sword called "Szczerbiec" ("Chipper") would become the coronation sword of Poland's Kings.
Bolesław I was a remarkable politician, strategist, and statesman. He not only turned Poland into a country comparable to older western monarchies, but he raised it to the front rank of European states. Bolesław conducted successful military campaigns in the west, south and east. He consolidated the Polish lands and conquered territories that lie outside the borders of modern Poland, including Slovakia, Moravia, Red Ruthenia, Meissen, Lusatia, and Bohemia. He was a powerful mediator in Central European affairs.
Finally, as the culmination of his reign, he had himself crowned King of Poland (1025), the first Polish ruler to do so.
He was an able administrator who established the "Prince's Law" and built many forts, churches, monasteries and bridges. He introduced the first Polish monetary unit, the "grzywna", divided into 240 "denarii", and minted his own coinage.
Bolesław I is widely considered to have been one of Poland's most capable and accomplished Piast rulers.
Life.
Youth.
Bolesław I was born in 967, in Poznań as the first child of Mieszko I, Duke of Poland and his first wife, the Bohemian princess Dobrawa. At age six he may have been sent to the Imperial court in Germany as a hostage, according to the agreements of the Imperial Diet of Quedlinburg (although some historians now dispute this detail). Another theory holds that Bolesław spent some time during the 980s at the court of his maternal uncle, Duke Boleslav II the Pious of Bohemia.
In 984 Mieszko I arranged the marriage of the eighteen-year-old Bolesław with the daughter of Rikdag, Margrave of Meissen. It's believed that following the wedding he became the ruler of Lesser Poland with his capital at Kraków. The death of Margrave Rikdag in 985 left the marriage without any political value, and shortly thereafter Bolesław repudiated his wife.
At the end of 985, probably at the instigation of Boleslav II the Pious, Bolesław married an unknown Hungarian princess with whom he had a son, Bezprym. However, this union also proved short-lived, probably because of the deterioration in political relations between Poland and Hungary, and around 987 the union was dissolved.
By 989, and perhaps as early as 987, Bolesław married Emnilda, daughter of Dobromir, most likely a Slavic prince of Lusatia. Other historians have argued that Emnilda was a Moravian princess, or a daughter of the last independent prince of the Vistulans, before their incorporation into the Polish state. Through this marriage he had, among others, the future king Mieszko II. At this time Bolesław's rule in Lesser Poland may have been at Bohemian fief. Presuming that it was, he added this province to Poland only after the death of Duke Boleslav II the Pious in 999. However assuming that Mieszko I took control of Lesser Poland in 990 (which is likely), then Bolesław I was bestowed the rule in Lesser Poland by his father but without its territory being included in the Polish realm. Bolesław does not appear in the surviving summary of the "Dagome Iudex" document, and as such it may be supposed that Lesser Poland was already known as Bolesław's inheritance, while his two surviving half-brothers Mieszko and Lambert, sons of Mieszko I by his second wife Oda, were to divide the rest of the realm between themselves. Another theory explains Bolesław's absence from the document through an old Slavic custom whereby children received their inheritance as soon as they reached the age of majority. Thus Bolesław might have received Kraków as his part of his father's legacy before the writing of the "Dagome iudex".
Accession.
The circumstances in which Bolesław took control of the country following the passing of his father, Mieszko, anticipated what would later become a prevalent practice among the Piast dynasty. It consisted of struggle for control, usually a military one, among the offspring of nearly every deceased monarch of the Piast dynasty. Bolesław was no different, and shortly after the death of Mieszko I (25 May 992), he banished his stepmother Oda and his two half-brothers, as they were competitors to the throne. The exact circumstances of Bolesław's ascension to the Ducal throne are unknown, but it is known that by June, he was the unquestioned ruler of Poland – as Emperor Otto III asked for his military aid in the summer of 992. Immediately after gaining the full control over Poland, Bolesław also quelled the opposition of powerful families by blinding two of their leaders, the magnates Odylen and Przybywoj. As cruel a sentence as this was, it proved most effective as it resulted in such obedience of his subjects that from that point on there was no mention of any challenge to his position whatsoever.
Extent of his domains.
Bolesław inherited from his father a realm that was close in dimensions to modern-day Poland. It centered on the core of Polanian country, the later Greater Poland ("Wielkopolska"). Greater Poland encompassed the valley of river Warta, stretched to the north to the Noteć river and to the south it encompassed Kalisz. Outside of this core the nascent Poland included the surrounding areas subdued by Bolesław's father, Mieszko I which included: parts of Pomerania to the north, including Kołobrzeg in the west and Gdańsk in the east, Mazovia with its capital at Płock to the east and Silesia to the south-west. It is disputed whether Lesser Poland, centered around Kraków, was incorporated into the Polish realm by Mieszko I before 992 or whether it was added by Bolesław in 999. Either way by the year 1000 Bolesław was the lord of a domain larger than contemporary England, Denmark, León or Burgundy.
Duke of Poland.
First years (992–1000).
It appears, from the lack of any record of international activity, that Bolesław spent the first years as ruler more concerned about gaining the throne and remaining on it than trying to increase the size of his dominion. It is during this period of consolidation of power that he allied himself with Otto III, Holy Roman Emperor, and in 995 he aided Otto in his expedition against the Lusatians.
Endeavoring to extend his influence to the territory of the Prussians, Bolesław encouraged Christianizing missions in the Prussian lands. Most famous of those was the mission of Vojtěch from the Bohemian princely Slavník clan, former bishop of Prague. Known as Adalbert of Prague upon the death of Adalbert of Magdeburg in 981, Adalbert's mission took place in 997 and ended in the missionary's martyrdom at the hands of the pagan Prussians. This took place in April 997 on the Baltic Sea coast in the vicinity of Truso (a medieval emporia near modern city of Elbląg). The remains of the missionary were held for ransom by the Prussians and Bohemian Přemyslid rulers refused to pay for Adalbert's body. Consequently it was purchased by Duke Bolesław, according to one story, in exchange for its weight in gold, and buried in Gniezno. In 999 Bishop Adalbert was canonized as Saint Adalbert by Pope Sylvester II. He was later made the patron saint of Bohemia, Poland, Hungary, and Prussia. Canonization of Adalbert/Vojtěch increased the prestige of the Polish church in Europe and the prestige of Polish state on the international arena.
Congress of Gniezno and alliance with the Holy Roman Empire (1000–1002).
By the year 1000, Bolesław had consolidated his position as Duke ("Dux") of Poland. Not only did he not meet any internal opposition, but he furthermore had gained the respect of Holy Roman Emperor Otto III (980–1002). Consequently in the year 1000, Otto III visited Poland under the pretext of a pilgrimage to the grave of his friend, the recently canonized Bishop Adalbert (Vojtěch). In addition to the religious motivation, Otto III's voyage also carried a strong political agenda: he had intentions to renew the Holy Roman Empire based on a federal concept he called "Renovatio Imperii Romanorum". Within the federal framework, Polish and Hungarian duchies were to be upgraded to eastern "federati" of the empire.
The Emperor needed to assess Poland's strength and establish its status within the Holy Roman Empire. The ensuing Congress of Gniezno, where Bolesław entertained his distinguished guest, is one of the most famous episodes of medieval Polish history. During the time the emperor spent in Poland, Bolesław did not hide the wealth of his country, in fact he showed off its affluence at every step as he tried to dazzle the emperor. Among other gifts the Polish ruler presented to Otto III were 300 armored knights, while the Emperor responded with a gift of a copy of the lance of Saint Maurice. Evidently Otto III was impressed with what he saw and decided that Poland should be treated as a kingdom on par with Germany and Italy, not merely as a tributary duchy like Bohemia. Since Otto III had intentions to renew the Empire it was towards this end that the Emperor placed his Imperial crown on Bolesław's brow and invested him with the titles "frater et cooperator Imperii" ("Brother and Partner of the Empire") and "populi Romani amicus et socius". He also raised Bolesław to the dignity of "patricius" or "elder of the Roman nation". This episode has long been a subject of debate among historians. Some historians see this as an act of favor between an Emperor and his vassal, others as a gesture of friendship between equals. Could placing of the Imperial crown on Bolesław's head mean that the Emperor crowned the Polish Duke? Most modern historians agree that it could not. Though it was undoubtedly a sign of Otto's respect for the Polish ruler, it could not truly mean Bolesław was King as only the Pope had the authority to invest a prince with the crown and elevate his realm to a status of a kingdom. According to one source afterwards Bolesław traveled with the Emperor to Aix-la-Chapelle where Otto III had the tomb of Charlemagne opened. From there Otto III is reputed to have removed the Imperial throne itself and presented it to the Polish Duke.
Other political talks took place as well. Otto III decided that Poland will no longer be required to pay tribute to the Empire. Gniezno was confirmed as an Archbishopric and a Metropolitan See for the Polish area. Three new Bishoprics were created and confirmed with papal consent. They were placed at Kraków, Wrocław and Kolobrzeg. The Poznań missionary Bishopric was confirmed as subject directly to the Vatican. Bolesław and his heirs gained the right of investiture of bishops. The future marriage of Bolesław's son Mieszko to Richeza (Polish: "Rycheza"), niece of Otto III, was also probably agreed upon at this point.
The untimely death of Otto III at age 22 in 1002 upset the ambitious "renovatio" plans, which were never fully implemented. Henry II, Otto III's less idealistic successor, and an opponent of Otto's policies, reversed the course of Imperial policy towards the east.
Occupation of Meissen, Lusatia, Bautzen and the intervention in Bohemia (1002–1003).
The excellent relations Poland and Holy Roman Empire enjoyed during the reign of Otto III quickly deteriorated following his death. Bolesław supported Eckard I, Margrave of Meissen, for the German throne. When Eckard was assassinated in April, Bolesław lent his support to Henry IV, Duke of Bavaria, and helped him ascend to the German throne as Henry II. Bolesław took advantage of internal strife following the Emperor's death and occupied important areas to the west of the Oder: Margraviate of Meissen and March of Lusatia, including strongholds Budziszyn and Strzala. Bolesław claimed a hereditary right to Meissen as a relative of its former ruler Margrave Rikdag (only through marriage; he was the former husband of his daughter). Henry II accepted Bolesław's gains and allowed the Polish Duke to keep Lusatia as a fief. The one exception was Meissen, which Bolesław was not allowed to keep. Though at this point Polish–German relations were normalized, soon thereafter Henry II organized a failed assassination attempt on Bolesław's life and relations between the two countries were severed.
In the same year (1003) Bolesław became entangled in Bohemian affairs when Duke Vladivoj died. Bolesław helped a pretender, Boleslav III the Red, to gain the throne. Boleslav III, however, undermined his position by ordering a massacre of the leading nobles, the Vršovci, at Vyšehrad. The nobles who survived the massacre secretly sent messengers to Bolesław and entreated him to come to their aid. The Polish Duke willingly agreed, and invited Boleslav III to visit him at his castle in Kraków. There, Boleslav III was trapped, blinded and imprisoned, probably dying in captivity some thirty years later. Bolesław I, claiming the Bohemian Ducal throne for himself, invaded Bohemia in 1003 and took Prague without any serious opposition, ruling as Boleslav IV for a little over a year. It is also likely that Polish forces took control of Moravia and Upper Hungary (present day Slovakia) in 1003 as well. The proper conquest date of the Hungarian territories is 1003 or 1015 and this area stayed a part of Poland until 1018.
Polish-German War (1002–1018).
As mentioned above, Bolesław had taken control of the marches of Lusatia, Sorbian Meissen, and the cities of Budziszyn (Bautzen) and Meissen in 1002, and refused to pay the tribute to the Empire from the conquered territories.
Henry II, allied with the Lutici, answered with an offensive a year later. Though the first attack was not successful, already in the autumn of 1004 the German forces deposed Bolesław from the Bohemian throne. Bolesław did manage to keep Moravia and Slovakia, however, over which he exercised control until 1018. During the next part of the offensive Henry II retook Meissen and in 1005 his army advanced as far into Poland as the city of Poznań where a peace treaty was signed. According to the peace treaty Bolesław lost Lusatia and Meissen and likely gave up his claim to the Bohemian throne. Also in 1005, a pagan rebellion in Pomerania overturned Boleslaw's rule and resulted in the destruction of the just implemented local bishopric.
In 1007 Henry denounced the Peace of Poznań, which caused Bolesław's attack on the Archbishopric of Magdeburg as well as the re-occupation of marches of Lusatia and Meissen including the city of Bautzen. The German counter-offensive began three years later, in 1010. It was of no significant consequence, beyond some pillaging in Silesia. In 1012 a five-year peace was signed.
Bolesław broke the peace however, and once again invaded Lusatia. Bolesław's forces pillaged and burned the city of Lubusz (Lebus). In 1013 a peace accord was signed at Merseburg. As part of the treaty Bolesław paid homage to Henry II for the March of Lusatia and Sorbian Meissen as fiefs. A marriage of Bolesław's son Mieszko with Richeza of Lotharingia, daughter of the Count Palatine Ezzo of Lotharingia and granddaughter of Emperor Otto II was also performed.
In 1014 Bolesław sent his son Mieszko to Bohemia in order to form an alliance with duke Oldrich against Emperor Henry. Bolesław also refused to aid the Emperor militarily in his Italian expedition. This led to imperial intervention in Poland and so in 1015 a war erupted once again. The war started out well for the Emperor as he was able to defeat the Polish forces at Ciani. Once the imperial forces crossed the river Oder, Bolesław sent a detachment of Moravian knights in a diversionary attack against the Eastern March of the empire. Soon thereafter the imperial army retreated from Poland without any permanent gains. Following this Bolesław's forces took the initiative. The Margrave of Meissen, Gero II, was defeated and killed during a clash with the Polish forces late in 1015.
Later that year, Bolesław's son Mieszko was sent to plunder Meissen. His attempt at conquering the city however, failed. In 1017 Bolesław defeated Margrave Henry V of Bavaria. In 1017 with Czech and Wendish support Henry II once again invaded Poland, however, once again to very little effect. He did besiege cities of Głogów and Niemcza, but was unable to take them. Taking advantage of Czech troops' involvement, Bolesław ordered his son to invade Bohemia, where Mieszko met very little resistance. On 30 January 1018, the Peace of Bautzen (which made Bolesław a clear winner), was signed. The Polish ruler was able to keep the contested marches of Lusatia and Sorbian Meissen not as fiefs, but as part of Polish territory, and also received military aid in his expedition against Kievan Rus. Also, Bolesław (then a widower) reinforced his dynastic bonds with the German nobility through his marriage with Oda, daughter of Margrave Eckard I of Meissen. The wedding took place four days later, on 3 February in the castle of "Cziczani" (also "Sciciani", at the site of either modern Groß-Seitschen or Zützen).
Intervention in the Kievan Succession (1015–1019).
Bolesław organized his first expedition against his eastern neighbor in 1015, but the decisive engagements were to take place in 1018 after the peace of Budziszyn was already signed. At the request of his son-in-law Sviatopolk I of Kiev, the Polish duke invaded Kievan Rus' with an army of between 2,000–5,000 Polish warriors, in addition to Thietmar's reported 1,000 Pechenegs, 300 German knights, and 500 Hungarian mercenaries. After collecting his forces during June, Boleslaw led his troops to the border in July and on 23 July at the banks of the Bug River, near Wielen, he defeated the forces of Yaroslav the Wise prince of Kiev, in what became known as the Battle at Bug river. All primary sources agree that the Polish prince was victorious in battle. Yaroslav retreated north to Novgorod, rather than to Kiev. The victory opened the road to Kiev, already under harassment from Boleslaw's Pecheneg allies. The city, which suffered from fires caused by the Pecheneg siege, surrendered upon seeing the main Polish force on 14 August. The entering army, led by Bolesław, was ceremonially welcomed by the local archbishop and the family of Vladimir I of Kiev. Bolesław may have deployed his troops in the capital of Rus for no more than six months (see Kiev Expedition of 1018) but had to recall them eventually due to popular uprising against the Poles. According to popular legend Bolesław notched his sword (Szczerbiec) hitting the Golden Gate of Kiev. During this campaign Poland re-annexed the Red Strongholds, later called Red Ruthenia, lost by Bolesław's father in 981.
In 1015 Bolesław sent a detachment of Polish horsemen to aid his nephew Canute the Great, son of his sister Swietoslawa, in his conquest of England.
Coronation.
Historians disputed the exact date of Bolesław's coronation. Some believe that since 1000 the Polish ruler asked to the Pope a consent for his coronation, following the Congress of Gniezno. Independent German sources clearly confirmed that after Henry II's death in 1024, Bolesław took advantage of the interregnum in Germany and crowned himself King in 1025 (the exact date or place is unknown), thus raising Poland to the rank of a kingdom before its neighbor Bohemia. He was the first Polish king ("rex"), his predecessors having been considered dukes ("dux") by the Holy Roman Empire and the papacy. Others (like Johannes Fried) believes that the coronation of 1025 was only the renewal of a previous coronation performed in 1000 (multiple coronations are common at the time).
Wipo of Burgundy in his Chronicle describes this event:
Hence it's assumed that Bolesław received permission for his coronation from Pope John XIX (who at that point had a bad relationship with the Holy Roman Empire). Stanisław Zakrzewski put forward the theory that the coronation had the tacit consent of Conrad II and the Papacy only confirmed this fact. This was further confirmed by Jarosław Sochacki, who added other facts who supported Zakrzewski's theory:
Death and Burial.
Bolesław I died shortly after his coronation, most likely from an illness. The whereabouts of Boleslaw's burial are uncertain. According to Jan Długosz (and followed by modern historians and archaeologists) he was buried in the Archcathedral Basilica of St. Peter and St. Paul, Poznań. In the 14th century, King Casimir III the Great ordened the build of a Gothic sarcophagus which he transferred Boleslaw's remains. The sarcophagus was partially destroyed in 1772 during a fire, and completely a few years later (1790) due to the collapse of the south tower. Then, the remains were moved to the Chapter house, where three bone fragments where donated to Tadeusz Czacki (in 1801, at his request). Czacki, a notable Polish historian, pedagogue and numismatist, placed one of the bone fragments in his ancestral mausoleum in Poryck (now Pavlivka) in the Volhynia region; the other two where gave to Princess Izabela Czartoryska "née" Flemming, who placed them in her recently founded Czartoryski Museum in Puławy. After many historical twists, the burial place of Bolesław I ultimately remained at Poznań Cathedral, in the namely Golden Chapel inside the temple. Is known the content of his Epitaph, which in part came from the original tombstone, it's also one of the first sources (dated to the period immediately after Bolesław's death, probably during the reign of Mieszko II) who gave the King his widely known nickname of "Brave" (Polish: "Chrobry") -later Gallus Anonymus in the Chapter 6 of his "Gesta principum Polonorum" named the Polish ruler as "Bolezlavus qui dicebatur Gloriosus seu Chrabri"-.
Legacy.
Military.
At the time of his death Bolesław left Poland larger than the land he had inherited: he had added to his domains the long-contested marches of Lusatia and Sorbian Meissen as well as Red Ruthenia and possibly Lesser Poland. Militarily, at the time, Poland was unquestionably a considerable power as Bolesław was able to fight successful campaigns against both Holy Roman Empire and the Kievan Rus. On the other hand it must be highlighted that his long-term involvement in the war against Germany allowed Western Pomerania to gain independence from the Polish aegis. Another negative side of Bolesław's drawn out military campaigns was a damaging influence on the economy of his kingdom. With the passing of each year, Bolesław needed ever-increasing amounts to finance his wars, especially when fought on two fronts; in Germany and Kiev. Unceasing war had placed ever-increasing fiscal obligations on his subjects, which in turn caused negative sentiment, sentiment that increased throughout his reign, and that would erupt into popular revolt soon after his death.
Economy.
Bolesław was a gifted and organized administrator. He was largely responsible for fully implementing the "Prince's Law" throughout the Polish lands. The Prince's Law created a sort of nationalized economy, controlled by the state, whose sole duty it was to finance the prince's spending needs. These needs were considerable, as the Duke was responsible for all manner of building projects. The foundation of the "Prince's Law" lay in a network of fortified towns called grody, but the ruler also commissioned the building of churches, monasteries, roads, bridges etc., in short the development of an infrastructure. The building projects were financed by collecting taxes in money or goods. Also peasants were required to house the monarch or provide the prince with different manner of goods and services which included communications, hunting, military or others. To produce necessary goods Bolesław organized a network of service settlements that specialized each in manufacturing about 30 different goods, such as: barrels, arches, metal wares, spears, as well as settlements responsible for animal husbandry, i.e., swine, horses or cattle. Hundreds of villages were thus specialized and named to reflect their particular job. To this day one may find scores of settlements in Poland with names left over from that era, such as: Szewce (cobblers), Kuchary (cooks) or Kobylniki (mare breeders). This system functioned well enough to support Bolesław throughout his 33-year reign.
Political.
Increasing both the internal and external strength of the realm was of paramount importance to Bolesław, especially in the face of increasing pressure from the magnates. The magnates demanded a larger share in the administration of the country while Bolesław sought to strengthen the central authority of the ruler. Bolesław's coronation, sometime in 1025, was aimed precisely to reinforce his leading position. In general an overall integration of the country took place during his reign.
Bolesław was able to establish an independent Polish church structure with a Metropolitan See at Gniezno, with papal and imperial sanction. His work laid a foundation for the use of designation "Poland" that was to unite all regions of the realm, as well as for the use of one symbol to represent the supreme authority of the prince. The symbol was a sign of Gniezno's knightly class: the white eagle.
Marriages and Issue.
First marriage: 984–985
An unknown daughter of Rikdag, Margrave of Meissen, variously named Henilda, Hemnilda, Herminilda or Oda. After Rikdag's death in 985, she was repudiated by her husband and sent away.
Issue:
Second marriage: 986 – 987/89
An unknown Hungarian princess formerly believed to be Judith, daughter of Géza, Grand Prince of the Hungarians. Around 987, as a consequence of the deterioration in the political relations between Poland and Hungary, she was repudiated.
Issue:
Third marriage: 987/89 – 1013
Emnilda, daughter of Dobromir, a Western Slavic prince.
Issue:
Fourth marriage: 1018–1025
Oda (b. ca. 995 – d. aft. 1025), daughter of Eckard I, Margrave of Meissen. She was nicknamed the Younger (Polish: "Młodsza") probably in reference to either Bolesław's step-mother or first wife.
Issue:

</doc>
<doc id="39029" url="http://en.wikipedia.org/wiki?curid=39029" title="Bamboo">
Bamboo

Bamboo (Bambuseae) is a tribe of flowering perennial evergreen plants in the grass family Poaceae, subfamily Bambusoideae, tribe Bambuseae. Giant bamboos are the largest members of the grass family. In bamboos, the internodal regions of the stem are hollow and the vascular bundles in the cross section are scattered throughout the stem instead of in a cylindrical arrangement. The dicotyledonous woody xylem is also absent. The absence of secondary growth wood causes the stems of monocots, even of palms and large bamboos, to be columnar rather than tapering.
Bamboos are some of the fastest-growing plants in the world, due to a unique rhizome-dependent system. Certain species of bamboo can grow 35 inches within a 24-hour period, at a rate of 3 cm/h (a growth of approximately 1 millimeter (or 0.02 inches) every 2 minutes). Bamboos are of notable economic and cultural significance in South Asia, Southeast Asia and East Asia, being used for building materials, as a food source, and as a versatile raw product. Bamboo has a higher compressive strength than wood, brick or concrete and a tensile strength that rivals steel.
The word bamboo comes from the Kannada term "bambu", which was introduced to English through Malay.
Genus and geography.
More than 10 genera are divided into about 1,450 species. Bamboo species are found in diverse climates, from cold mountains to hot tropical regions. They occur across East Asia, from 50°N latitude in Sakhalin through to Northern Australia, and west to India and the Himalayas. They also occur in sub-Saharan Africa, and in the Americas from the mid-Atlantic United States south to Argentina and Chile, reaching their southernmost point at 47°S latitude. Continental Europe is not known to have any native species of bamboo.
Recently, some attempts have been made to grow bamboo on a commercial basis in the Great Lakes region of east-central Africa, especially in Rwanda. In the United States, several companies are growing, harvesting, and distributing species such as "Phyllostachys nigra" (Henon) and "Phyllostachys edulis" (Moso).
Bamboo grows in two main forms: the woody bamboos (Arundinarieae and Bambuseae) and the understory herbaceous bamboos (Olyreae). Molecular analysis suggests that there are 3–5 major lineages of bamboo. Four major lineages are currently recognized: temperate woody, paleotropical woody, neotropical woody and herbaceous.
Ecology.
Bamboo is one of the fastest-growing plants on Earth, with reported growth rates of 250 cm in 24 hours. However, the growth rate is dependent on local soil and climatic conditions, as well as species, and a more typical growth rate for many commonly cultivated bamboos in temperate climates is in the range of 3 – per day during the growing period. Primarily growing in regions of warmer climates during the late Cretaceous period, vast fields existed in what is now Asia. Some of the largest timber bamboo can grow over 30 m tall, and be as large as 15 – in diameter. However, the size range for mature bamboo is species dependent, with the smallest bamboos reaching only several inches high at maturity. A typical height range that would cover many of the common bamboos grown in the United States is 4.6 -, depending on species. Anji County of China, known as the "Town of Bamboo", provides the optimal climate and soil conditions to grow, harvest, and process some of the most valued bamboo poles available worldwide.
Unlike all trees, individual bamboo stems, or culms, emerge from the ground at their full diameter and grow to their full height in a single growing season of three to four months. During these several months, each new shoot grows vertically into a culm with no branching out until the majority of the mature height is reached. Then, the branches extend from the nodes and leafing out occurs. In the next year, the pulpy wall of each culm slowly hardens. During the third year, the culm hardens further. The shoot is now considered a fully mature culm. Over the next 2–5 years (depending on species), fungus begins to form on the outside of the culm, which eventually penetrates and overcomes the culm. Around 5–8 years later (species and climate dependent), the fungal growths cause the culm to collapse and decay. This brief life means culms are ready for harvest and suitable for use in construction within about three to seven years. Individual bamboo culms do not get any taller or larger in diameter in subsequent years than they do in their first year, and they do not replace any growth lost from pruning or natural breakage. Bamboos have a wide range of hardiness depending on species and locale. Small or young specimens of an individual species will produce small culms initially. As the clump and its rhizome system mature, taller and larger culms will be produced each year until the plant approaches its particular species limits of height and diameter.
Many tropical bamboo species will die at or near freezing temperatures, while some of the hardier or so-called temperate bamboos can survive temperatures as low as -29 °C. Some of the hardiest bamboo species can be grown in places as cold as USDA Plant Hardiness Zones 5–6, although they typically will defoliate and may even lose all above-ground growth, yet the rhizomes will survive and send up shoots again the next spring. In milder climates, such as USDA Zone 8 and above, some hardy bamboo may remain fully leafed out year-round.
Mass flowering.
Most bamboo species flower infrequently. In fact, many bamboos only flower at intervals as long as 65 or 120 years. These taxa exhibit mass flowering (or gregarious flowering), with all plants in a particular "cohort" flowering over a several-year period. Any plant derived through clonal propagation from this cohort will also flower regardless of whether it has been planted in a different location. The longest mass flowering interval known is 130 years, and it is for the species "Phyllostachys bambusoides" (Sieb. & Zucc.). In this species, all plants of the same stock flower at the same time, regardless of differences in geographic locations or climatic conditions, and then the bamboo dies. The lack of environmental impact on the time of flowering indicates the presence of some sort of "alarm clock" in each cell of the plant which signals the diversion of all energy to flower production and the cessation of vegetative growth. This mechanism, as well as the evolutionary cause behind it, is still largely a mystery.
One hypothesis to explain the evolution of this semelparous mass flowering is the "predator satiation hypothesis" which argues that by fruiting at the same time, a population increases the survival rate of their seeds by flooding the area with fruit, so, even if predators eat their fill, seeds will still be left over. By having a flowering cycle longer than the lifespan of the rodent predators, bamboos can regulate animal populations by causing starvation during the period between flowering events. Thus the death of the adult clone is due to resource exhaustion, as it would be more effective for parent plants to devote all resources to creating a large seed crop than to hold back energy for their own regeneration.
Another, the "fire cycle hypothesis", argues that periodic flowering followed by death of the adult plants has evolved as a mechanism to create disturbance in the habitat, thus providing the seedlings with a gap in which to grow. This argues that the dead culms create a large fuel load, and also a large target for lightning strikes, increasing the likelihood of wildfire. Because bamboos can be aggressive as early successional plants, the seedlings would be able to outstrip other plants and take over the space left by their parents.
However, both have been disputed for different reasons. The predator satiation hypothesis does not explain why the flowering cycle is 10 times longer than the lifespan of the local rodents, something not predicted. The bamboo fire cycle hypothesis is considered by a few scientists to be unreasonable; they argue that fires only result from humans and there is no natural fire in India. This notion is considered wrong based on distribution of lightning strike data during the dry season throughout India. However, another argument against this is the lack of precedent for any living organism to harness something as unpredictable as lightning strikes to increase its chance of survival as part of natural evolutionary progress.
The mass fruiting also has direct economic and ecological consequences, however. The huge increase in available fruit in the forests often causes a boom in rodent populations, leading to increases in disease and famine in nearby human populations. For example, devastating consequences occur when the "Melocanna bambusoides" population flowers and fruits once every 30–35 years around the Bay of Bengal. The death of the bamboo plants following their fruiting means the local people lose their building material, and the large increase in bamboo fruit leads to a rapid increase in rodent populations. As the number of rodents increases, they consume all available food, including grain fields and stored food, sometimes leading to famine. These rats can also carry dangerous diseases, such as typhus, typhoid, and bubonic plague, which can reach epidemic proportions as the rodents increase in number. The relationship between rat populations and bamboo flowering was examined in a 2009 "Nova" documentary .
In any case, flowering produces masses of seeds, typically suspended from the ends of the branches. These seeds will give rise to a new generation of plants that may be identical in appearance to those that preceded the flowering, or they may produce new cultivars with different characteristics, such as the presence or absence of striping or other changes in coloration of the culms.
Several bamboo species are never known to set seed even when sporadically flowering has been reported. "Bambusa vulgaris", "Bambusa balcooa" and "Dendrocalamus stocksii" are common examples of such bamboo.
As animal diet.
Soft bamboo shoots, stems, and leaves are the major food source of the giant panda of China, the red panda of Nepal and the bamboo lemurs of Madagascar. Rats will eat the fruits as described above. Mountain gorillas of Africa also feed on bamboo, and have been documented consuming bamboo sap which was fermented and alcoholic; chimps and elephants of the region also eat the stalks.
The larvae of the bamboo borer (the moth "Omphisa fuscidentalis") of Laos, Myanmar, Thailand and Yunnan Province, China, feeds off the pulp of live bamboo. In turn, these caterpillars are considered a local delicacy.
Cultivation.
Commercial timber.
Timber is harvested from both cultivated and wild stands, and some of the larger bamboos, particularly species in the genus "Phyllostachys", are known as "timber bamboos".
Harvesting.
Bamboo used for construction purposes must be harvested when the culms reach their greatest strength and when sugar levels in the sap are at their lowest, as high sugar content increases the ease and rate of pest infestation.
Harvesting of bamboo is typically undertaken according to the following cycles:
1) Life cycle of the culm:
As each individual culm goes through a 5– to 7-year life cycle, culms are ideally allowed to reach this level of maturity prior to full capacity harvesting. The clearing out or thinning of culms, particularly older decaying culms, helps to ensure adequate light and resources for new growth. Well-maintained clumps may have a productivity three to four times that of an unharvested wild clump. Consistent with the life cycle described above, bamboo is harvested from two to three years through to five to seven years, depending on the species.
2) Annual cycle:
As all growth of new bamboo occurs during the wet season, disturbing the clump during this phase will potentially damage the upcoming crop. Also during this high rainfall period, sap levels are at their highest, and then diminish towards the dry season. Picking immediately prior to the wet/growth season may also damage new shoots. Hence, harvesting is best a few months prior to the start of the wet season.
3) Daily cycle:
During the height of the day, photosynthesis is at its peak, producing the highest levels of sugar in sap, making this the least ideal time of day to harvest. Many traditional practitioners believe the best time to harvest is at dawn or dusk on a waning moon.
Leaching.
Leaching is the removal of sap after harvest. In many areas of the world, the sap levels in harvested bamboo are reduced either through leaching or postharvest photosynthesis.
Examples of this practice include:
In the process of water leaching, the bamboo is dried slowly and evenly in the shade to avoid cracking in the outer skin of the bamboo, thereby reducing opportunities for pest infestation.
Durability of bamboo in construction is directly related to how well it is handled from the moment of planting through harvesting, transportation, storage, design, construction and maintenance. Bamboo harvested at the correct time of year and then exposed to ground contact or rain, will break down just as quickly as incorrectly harvested material.
Ornamental bamboos.
The two general patterns for the growth of bamboo are "clumping" (sympodial) and "running" (monopodial). Clumping bamboo species tend to spread slowly, as the growth pattern of the rhizomes is to simply expand the root mass gradually, similar to ornamental grasses. "Running" bamboos, on the other hand, need to be controlled during cultivation because of their potential for aggressive behavior. They spread mainly through their roots and/or rhizomes, which can spread widely underground and send up new culms to break through the surface. Running bamboo species are highly variable in their tendency to spread; this is related to both the species and the soil and climate conditions. Some can send out runners of several metres a year, while others can stay in the same general area for long periods. If neglected, over time they can cause problems by moving into adjacent areas.
Bamboos seldom and unpredictably flower, and the frequency of flowering varies greatly from species to species. Once flowering takes place, a plant will decline and often die entirely. Although there are always a few species of bamboo in flower at any given time, collectors desiring to grow specific bamboo typically obtain their plants as divisions of already-growing plants, rather than waiting for seeds to be produced.
Regular observations will indicate major growth directions and locations. Once the rhizomes are cut, they are typically removed; however, rhizomes take a number of months to mature, and an immature, severed rhizome will usually cease growing if left in-ground. If any bamboo shoots come up outside of the bamboo area afterwards, their presence indicates the precise location of the removed rhizome. The fibrous roots that radiate from the rhizomes do not produce more bamboo.
Bamboo growth is also controlled by surrounding the plant or grove with a physical barrier. Typically, concrete and specially rolled HDPE plastic are the materials used to create the barrier, which is placed in a 60– to 90-cm-deep ditch around the planting, and angled out at the top to direct the rhizomes to the surface. (This is only possible if the barrier is installed in a straight line.) If the containment area is small, this method can be detrimental to ornamental bamboo, as the bamboo within can become rootbound and start to display the signs of any unhealthy containerized plant. In addition, rhizomes can escape over the top, or beneath the barrier if it is not deep enough. Strong rhizomes and tools can penetrate plastic barriers, so care must be taken. In small areas, regular maintenance may be the best method for controlling the running bamboos. Barriers and edging are unnecessary for clump-forming bamboos, although these may eventually need to have portions removed if they become too large.
The ornamental plant sold in containers and marketed as "lucky bamboo" is actually an entirely unrelated plant, "Dracaena sanderiana". It is a resilient member of the lily family that grows in the dark, tropical rainforests of Southeast Asia and Africa. Lucky bamboo has long been associated with the Eastern practice of "feng shui" and images of the plant widely available on the Web are often used to depict bamboo. On a similar note, Japanese knotweed is also sometimes mistaken for a bamboo, but it grows wild and is considered an invasive species.
"Phyllostachys" species of bamboo are also considered invasive and illegal to sell or propagate in some areas of the US.
Uses.
Culinary.
Although the shoots (new culms that come out of the ground) of bamboo contain a toxin taxiphyllin (a cyanogenic glycoside) that produces cyanide in the gut, proper processing renders them edible. They are used in numerous Asian dishes and broths, and are available in supermarkets in various sliced forms, in both fresh and canned versions. The golden bamboo lemur ingests many times the quantity of the taxiphyllin-containing bamboo that would kill a human.
The bamboo shoot in its fermented state forms an important ingredient in cuisines across the Himalayas. In Assam, India, for example, it is called "khorisa". In Nepal, a delicacy popular across ethnic boundaries consists of bamboo shoots fermented with turmeric and oil, and cooked with potatoes into a dish that usually accompanies rice ("alu tama (आलु तामा)" in Nepali).
In Indonesia, they are sliced thin and then boiled with "santan" (thick coconut milk) and spices to make a dish called "gulai rebung". Other recipes using bamboo shoots are "sayur lodeh" (mixed vegetables in coconut milk) and "lun pia" (sometimes written "lumpia": fried wrapped bamboo shoots with vegetables). The shoots of some species contain toxins that need to be leached or boiled out before they can be eaten safely.
Pickled bamboo, used as a condiment, may also be made from the pith of the young shoots.
The sap of young stalks tapped during the rainy season may be fermented to make "ulanzi" (a sweet wine) or simply made into a soft drink. Bamboo leaves are also used as wrappers for steamed dumplings which usually contains glutinous rice and other ingredients.
Pickled bamboo shoots (Nepali:"तामा tama") are cooked with black-eyed beans as a delicacy food in Nepal. Many Nepalese restaurant around the world serve this dish as "aloo bodi tama". Fresh bamboo shoots are sliced and pickled with mustard seeds and turmeric and kept in glass jar in direct sunlight for the best taste. It is used alongside many dried beans in cooking during winter months. Baby shoots (Nepali: tusa) of a very different variety of bamboo (Nepali: निगालो Nigalo) native to Nepal is cooked as a curry in Hilly regions.
In Sambalpur, India, the tender shoots are grated into juliennes and fermented to prepare "kardi". The name is derived from the Sanskrit word for bamboo shoot, "karira". This fermented bamboo shoot is used in various culinary preparations, notably "amil", a sour vegetable soup. It is also made into pancakes using rice flour as a binding agent. The shoots that have turned a little fibrous are fermented, dried, and ground to sand-sized particles to prepare a garnish known as "hendua". It is also cooked with tender pumpkin leaves to make sag green leaves.
In Konkani cuisine, the tender shoots (kirlu) are grated and cooked with crushed jackfruit seeds to prepare 'kirla sukke'.
The empty hollow in the stalks of larger bamboo is often used to cook food in many Asian cultures. Soups are boiled and rice is cooked in the hollows of fresh stalks of bamboo directly over a flame. Similarly, steamed tea is sometimes rammed into bamboo hollows to produce compressed forms of Pu-erh tea. Cooking food in bamboo is said to give the food a subtle but distinctive taste.
In addition, bamboo is frequently used for cooking utensils within many cultures, and is used in the manufacture of chopsticks. In modern times, some see bamboo tools as an ecofriendly alternative to other manufactured utensils.
Medicine.
Bamboo is used in Chinese medicine for treating infections and healing. In northern Indian state of Assam, the fermented bamboo paste known as "khorisa" is known locally as a folk remedy for the treatment of impotence, infertility, and menstrual pains.
Construction.
Bamboo, like true wood, is a natural composite material with a high strength-to-weight ratio useful for structures.
In its natural form, bamboo as a construction material is traditionally associated with the cultures of South Asia, East Asia and the South Pacific, to some extent in Central and South America, and by extension in the aesthetic of Tiki culture. In China and India, bamboo was used to hold up simple suspension bridges, either by making cables of split bamboo or twisting whole culms of sufficiently pliable bamboo together. One such bridge in the area of Qian-Xian is referenced in writings dating back to 960 AD and may have stood since as far back as the third century BC, due largely to continuous maintenance.
Bamboo has also long been used as scaffolding; the practice has been banned in China for buildings over six stories, but is still in continuous use for skyscrapers in Hong Kong. In the Philippines, the nipa hut is a fairly typical example of the most basic sort of housing where bamboo is used; the walls are split and woven bamboo, and bamboo slats and poles may be used as its support. In Japanese architecture, bamboo is used primarily as a supplemental and/or decorative element in buildings such as fencing, fountains, grates and gutters, largely due to the ready abundance of quality timber.
Various structural shapes may be made by training the bamboo to assume them as it grows. Squared sections of bamboo are created by compressing the growing stalk within a square form. Arches may similarly be created by forcing the bamboo's growth into the desired form, costing much less than it would to obtain the same shape with regular wood timber. More traditional forming methods, such as the application of heat and pressure, may also be used to curve or flatten the cut stalks.
Bamboo can be cut and laminated into sheets and planks. This process involves cutting stalks into thin strips, planing them flat, and boiling and drying the strips; they are then glued, pressed and finished. Long used in China and Japan, entrepreneurs started developing and selling laminated bamboo flooring in the West during the mid-1990s; products made from bamboo laminate, including flooring, cabinetry, furniture and even decorations, are currently surging in popularity, transitioning from the boutique market to mainstream providers such as Home Depot. The bamboo goods industry (which also includes small goods, fabric, etc.) is expected to be worth $25 billion by 2012. The quality of bamboo laminate varies among manufacturers and varies according to the maturity of the plant from which it was harvested (six years being considered the optimum); the sturdiest products fulfill their claims of being up to three times harder than oak hardwood while others may be softer than standard hardwood.
Bamboo intended for use in construction should be treated to resist insects and rot. The most common solution for this purpose is a mixture of borax and boric acid. Another process involves boiling cut bamboo to remove the starches that attract insects.
Bamboo has been used as reinforcement for concrete in those areas where it is plentiful, though dispute exists over its effectiveness in the various studies done on the subject. Bamboo does have the necessary strength to fulfil this function, but untreated bamboo will swell with water absorbed from the concrete, causing it to crack. Several procedures must be followed to overcome this shortcoming.
Several institutes, businesses, and universities are researching the use of bamboo as an ecological construction material. In the United States and France, it is possible to get houses made entirely of bamboo, which are earthquake- and cyclone-resistant and internationally certified. In Bali, Indonesia, an international K-12 school, the , is constructed entirely of bamboo, for its beauty and advantages as a sustainable resource. There are three ISO standards for bamboo as a construction material.
In parts of India, bamboo is used for drying clothes indoors, both as a rod high up near the ceiling to hang clothes on, and as a stick wielded with acquired expert skill to hoist, spread, and to take down the clothes when dry. It is also commonly used to make ladders, which apart from their normal function, are also used for carrying bodies in funerals. In Maharashtra, the bamboo groves and forests are called Veluvana, the name "velu" for bamboo is most likely from Sanskrit, while "vana" means forest.
Furthermore, bamboo is also used to create flagpoles for saffron-coloured, Hindu religious flags, which can be seen fluttering across India, especially in Bihar and Uttar Pradesh, as well as in Guyana and Suriname in South America.
Bamboo was used for the structural members of the India pavilion at Expo 2010 in Shanghai. The pavilion is the world’s largest bamboo dome, about 34 m in diameter, with bamboo beams/members overlaid with a ferro-concrete slab, waterproofing, copper plate, solar PV panels, a small windmill, and live plants. A total of 30 km of bamboo was used. The dome is supported on 18-m-long steel piles and a series of steel ring beams. The bamboo was treated with borax and boric acid as a fire retardant and insecticide and bent in the required shape. The bamboo sections were joined with reinforcement bars and concrete mortar to achieve the necessary lengths.
Textiles.
Since the fibers of bamboo are very short (less than 3 mm), they are not usually transformed into yarn by a natural process. The usual process by which textiles labeled as being made of bamboo are produced uses only rayon made from the fibers with heavy employment of chemicals. To accomplish this, the fibers are broken down with chemicals and extruded through mechanical spinnerets; the chemicals include lye, carbon disulfide and strong acids. Retailers have sold both end products as "bamboo fabric" to cash in on bamboo's current ecofriendly cachet; however, the Canadian Competition Bureau and the US Federal Trade Commission, as of mid-2009, are cracking down on the practice of labeling bamboo rayon as natural bamboo fabric. Under the guidelines of both agencies, these products must be labeled as rayon with the optional qualifier "from bamboo".
As a writing surface.
Bamboo was in widespread use in early China as a medium for written documents. The earliest surviving examples of such documents, written in ink on string-bound bundles of bamboo strips (or "slips"), date from the fifth century BC during the Warring States period. However, references in earlier texts surviving on other media make it clear that some precursor of these Warring States period bamboo slips was in use as early as the late Shang period (from about 1250 BC).
Bamboo or wooden strips were the standard writing material during the Han dynasty, and excavated examples have been found in abundance. Subsequently, paper began to displace bamboo and wooden strips from mainstream uses, and by the fourth century AD, bamboo slips had been largely abandoned as a medium for writing in China.
Several paper industries are surviving on bamboo forests. Ballarpur (Chandrapur, Maharstra) paper mills use bamboo for paper production.
Bamboo fiber has been used to make paper in China since early times. A high-quality, handmade paper is still produced in small quantities. Coarse bamboo paper is still used to make spirit money in many Chinese communities.
Bamboo pulps are mainly produced in China, Myanmar, Thailand and India, and are used in printing and writing papers. The most common bamboo species used for paper are "Dendrocalamus asper" and "Bamboo bluemanea". It is also possible to make dissolving pulp from bamboo. The average fiber length is similar to hardwoods, but the properties of bamboo pulp are closer to softwood pulps due to it having a very broad fiber length distribution. With the help of molecular tools, it is now possible to distinguish the superior fiber-yielding species/varieties even at juvenile stages of their growth, which can help in unadulterated merchandise production.
Weapons.
Bamboo has often been used to construct weapons and is still incorporated in several Asian martial arts.
Other uses.
Bamboo has traditionally been used to make a wide range of everyday utensils, particularly in Japan, where archaeological excavations have uncovered bamboo baskets dating to the Late Jomon period (2000–1000 BC). 
Bamboo has a long history of use in Asian furniture. Chinese bamboo furniture is a distinct style based on a millennia-long tradition.
Several manufacturers offer bamboo bicycles, surfboards, snowboards, and skateboards.
Due to its flexibility, bamboo is also used to make fishing rods. The split cane rod is especially prized for fly fishing.
Bamboo has been traditionally used in Malaysia as a firecracker called a "meriam buluh" or bamboo cannon. Four-foot-long sections of bamboo are cut, and a mixture of water and calcium carbide are introduced. The resulting acetylene gas is ignited with a stick, producing a loud bang.
Bamboo can be used in water desalination. A bamboo filter is used to remove the salt from seawater.
Food is cooked in East Timor in bamboo in fire. This is called "Tukir".
Many minority groups in remote areas that have water access in Asia use bamboo that is 3–5 years old to make rafts. They use 8 to 12 poles, 6–7 metres (20 to 24 feet) long, laid together side by side to a width of about 1 metre (3.5 feet). Once the poles are lined up together, they cut a hole crosswise through the poles at each end and use a small bamboo pole pushed through that hole like a screw to hold all the long bamboo poles together. Floating houses use whole bamboo stalks tied together in a big bunch to support the house floating in the water. Bamboo is also used to make eating utensils such as chop sticks, trays, and tea scoops.
The Song Dynasty (960–1279 AD) Chinese scientist and polymath Shen Kuo (1031–1095) used the evidence of underground petrified bamboo found in the dry northern climate of Yan'an, Shanbei region, Shaanxi province to support his geological theory of gradual climate change.
Symbolism and culture.
Bamboo's long life makes it a Chinese symbol of uprightness, while in India it is a symbol of friendship. The rarity of its blossoming has led to the flowers' being regarded as a sign of impending famine. This may be due to rats feeding upon the profusion of flowers, then multiplying and destroying a large part of the local food supply. The most recent flowering began in May 2006 (see Mautam). Bamboo is said to bloom in this manner only about every 50 years (see 28–60 year examples in ).
In Chinese culture, the bamboo, plum blossom, orchid, and chrysanthemum (often known as "méi lán zhú jú" 梅兰竹菊) are collectively referred to as the Four Gentlemen. These four plants also represent the four seasons and, in Confucian ideology, four aspects of the "junzi" ("prince" or "noble one"). The pine ("sōng" 松), the bamboo ("zhú" 竹), and the plum blossom ("méi" 梅) are also admired for their perseverance under harsh conditions, and are together known as the "Three Friends of Winter" (岁寒三友 "suìhán sānyǒu") in Chinese culture. The "Three Friends of Winter" is traditionally used as a system of ranking in Japan, for example in sushi sets or accommodations at a traditional "ryokan". Pine ("matsu" 松) is of the first rank, bamboo ("také" 竹) is of second rank, and plum ("ume" 梅) is of the third.
The Bozo ethnic group of West Africa take their name from the Bambara phrase "bo-so", which means "bamboo house". Bamboo is also the national plant of St. Lucia.
Bamboo, noble and useful.
Bamboo, one of the "Four Gentlemen" (bamboo, orchid, plum blossom and chrysanthemum), plays such an important role in traditional Chinese culture that it is even regarded as a behavior model of the gentleman. As bamboo has some features such as uprightness, tenacity and hollow heart, people endow bamboo with integrity, elegance and plainness, though it is not physically strong. Ancient Chinese poets wrote countless poems to praise bamboo, but actually they were truly talking about people like bamboo and express their understanding of what a real gentleman should be like. According to laws, an ancient poet, Bai Juyi (772–846), thought that to be a gentleman, a man does not need to be physically strong, but he must be mentally strong, upright, and perseverant. Just as a bamboo is hollow-hearted, he should open his heart to accept anything of benefit and never have arrogance or prejudice.
Bamboo is not only a symbol of gentleman, but also an important role in Buddhism. In the first century, Buddhism was introduced into China. As canons of Buddhism do not allow its believers to do anything cruel to animals, flesh and egg were not allowed in the diet. However, people need something nutritious to live; thus, the tender bamboo shoot ("sǔn"筍 in Chinese) became a good choice. The bamboo shoot is nutritious, and eating it does not violate the canon. With thousands of years’ development, how to eat bamboo shoot has become a part of cuisine system, especially for monks. A Buddhist monk, Zan Ning, wrote a manual of the bamboo shoot called ""Sǔn Pǔ"筍譜". He offered descriptions and recipes for many kinds of bamboo shoots.
Bamboo shoot has always been a traditional dish on the Chinese dinner table, especially in southern China.
In ancient times, as long as people had money to buy a big house with yard, they would always plant bamboos in their garden. Bamboo is a necessary element of Chinese culture, or even in the whole Asian civilization. People plant bamboos, eat bamboo shoots, paint bamboos, write poems for bamboos, and speak highly of gentlemen who are like bamboos. Bamboo is not only a plant, but also a part of people’s lives.
In Japan, a bamboo forest sometimes surrounds a Shinto shrine as part of a sacred barrier against evil. Many Buddhist temples also have bamboo groves.
Bamboo plays an important part of the culture of Vietnam. Bamboo symbolizes the spirit of Vovinam (a Vietnamese martial arts): "cương nhu phối triển" (coordination between hard and soft (martial arts)). Bamboo also symbolizes the Vietnamese hometown and Vietnamese soul: the gentlemanlike, straightforwardness, hard working, optimism, unity, and adaptability. A Vietnamese proverb says, "When the bamboo is old, the bamboo sprouts appear", the meaning being Vietnam will never be annihilated; if the previous generation dies, the children take their place. Therefore, the Vietnam nation and Vietnamese value will be maintained and developed eternally. Traditional Vietnamese villages are surrounded by thick bamboo hedges ("lũy tre").
In mythology.
Several Asian cultures, including that of the Andaman Islands, believe humanity emerged from a bamboo stem.
In Philippine mythology, one of the more famous creation accounts tells of the first man, "Malakás" ("Strong"), and the first woman, "Maganda" ("Beautiful"), each emerged from one half of a split bamboo stem on an island formed after the battle between Sky and Ocean. In Malaysia, a similar story includes a man who dreams of a beautiful woman while sleeping under a bamboo plant; he wakes up and breaks the bamboo stem, discovering the woman inside. The Japanese folktale "Tale of the Bamboo Cutter" ("Taketori Monogatari") tells of a princess from the Moon emerging from a shining bamboo section. Hawaiian bamboo "('ohe)" is a "kinolau" or body form of the Polynesian creator god Kāne.
A bamboo cane is also the weapon of Vietnamese legendary hero, Saint Giong, who had grown up immediately and magically since the age of three because of his wish to liberate his land from Ân invaders. An ancient Vietnamese legend (The Hundred-knot Bamboo Tree) tells of a poor, young farmer who fell in love with his landlord's beautiful daughter. The farmer asked the landlord for his daughter's hand in marriage, but the proud landlord would not allow her to be bound in marriage to a poor farmer. The landlord decided to foil the marriage with an impossible deal; the farmer must bring him a "bamboo tree of 100 nodes". But Gautama Buddha ("Bụt") appeared to the farmer and told him that such a tree could be made from 100 nodes from several different trees. "Bụt" gave to him four magic words to attach the many nodes of bamboo: "Khắc nhập, khắc xuất", which means "joined together immediately, fell apart immediately". The triumphant farmer returned to the landlord and demanded his daughter. Curious to see such a long bamboo, the landlord was magically joined to the bamboo when he touched it, as the young farmer said the first two magic words. The story ends with the happy marriage of the farmer and the landlord's daughter after the landlord agreed to the marriage and asked to be separated from the bamboo.
In a Chinese legend, the Emperor Yao gave two of his daughters to the future Emperor Shun as a test for his potential to rule. Shun passed the test of being able to run his household with the two emperor's daughters as wives, and thus Yao made Shun his successor, bypassing his unworthy son. Later, Shun drowned in the Xiang River. The tears his two bereaved wives let fall upon the bamboos growing there explains the origin of spotted bamboo. The two women later became goddesses.

</doc>
<doc id="39068" url="http://en.wikipedia.org/wiki?curid=39068" title="Digital electronics">
Digital electronics

Digital electronics, or digital (electronic) circuits, are electronics that represent signals by discrete bands of analog levels, rather than by continuous ranges (as used in analogue electronics). All levels within a band represent the same signal state. Because of this discretization, relatively small changes to the analog signal levels due to manufacturing tolerance, signal attenuation or parasitic noise do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry.
In most cases the number of these states is two, and they are represented by two voltage bands: one near a reference value (typically termed as "ground" or zero volts), and the other a value near the supply voltage. These correspond to the "false" ("0") and "true" ("1") values of the Boolean domain, respectively, yielding binary code.
Digital techniques are useful because it is easier to get an electronic device to switch into one of a number of known states than to accurately reproduce a continuous range of values.
Digital electronic circuits are usually made from large assemblies of logic gates, simple electronic representations of Boolean logic functions.
Advantages.
An advantage of digital circuits when compared to analog circuits is that signals represented digitally can be transmitted without degradation due to noise. For example, a continuous audio signal transmitted as a sequence of 1s and 0s, can be reconstructed without error, provided the noise picked up in transmission is not enough to prevent identification of the 1s and 0s. An hour of music can be stored on a compact disc using about 6 billion binary digits.
In a digital system, a more precise representation of a signal can be obtained by using more binary digits to represent it. While this requires more digital circuits to process the signals, each digit is handled by the same kind of hardware, resulting in an easily scalable system. In an analog system, additional resolution requires fundamental improvements in the linearity and noise characteristics of each step of the signal chain.
Computer-controlled digital systems can be controlled by software, allowing new functions to be added without changing hardware. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands.
Information storage can be easier in digital systems than in analog ones. The noise-immunity of digital systems permits data to be stored and retrieved without degradation. In an analog system, noise from aging and wear degrade the information stored. In a digital system, as long as the total noise is below a certain level, the information can be recovered perfectly.
Disadvantages.
In some cases, digital circuits use more energy than analog circuits to accomplish the same tasks, thus producing more heat which increases the complexity of the circuits such as the inclusion of heat sinks. In portable or battery-powered systems this can limit use of digital systems.
For example, battery-powered cellular telephones often use a low-power analog front-end to amplify and tune in the radio signals from the base station. However, a base station has grid power and can use power-hungry, but very flexible software radios. Such base stations can be easily reprogrammed to process the signals used in new cellular standards.
Digital circuits are sometimes more expensive, especially in small quantities.
Most useful digital systems must translate from continuous analog signals to discrete digital signals. This causes quantization errors. Quantization error can be reduced if the system stores enough digital data to represent the signal to the desired degree of fidelity. The Nyquist-Shannon sampling theorem provides an important guideline as to how much digital data is needed to accurately portray a given analog signal.
In some systems, if a single piece of digital data is lost or misinterpreted, the meaning of large blocks of related data can completely change. Because of the cliff effect, it can be difficult for users to tell if a particular system is right on the edge of failure, or if it can tolerate much more noise before failing.
Digital fragility can be reduced by designing a digital system for robustness. For example, a parity bit or other error management method can be inserted into the signal path. These schemes help the system detect errors, and then either correct the errors, or at least ask for a new copy of the data. In a state-machine, the state transition logic can be designed to catch unused states and trigger a reset sequence or other error recovery routine.
Digital memory and transmission systems can use techniques such as error detection and correction to use additional data to correct any errors in transmission and storage.
On the other hand, some techniques used in digital systems make those systems more vulnerable to single-bit errors. These techniques are acceptable when the underlying bits are reliable enough that such errors are highly unlikely.
A single-bit error in audio data stored directly as linear pulse code modulation (such as on a CD-ROM) causes, at worst, a single click. Instead, many people use audio compression to save storage space and download time, even though a single-bit error may corrupt the entire song.
Design issues in digital circuits.
Digital circuits are made from analog components. The design must assure that the analog nature of the components doesn't dominate the desired digital behavior. Digital systems must manage noise and timing margins, parasitic inductances and capacitances, and filter power connections.
Bad designs have intermittent problems such as "glitches", vanishingly fast pulses that may trigger some logic but not others, "runt pulses" that do not reach valid "threshold" voltages, or unexpected ("undecoded") combinations of logic states.
Additionally, where clocked digital systems interface to analog systems or systems that are driven from a different clock, the digital system can be subject to metastability where a change to the input violates the set-up time for a digital input latch. This situation will self-resolve, but will take a random time, and while it persists can result in invalid signals being propagated within the digital system for a short time.
Since digital circuits are made from analog components, digital circuits calculate more slowly than low-precision analog circuits that use a similar amount of space and power. However, the digital circuit will calculate more repeatably, because of its high noise immunity. On the other hand, in the high-precision domain (for example, where 14 or more bits of precision are needed), analog circuits require much more power and area than digital equivalents.
Construction.
A digital circuit is often constructed from small electronic circuits called logic gates that can be used to create combinational logic. Each logic gate represents a function of boolean logic. A logic gate is an arrangement of electrically controlled switches, better known as transistors.
Each logic symbol is represented by a different shape. The actual set of shapes was introduced in 1984 under IEEE/ANSI standard 91-1984. "The logic symbol given under this standard are being increasingly used now and have even started appearing in the literature published by manufacturers of digital integrated circuits."
The output of a logic gate is an electrical flow or voltage, that can, in turn, control more logic gates.
Logic gates often use the fewest number of transistors in order to reduce their size, power consumption and cost, and increase their reliability.
Integrated circuits are the least expensive way to make logic gates in large volumes. Integrated circuits are usually designed by engineers using electronic design automation software (see below for more information).
Another form of digital circuit is constructed from lookup tables, (many sold as "programmable logic devices", though other kinds of PLDs exist). Lookup tables can perform the same functions as machines based on logic gates, but can be easily reprogrammed without changing the wiring. This means that a designer can often repair design errors without changing the arrangement of wires. Therefore, in small volume products, programmable logic devices are often the preferred solution. They are usually designed by engineers using electronic design automation software.
When the volumes are medium to large, and the logic can be slow, or involves complex algorithms or sequences, often a small microcontroller is programmed to make an embedded system. These are usually programmed by software engineers.
When only one digital circuit is needed, and its design is totally customized, as for a factory production line controller, the conventional solution is a programmable logic controller, or PLC. These are usually programmed by electricians, using ladder logic.
Structure of digital systems.
Engineers use many methods to minimize logic functions, in order to reduce the circuit's complexity. When the complexity is less, the circuit also has fewer errors and less electronics, and is therefore less expensive.
The most widely used simplification is a minimization algorithm like the Espresso heuristic logic minimizer within a CAD system, although historically, binary decision diagrams, an automated Quine–McCluskey algorithm, truth tables, Karnaugh maps, and Boolean algebra have been used.
Representation.
Representations are crucial to an engineer's design of digital circuits. Some analysis methods only work with particular representations.
The classical way to represent a digital circuit is with an equivalent set of logic gates. Another way, often with the least electronics, is to construct an equivalent system of electronic switches (usually transistors). One of the easiest ways is to simply have a memory containing a truth table. The inputs are fed into the address of the memory, and the data outputs of the memory become the outputs.
For automated analysis, these representations have digital file formats that can be processed by computer programs. Most digital engineers are very careful to select computer programs ("tools") with compatible file formats.
Combinational vs. Sequential.
To choose representations, engineers consider types of digital systems. Most digital systems divide into "combinational systems" and "sequential systems." A combinational system always presents the same output when given the same inputs. It is basically a representation of a set of logic functions, as already discussed.
A sequential system is a combinational system with some of the outputs fed back as inputs. This makes the digital machine perform a "sequence" of operations. The simplest sequential system is probably a flip flop, a mechanism that represents a binary digit or "bit".
Sequential systems are often designed as state machines. In this way, engineers can design a system's gross behavior, and even test it in a simulation, without considering all the details of the logic functions.
Sequential systems divide into two further subcategories. "Synchronous" sequential systems change state all at once, when a "clock" signal changes state. "Asynchronous" sequential systems propagate changes whenever inputs change. Synchronous sequential systems are made of well-characterized asynchronous circuits such as flip-flops, that change only when the clock changes, and which have carefully designed timing margins.
Synchronous Systems.
The usual way to implement a synchronous sequential state machine is to divide it into a piece of combinational logic and a set of flip flops called a "state register." Each time a clock signal ticks, the state register captures the feedback generated from the previous state of the combinational logic, and feeds it back as an unchanging input to the combinational part of the state machine. The fastest rate of the clock is set by the most time-consuming logic calculation in the combinational logic.
The state register is just a representation of a binary number. If the states in the state machine are numbered (easy to arrange), the logic function is some combinational logic that produces the number of the next state.
Asynchronous Systems.
As of 2014, almost all digital machines are synchronous designs because it is easier to create and verify a synchronous design. However, asynchronous logic is thought can be superior because its speed is not constrained by an arbitrary clock; instead, it runs at the maximum speed of its logic gates. Building an asynchronous system using faster parts makes the circuit faster.
Many systems need circuits that allow external unsynchronized signals to enter synchronous logic circuits. These are inherently asynchronous in their design and must be analyzed as such. Examples of widely used asynchronous circuits include synchronizer flip-flops, switch debouncers and arbiters.
Asynchronous logic components can be hard to design because all possible states, in all possible timings must be considered. The usual method is to construct a table of the minimum and maximum time that each such state can exist, and then adjust the circuit to minimize the number of such states. Then the designer must force the circuit to periodically wait for all of its parts to enter a compatible state (this is called "self-resynchronization"). Without such careful design, it is easy to accidentally produce asynchronous logic that is "unstable," that is, real electronics will have unpredictable results because of the cumulative delays caused by small variations in the values of the electronic components. 
Register Transfer Systems.
Many digital systems are data flow machines. These are usually designed using synchronous register transfer logic, using hardware description languages such as VHDL or Verilog.
In register transfer logic, binary numbers are stored in groups of flip flops called registers. The outputs of each register are a bundle of wires called a "bus" that carries that number to other calculations. A calculation is simply a piece of combinational logic. Each calculation also has an output bus, and these may be connected to the inputs of several registers. Sometimes a register will have a multiplexer on its input, so that it can store a number from any one of several buses. Alternatively, the outputs of several items may be connected to a bus through buffers that can turn off the output of all of the devices except one. A sequential state machine controls when each register accepts new data from its input.
Asynchronous register-transfer systems (such as computers) have a general solution. In the 1980s, some researchers discovered that almost all synchronous register-transfer machines could be converted to asynchronous designs by using first-in-first-out synchronization logic. In this scheme, the digital machine is characterized as a set of data flows. In each step of the flow, an asynchronous "synchronization circuit" determines when the outputs of that step are valid, and presents a signal that says, "grab the data" to the stages that use that stage's inputs. It turns out that just a few relatively simple synchronization circuits are needed.
Computer Design.
The most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or "word" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself.A "specialized computer" is usually a conventional computer with special-purpose control logic or microprogram.
In this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines.
Almost all computers are synchronous. However, true asynchronous computers have also been designed. One example is the Aspida DLX core. Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest componment, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.
Computer Architecture.
Computer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers.
Automated design tools.
To save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called "electronic design automation tools" or just "EDA."
Simple truth table-style descriptions of logic are often optimized with EDA that automatically produces reduced systems of logic gates or smaller lookup tables that still produce the desired outputs. The most common example of this kind of software is the Espresso heuristic logic minimizer.
Most practical algorithms for optimizing large logic systems use algebraic manipulations or binary decision diagrams, and there are promising experiments with genetic algorithms and annealing optimizations.
To automate costly engineering processes, some EDA can take state tables that describe state machines and automatically produce a truth table or a function table for the combinational logic of a state machine. The state table is a piece of text that lists each state, together with the conditions controlling the transitions between them and the belonging output signals.
It is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog.
Often, real logic systems are designed as a series of sub-projects, which are combined using a "tool flow." The tool flow is usually a "script," a simplified computer language that can invoke the software design tools in the right order.
Tool flows for large logic systems such as microprocessors can be thousands of commands long, and combine the work of hundreds of engineers.
Writing and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board.
Parts of tool flows are "debugged" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior.
Once the input data is believed correct, the design itself must still be verified for correctness. Some tool flows verify designs by first producing a design, and then scanning the design to produce compatible input data for the tool flow. If the scanned data matches the input data, then the tool flow has probably not introduced errors.
The functional verification data are usually called "test vectors." The functional test vectors may be preserved and used in the factory to test that newly constructed logic works correctly. However, functional test patterns don't discover common fabrication faults. Production tests are often designed by software tools called "test pattern generators". These generate test vectors by examining the structure of the logic and systematically generating tests for particular faults. This way the fault coverage can closely approach 100%, provided the design is properly made testable (see next section).
Once a design exists, and is verified and testable, it often needs to be processed to be manufacturable as well. Modern integrated circuits have features smaller than the wavelength of the light used to expose the photoresist. Manufacturability software adds interference patterns to the exposure masks to eliminate open-circuits, and enhance the masks' contrast.
Design for testability.
There are several reasons for testing a logic circuit. When the circuit is first developed, it is necessary to verify that the design circuit meets the required functional and timing specifications. When multiple copies of a correctly designed circuit are being manufactured, it is essential to test each copy to ensure that the manufacturing process has not introduced any flaws.
A large logic machine (say, with more than a hundred logical variables) can have an astronomical number of possible states. Obviously, in the factory, testing every state is impractical if testing each state takes a microsecond, and there are more states than the number of microseconds since the universe began. Unfortunately, this ridiculous-sounding case is typical.
Fortunately, large logic machines are almost always designed as assemblies of smaller logic machines. To save time, the smaller sub-machines are isolated by permanently installed "design for test" circuitry, and are tested independently.
One common test scheme known as "scan design" moves test bits serially (one after another) from external test equipment through one or more serial shift registers known as "scan chains". Serial scans have only one or two wires to carry the data, and minimize the physical size and expense of the infrequently used test logic.
After all the test data bits are in place, the design is reconfigured to be in "normal mode" and one or more clock pulses are applied, to test for faults (e.g. stuck-at low or stuck-at high) and capture the test result into flip-flops and/or latches in the scan shift register(s). Finally, the result of the test is shifted out to the block boundary and compared against the predicted "good machine" result.
In a board-test environment, serial to parallel testing has been formalized with a standard called "JTAG" (named after the "Joint Test Action Group" that proposed it).
Another common testing scheme provides a test mode that forces some part of the logic machine to enter a "test cycle." The test cycle usually exercises large independent parts of the machine.
Trade-offs.
Several numbers determine the practicality of a system of digital logic: cost, reliability, fanout and speed. Engineers explored numerous electronic devices to get an ideal combination of these traits.
Cost.
The cost of a logic gate is crucial. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, engineers always used the cheapest available electronic switches that could still fulfill the requirements.
The earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). To everyone's surprise, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost.
With the rise of integrated circuits, reducing the absolute number of chips used represented another way to save costs. The goal of a designer is not just to make the simplest circuit, but to keep the component count down. Sometimes this results in slightly more complicated designs with respect to the underlying digital logic but nevertheless reduces the number of components, board size, and even power consumption.
For example, in some logic families, NAND gates are the simplest digital gate to build. All other logical operations can be implemented by NAND gates. If a circuit already required a single NAND gate, and a single chip normally carried four NAND gates, then the remaining gates could be used to implement other logical operations like logical and. This could eliminate the need for a separate chip containing those different types of gates.
Reliability.
The "reliability" of a logic gate describes its mean time between failure (MTBF). Digital machines often have millions of logic gates. Also, most digital machines are "optimized" to reduce their cost. The result is that often, the failure of a single logic gate will cause a digital machine to stop working.
Digital machines first became useful when the MTBF for a switch got above a few hundred hours. Even so, many of these machines had complex, well-rehearsed repair procedures, and would be nonfunctional for hours because a tube burned-out, or a moth got stuck in a relay. Modern transistorized integrated circuit logic gates have MTBFs greater than 82 billion hours (8.2×1010) hours, and need them because they have so many logic gates.
Fanout.
Fanout describes how many logic inputs can be controlled by a single logic output without exceeding the current ratings of the gate. The minimum practical fanout is about five. Modern electronic logic using CMOS transistors for switches have fanouts near fifty, and can sometimes go much higher.
Speed.
The "switching speed" describes how many times per second an inverter (an electronic representation of a "logical not" function) can change from true to false and back. Faster logic can accomplish more operations in less time. Digital logic first became useful when switching speeds got above fifty hertz, because that was faster than a team of humans operating mechanical calculators. Modern electronic digital logic routinely switches at five gigahertz (5×109 hertz), and some laboratory systems switch at more than a terahertz (1×1012 hertz).
Logic families.
Design started with relays. Relay logic was relatively inexpensive and reliable, but slow. Occasionally a mechanical failure would occur. Fanouts were typically about ten, limited by the resistance of the coils and arcing on the contacts from high voltages.
Later, vacuum tubes were used. These were very fast, but generated heat, and were unreliable because the filaments would burn out. Fanouts were typically five to seven, limited by the heating from the tubes' current. In the 1950s, special "computer tubes" were developed with filaments that omitted volatile elements like silicon. These ran for hundreds of thousands of hours.
The first semiconductor logic family was resistor–transistor logic. This was a thousand times more reliable than tubes, ran cooler, and used less power, but had a very low fan-in of three. Diode–transistor logic improved the fanout up to about seven, and reduced the power. Some DTL designs used two power-supplies with alternating layers of NPN and PNP transistors to increase the fanout.
Transistor–transistor logic (TTL) was a great improvement over these. In early devices, fanout improved to ten, and later variations reliably achieved twenty. TTL was also fast, with some variations achieving switching times as low as twenty nanoseconds. TTL is still used in some designs.
Emitter coupled logic is very fast but uses a lot of power. It was extensively used for high-performance computers made up of many medium-scale components (such as the Illiac IV).
By far, the most common digital integrated circuits built today use CMOS logic, which is fast, offers high circuit density and low-power per gate. This is used even in large, fast computers, such as the IBM System z.
Recent developments.
In 2009, researchers discovered that memristors can implement a boolean state storage (similar to a flip flop, implication and logical inversion), providing a complete logic family with very small amounts of space and power, using familiar CMOS semiconductor processes.
The discovery of superconductivity has enabled the development of rapid single flux quantum (RSFQ) circuit technology, which uses Josephson junctions instead of transistors. Most recently, attempts are being made to construct purely optical computing systems capable of processing digital information using nonlinear optical elements.

</doc>
<doc id="39071" url="http://en.wikipedia.org/wiki?curid=39071" title="Samuel de Champlain">
Samuel de Champlain

Samuel de Champlain (] born Samuel Champlain; on or before August 13, 1574 – December 25, 1635), "The Father of New France", was a French navigator, cartographer, draughtsman, soldier, explorer, geographer, ethnologist, diplomat, and chronicler. He founded New France and Quebec City on July 3, 1608. He is important to Canadian history because he made the first accurate map of the coast and he helped establish the settlements.
Born into a family of mariners, Champlain, while still a young man, began exploring North America in 1603 under the guidance of François Gravé Du Pont, From 1604 to 1607 Champlain participated in the exploration and settlement of the first permanent European settlement north of Florida, Port Royal, Acadia (1605). Then, in 1608, he established the French settlement that is now Quebec City. Champlain was the first European to explore and describe the Great Lakes, and published maps of his journeys and accounts of what he learned from the natives and the French living among the Natives. He formed relationships with local Montagnais and Innu and later with others farther west (Ottawa River, Lake Nipissing, or Georgian Bay), with Algonquin and with Huron Wendat, and agreed to provide assistance in their wars against the Iroquois.
In 1620, Louis XIII ordered Champlain to cease exploration, return to Quebec, and devote himself to the administration of the country. In every way but formal title, Samuel "de" Champlain served as Governor of New France, a title that may have been formally unavailable to him owing to his non-noble status. He established trading companies that sent goods, primarily fur, to France, and oversaw the growth of New France in the St. Lawrence River valley until his death in 1635.
Champlain is memorialized as the "Father of New France" and "Father of Acadia", and many places, streets, and structures in northeastern North America bear his name, or have monuments established in his memory. The most notable of these is Lake Champlain, which straddles the border between northern New York and Vermont, extending slightly across the border into Canada. In 1609 he led an expedition up the Richelieu River and explored a long, narrow lake situated between the Green Mountains of present-day Vermont and the Adirondack Mountains of present-day New York; he named the lake after himself as the first European to map and describe it.
Birth year, location, and family.
Champlain was born to Antoine Champlain (also written "Anthoine Chappelain" in some records) and Marguerite Le Roy, in either Hiers-Brouage, or the port city of La Rochelle, in the French province of Aunis. He was born on or before August 13, 1574 according to a recent baptism record found by Jean-Marie Germe, French genealogist. Although in 1870, the Canadian Catholic priest Laverdière, in the first chapter of his "Œuvres de Champlain", accepted Pierre-Damien Rainguet's estimate and tried to justify it, his calculations were based on assumptions now believed, or proven, to be incorrect. Although Léopold Delayant (member, secretary, then president of "l'Académie des belles-lettres, sciences et arts de La Rochelle") wrote as early as 1867 that Rainguet's estimate was wrong, the books of Rainguet and Laverdière have had a significant influence. The 1567 date was carved on numerous monuments dedicated to Champlain and is widely regarded as accurate. In the first half of the 20th century, some authors disagreed, choosing 1570 or 1575 instead of 1567. In 1978 Jean Liebel published groundbreaking research about these estimates of Champlain's birth year and concluded, "Samuel Champlain was born about 1580 in Brouage." Liebel asserts that some authors, including the Catholic priests Rainguet and Laverdière, preferred years when Brouage was under Catholic control (which include 1567, 1570, and 1575).
Champlain claimed to be from Brouage in the title of his 1603 book, and to be "Saintongeois" in the title of his second book (1613). He belonged to either a Protestant family, or a tolerant Roman Catholic one, since Brouage was most of the time a Catholic city in a Protestant region, and his Old Testament first name (Samuel) was not usually given to Catholic children. The exact location of his birth is thus also not known with certainty, but at the time of his birth his parents were living in Brouage.
Born into a family of mariners (both his father and uncle-in-law were sailors, or navigators), Samuel Champlain learned to navigate, draw, make nautical charts, and write practical reports. His education did not include Ancient Greek or Latin, so he did not read or learn from any ancient literature. As each French fleet had to assure its own defense at sea, Champlain sought to learn fighting with the firearms of his time: he acquired this practical knowledge when serving with the army of King Henry IV during the later stages of France's religious wars in Brittany from 1594 or 1595 to 1598, beginning as a quartermaster responsible for the feeding and care of horses. During this time he claimed to go on a "certain secret voyage" for the king, and saw combat (including maybe the Siege of Fort Crozon, at the end of 1594). By 1597 he was a "capitaine d'une compagnie" serving in a garrison near Quimper.
Early travels.
In 1598, his uncle-in-law, a navigator whose ship "Saint-Julien" was chartered to transport Spanish troops to Cadiz pursuant to the Treaty of Vervins, gave Champlain the opportunity to accompany him. After a difficult passage, he spent some time in Cadiz before his uncle, whose ship was then chartered to accompany a large Spanish fleet to the West Indies, again offered him a place on the ship. His uncle, who gave command of the ship to Jeronimo de Vallebrera, instructed the young Champlain to watch over the ship. This journey lasted two years, and gave Champlain the opportunity to see or hear about Spanish holdings from the Caribbean to Mexico City. Along the way he took detailed notes, and wrote an illustrated report on what he learned on this trip, and gave this secret report to King Henry, who rewarded Champlain with an annual pension. This report was published for the first time in 1870, by Laverdière, as "Brief Discours des Choses plus remarquables que Sammuel Champlain de Brouage a reconneues aux Indes Occidentalles au voiage qu'il en a faict en icettes en l'année 1599 et en l'année 1601, comme ensuite" (and in English as "Narrative of a Voyage to the West Indies and Mexico 1599–1602"). The authenticity of this account as a work written by Champlain has frequently been questioned, due to inaccuracies and discrepancies with other sources on a number of points; however, recent scholarship indicates that the work probably was authored by Champlain.
On Champlain's return to Cadiz in August 1600, his uncle, who had fallen ill, asked him to look after his business affairs. This Champlain did, and when his uncle died in June 1601, Champlain inherited his substantial estate. It included an estate near La Rochelle, commercial properties in Spain, and a 150-ton merchant ship. This inheritance, combined with the king's annual pension, gave the young explorer a great deal of independence, as he was not dependent on the financial backing of merchants and other investors. From 1601 to 1603 Champlain served as a geographer in the court of King Henry. As part of his duties he traveled to French ports and learned much about North America from the fishermen that seasonally traveled to coastal areas from Nantucket to Newfoundland to capitalize on the rich fishing grounds there. He also made a study of previous French failures at colonization in the area, including that of Pierre de Chauvin at Tadoussac. When Chauvin forfeited his monopoly on fur trade in North America in 1602, responsibility for renewing the trade was given to Aymar de Chaste. Champlain approached de Chaste about a position on the first voyage, which he received with the king's assent.
Champlain's first trip to North America was as an observer on a fur-trading expedition led by François Gravé Du Pont. Du Pont was a navigator and merchant who had been a ship's captain on Chauvin's expedition, and with whom Champlain established a firm lifelong friendship. He educated Champlain about navigation in North America, including the Saint Lawrence River, and in dealing with the natives there (and in Acadia after). The "Bonne-Renommée" (the "Good Fame") arrived at Tadoussac on March 15, 1603. Champlain was anxious to see for himself all of the places that Jacques Cartier had seen and described about sixty years earlier, and wanted to go even further than Cartier, if possible. Champlain created a map of the Saint Lawrence on this trip and, after his return to France on September 20, published an account as "Des Sauvages: ou voyage de Samuel Champlain, de Brouages, faite en la France nouvelle l'an 1603" ("Concerning the Savages: or travels of Samuel Champlain of Brouages, made in New France in the year 1603"). Included in his account were meetings with Begourat, a chief of the Montagnais at Tadoussac, in which positive relationships were established between the French and the many Montagnais gathered there, with some Algonquin friends.
Promising to King Henry to report on further discoveries, Champlain joined a second expedition to New France in the spring of 1604. This trip, once again an exploratory journey without women and children, lasted several years, and focused on areas south of the St. Lawrence River, in what later became known as Acadia. It was led by Pierre Dugua de Mons, a noble and Protestant merchant who had been given a fur trading monopoly in New France by the king. Dugua asked Champlain to find a site for winter settlement. After exploring possible sites in the Bay of Fundy, Champlain selected Saint Croix Island in the St. Croix River as the site of the expedition's first winter settlement. After enduring a harsh winter on the island the settlement was relocated across the bay where they established Port Royal. Until 1607, Champlain used that site as his base, while he explored the Atlantic coast. Dugua was forced to leave the settlement for France in September 1605, because he learned that his monopoly was at risk. His monopoly was rescinded by the king in July 1607 under pressure from other merchants and proponents of free trade, leading to the abandonment of the settlement.
In 1605 and 1606, Champlain explored the North American coast as far south as Cape Cod, searching for sites for a permanent settlement. Minor skirmishes with the resident Nausets dissuaded him from the idea of establishing one near present-day Chatham, Massachusetts. He named the area Mallebar ("bad bar").
Founding of Quebec City.
In the spring of 1608, Dugua wanted Champlain to start a new French colony on the shores of the St. Lawrence. Dugua equipped, at his own expense, a fleet of three ships with workers, that left the French port of Honfleur. The main ship, called the "Don-de-Dieu" (the "Gift of God"), was commanded by Champlain. Another ship, the "Lévrier" (the "Hunt Dog"), was commanded by his friend Du Pont. The small group of male settlers arrived at Tadoussac on the lower St. Lawrence in June. Because of the dangerous strength of the Saguenay River ending there, they left the ships and continued up the "Big River" in small boats bringing the men and the materials.
On July 3, 1608, Champlain landed at the "point of Quebec" and set about fortifying the area by the erection of three main wooden buildings, each two stories tall, that he collectively called the "Habitation", with a wooden stockade and a moat 12 ft wide surrounding them. This was the very beginning of Quebec City. Gardening, exploring, and fortifying this place became great passions of Champlain for the rest of his life.
In the 1620s, the "Habitation" at Quebec was mainly a store for the "Compagnie des Marchands" (Traders Company), and Champlain lived in the wooden "Fort Saint Louis" newly built up the hill (south from the present-day "Château Frontenac" Hotel), near the only two houses built by the two settler families (the ones of Louis Hébert and Guillaume Couillard, his son-in-law).
Murder of the King.
In May 1610, King Henry was assassinated by a Catholic fanatic, and rule fell to his wife, Marie de' Medici, as regent for the nine-year-old Louis XIII. Marie was a staunch Catholic with little interest in New France, and many of Champlain's Protestant financial supporters, including Pierre Dugua de Mons, were denied access to court. Champlain, on hearing the news, returned to France in September 1610 to establish new political connections in support of efforts at colonization.
Marriage.
One route Champlain may have chosen to improve his access to the court of the regent was his decision to enter into marriage with the twelve-year-old Hélène Boullé. She was the daughter of Nicolas Boullé, a man charged with carrying out royal decisions at court. The marriage contract was signed on December 27, 1610 in presence of Dugua, who had dealt with the father (a Protestant like him), and the couple was married three days later. The terms of the contract called for the marriage to be consummated two years later.
Champlain's marriage was initially quite troubled, as Hélène rebelled when she was told to join him in August 1613. Their relationship, while it apparently lacked any physical connection, recovered and was apparently good for many years. Hélène lived in Quebec for several years, but returned to Paris and eventually decided to enter a convent. The couple had no children, although Champlain did adopt three Montagnais girls named Faith, Hope, and Charity in the winter of 1627-28.
Relations and war with natives.
During the summer of 1609, Champlain attempted to form better relations with the local native tribes. He made alliances with the Wendat (called "Huron" by the French) and with the Algonquin, the Montagnais and the Etchemin, who lived in the area of the St. Lawrence River. These tribes demanded that Champlain help them in their war against the Iroquois, who lived farther south. Champlain set off with nine French soldiers and 300 natives to explore the "Rivière des Iroquois" (now known as the Richelieu River), and became the first European to map Lake Champlain. Having had no encounters with the Iroquois at this point many of the men headed back, leaving Champlain with only 2 Frenchmen and 60 natives.
On July 29, somewhere in the area near Ticonderoga and Crown Point, New York (historians are not sure which of these two places, but Fort Ticonderoga historians claim that it occurred near its site), Champlain and his party encountered a group of Iroquois. In a battle begun the next day, two hundred Iroquois advanced on Champlain's position, and one of his guides pointed out the three Iroquois chiefs. In his account of the battle, Champlain recounts firing his arquebus and killing two of them with a single shot, after which one of his men killed the third. The Iroquois turned and fled. This action set the tone for poor French-Iroquois relations for the rest of the century.
The Battle of Sorel occurred on June 19, 1610, with Samuel de Champlain supported by the Kingdom of France and his allies, the Wyandot people, Algonquin people and Innu people against the Mohawk people in New France at present day Sorel-Tracy, Quebec. The forces of Champlain armed with the arquebus engaged and killed or captured nearly all of the Mohawks. The battle ended major hostilities with the Mohawks for twenty-years.
Exploration of New France.
On March 29, 1613, arriving back in New France, he first ensured that his new royal commission be proclaimed. Champlain set out on May 27 to continue his exploration of the Huron country and in hopes of finding the "northern sea" he had heard about (probably Hudson Bay). He traveled the Ottawa River, later giving the first description of this area. It was in June that he met with Tessouat, the Algonquin chief of Allumettes Island, and offered to build the tribe a fort if they were to move from the area they occupied, with its poor soil, to the locality of the Lachine Rapids.
By August 26 Champlain was back in Saint-Malo. There he wrote an account of his life from 1604 to 1612 and his journey up the Ottawa river, his "Voyages" and published another map of New France. In 1614 he formed the "Compagnie des Marchands de Rouen et de Saint-Malo" and "Compagnie de Champlain", which bound the Rouen and Saint-Malo merchants for eleven years. He returned to New France in the spring of 1615 with four Recollects in order to further religious life in the new colony. The Roman Catholic Church was eventually given "en seigneurie" large and valuable tracts of land estimated at nearly 30% of all the lands granted by the French Crown in New France.
Champlain continued to work to improve relations with the natives promising to help them in their struggles against the Iroquois. With his native guides he explored further up the Ottawa River and reached Lake Nipissing. He then followed the French River until he reached the fresh-water sea he called Lac Attigouautau (now Lake Huron).
In 1615, Champlain was escorted through the area that is now Peterborough, Ontario by a group of Hurons. He used the ancient portage between Chemong Lake and Little Lake (now Chemong Road), and stayed for a short period of time near what is now Bridgenorth.
Military expedition.
On September 1, at Cahiagué (A Huron community on what is now called Lake Simcoe), he and the northern tribes started a military expedition against the Iroquois. The party passed Lake Ontario at its eastern tip where they hid their canoes and continued their journey by land. They followed the Oneida River until they arrived at the main Onondaga fort on October 10, 1615. The exact location of this place is still a matter of debate. Although the traditional location, Nichols Pond, is regularly disproved by professional and amateur archeologists many still claim that Nichols Pond is the location of the battle. 10 mi south of Canastota, New York. Champlain attacked the stockaded Oneida Indian village. He was accompanied by 10 Frenchmen and 300 Huron Indians. Pressured by the Hurons to attack prematurely, the assault failed. Champlain was wounded twice in the leg by arrows, one in his knee. The conflict ended on October 16 when the French and Huron were forced to flee.
Although he did not want to, the Hurons insisted that Champlain spend the winter with them. During his stay he set off with them in their great deer hunt, during which he became lost and was forced to wander for three days living off game and sleeping under trees until he met up with a band of aboriginals by chance. He spent the rest of the winter learning "their country, their manners, customs, modes of life". On May 22, 1616, he left the Huron country and returned to Quebec before heading back to France on July 2.
Improving administration in New France.
Champlain returned to New France in 1620 and was to spend the rest of his life focusing on administration of the territory rather than exploration. Champlain spent the winter building Fort Saint-Louis on top of Cape Diamond. By mid-May he learned that the fur trading monopoly had been handed over to another company led by the Caen brothers. After some tense negotiations, it was decided to merge the two companies under the direction of the Caens. Champlain continued to work on relations with the natives and managed to impose on them a chief of his choice. He also negotiated a peace treaty with the Iroquois.
Champlain continued to work on the fortifications of what became Quebec City, laying the first stone on May 6, 1624. On August 15 he once again returned to France where he was encouraged to continue his work as well as to continue looking for a passage to China, something widely believed to exist at the time. By July 5 he was back at Quebec and continued expanding the city.
In 1627 the Caen brothers' company lost its monopoly on the fur trade, and Cardinal Richelieu (who had joined the Royal Council in 1624 and rose rapidly to a position of dominance in French politics that he would hold until his death in 1642) formed the Compagnie des Cent-Associés (the Hundred Associates) to manage the fur trade. Champlain was one of the 100 investors, and its first fleet, loaded with colonists and supplies, set sail in April 1628.
Champlain had overwintered in Quebec. Supplies were low, and English merchants pillaged Cap Tourmente in early July 1628. A war had broken out between France and England, and Charles I of England had issued letters of marque that authorized the capture of French shipping and its colonies in North America. Champlain received a summons to surrender on July 10 from some heavily armed, English based Scottish merchants, the Kirke brothers. Champlain refused to deal with them, misleading them to believe that Quebec's defenses were better than they actually were (Champlain had only 50 pounds of gunpowder to defend the community). Successfully bluffed, they withdrew, but encountered and captured the French supply fleet, cutting off that year's supplies to the colony. By the spring of 1629 supplies were dangerously low and Champlain was forced to send people to Gaspé and into Indian communities to conserve rations. On July 19, the Kirke brothers arrived before Quebec after intercepting Champlain's plea for help, and Champlain was forced to surrender the colony. Many colonists were taken first to England and then France by the Kirkes, but Champlain remained in London to begin the process of regaining the colony. A peace treaty had been signed in April 1629, three months before the surrender, and, under the terms of that treaty, Quebec and other prizes taken by the Kirkes after the treaty were supposed to be returned. It was not until the 1632 Treaty of Saint-Germain-en-Laye that Quebec was formally given back to France. (David Kirke was rewarded when Charles I knighted him and gave him a charter for Newfoundland.) Champlain reclaimed his role as commander of New France on behalf of Richelieu on March 1, 1633, having served in the intervening years as commander in New France "in the absence of my Lord the Cardinal de Richelieu" from 1629 to 1635. In 1632 Champlain published "Voyages de la Nouvelle France", which was dedicated to Cardinal Richelieu, and "Traitté de la marine et du devoir d'un bon marinier", a treatise on leadership, seamanship, and navigation. (Champlain made more than twenty-five round-trip crossings of the Atlantic in his lifetime, without losing a single ship.)
Last return, and last years working in Quebec.
Champlain returned to Quebec on May 22, 1633, after an absence of four years. Richelieu gave him a commission as Lieutenant General of New France, along with other titles and responsibilities, but not that of Governor. Despite this lack of formal status, many colonists, French merchants, and Indians treated him as if he had the title; writings survive in which he is referred to as "our governor". On August 18, 1634, he sent a report to Richelieu stating that he had rebuilt on the ruins of Quebec, enlarged its fortifications, and established two more habitations. One was 15 leagues upstream, and the other was at Trois-Rivières. He also began an offensive against the Iroquois, reporting that he wanted them either wiped out or "brought to reason".
Illness, last wills, death, and burial.
Champlain suffered a severe stroke in October 1635, and died on 25 December 1635, leaving no immediate heirs. Jesuit records tell us he died in the care of his friend and confessor Charles Lallemant.
Although his will (drafted in November 17, 1635) gave much of his French property to his wife Hélène, he made significant bequests to the Catholic missions and to individuals in the colony of Quebec. However, Marie Camaret, a cousin on his mother's side, challenged the will in Paris and had it successfully overturned. It is unclear exactly what happened to his estate.
He was temporarily buried in the church while a standalone chapel was built to hold his remains in the upper part of the city. Unfortunately, this small building, along many others, was destroyed by a large fire in 1640. Though immediately rebuilt, no traces of it exist anymore: his exact burial site is still unknown, despite much research since about 1850, including several archaeological digs in the city. There is general agreement that the previous Champlain chapel site, and the remains of Champlain, should be somewhere near the Notre-Dame de Québec Cathedral.
The search for Champlain's remains supplies a key plot-line in the crime writer Louise Penny's 2010 novel, "Bury Your Dead".
Memorials.
Many sites and landmarks have been named to honour Champlain, who remains, to this day, a prominent historical figure in many parts of Acadia, Ontario, Quebec, New York, and Vermont. They include:
Bibliography.
These are works that are known to have been written by Champlain:
Further reading.
</dl>

</doc>
<doc id="39083" url="http://en.wikipedia.org/wiki?curid=39083" title="The Stoned Guest">
The Stoned Guest

This musical work, while touted as "P. D. Q. Bach's Half-Act Opera: The Stoned Guest," is actually the work of Peter Schickele. The title is a play on the "stone guest" character in "Don Giovanni" by Mozart, as well as the opera "The Stone Guest" by Alexander Sergeyevich Dargomïzhsky. The work is a parody of classical opera, although some critics consider it to be the equal of many classical works in technical ability. The opera appears on the 1970 album of the same name.
The loose story combines elements of "Don Giovanni" with elements of "Carmen" by Georges Bizet. Some character names, such as "Don Octave" and "Donna Ribalda" play on the Mozart opera, referring to Don Ottavio and Donna Elvira respectively, while the castanet-clicking "Carmen Ghia" plays on the title character of Bizet's opera (and puns on the Volkswagen Karmann Ghia). The "Commendatoreador" plays on both operas at once, being a combination of "Il Commendatore" and the toreador Escamillo. The orchestral accompaniment for Donna Ribalda's opening aria, "Let's face it—I'm lost", bears more than a passing resemblance to the "Rex tremendae majestatis" from Mozart's "Requiem".
At one point in the opera, the rival divas Carmen Ghia and Donna Ribalda break character in the middle of a "recitative", so the singers can hold a conversation (still in recitative) about their singing careers. At a subsequent point, they have a contest to see who can hold a note the longest. The ending parodies many classical operas, "Don Giovanni" in particular, by apparently ending in tragedy, then having a completely unmotivated happy ending.

</doc>
<doc id="39086" url="http://en.wikipedia.org/wiki?curid=39086" title="Waylon Jennings">
Waylon Jennings

Waylon Arnold Jennings (pronounced ; June 15, 1937 – February 13, 2002) was an American singer, songwriter, musician, and actor. Jennings began playing guitar at 8 and began performing at 12 on KVOW radio. His first band was "The Texas Longhorns". Jennings worked as a D.J. on KVOW, KDAV, KYTI, and KLLL. In 1958, Buddy Holly arranged Jennings's first recording session, of "Jole Blon" and "When Sin Stops (Love Begins)". Holly hired him to play bass. In Clear Lake, Iowa, Jennings gave up his seat on the ill-fated flight that crashed and killed Holly, J. P. Richardson, and others. The day of the flight was later known as The Day the Music Died. Jennings then worked as a D.J. in Coolidge, Arizona, and Phoenix. He formed a rockabilly club band, The Waylors. He recorded for independent label Trend Records and A&M Records, before succeeding with RCA Victor after achieving creative control.
During the 1970s, Jennings joined the Outlaw movement. He released critically acclaimed albums "Lonesome, On'ry and Mean" and "Honky Tonk Heroes", followed by hit albums "Dreaming My Dreams" and "Are You Ready for the Country". In 1976 he released the album "Wanted! The Outlaws" with Willie Nelson, Tompall Glaser, and Jessi Colter, the first platinum country music album. That success was followed by "Ol' Waylon", and the hit song "Luckenbach, Texas". By the early 1980s, Jennings was struggling with a cocaine addiction, which he quit in 1984. Later he joined the country supergroup The Highwaymen with Nelson, Kris Kristofferson, and Johnny Cash. During that period, Jennings released the successful album "Will the Wolf Survive". He toured less after 1997, to spend more time with his family. Between 1999 and 2001, his appearances were limited by health problems. On February 13, 2002, Jennings died from complications of diabetes.
Jennings also appeared in movies and television series. He was the balladeer for "The Dukes of Hazzard"; composing and singing the show's theme song. In 2001 he was inducted into the Country Music Hall of Fame, which he chose not to attend. In 2007 he was posthumously awarded the Cliffie Stone Pioneer Award by the Academy of Country Music.
Early life.
Waylon Arnold Jennings was born on June 15, 1937 on the J.W Bittner farm, near Littlefield, Texas. He was the son of Lorene Beatrice (née Shipley) and William Albert Jennings. The Jennings family line descended from Irish and Black-Dutch. Meanwhile, the Shipley family moved from Tennessee and settled in Texas. The Shipley line descended from Cherokee and Comanche families.
The name on his birth certificate was Wayland, meaning land by the highway. His name was changed after a Baptist preacher visited Jennings's parents and congratulated his mother for naming him after the Wayland Baptist University in Plainview, Texas. Lorene Jennings, who had been unaware of the college, changed the spelling to Waylon. Jennings later expressed in his autobiography, "I didn't like Waylon. It sounded corny and hillbilly, but it's been good to me, and I'm pretty well at peace with it right now."
After working as a laborer on the Bittner farm, Jennings' father moved the family to Littlefield and established a retail creamery. When Jennings was eight, his mother taught him to play guitar with the tune "Thirty Pieces of Silver". Jennings used to practice with his relatives' guitars, until his mother bought him a used Stella, and later ordered a Harmony Patrician. Early influences were Bob Wills, Floyd Tillman, Ernest Tubb, Hank Williams, Carl Smith and Elvis Presley.
Beginning at family gatherings, Jennings advanced to perform at the Youth Center with Anthony Bonanno followed by appearances at the local Jaycees and Lions club. He won a talent show at Channel 13, in Lubbock, Texas, singing "Hey Joe". He later made frequent performances at the Palace Theater in Littlefield, during local talent night.
Music career.
Beginnings in music.
The 12-year-old Jennings auditioned for a spot on KVOW in Littlefield, Texas. Owner J.B. McShan, along with Emil Macha, recorded Jennings's performance. McShan liked his style and hired him for a weekly 30-minute program. Following this successful introduction, Jennings formed his own band. He asked Macha to play bass for him, and gathered other friends and acquaintances to form "The Texas Longhorns". The style of the band, a mixture of country and western and bluegrass, was often not well received.
At age sixteen, after several disciplinary infractions, tenth-grader Jennings was convinced to drop out of high school by the superintendent. Upon leaving school, he worked for his father in the produce store, also taking temporary jobs. Jennings felt that music, his favorite activity, would turn into his career. The next year, Jennings and The Texas Longhorns recorded a demo of the songs "Stranger in My Home" and "There'll Be a New Day" at KFYO radio in Lubbock, Texas. Meanwhile, he drove a truck for the Thomas Land Lumber Company, and a cement truck for the Roberts Lumber Company. Tired of the owner, and after a minor driving accident, Jennings quit. He and other local musicians often performed at country radio station KDAV; it was during this time that he met Buddy Holly at a Lubbock restaurant. He and Holly became friends, often meeting during local shows. Jennings also attended Holly's performances on KDAV's "Sunday Party".
In addition to performing on air for KVOW, Jennings started to work as a D.J. in 1956, and moved to Lubbock. His program ran for six hours, from four in the afternoon to ten in the evening. Jennings played two hours of country classics, two of current country and two of mixed recordings. During those final two hours, Jennings played artists such as Chuck Berry and Little Richard. The owner reprimanded him each time he aired the recordings, and when he then played two Richard records in a row, the owner fired him.
During his time at KVOW, Jennings was visited by D.J Sky Corbin, who worked at KLVT in Levelland, Texas. Corbin was impressed with his voice, and decided to visit Jennings at the station after hearing him sing a jingle to the tune of Hank Snow's "I'm Moving On". Jennings expressed his economic struggle to live on a US$50-a week salary. Corbin invited Jennings to visit KLVT, where he eventually took Corbin's then-vacated position. The Corbin family later purchased KLLL, in Lubbock. They changed the format of the station to country, becoming the main competition of KDAV. The Corbins hired Jennings as the station's first D.J.
Jennings produced commercials and created jingles with the rest of the D.J's. As their popularity increased, the D.J's made public appearances. Jennings' events included live performances. During one performance, Buddy's father, L.O Holley, approached them with his son's latest record, and requested them to play it at the station. L.O mentioned his son's intention to start producing artists himself, and Corbin recommended Jennings. After returning from his England tour, Buddy Holly visited KLLL.
Holly took Jennings as his first artist. He outfitted him with new clothes, and worked with him to improve his image. He arranged a session for Jennings at Norman Petty's recording studios in Clovis, New Mexico. On September 10 Jennings recorded the songs "Jole Blon" and "When Sin Stops (Love Begins)" with Holly and Tommy Allsup on guitars with saxophonist King Curtis. Holly then hired Jennings to play electric bass for him during his "Winter Dance Party Tour".
Winter Dance Party Tour.
Before the tour, Holly vacationed with his wife in Lubbock, and visited Jennings' radio station in December 1958. Jennings and Sky Corbin performed the hand claps to Holly's tune "You're the One". Jennings and Holly soon left for New York City, arriving on January 15, 1959. Jennings stayed at Holly's apartment by Washington Square Park, on the days prior to a meeting scheduled on the headquarters of the "General Artists Corporation", that organized the tour. They later took a train to Chicago to join the band.
The tour began in Milwaukee, Wisconsin, on January 23, 1959. The amount of travel created logistical problems. The distance between venues had not been considered when scheduling each performance. Adding to the disarray, the tour buses were not equipped for the weather and twice broke down. After a show in Clear Lake, Iowa, Buddy Holly chartered a plane for himself, Jennings and Allsup to avoid the long bus trip to Moorhead, Minnesota. Jennings gave up his seat to J. P. Richardson, who was suffering from the flu and complaining about how uncomfortable the bus was for a man of his size. A friendly banter between Holly and Jennings then ensued on the situation that would come back to haunt Jennings in later years: Holly jokingly told Jennings, "I hope your ol' bus freezes up!" Jennings jokingly replied, "Well, I hope your ol' plane crashes!" During the early morning hours of February 3, 1959, later known as The Day the Music Died, the charter crashed outside Clear Lake, killing all on board.
Jennings's family heard on the radio that "Buddy Holly and his band had been killed". After calling his family, Jennings called Sky Corbin at KLLL from Fargo to say that he was alive. The General Artists Corporation promised to pay a first class ticket for Jennings and the band to assist Holly's funeral in Lubbock, in exchange for them playing that night in Moorhead, Minnesota. After the first show, they were initially denied their payment by the venue, but after Jennings' persistence, they were paid. The flights were never paid, and Jennings and Allsup continued the tour for two more weeks, featuring Jennings as the lead singer. They were paid less than half of the original agreed salary, and upon returning to New York, Jennings put Holly's guitar and amplifier in a locker in Grand Central Station and mailed the keys to Maria Elena Holly. Then, he returned to Lubbock. Jennings later admitted that he felt responsibility for the crash.
"Jole Blon" was released on Brunswick in March 1959 with limited success. Now unemployed, he returned to KLLL. Still affected by the death of Holly, his performance at the station worsened. He left the station after he was denied a raise, and later worked briefly for the competition, KDAV.
Phoenix and the Nashville Sound.
Due to Maxine's father's illness, Jennings had to shuttle between Arizona and Texas. While his family lived back in Littlefield, Jennings found a job briefly at KOYL in Odessa, Texas. He moved with his family to Coolidge, Arizona, where his wife's sister lived. He found a job performing at the "Galloping Goose" bar, where he was heard by Earl Perrin, who offered him a spot on KCKY. Jennings also played during the intermission on drive-in theaters and on bars. After a successful a performance at the Cross Keys Club in Phoenix, Arizona, he was approached by contractors who were building a club for Jimmy D. Musiel, called "JD's". Musiel employed Jennings as his main artist and designed the club around his act.
He formed his backing band, The Waylors with bassist Paul Foster, guitarist Jerry Gropp and drummer Richie Albright. Jennings and his band performed at the newly opened nightspot in Scottsdale, where they soon earned a strong local fanbase. At JD's, Jennings developed his "rock tempered" style of country music, that would define him on his later career.
In 1961, Jennings signed a recording contract with Trend Records, and experienced moderate success with his single, "Another Blue Day". His friend, Don Bowman took demos of Jennings to Jerry Moss, who at the time was starting A&M Records with associate Herb Alpert. On July 9, 1963, Jennings signed a contract with A&M that granted him five percent of record sales. At A&M he recorded "Love Denied" backed with "Rave On", and "Four Strong Winds" backed with "Just to Satisfy You". He followed up by recording demos of "The Twelfth of Never", "Kisses Sweeter than Wine" and "Don't Think Twice, It's All Right"; and also produced the single "Sing the Girls a Song, Bill", backed with "The Race Is On". The singles were released between April and October 1964.
His records had little success, because A&M's main releases were folk music rather than country. He had a few hits on local radio in Phoenix, with "Four Strong Winds" and "Just To Satisfy You" (co-written with Bowman). Meanwhile, he recorded an album on BAT records, called "JD's". After 500 copies were sold at the club, another 500 copies were pressed by the Sounds label. He also played lead guitar for Patsy Montana on a 1964 album.
Singer Bobby Bare heard Jennings' "Just to Satisfy You" on his car radio while passing through Phoenix, eventually recording it and "Four Strong Winds" After stopping in Phoenix to attend to a Jennings performance at JD's, while driving to Las Vegas, Bare stopped and called Chet Atkins in Nashville, suggesting that he needed to sign Jennings.
When he was made aware of the new deal, Waylon wasn't sure if he should quit his gig at JD's. He then went to get the advice of his friend, RCA artist Willie Nelson, who had gone to see one of Waylon's shows. When Willie and Waylon met, after talking about the possibilities and considering Waylon's profits at the club, Nelson suggested that Waylon should stay in Phoenix and not to move to Nashville.
Nonetheless, Jennings decided to accept the offer, and asked Herb Alpert to release him from his contract with A&M. Alpert agreed, though later A&M would compile all of Jennings' singles and unreleased material the label had and release it as "Don't Think Twice". Atkins formally signed Jennings to RCA Victor in 1965. On August 21, Jennings made his first appearance on the "Billboard's" Hot Country Songs chart with "That's the Chance I'll Have to Take".
In 1966, Jennings released his debut album for RCA "Folk-Country", followed by "Leavin' Town", and "Nashville Rebel". "Leavin' Town" resulted in significant chart success as the first two singles "Anita, You're Dreaming" and "Time to Burn Again" both peaking at No. 17 on the Billboard Hot Country Singles chart. The album's third single, a cover of Gordon Lightfoot's "(That's What You Get) For Lovin' Me", became Jennings' first top 10 single, peaking at No. 9. "Nashville Rebel" was the soundtrack to an independent film of the same name, starring Jennings. The single "Green River" charted on "Billboard" country singles at #11. In 1967 Jennings released a hit single, "Just to Satisfy You". During an interview, Jennings remarked that the song was a "pretty good example" of the influence of his work with Buddy Holly and rockabilly music. Jennings produced midchart albums that sold well, including "Just to Satisfy You", that included the same-named hit single of 1967. Jennings' singles enjoyed success. "The Chokin' Kind" peaked at number eight on "Billboard's" Hot Country Singles in 1967, while "Only Daddy That'll Walk the Line" hit number two the following year. In 1969, his collaboration with The Kimberlys on the single "MacArthur Park" earned a Grammy Award for Best Country Performance by a Duo or Group. His single "Brown Eyed Handsome Man" reached number three at the Hot Country Singles chart by the end of the year.
During this time, Jennings rented an apartment in Nashville with singer Johnny Cash. Jennings and Cash were both managed by "Lucky" Moeller's booking agency Moeller Talent, Inc. The tours organized by the agency were unproductive, with the artists being booked to venues located far from each other in close dates. After paying for the accommodation and travel expenditures, Jennings' profits were reduced, with him frequently requesting advances from the agency or RCA Records to play the next venue. While playing three-hundred days on the road, Jennings' debt increased along with his consumption of amphetamines, as he believed himself to be trapped on the circuit.
In 1972 Jennings released "Ladies Love Outlaws". The single that headlined the album became a hit for Jennings, and was his first approach to Outlaw Country. Jennings was accustomed to performing and recording with his own band, The Waylors, a practice that was not encouraged by powerful Nashville producers. Over time, however, Jennings felt limited by the Nashville sound's lack of artistic freedom. The music style publicized as "Countrypolitan" was characterized by orchestral arrangements, and the absence of traditional country music instruments. The producers did not let Jennings play his own guitar or select material to record.
Outlaw Country.
In an interview, Jennings recalled the restrictions of the Nashville establishment: "They wouldn't let you do anything. You had to dress a certain way: you had to do everything a certain way... They kept trying to destroy me... I just went about my business and did things my way... You start messing with my music, I get mean." By 1972, after the release of "Ladies Love Outlaws", his recording contract was nearing an end. Sick with hepatitis, Jennings was hospitalized. Afflicted by disease, debt and the music industry, he was considering retirement. Albright visited him and convinced him to continue. Albright talked to him about making Neil Reshen his new manager. Meanwhile, Jennings requested a US$25,000 royalty advance from RCA Records to cover his living expenses during his recovery. The same day he met Rashen, RCA sent Jerry Bradley to offer Jennings US$5,000 as a bonus for signing a new 5% royalty deal with RCA, the same terms he had accepted in 1965. After reviewing with Reshen, he rejected the offer and hired Reshen.
Reshen started to renegotiate Jennings' recording and touring contracts. At a meeting in a Nashville airport, Jennings introduced Reshen to Willie Nelson. By the end of the meeting, Reshen had become Nelson's manager as well. Jennings's new deal gained him a $75,000 advance and artistic control. Reshen advised Jennings to keep the beard that he had grown in the hospital, to match the image of outlaw country.
By 1973, Nelson had returned to music, finding success with Atlantic Records. Now based in Austin, Texas, he had made inroads into the rock and roll press by attracting rock audiences. Atlantic Records was now attempting to sign Jennings, but Nelson's rise to popularity persuaded RCA to renegotiate with Jennings before losing another potential star.
In 1973, Jennings released "Lonesome, On'ry and Mean" and "Honky Tonk Heroes", the first albums recorded and released under his creative control. The release of these albums heralded a major turning point for Jennings, kicking off his most critically and commercially successful years. More hit albums followed with "This Time" and "The Ramblin' Man", both released in 1974. The title tracks of both albums topped the "Billboard" country singles chart, with the self-penned "This Time" becoming Jennings' first No. 1 single. "Dreaming My Dreams", released in 1975, included the No. 1 single "Are You Sure Hank Done It This Way" and was his first album to be certified gold by the RIAA; it was also the first of his next six consecutive, solo studio albums to be certified gold or higher. In 1976, Jennings released "Are You Ready for the Country", Jennings wanted the record to be produced by Los Angeles producer Ken Mansfield. Although RCA denied the request, Jennings and The Waylors went to Los Angeles and recorded with Mansfield at his expense. After a month, Jennings presented the master tape to Chet Atkins, who decided to release it. The album hit number one on "Billboard"'s country albums three times the same year, topping the charts for 10 weeks. It was named country album of the year in 1976 by "Record World Magazine" and it was certified gold by the RIAA.
In 1976 Jennings released the album "Wanted! The Outlaws", recorded with Willie Nelson, Tompall Glaser, and Jessie Colter for RCA. The album was the first country music album certified platinum. The following year, RCA issued "Ol' Waylon", an album that produced a hit duet with Nelson, "Luckenbach, Texas". The album "Waylon and Willie" followed in 1978, producing the hit single "Mammas Don't Let Your Babies Grow Up to Be Cowboys". Jennings released "I've Always Been Crazy", also in 1978. The same year, at the peak of his success, Jennings began to feel limited by the outlaw movement. Jennings referred to the over-exploitation of the image in the song "Don't You Think This Outlaw Bit Has Done Got Out of Hand?", claiming that the movement had become a "self-fulfilling prophecy". In 1979 he released "Greatest Hits", which was certified gold the same year, and quintuple platinum in 2002.
Also in 1979, Jennings joined the cast of the CBS series "The Dukes of Hazzard" as The Balladeer, the narrator. The only episode to feature him in person was "Welcome, Waylon Jennings", during the seventh season. Jennings played himself, presented as an old friend of the Duke family. For the show, he also wrote and sang the theme song "Good Ol' Boys", which became the biggest hit of his career. Released as a single in promotion with the show, it became Jennings' twelfth single to reach number one on the Billboard Country Singles chart. It was also a crossover hit, peaking at twenty-one on the Billboard Hot 100.
Later years.
In the mid-1980s, Johnny Cash, Kris Kristofferson, Nelson, and Jennings formed a successful group called The Highwaymen. Aside from his work with The Highwaymen, Jennings' released a gold album "WWII" (1982) with Willie Nelson.
In 1985, Jennings joined with USA for Africa to record "We Are the World," but he left the studio because of a dispute over the song's lyrics that were to be sung in Swahili. Ironically, after Jennings left the session, the idea was dropped at the prompting of Stevie Wonder, who pointed out that Ethiopians did not speak Swahili. By this time, his sales had decreased. After the release of "Sweet Mother Texas", Jennings signed with Music Corporation of America. The debut release with the label "Will the Wolf Survive" (1985) peaked at number one in "Billboard's" Country albums in 1986. Jennings's initial success tailed off, and in 1990, he signed with Epic Records. His first release, "The Eagle", became his final top 10 album. During the late 80s-early 90s, Jennings and his contemporaries such as Faron Young, Merle Haggard, and George Jones were gradually evicted from the airwaves in favor of a younger generation of pop-influenced country artists like Alan Jackson and Reba McEntire.
Also in 1985, he made a cameo appearance in the live-action children's film "". In the movie, he plays a turkey farm truck driver that gives Big Bird a lift. He also sings one of the film's songs, entitled "Ain't No Road Too Long".
In 1993, in collaboration with Rincom Children's Entertainment, Jennings recorded an album of children's songs, "Cowboys, Sisters, Rascals & Dirt", which included "Shooter's Theme," a tribute to his 14-year-old with the theme of "a friend of mine".
Although his record sales and radio play dwindled during the 90s, Jennings continued to draw large crowds at his live performances. In 1997, after the "Lollapalooza tour", he decreased his tour schedule and became centered on his family.
In 1998, Jennings teamed up with Bare, Jerry Reed, and Mel Tillis to form The Old Dogs. The group recorded a double album of songs by Shel Silverstein.
In mid-1999, Jennings assembled what he referred to as his "hand-picked dream team" and formed Waylon & The Waymore Blues Band. Consisting primarily of former Waylors, the 13-member group performed concerts from 1999 to 2001. In January 2000, Jennings recorded what would become his final album at Nashville's historic Ryman Auditorium, "".
Music style and image.
Jennings' music was characterized by his "powerful" singing voice, noted by his "rough-edged quality," as well as his phrasing and texture.
He was also recognized for his "spanky-twang" guitar style. To create his sound, he used a mixture of thumb and fingers during the rhythmic parts, while using picks for the lead runs. He combined hammer-on and pull-off riffs, with eventual upper-fret double stops and modulation effects. Jennings played a 1953 Fender Telecaster, a used guitar that was a gift from The Waylors. Jennings's bandmates adorned his guitar with a distinctive leather cover that featured a black background with a white floral work. Jennings further customized it by filing down the frets to lower the strings on the neck to obtain the slapping sound. Among his other guitars, Jennings used a 1950 Fender Broadcaster from the mid-1970s, until he gave it to guitarist Reggie Young in 1993. The leather covers of his guitars were carved by leather artist Terry Lankford.
His signature image was characterized by his long hair and beard, as well as his black hat and the black leather vest he wore during his appearances.
Personal life.
In 1969, Jennings met and married Jessi Colter.[5] Jennings and Colter then moved to Nashville, Tennessee. Colter and Jennings had one son, Waylon Albright "Shooter" Jennings (born May 19, 1979). In the early 1980s, Colter and Jennings nearly divorced due to his addiction to drugs and other forms of substance abuse.[6] However, they remained together until Jennings's death in 2002.
In 1997, he gave up touring to be close to his family. To set an example about the importance of education to his son Waylon Albright, Jennings earned a GED at age 52.
Addiction and recovery.
Jennings started to consume amphetamines while he lived with Johnny Cash during the mid-1960s. Jennings later stated, "Pills were the artificial energy on which Nashville ran around the clock." In 1977, Jennings was arrested by federal agents for conspiracy and possession of cocaine with intent to distribute. A private courier warned the Drug Enforcement Administration about the package sent to Jennings by a New York colleague that contained 27 grams of cocaine. The DEA and the police searched Jennings's recording studio. They found no evidence, because while they were waiting for a search warrant, Jennings disposed of the cocaine. The charges were later dropped and Jennings was released. The episode was recounted in Jennings's song "Don't You Think This Outlaw Bit's Done Got Outta Hand?"
During the early 1980s, his cocaine addiction intensified. Jennings claimed to have spent $1,500 a day on his habit, draining his personal finances and leaving him bankrupt with debt of up to $2.5 million. Though he insisted on repaying the debt and did additional tours to earn the funds, his work became less focused and his tours deteriorated. Jennings decided to quit his addictions, leased a home in the Phoenix, Arizona, area and spent a month detoxing himself, intending to start using cocaine again in a more controlled fashion afterward. In 1984, he quit cocaine. Jennings claimed that his son Shooter Jennings was his main inspiration to quit permanently.
Illness and death.
Jennings's health had been deteriorating for years before his death. After quitting cocaine, he ended his habit of smoking six packs of cigarettes daily in 1988. The same year he underwent heart bypass surgery. By 2000 his diabetes worsened, and the pain reduced his mobility, forcing Jennings to end most touring. Later the same year he underwent surgery to improve his leg circulation. In December 2001 his left foot was amputated at a hospital in Phoenix, Arizona. On February 13, 2002, Jennings died in his sleep of diabetic complications in Chandler, Arizona. He was buried in the Mesa City Cemetery, in Mesa, Arizona. At the funeral ceremony, on February 15, Jessi Colter sang "Storms Never Last" for the attendees, who included Jennings's close friends and fellow musicians.
Recognition.
Between 1966 and 1995, fifty-four Jennings albums charted, with 11 reaching number one. Meanwhile between 1965 and 1991, ninety-six singles charted, with 16 number ones.
In October 2001, Jennings was inducted into the Country Music Hall of Fame. In one final act of defiance, he did not attend the ceremony and opted instead to send son Buddy Dean Jennings.
On July 6, 2006, Jennings was inducted to Hollywood's Rock Wall in Hollywood, California. On June 20, 2007, Jennings was posthumously awarded the Cliffie Stone Pioneer Award by the Academy of Country Music.
Legacy.
Jennings's music had a major influence on several neotraditionalist and alternative country artists, including Hank Williams Jr., The Marshall Tucker Band, Travis Tritt, Steve Earle, Jamey Johnson, John Anderson, his son, Shooter Jennings and Hank Williams III.
In 2008, his first posthumous album, "Waylon Forever", was released. The album consisted of songs recorded with his son Shooter when he was 16.
In 2012, ' a three-volume project, consisting in covers of Jennings's songs by different artists was released. The same year, it was announced for September the release of ', a set of 12 songs recorded by Jennings and bassist Robby Turner before his death in 2002. Jennings's family was reluctant to release any new material because they did not feel comfortable at the time. The songs only featured Jennings and Turner on the bass, while further accompaniment would be added later. Ten years after, Turner completed the recordings with the help of former Waylors. The Jennings family approved the release despite the launch of a new business focused on his estate. Shooter Jennings arranged deals for a clothing line, while also launching a renewed website, and started talks with different producers about the making of a biographical film.
References.
Bibliography.
</dl>

</doc>
<doc id="39122" url="http://en.wikipedia.org/wiki?curid=39122" title="Z/OS">
Z/OS

z/OS is a 64-bit operating system for mainframe computers, produced by IBM. It derives from and is the successor to OS/390, which in turn followed a string of MVS versions. Like OS/390, z/OS combines a number of formerly separate, related products, some of which are still optional. z/OS offers the attributes of modern operating systems but also retains much of the functionality originating in the 1960s and each subsequent decade that is still found in daily use (backward compatibility is one of z/OS's central design philosophies). z/OS was first introduced in October 2000. The newest version is z/OS Version 2 Release 1.
Major characteristics.
z/OS supports stable mainframe systems and standards such as CICS, IMS, DB2, RACF, SNA, WebSphere MQ, record-oriented data access methods, REXX, CLIST, SMP/E, JCL, TSO/E, and ISPF, among others. However, z/OS also supports 64-bit Java, C, C++, and UNIX (Single UNIX Specification) APIs and applications through UNIX System Services  — The Open Group certifies z/OS as a compliant UNIX operating system — with UNIX/Linux-style hierarchical HFS and zFS file systems. As a result, z/OS hosts a broad range of commercial and open source software of any vintage. z/OS can communicate directly via TCP/IP, including IPv6, and includes standard HTTP servers (one from Lotus, the other Apache-derived) along with other common services such as FTP, NFS, and CIFS/SMB. Another central design philosophy is support for extremely high quality of service (QoS), even within a single operating system instance, although z/OS has built-in support for Parallel Sysplex clustering.
z/OS has a Workload Manager (WLM) and dispatcher which automatically manages numerous concurrently hosted units of work running in separate key-protected address spaces according to dynamically adjustable goals. This capability inherently supports multi-tenancy within a single operating system image. However, modern IBM mainframes also offer two additional levels of virtualization: LPARs and (optionally) z/VM. These new functions within the hardware, z/OS, and z/VM — and Linux and OpenSolaris support — have encouraged development of new applications for mainframes. Many of them utilize the WebSphere Application Server for z/OS middleware.
From its inception z/OS has supported tri-modal addressing (24-bit, 31-bit, and 64-bit). Up through Version 1.5, z/OS itself could start in either 31-bit ESA/390 or 64-bit z/Architecture mode, so it could function on older hardware albeit without 64-bit application support on those machines. (Only the newer z/Architecture hardware manufactured starting in the year 2000 can run 64-bit code.) IBM support for z/OS 1.5 ended on March 31, 2007. Now z/OS is only supported on z/Architecture mainframes and only runs in 64-bit mode. z/Architecture hardware always starts running in 31-bit mode, but current z/OS releases quickly switch to 64-bit mode and will not run on hardware that does not support 64-bit mode. Application programmers can still use any addressing mode: all applications, regardless of their addressing mode(s), can coexist without modification, and IBM maintains commitment to tri-modal backward compatibility. However, increasing numbers of middleware products and applications, such as DB2 Version 8 and above, now require and exploit 64-bit addressing.
IBM markets z/OS as its flagship operating system, suited for continuous, high-volume operation with high security and stability.
z/OS is available under standard license pricing as well as via System z New Application License Charges (zNALC) and "System z Solution Edition," two lower priced offerings aimed at supporting newer applications ("new workloads"). U.S. standard commercial z/OS pricing starts at about $125 per month, including support, for the smallest zNALC installation running the base z/OS product plus a typical set of optional z/OS features.
z/OS introduced Variable Workload License Charges (VWLC) and Entry Workload License Charges (EWLC) which are sub-capacity billing options. VWLC and EWLC customers only pay for peak monthly z/OS usage, not for full machine capacity as with the previous OS/390 operating system. VWLC and EWLC are also available for most IBM software products running on z/OS, and their peaks are separately calculated but can never exceed the z/OS peak. To be eligible for sub-capacity licensing, a z/OS customer must be running in 64-bit mode (which requires z/Architecture hardware), must have completely eliminated OS/390 from the system, and must e-mail IBM monthly sub-capacity reports. Sub-capacity billing substantially reduces software charges for most IBM mainframe customers. Advanced Workload License Charges (AWLC) is the successor to VWLC on mainframe models starting with the zEnterprise 196, and EAWLC is an option on zEnterprise 114 models. AWLC and EAWLC offer further sub-capacity discounts.
Other features.
64-bit memory support.
Within each address space, z/OS typically only permits the placement of data above the 2 GB "bar", not code. z/OS enforces this distinction primarily for performance reasons. There are no architectural impediments to allowing more than 2 GB of application code per address space. IBM has started to allow Java code running on z/OS to execute above the 2 GB bar, again for performance reasons.
Memory is obtained as "Large Memory Objects" in multiples of 1 MB (with the expectation that applications and middleware will manage memory allocation within these large pieces). There are three types of large memory objects:
Release history.
IBMs release scheduling introduces new releases of z/OS each September.
IBM supports z/OS release coexistence and fallback on an "N+2" basis. For example, IBM customers running Release 9 can upgrade directly to Release 11 or Release 10, and both releases can operate concurrently within the same Sysplex (cluster) and without conflict using the same datasets, configurations, security profiles, etc. (coexistence), as long as so-called "toleration maintenance" is installed in the older release. If there is a problem with Release 11, the customer can return to Release 9 without experiencing dataset, configuration, or security profile compatibility problems (fallback) until ready to try moving forward again. z/OS customers using Parallel Sysplex (clustering) can operate N+2 releases (e.g. Release 9 and Release 11, or Release 9 and Release 10) in mixed release configurations, in production, as long as required to complete release upgrades. Supported release mixing within a cluster is one of the key strategies of z/OS for avoiding service outages.
IBM's standard support period for each z/OS release is three years. Most z/OS customers take advantage of the N+2 support model, and skip every other release. Thus most z/OS customers are either "odd" or "even."
IBM releases individual small enhancements and corrections (a.k.a. PTFs) for z/OS as needed, when needed. IBM labels critical PTFs as "HIPER" (High Impact PERvasive). IBM also "rolls up" multiple patches into a Recommended Service Update (RSU). RSUs are released periodically (in the range of every one to three months) and undergo additional testing. Although z/OS customers vary in their maintenance practices, IBM encourages every z/OS customer to adopt a reasonable preventive maintenance strategy, to avoid known problems before they might occur.

</doc>
<doc id="39139" url="http://en.wikipedia.org/wiki?curid=39139" title="Proso millet">
Proso millet

Panicum miliaceum (Kannada:ಬರಗು), with many common names including proso millet, common millet, broomtail millet, hog millet, red millet,and white millet, is a grass species used as a crop. Both the wild ancestor and the location of domestication of proso millet are unknown, but it first appears as a crop in both Transcaucasia and China about 7,000 years ago, suggesting it may have been domesticated independently in each area. It is still extensively cultivated in India, Russia, Ukraine, the Middle East, Turkey and Romania. In the United States, proso is mainly grown for birdseed. It is sold as health food, and due to its lack of gluten, it can be included in the diets of people who cannot tolerate wheat.
The name comes from the pan-Slavic general and generic name for millet (Russian, Serbian, Macedonian, Bulgarian: просо and Polish, Czech, Slovakian, Slovenian, Croatian: "proso").
Proso is well adapted to many soil and climatic conditions; it has a short growing season, and needs little water. The water requirement of proso is probably the lowest of any major cereal. It is an excellent crop for dryland and no-till farming. Proso millet is an annual grass whose plants reach an average height of 100 cm (4 feet.). Like corn, it has a C4 photosynthesis. The seedheads grow in bunches. The seeds are small (2–3 mm or 0.1 inch) and can be cream, yellow, orange-red, or brown in colour.
Proso is an annual grass like all other millets, but it is not closely related to pearl millet, foxtail millet, finger millet, or the barnyard millets.
History and domestication.
Unlike the foxtail millet, the wild ancestor of the proso millet has not yet been satisfactorily identified. Weedy forms of this grain are found in central Asia, covering a widespread area from the Caspian Sea east to Xinjiang and Mongolia, and it may be that these semiarid areas may harbor "genuinely wild "P. miliaceum" forms." This millet has been reportedly found in Neolithic sites in Georgia (dated to the fifth and fourth millennia BC), as well as excavated Yangshao culture farming villages east in China. Proso millet appears to have reached Europe not long after its appearance in Georgia, first appearing in east and central Europe; however, the grain needed a few thousand more years to cross into Italy, Greece, and Iran, and the earliest evidence for its cultivation in the Near East is a find in the ruins of Nimrud, Iraq dated to about 700 BC.
While proso millet is not a member of the Neolithic Near East crop assemblage, it arrived in Europe no later than the time these introductions did, and proso millet as an independent domestication could predate the arrival of the Near East grain crops.
Cultivation.
Proso millet is a relatively low demanding crop and diseases aren’t known. That’s why Proso millet is often used in organic farming systems in Europe. In the United States it is often used as an intercrop. Thereby, proso millet can help to avoid a summer fallow, and continuous crop rotation can be achieved. Its superficial root system and its resistance to atrazine residue make proso millet a good intercrop between two water and pesticide demanding crops. The stubbles of the last crop, by allowing more heat into the soil, result in a faster and earlier millet growth. While millet occupies the ground, because of its superficial root system, the soil can replenish in water for the next crop. The later, for example a winter wheat, can in turn benefits from the millet stubbles, which can notably act as snow accumulators.
Climate and soil requirements.
Due to its C4 photosynthetic system, Proso millet is thermophilic like maize. Therefore shady locations of the field should be avoided. It is sensitive to cold temperatures lower than 10 to 13 degrees Celsius. Proso millet is highly drought resistant which makes it of interest to regions with low water availability and longer periods without rain.
The soil should be lightly or medium-heavy. Due to its flat root systems, soil compaction must be prohibited. Furthermore Proso millet doesn’t tolerate wetness due to dammed-up water.
Seedbed & Sowing.
The seedbed should be fine crumbled like for sugar beet and rapeseed. In Europe proso millet is sowed between mid April and the end of May. 500g/are of seeds are required which comes up to 500 grains/m2. In organic farming this amount should be increased if a harrow weeder is used. For sowing the normal sowing machines could be used similar they are used for other crops like wheat. A distance between the rows among 16 to 25 centimeters is recommended if the farmer uses an inter row cultivator. The sowing depth should be 1.5 up to 2 cm in optimal soil or 3 to 4 cm in dry soil. Rolling of the ground after sowing is helpful for further cultivation. Cultivation in no till farming systems is also possible and often done in the US. The sowing then could be done two weeks later.
Field management.
Only a few diseases and pests are known but they aren’t very important. Bigger problems are weeds. The critical phase is the juvenile development. The formation of the grains happens in the 3 up to 5 leaf stadium. After that all nutrients should be available for the proso millet. Therefore it is necessary to oppress the development of weeds. In conventional farming herbicides could be used. In organic farming it’s possible to use harrow weeders and inter row cultivators. For that special sowing parameters described in the chapter above are needed.
For a good development of the plant fertilization with 50 to 75 kg nitrogen per hectare is recommended. Planting proso millet in a crop rotation after maize should be avoided due to its same weed spectrum. Because proso millet is an undemanding crop, it also could be at the end of the crop rotation.
Harvesting & postharvest treatments.
Harvest time is at the end of August until mid September. To determine the best harvest time isn’t that easy because the ripeness of the grains isn’t synchronized. The grains on the top of the panicle are ripe first while the grains in the lower parts need more time. That’s why it’s necessary to find a compromise and catch the date when the yield is highest.
Harvesting could be done with a conventional combine harvester at moisture of the grains about 15-20%. Usually proso millet is mowed at windrows first because the plants aren’t dry like wheat. There they could wither which makes the threshing easier. Then the harvest is done with a pickup attached to the combine.
The possible yield is between 2.5 and 4.5 tons per hectare under optimal conditions. Studies in Germany showed that even a higher yield could be reached.
Uses.
Proso millet is one of the few types of millet not cultivated in Africa.
In the United States, former Soviet Union, and some South American countries, it is primarily grown for livestock feed. As a grain fodder, it is very deficient in lysine and needs complementation.
Proso millet is also a poor fodder due to its low leaf:stem ratio and a possible irritant effect due to its hairy stem. Foxtail millet, having a higher leaf:stem ratio and less hairy stems, is preferred as fodder, particularly the variety called moha, which is a high quality fodder.
In order to promote millet cultivation, other potential uses have been considered recently. For example, starch derived from millets has been shown to be a good substrate for fermentation and malting with grains having similar starch contents as wheat grains. A recently published study suggested that starch derived from Proso millet can be converted to ethanol with an only moderately lower efficiency than starch derived from corn. The development of varieties with highly fermentable characteristics could improve ethanol yield to that of highly fermentable corn. Since Proso Millet is compatible with low input agriculture, cultivation on marginal soils for biofuel production could represent an important new market, for example for farmers in the High Plains of the US.
The demand for more diverse and healthier cereal-based foods is increasing, particularly in affluent countries. This could create new markets for proso millet products in human nutrition. Protein content in proso millet grains is comparable with that of wheat, but the share of essential amino acids (leucine, isoleucine and methionine) is substantially higher in proso millet. In addition, health-promoting phenolic compounds contained in the grains are readily bioaccessible and high Calcium contents favor bone strengthening and dental health. Among the most commonly consumed products are ready-to-eat breakfast cereals made purely from millet flour as well as a variety of noodles and bakery products, which are, however, often produced from mixtures with wheat flour to improve their sensory quality.

</doc>
<doc id="39181" url="http://en.wikipedia.org/wiki?curid=39181" title="Cuneiform (disambiguation)">
Cuneiform (disambiguation)

Cuneiform (from the Latin word for "wedge-shaped") can refer to:

</doc>
<doc id="39190" url="http://en.wikipedia.org/wiki?curid=39190" title="William Thurston">
William Thurston

William Paul Thurston (October 30, 1946 – August 21, 2012) was an American mathematician. He was a pioneer in the field of low-dimensional topology. In 1982, he was awarded the Fields Medal for his contributions to the study of 3-manifolds. From 2003 until his death he was a professor of mathematics and computer science at Cornell University.
Mathematical contributions.
Foliations.
His early work, in the early 1970s, was mainly in foliation theory, where he had a dramatic impact. His more significant results include:
In fact, Thurston resolved so many outstanding problems in foliation theory in such a short period of time that it led to a kind of exodus from the field, where advisors counselled students against going into foliation theory because Thurston was "cleaning out the subject" (see "On Proof and Progress in Mathematics", especially section 6 ).
The geometrization conjecture.
His later work, starting around the mid-1970s, revealed that hyperbolic geometry played a far more important role in the general theory of 3-manifolds than was previously realised. Prior to Thurston, there were only a handful of known examples of hyperbolic 3-manifolds of finite volume, such as the Seifert–Weber space. The independent and distinct approaches of Robert Riley and Troels Jørgensen in the mid-to-late 1970s showed that such examples were less atypical than previously believed; in particular their work showed that the figure-eight knot complement was hyperbolic. This was the first example of a hyperbolic knot.
Inspired by their work, Thurston took a different, more explicit means of exhibiting the hyperbolic structure of the figure eight knot complement. He showed that the figure eight knot complement could be decomposed as the union of two regular ideal hyperbolic tetrahedra whose hyperbolic structures matched up correctly and gave the hyperbolic structure on the figure eight knot complement. By utilizing Haken's normal surface techniques, he classified the incompressible surfaces in the knot complement. Together with his analysis of deformations of hyperbolic structures, he concluded that all but 10 Dehn surgeries on the figure eight knot resulted in irreducible, non-Haken non-Seifert-fibered 3-manifolds. These were the first such examples; previously it had been believed that except for certain Seifert fiber spaces, all irreducible 3-manifolds were Haken. These examples were actually hyperbolic and motivated his next revolutionary theorem.
Thurston proved that in fact most Dehn fillings on a cusped hyperbolic 3-manifold resulted in hyperbolic 3-manifolds. This is his celebrated hyperbolic Dehn surgery theorem.
To complete the picture, Thurston proved a hyperbolization theorem for Haken manifolds. A particularly important corollary is that many knots and links are in fact hyperbolic. Together with his hyperbolic Dehn surgery theorem, this showed that closed hyperbolic 3-manifolds existed in great abundance.
The geometrization theorem has been called "Thurston's Monster Theorem," due to the length and difficulty of the proof. Complete proofs were not written up until almost 20 years later. The proof involves a number of deep and original insights which have linked many apparently disparate fields to 3-manifolds.
Thurston was next led to formulate his geometrization conjecture. This gave a conjectural picture of 3-manifolds which indicated that all 3-manifolds admitted a certain kind of geometric decomposition involving eight geometries, now called Thurston model geometries. Hyperbolic geometry is the most prevalent geometry in this picture and also the most complicated. The conjecture was proved by Grigori Perelman in 2002–2003.
Orbifold theorem.
In his work on hyperbolic Dehn surgery, Thurston realized that orbifold structures naturally arose. Such structures had been studied prior to Thurston, but his work, particularly the next theorem, would bring them to prominence. In 1981, he announced the orbifold theorem, an extension of his geometrization theorem to the setting of 3-orbifolds. Two teams of mathematicians around 2000 finally finished their efforts to write down a complete proof, based mostly on Thurston's lectures given in the early 1980s in Princeton. His original proof relied partly on Hamilton's work on the Ricci flow.
Education and career.
Thurston was born in Washington, D.C. to a homemaker and an aeronautical engineer. He received his bachelor's degree from New College (now New College of Florida) in 1967. For his undergraduate thesis he developed an intuitionist foundation for topology. Following this, he earned a doctorate in mathematics from the University of California, Berkeley, in 1972. His Ph.D. advisor was Morris W. Hirsch and his dissertation was on "Foliations of Three-Manifolds which are Circle Bundles".
After completing his Ph.D., he spent a year at the Institute for Advanced Study, then another year at MIT as Assistant Professor. In 1974, he was appointed Professor of Mathematics at Princeton University. In 1991, he returned to UC-Berkeley as Professor of Mathematics and in 1993 became Director of the Mathematical Sciences Research Institute. In 1996, his wife Julian, who had earlier been his Ph.D. student at Princeton University, made a career switch to veterinary medicine, and began her studies at the UC Davis School of Veterinary Medicine. Bill and Julian moved to Davis, California, where Bill became Professor of Mathematics at UC Davis. In 2000, their first child Jade was born, and in 2003 their second child Liam was born. Bill and Julian had visited Ithaca in 1997 for a family celebration for his mother's 80th birthday. They were enchanted by the beauty of Ithaca, and in 2003 the family moved to Ithaca, NY, where Bill became Professor of Mathematics at Cornell University.
His Ph.D. students include Martin Bridgeman, Danny Calegari, Richard Canary, Suhyoung Choi, Renaud Dreyer, David Gabai, William Goldman, Benson Farb, Sergio Fenley, Detlef Hardorp, Craig Hodgson, Christopher Jerdonek, Richard Kenyon, Steven Kerckhoff, Silvio Levy, Robert Meyerhoff, Yair Minsky, Lee Mosher, Igor Rivin, Nicolau Saldanha, Oded Schramm, Richard Schwartz, William Floyd, Biao Wang and Jeffrey Weeks. His son Dylan Thurston is an associate professor of mathematics at Indiana University.
In later years Thurston widened his attention to include mathematical education and bringing mathematics to the general public. He has served as mathematics editor for Quantum Magazine, a youth science magazine, and was one of the founders of The Geometry Center. As director of Mathematical Sciences Research Institute from 1992 to 1997, he initiated a number of programs designed to increase awareness of mathematics among the public.
In 2005 Thurston won the first AMS Book Prize, for "Three-dimensional Geometry and Topology".
The prize "recognizes an outstanding research book that makes a seminal contribution to the research literature".
In 2012, Thurston was awarded the Leroy P Steele Prize by the AMS for seminal contribution to research. The citation described his work as having "revolutionized 3-manifold theory".
He died on August 21, 2012 in Rochester, New York, of a sinus mucosal melanoma that was diagnosed in 2011.
Thurston and his family had been in the process of moving back to Davis, CA, where he was to rejoin the mathematics faculty at UC Davis while his wife completed her veterinary medical degree. Thurston died before he could make the move to California. He had remained with his brother George in Rochester, NY, while his family went ahead of him to California to get settled, waiting for him to gain better physical strength for making the cross-country trip to California to join them. Bill's health declined rapidly, and the family returned to Rochester to be with him during his final days.
In Thurston's last days, he sometimes used American Sign Language to communicate with his children, Liam and Jade. Bill and Julian had spent a year studying ASL when Jade was an infant, and the family had become somewhat fluent. He also communicated by writing on one of his many pads of paper. One of his last written messages was, "Treasure Island," and this reference remains mysterious to his family.
Thurston has an Erdős number of 2, via John Horton Conway. Paths of length 3 are many; for example, Allan R. Wilks is a co-author, and Ronald Graham is a co-author with Wilks. Kenneth Steiglitz is a co-author, and Daniel J. Kleitman is a co-author with Steiglitz. Joel Hass is a co-author, and Laszlo Lovasz is a co-author with Hass. Graham, Lovasz and Kleitman are all co-authors with Erdős.
Selected works.
"William P. Thurston, . Notices of the AMS 37:7 (September 1990) pp 844–850

</doc>
<doc id="39204" url="http://en.wikipedia.org/wiki?curid=39204" title="Battle of Bennington">
Battle of Bennington

The Battle of Bennington was a battle of the American Revolutionary War, part of the Saratoga campaign, that took place on August 16, 1777, in Walloomsac, New York, about 10 mi from its namesake Bennington, Vermont. A rebel force of 2,000 men, primarily composed of New Hampshire and Massachusetts militiamen, led by General John Stark, and reinforced by men led by Colonel Seth Warner and members of the Green Mountain Boys, decisively defeated a detachment of General John Burgoyne's army led by Lieutenant Colonel Friedrich Baum, and supported by additional men under Lieutenant Colonel Heinrich von Breymann.
Baum's detachment was a mixed force of 700 composed of dismounted Brunswick dragoons, Canadians, Loyalists, and Indians. He was sent by Burgoyne to raid Bennington in the disputed New Hampshire Grants area for horses, draft animals, and other supplies. Believing the town to be only lightly defended, Burgoyne and Baum were unaware that Stark and 1,500 militiamen were stationed there. After a rain-caused standoff, Stark's men enveloped Baum's position, taking many prisoners, and killing Baum. Reinforcements for both sides arrived as Stark and his men were mopping up, and the battle restarted, with Warner and Stark driving away Breymann's reinforcements with heavy casualties.
The battle was a decisive victory for the rebel cause, as it reduced Burgoyne's army in size by almost 1,000 men, led his Indian support to largely abandon him, and deprived him of needed supplies such as cavalry and draft horses and food, all factors that contributed to Burgoyne's eventual surrender at Saratoga. The victory also galvanized colonial support for the independence movement, and played a role in bringing France into the war on the rebel side. The battle anniversary is celebrated in the state of Vermont as Bennington Battle Day.
Background.
With the American Revolutionary War two years old, the British changed their plans. Giving up on the rebellious New England colonies, they decided to split the Thirteen Colonies and isolate New England from what the British believed to be the more loyal southern colonies. The British command devised a grand plan to divide the colonies via a three-way pincer movement. The western pincer, under the command of Barry St. Leger, was repulsed when the Siege of Fort Stanwix failed, and the southern pincer, which was to progress up the Hudson valley from New York City, never started since General William Howe decided instead to capture Philadelphia.
The northern pincer, proceeding southward from Montreal, enjoyed the most success. After the British victories at Hubbardton, Fort Ticonderoga, and Fort Anne, General John Burgoyne proceeded with the Saratoga campaign, with the goal of capturing Albany and gaining control of the Hudson River Valley, where Burgoyne's force could (as the plan went) meet the other pincers, dividing the colonies in two.
British forces.
Burgoyne's progress towards Albany had initially met with great success, including the scattering of Seth Warner's men in the Battle of Hubbardton. However, his advance had slowed to a crawl by late July, due to logistical difficulties, exacerbated by the American destruction of a key road, and the army's supplies began to dwindle. Burgoyne's concern over supplies was magnified in early August when he received word from Howe that he (Howe) was going to Philadelphia, and was not in fact going to advance up the Hudson River valley. In response to a proposal first made on July 22 by the commander of his German troops, Baron Riedesel, Burgoyne sent a detachment of about 800 troops under the command of Lieutenant Colonel Friedrich Baum from Fort Miller on a foraging mission to acquire horses for the German dragoons, draft animals to assist in moving the army, and to harass the enemy. Baum's detachment was primarily made up of dismounted Brunswick dragoons of the Prinz Ludwig regiment. Along the way it was joined by local companies of Loyalists, some Canadians and about 100 Indians, and a company of British sharpshooters. Baum was originally ordered to proceed to the Connecticut River valley where they believed horses could be procured for the dragoons. However, as Baum was preparing to leave, Burgoyne verbally changed the goal to be a supply depot at Bennington, which was believed to be guarded by the remnants of Warner's brigade, about 400 colonial militia.
American forces.
Unknown to Burgoyne, the citizens of the New Hampshire Grants territory (which was then disputed between New York and the Vermont Republic) had appealed to the states of New Hampshire and Massachusetts for protection from the invading army following the British capture of Ticonderoga. New Hampshire responded on July 18 by authorizing John Stark to raise a militia for the defense of the people "or the annoyance of the enemy". Using funds provided by John Langdon, Stark raised 1,500 New Hampshire militiamen in the space of six days, more than ten percent of New Hampshire's male population over the age of sixteen. They were first marched to the Fort at Number 4 (modern Charlestown, New Hampshire), then crossed the river border into the Grants and stopped at Manchester, where Stark conferred with Warner. While in Manchester, General Benjamin Lincoln, whose promotion in preference to Stark had been the cause for Stark's resignation from the Continental Army, attempted to assert Army authority over Stark and his men. Stark refused, stating that he was solely responsible to the New Hampshire authorities. Stark then went on to Bennington with Warner as a guide, while Warner's men remained in Manchester. Lincoln returned to the American camp at Stillwater, where he and General Philip Schuyler hatched a plan for Lincoln, with 500 men, to join with Stark and Warner in actions to harass Burgoyne's communications and supply lines at Skenesboro. Baum's movements significantly altered these plans.
Prelude.
Baum's Germans left Burgoyne's camp at Fort Edward on August 9 and marched to Fort Miller, where they waited until they were joined by the Indians and a company of British marksmen. The company marched off toward Bennington on August 11. In minor skirmishes along the way they learned from prisoners taken that a sizable force was in place at Bennington. On August 14 Baum's men encountered a detachment of Stark's men that had been sent out to investigate reports of Indians in the area. Stark's men retreated, destroying a bridge to delay Baum's advance. Stark, on receiving word of the approaching force, sent a request to Manchester for support, and then moved his troops out of Bennington toward Baum's force, setting up a defensive line. Baum sent a message to Burgoyne following the first contact indicating that the American force was larger than expected, but that it was likely to retreat before him. He then advanced a few miles further until he neared Stark's position. He then realized that at least part of his first message was incorrect, so he sent a second message to Burgoyne, requesting reinforcements.
It rained for the next day and a half, preventing battle. During this time, Baum's men constructed a small redoubt at the crest of the hill and hoped that the weather would prevent the Americans from attacking before reinforcements arrived. Stark sent out skirmishers to probe the German lines, and managed to kill thirty Indians in spite of the difficulties of keeping their gunpowder dry. Reinforcements for both sides marched out on the 15th; travel was quite difficult due to the heavy rains. Burgoyne sent 550 men under Heinrich von Breymann, while Warner's company of about 350 Green Mountain Boys came south from Manchester under Lieutenant Samuel Safford's command.
Late on the night of August 15, Stark was awakened by the arrival of Parson Thomas Allen and a band of Massachusetts militiamen from nearby Berkshire County who insisted on joining his force. In response to the minister's fiery threat that his men would never come out again if they were not allowed to participate, Stark is reported to have said, "Would you go now on this dark and rainy night? Go back to your people and tell them to get some rest if they can, and if the Lord gives us sunshine to-morrow and I do not give you fighting enough, I will never call on you to come again." Stark's forces again swelled the next day with the arrival of some Stockbridge Indians, bringing his force (excluding Warner's men) to nearly 2,000 men.
Stark was not the only beneficiary of unexpected reinforcements. Baum's force grew by almost 100 when a group of local Loyalists arrived in his camp on the morning of August 16.
Battle.
On the afternoon of August 16, the weather cleared, and Stark ordered his men to be ready to attack. Stark is reputed to have rallied his troops by saying, "There are your enemies, the Red Coats and the Tories. They are ours, or this night Molly Stark sleeps a widow." Upon hearing that the militia had melted away into the woods, Baum assumed that the Americans were retreating or redeploying. However, Stark had decided to capitalize on weaknesses in the German's widely distributed position, and had sent sizable flanking parties to either side of his lines. These movements were assisted by a ruse employed by Stark's men that enabled them to get closer safely without alarming the opposing forces. The Germans, most of whom spoke no English, had been told that soldiers with bits of white paper in their hat were Loyalists, and should not be fired on; Stark's men had also heard this and many of them had suitably adorned their hats.
When the fighting broke out around 3:00 PM the German position was immediately surrounded by gunfire, which Stark described as "the hottest engagement I have ever witnessed, resembling a continual clap of thunder." The Loyalists and Indian positions were overrun, causing many of them to flee or surrender. This left Baum and his Brunswick dragoons trapped alone on the high ground. The Germans fought valiantly even after running low on powder and the destruction of their ammunition wagon. In desperation the dragoons led a sabre charge in an attempt to break through the enveloping forces. Baum was mortally wounded in this final charge, and the remaining Germans surrendered.
After the battle ended, while Stark's militiamen were busy disarming the prisoners and looting their supplies, Breymann arrived with his reinforcements. Seeing the Americans in disarray, they immediately pressed their attack. After hastily regrouping, Stark's forces tried to hold their ground against the new German onslaught, but began to fall back. Before their lines collapsed, Warner's men arrived on the scene to reinforce Stark's troops. Pitched battle continued until dark, when both sides disengaged. Breymann began a hasty retreat; he had lost one quarter of his force and all of his artillery pieces.
Aftermath.
Total German and British losses at Bennington were recorded at 207 dead and 700 captured; American losses included 30 Americans dead and 40 wounded. The battle was at times particularly brutal when Loyalists met Patriots, as in some cases they came from the same communities. The prisoners, who were first kept in Bennington, were eventually marched to Boston.
Burgoyne's army was readying to cross the Hudson at Fort Edward on August 17 when the first word of the battle arrived. Believing that reinforcements might be necessary, Burgoyne marched the army toward Bennington until further word arrived that Breymann and the remnants of his force were returning. Stragglers continued to arrive throughout the day and night, while word of the disaster spread within the camp.
The effect on Burgoyne's campaign was significant. Not only had he lost nearly 1,000 men, of which half were regulars, but he also lost the crucial Indian support. In a council following the battle, many of the Indians (who had traveled with him from Quebec) decided to go home. This loss severely hampered Burgoyne's reconnaissance efforts in the days to come. The failure to bring in nearby supplies meant that he had to rely on supply lines that were already dangerously long, and that he eventually broke in September. The shortage of supplies was a significant factor in his decision to surrender at Saratoga, following which France entered the war.
American Patriots reacted to news of the battle with optimism. Especially after Burgoyne's Indian screen left him, small groups of local Patriots began to emerge to harass the fringes of British positions. Interestingly, a significant portion of Stark's force returned home and did not again become influential in the campaign until appearing at Saratoga on October 13 to complete the encirclement of Burgoyne's army.
John Stark's reward from the New Hampshire General Assembly for "the Memorable Battle of Bennington" was "a compleat suit of Clothes becoming his Rank". A reward that Stark likely valued the highest was a message of thanks from John Hancock, president of the Continental Congress, which included a commission as "brigadier in the army of the United States".
Order of battle.
The battle forces are generally described as in Morrissey. His numbers are generally consistent with other sources on the British units, although there is disagreement across a wide array of sources on the number of troops under Breymann, which are generally listed at either approximately 550 or 650. Morrissey is also incorrect in identifying some of the American units. He identifies William Gregg as having a separate command; Gregg apparently led several companies in Nichols' regiment. Morrissey also failed to include the Massachusetts militia, and misidentified Langdon's company, erroneously believing they may have been from Worcester, Massachusetts. (Militia companies from the Worcester area marched on Bennington, with some companies arriving the day after the battle.) Langdon originally raised his company in 1776, but it did not become a cavalry unit until 1778.
Commemorations.
August 16 is a legal holiday in Vermont, known as Bennington Battle Day. The battlefield, now a New York state historic site, was designated a National Historic Landmark on January 20, 1961, and added to the National Register of Historic Places on October 15, 1966. In the 1870s, the local historic society in Bennington commissioned the design and construction of the Bennington Battle Monument, which was complete in 1889 and dedicated in 1891 with ceremonies attended by President Benjamin Harrison. The Monument, an obelisk 306 ft high, is also listed on the National Register of Historic Places. Although the monument was not ready in time to mark the centennial of the battle, the 100th anniversary of the battle was marked by speeches attended by President Rutherford B. Hayes.
References.
</dl>

</doc>
<doc id="39216" url="http://en.wikipedia.org/wiki?curid=39216" title="Bangers and mash">
Bangers and mash

Bangers and mash, also known as sausages and mash, is a traditional British Isles dish made of mashed potatoes and sausages, the latter of which may consist of a variety of flavoured sausage made of pork or beef or a Cumberland sausage. It is sometimes served with onion gravy, fried onions, baked beans, and peas. It is mostly eaten in the United Kingdom, Canada, Australia, and New Zealand.
This dish, even when cooked at home, may be thought of as an example of pub grub—relatively quick and easy to make in large quantities. More up-market varieties, with exotic sausages and mashes, are sold in gastropubs, as well as less alternatives being available in regular public houses.
Etymology.
Although it is sometimes stated that the term "bangers" has its origins in World War II, the term was actually in use at least as far back as 1919. The term "bangers" is attributed (in common usage in the UK) to the fact that sausages, particularly the kind made during World War II under rationing, were made with water so they were more likely to explode under high heat if not cooked carefully; modern sausages do not have this attribute.
In popular culture.
Peter Sellers recorded a song with Sophia Loren, "Bangers and Mash" (1961), extolling their virtues: "No wonder you're so bony Joe, and skinny as a rake. Well then, give us a bash at the bangers and mash me mother used to make".
References.
Notes

</doc>
<doc id="39237" url="http://en.wikipedia.org/wiki?curid=39237" title="Israel Defense Forces">
Israel Defense Forces

The Israel Defense Forces (IDF; Hebrew: צְבָא הַהֲגָנָה לְיִשְׂרָאֵל   , "lit." "The Army of Defense for Israel", commonly known in Israel by the Hebrew acronym "Tzahal" (צה״ל), are the military forces of the State of Israel. They consist of the ground forces, air force, and navy. The IDF is headed by its Chief of General Staff, the "Ramatkal", subordinate to the Defense Minister of Israel; Lieutenant general ("Rav Aluf") Gadi Eizenkot has served as Chief of Staff since 2015.
An order from Defense Minister David Ben-Gurion on 26 May 1948 officially set up the Israel Defense Forces as a conscript army formed out of the paramilitary group Haganah, incorporating the militant groups Irgun and Lehi. The IDF served as Israel's armed forces in all the country's major military operations—including the 1948 War of Independence, 1951–1956 Retribution operations, 1956 Sinai War, 1964–1967 War over Water, 1967 Six-Day War, 1967–1970 War of Attrition, 1968 Battle of Karameh, 1973 Operation Spring of Youth, 1973 Yom Kippur War, 1976 Operation Entebbe, 1978 Operation Litani, 1982 Lebanon War, 1982–2000 South Lebanon conflict, 1987–1993 First Intifada, 2000–2005 Second Intifada, 2002 Operation Defensive Shield, 2006 Lebanon War, 2008-2009 Operation Cast Lead, 2012 Operation Pillar of Defense, and 2014 Operation Protective Edge . The number of wars and border conflicts in which the IDF has been involved in its short history makes it one of the most battle-trained armed forces in the world. While originally the IDF operated on three fronts—against Lebanon and Syria in the north, Jordan and Iraq in the east, and Egypt in the south—after the 1979 Egyptian–Israeli Peace Treaty, it has concentrated its activities in southern Lebanon and the Palestinian Territories, including the First and the Second Intifada.
The Israel Defense Forces differs from most armed forces in the world in many ways. Differences include the mandatory conscription of women and its structure, which emphasizes close relations between the army, navy, and air force. Since its founding, the IDF has been specifically designed to match Israel's unique security situation. The IDF is one of Israeli society's most prominent institutions, influencing the country's economy, culture and political scene. In 1965, the Israel Defense Forces was awarded the Israel Prize for its contribution to education. The IDF uses several technologies developed in Israel, many of them made specifically to match the IDF's needs, such as the Merkava main battle tank, Achzarit armoured personnel carrier, high tech weapons systems, the Iron Dome missile defense system, Trophy active protection system for vehicles, and the Galil and Tavor assault rifles. The Uzi submachine gun was invented in Israel and used by the IDF until December 2003, ending a service that began in 1954. Following 1967, the IDF has close military relations with the United States, including development cooperation, such as on the F-15I jet, THEL laser defense system, and the Arrow missile defense system.
History.
The IDF traces its roots to Jewish paramilitary organizations in the New Yishuv, starting with the Second Aliyah (1904 to 1914). The first such organization was Bar-Giora, founded in September 1907. It was converted to Hashomer in April 1909, which operated until the British Mandate of Palestine came into being in 1920. Hashomer was an elitist organization with narrow scope, and was mainly created to protect against criminal gangs seeking to steal property. During World War I, the forerunners of the Haganah/IDF were the Zion Mule Corps and the Jewish Legion, both of which were part of the British Army. After the Arab riots against Jews in April 1920, the Yishuv's leadership saw the need to create a nationwide underground defense organization, and the Haganah was founded in June of the same year. The Haganah became a full-scale defense force after the 1936–1939 Arab revolt in Palestine with an organized structure, consisting of three main units—the Field Corps, Guard Corps, and the Palmach. During World War II the successor to the Jewish Legion of World War I was the Jewish Brigade.
The IDF was founded following the establishment of the State of Israel, after Defense Minister and Prime Minister David Ben-Gurion issued an order on 26 May 1948. The order called for the establishment of the Israel Defense Forces, and the abolishment of all other Jewish armed forces. Although Ben-Gurion had no legal authority to issue such an order, the order was made legal by the cabinet on 31 May.
The two other Jewish underground organizations, Irgun and Lehi, agreed to join the IDF if they would be able to form independent units and agreed not to make independent arms purchases. This was the background for the dispute which led to the Altalena Affair, following a confrontation regarding the weapons purchased by the Irgun. This resulted in a battle between Irgun members and the newly created IDF. It ended when the ship carrying the arms was shelled. Following the affair, all independent Irgun and Lehi units were either disbanded or merged into the IDF. The Palmach, a strong lobby within the Haganah, also joined the IDF with provisions, and Ben Gurion responded by disbanding its staff in 1949, after which many senior Palmach officers retired, notably its first commander, Yitzhak Sadeh.
The new army organized itself during the 1948 Arab–Israeli War when neighbouring Arab states attacked Israel. Twelve infantry and armored brigades formed: Golani, Carmeli, Alexandroni, Kiryati, Givati, Etzioni, the 7th, and 8th armored brigades, Oded, Harel, Yiftach, and Negev. After the war, some of the brigades were converted to reserve units, and others were disbanded. Directorates and corps were created from corps and services in the Haganah, and this basic structure in the IDF still exists today.
Immediately after the 1948 war, the Israel Defense Forces shifted to low intensity conflict against Arab Palestinian guerrillas. In the 1956 Suez Crisis, the IDF's first test of strength after 1949, the new army proved itself by capturing the Sinai Peninsula from Egypt, which was later returned. In the 1967 Six-Day War, Israel conquered the Sinai Peninsula, Gaza Strip, West Bank (including East Jerusalem) and Golan Heights from the surrounding Arab states, changing the balance of power in the region as well as the role of the IDF. In the following years leading up to the Yom Kippur War, the IDF fought a war of attrition against Egypt in the Sinai and a border war against the PLO in Jordan, culminating in the Battle of Karameh.
The surprise of the Yom Kippur War and its aftermath completely changed the IDF's procedures and approach to warfare. Organizational changes were made and more time was dedicated to training for conventional warfare. However, in the following years the army's role slowly shifted again to low-intensity conflict, urban warfare and counter-terrorism. It was involved in the Lebanese Civil War, initiating Operation Litani and later the 1982 Lebanon War, where the IDF ousted Palestinian guerilla organizations from Lebanon. Palestinian militancy has been the main focus of the IDF ever since, especially during the First and Second Intifadas, Operation Defensive Shield, the Gaza War, Operation Pillar of Defense, and Operation Protective Edge, causing the IDF to change many of its values and publish the IDF Spirit. The Islamic Shia organization Hezbollah has also been a growing threat, against which the IDF fought an asymmetric conflict between 1982 and 2000, as well as a full-scale war in 2006.
Etymology.
The Israeli cabinet ratified the name "Israel Defense Forces" (Hebrew: צְבָא הַהֲגָנָה לְיִשְׂרָאֵל), "Tzva HaHagana LeYisra'el", literally "army for the defense of Israel," on 26 May 1948. The other main contender was "Tzva Yisra'el" (Hebrew: צְבָא יִשְׂרָאֵל). The name was chosen because it conveyed the idea that the army's role was defense, and because it incorporated the name Haganah, the pre-state defensive organization upon which the new army was based. Among the primary opponents of the name were Minister Haim-Moshe Shapira and the Hatzohar party, both in favor of "Tzva Yisra'el".
Organization.
All branches of the IDF answer to a single General Staff. The Chief of the General Staff is the only serving officer having the rank of Lieutenant General ("Rav Aluf"). He reports directly to the Defense Minister and indirectly to the Prime Minister of Israel and the cabinet. Chiefs of Staff are formally appointed by the cabinet, based on the Defense Minister's recommendation, for three years, but the government can vote to extend their service to four (and in rare occasions even five) years. The current chief of staff is Gadi Eizenkot. He replaced Benny Gantz in 2015.
Structure.
The IDF includes the following bodies (those whose respective heads are members of the General Staff are in bold):
Related bodies.
The following bodies work closely with the IDF, but do not (or only partially) belong to its formal structure.
Ranks, uniforms and insignia.
Ranks.
Unlike most world armies, the IDF uses the same rank names in all corps, including the air force, and navy. All enlisted ranks, as well as some of the officer and NCO ranks, may be given as a result of time spent in service, and not for accomplishment or merit.
For ground forces' officers, rank insignia were brass on a red background; for the air force, silver on a blue background; and for the navy, the standard gold worn on the sleeve. Officer insignia were worn on epaulets on top of both shoulders. Insignia distinctive to each service were worn on the cap (see fig. 15).
Enlisted grades wore rank insignia on the sleeve, halfway between the shoulder and the elbow. For the army and air force, the insignia were white with blue interwoven threads backed with the appropriate corps color. Navy personnel wore gold-colored rank insignia sewn on navy blue material.
From the formation of the IDF until the late 1980s, sergeant major was a particularly important warrant officer rank, in line with usage in other armies. However, in the 1980s and 1990s the proliferating ranks of sergeant major became devalued, and now all professional NCO ranks are a variation on sergeant major ("rav samal") with the exception of "rav nagad".
All translations here are the official translations of the IDF's website.
Conscripts ("Hogrim") (Conscript ranks may be gained purely on time served)
Warrant Officers ("Nagadim")
Academic officers ("Ktzinim Akadema'im")
Officers ("Ktzinim")
Uniforms.
The Israel Defense Forces has several types of uniforms:
The first two resemble each other but the Madei Alef is made of higher quality materials in a golden-olive while the madei bet is in olive drab. The dress uniforms may also exhibit a surface shine
The service uniform for all ground forces personnel is olive green; navy and air force uniforms are beige (tan). The uniforms consist of a two-pocket shirt, combat trousers, sweater, jacket or blouse, and shoes or boots. The navy also has an all white dress uniform. The green fatigues are the same for winter and summer and heavy winter gear is issued as needed. Women's dress parallels the men's but may substitute a skirt for the trousers.
Headgear included a service cap for dress and semi-dress and a field cap or bush hat worn with fatigues. IDF personnel generally wear berets in lieu of the service cap and there are many beret colors issued to IDF Servicemen and Women. Paratroops are issued a maroon beret, Golani brown, Givati purple, Kfir Camouflage, Combat Engineers gray, IDF Naval and Air force personnel also have berets. Blue-grey for the IDF Air Corps and Navy-blue for the IDF Naval Forces. Other beret colors are: black for armored corps, Grey for mechanized infantry and turquoise artillery personnel; olive drab for infantry; grey for combat engineers; and purple for the Givati Brigade and brown for the Golani Brigade. For all other army personnel, except combat units, the beret for men was green and for women, black. Women in the navy wore a black beret with gold insignia. Males in the navy once wore a blue/black beret but replaced it with the US Navy's sailor cap.
Some corps or units have small variations in their uniforms – for instance, military policemen wear a white belt and police hat, Naval personnel have dress whites for parades, Paratroopers are issued a four pocket Tunic (shirt) meant to be worn untucked with a pistol belt cinched tight around the waist over the shirt. The IDF air corps has a dress uniform consisting of a pale blue shirt with dark blue trousers. Similarly, while most IDF soldiers are issued black leather boots, certain units issue reddish-brown leather boots for historical reasons — the paratroopers, combat medics, Nahal and Kfir brigades, as well as some SF units (Sayeret Matkal, Oketz, Duvdevan, Maglan, Counter-Terror School). Women were also formerly issued sandals, but this practice has ceased.
Insignia.
IDF soldiers have three types of insignia (other than rank insignia) which identify their corps, specific unit, and position.
A pin attached to the beret identifies a soldier's corps. Soldiers serving in staffs above corps level are often identified by the General Corps pin, despite not officially belonging to it, or the pin of a related corps. New recruits undergoing basic training ("tironut") do not have a pin. Beret colors are also often indicative of the soldier's corps, although most non-combat corps do not have their own beret, and sometimes wear the color of the corps to which the post they're stationed in belongs. Individual units are identified by a shoulder tag attached to the left shoulder strap. Most units in the IDF have their own tags, although those that do not, generally use tags identical to their command's tag (corps, directorate, or regional command).
While one cannot always identify the position/job of a soldier, two optional factors help make this identification: an aiguillette attached to the left shoulder strap and shirt pocket, and a pin indicating the soldier's work type (usually given by a professional course). Other pins may indicate the corps or additional courses taken. Finally, an optional battle pin indicates a war that a soldier has fought in.
Service.
Military service routes.
The military service is held in three different tracks:
Sometimes the IDF would also hold pre-military courses (קורס קדם צבאי or קד"צ) for soon to be regular service soldiers.
Special service routes.
Shoher will have the ability to serve in R&D units without having the engineering credentials if an officer finds him as worthy and could recommend him for the R&D units, R&D unit have the option to provide "על תקן מהנדס" certificate for few selected personal to allow person to work on life saving or flight equipment without having an Eng. license (the certificate isn't valid for medical R&D machinery), the certificate is provided by the highest in command in the research field (as an example for the Air Force it is the Chef of Equipment Group).
The Israeli Manpower Directorate (אגף משאבי אנוש) at the Israeli General Staff is the body which coordinates and assembles activities related to the control over human resources and its placement.
Regular service.
National military service is mandatory for all Israeli citizens over the age of 18, although Arab (but not Druze) citizens are exempted if they so please, and other exceptions may be made on religious, physical or psychological grounds (see Profile 21). The Tal law, which exempts ultra-orthodox Jews from service, has been the subject of several court cases as well as considerable legislative controversy.
Men serve three years in the IDF, while women serve two. The IDF women who volunteer for several combat positions often serve for three years, due to the longer period of training. Women in other positions, such as programmers, who also require lengthy training time, may also serve three years. Women in most combat positions are also required to serve in the reserve for several years after they leave regular service.
Some distinguished recruits are selected to be trained in order to eventually become members of special forces units. Every brigade in the IDF has its own special force branch.
Career soldiers are paid on average NIS 23,000 a month, fifty times the NIS 460 paid to conscripts.
In 1998-2000, only about 9% of those who refused to serve in the Israeli military were granted exemption.
Permanent service.
Permanent service is designed for soldiers who choose to continue serving in the army after their regular service, for a short or long period, and in many cases making the military their career. Permanent service usually begins immediately after the mandatory Regular service period, but there are also soldiers who get released from military at the end of the mandatory Regular service period and who get recruited back to the military as Permanent service soldiers in a later period.
Permanent service is based on a contractual agreement between the IDF and the permanent position holder. The service contract defines how long the soldier's service would be, and towards the end of the contract period a discussion may rise on the extension of the soldier's service duration. Many times, regular service soldiers are required to commit to a permanent service after the mandatory Regular service period, in exchange for assigning them in military positions which require a long training period.
In exchange for the Permanent service, the Permanent service soldiers receive full wages, and when serving for a long period as a permanent service soldier, they are also entitled for a pension from the army. This right is given to the Permanent service soldiers in a relatively early stage of their life in comparison to the rest of the Israeli retirees.
Reserve service.
After personnel complete their regular service, the IDF may call up men for:
In most cases, the reserve duty is carried out in the same unit for years, in many cases the same unit as the active service and by the same people. Many soldiers who have served together in active service continue to meet in reserve duty for years after their discharge, causing reserve duty to become a strong male bonding experience in Israeli society.
Although still available for call-up in times of crisis, most Israeli men, and virtually all women, do not actually perform reserve service in any given year. Units do not always call up all of their reservists every year, and a variety of exemptions are available if called for regular reserve service. Virtually no exemptions exist for reservists called up in a time of crisis, but experience has shown that in such cases (most recently, the 2006 Lebanon War) exemptions are rarely requested or exercised; units generally achieve recruitment rates above those considered fully manned.
Legislation (set to take effect by 13 March 2008) has proposed reform in the reserve service, lowering the maximum service age to 40, designating it as a purely emergency force, as well as many other changes to the structure (although the Defence Minister can suspend any portion of it at any time for security reasons). The age threshold for many reservists whose positions are not listed, though, will be fixed at 49.
Non-IDF service.
Other than the National Service ("Sherut Leumi"), IDF conscripts may serve in bodies other than the IDF in a number of ways.
The combat option is Israel Border Police ("Magav" – the exact translation from Hebrew means "border guard") service, part of the Israel Police. Some soldiers complete their IDF combat training and later undergo additional counter terror and Border Police training. These are assigned to Border Police units. The Border Police units fight side by side with the regular IDF combat units though to a lower capacity. They are also responsible for security in heavy urban areas such as Jerusalem and security and crime fighting in rural areas.
Non-combat services include the Mandatory Police Service ("Shaham") program, where youth serve in the Israeli Police, Israel Prison Service, or other wings of the Israeli Security Forces instead of the regular army service.
Women.
Israel is one of only a few nations that conscript women or deploy them in combat roles, although in practice, women can avoid conscription through a religious exemption and over a third of Israeli women do so. A study of women in the IDF from 2002 to 2005 found that women are often superior in discipline, motivation and marksmanship. However, the study noted that women still face gender discrimination in the IDF.
Civilian pilot and aeronautical engineer Alice Miller successfully petitioned the High Court of Justice to take the Israeli Air Force pilot training exams, after being rejected on grounds of gender. Though president Ezer Weizman, a former IAF commander, told Miller that she would be better off staying home and darning socks, the court eventually ruled in 1996 that the IAF could not exclude qualified women from pilot training. Even though Miller would not pass the exams, the ruling was a watershed, opening doors for women in new IDF roles. Female legislators took advantage of the momentum to draft a bill allowing women to volunteer for any position, if they could qualify.
In 2000, the Equality amendment to the Military Service law stated that the right of women to serve in any role in the IDF is equal to the right of men. Women have taken part in Israel’s military before and since the founding of the state in 1948. Women started to enter combat support and light combat roles in a few areas, including the Artillery Corps, infantry units and armored divisions. A few platoons named Karakal were formed for men and women to serve together in light infantry. By 2000 Karakal became a full-fledged battalion. Many women would also join the Border Police.
In June 2011, Maj. General Orna Barbivai became the first female major general in the IDF, replacing head of the directorate Maj. General Avi Zamir. Barbivai stated, "I am proud to be the first woman to become a major general and to be part of an organization in which equality is a central principle. 90 percent of jobs in the IDF are open to women and I am sure that there are other women who will continue to break down barriers."
In 2013, the IDF announced they would, for the first time, allow a (MTF) transgender woman to serve in the army as a female soldier.
As of 2010, 88% of all roles in the IDF are open to female candidates. and women could be found in 69% of all IDF positions. Elana Sztokman notes it would be "difficult to claim that women are equals in the IDF." The IDF concedes that fewer than 4 percent of women are in combat positions. Rather, they are concentrated in “combat-support” positions which command a lower compensation and status than combat positions. "And tellingly, there is only one female general in the entire IDF," she adds. In 2012 religious soldiers claimed they were promised they would not have to listen to women sing or lecture, but IAF Chief Rabbi Moshe Raved resigned because male religious soldiers were being required to do so. In January 2015 three women IDF singers performed in one of the IDF’s units. The performance was first disrupted by fifteen religious soldiers, who left in protest and then the Master Sergeant forced them to end the performance because it was disturbing the religious soldiers. An IDF spokesperson announced an investigation of the incident: "We are aware of the incident and already began examining it. The exclusion of woman is not consistent with the values of the IDF.” Defense Minister Moshe Ya'alon has also arranged for women to be excluded from recruitment centers catering to religious males. As the IDF recruits more religious soldiers, the rights of male religious soldiers and of women in the IDF come into conflict. Brig. Gen. Zeev Lehrer, who served on the chief of staff’s panel of the integration of women, noted “There is a clear process of ‘religionization’ in the army, and the story of the women is a central piece of it. There are very strong pressures at work to halt the process of integrating women into the army, and they are coming from the direction of religion.” Gender segregation or sex segregation is allowed in the IDF, and the IDF reached what it considers a "new milestone" in 2006, creating the first company of soldiers segregated in an all female unit, the Nachshol (Hebrew for "giant wave") Reconnaissance Company. “We are the only unit in the world made up entirely of female combat soldiers,” said Nachshol Company Commander Cpt. Dana Ben-Ezra. “My girls often carry out tasks more difficult than those of male combat soldiers.”
Minorities in the IDF.
Non-Jewish minorities tended to serve in one of several special units: the Minorities Unit, also known as Unit 300; the Druze Reconnaissance Unit; and the Trackers Unit, which comprised mostly Negev Bedouins. In 1982 the IDF general staff decided to integrate the armed forces by opening up other units to minorities, while placing some Jewish conscripts in the Minorities Unit. Until 1988 the intelligence corps and the air force remained closed to minorities.
Druze and Circassians.
Although Israel has a majority of Jewish soldiers, large numbers of Druze and Circassian men are subject to mandatory conscription to the IDF just like Israeli Jews. Originally, they served in the framework of a special unit called "The Minorities' Unit", which still exists today, in the form of the independent Herev ("Sword") battalion. However, since the 1980s Druze soldiers have increasingly protested this practice, which they considered a means of segregating them and denying them access to elite units (like sayeret units). The army has increasingly admitted Druze soldiers to regular combat units and promoted them to higher ranks from which they had been previously excluded. In recent years, several Druze officers have reached ranks as high as Major General and many have received commendations for distinguished service. In proportion to their numbers, the Druze people achieve much higher—documented—levels in the Israeli army than other soldiers. Nevertheless, some Druze still charge that discrimination continues, such as exclusion from the Air Force, although the official low security classification for Druze has been abolished for some time. The first Druze aircraft navigator completed his training course in 2005; his identity is protected as are those of all air force pilots. During the Israeli War of Independence, many Druze who had initially sided with the Arabs deserted their ranks to either return to their villages or side with Israel in various capacities.
Since the late 1970s the Druze Initiative Committee, centered at the village of Beit Jan and linked to the Israeli Communist Party, has campaigned to abolish Druze conscription.
Military service is a tradition among some of the Druze population, with most opposition in Druze communities of the Golan Heights; 83 percent of Druze boys serve in the army, according to the IDF's statistics. According to the Israeli army, 369 Druze soldiers have been killed in combat operations since 1948.
Bedouins and Israeli Arabs.
By law, all Israeli citizens are subject to conscription. The Defense Minister has complete discretion to grant exemption to individual citizens or classes of citizens. A long-standing policy dating to Israel's early years extends an exemption to all other Israeli minorities (most notably Israeli Arabs). However, there is a long-standing government policy of encouraging Bedouins to volunteer and of offering them various inducements, and in some impoverished Bedouin communities a military career seems one of the few means of (relative) social mobility available. Also, Muslims and Christians are accepted as volunteers, even at an age greater than 18.
From among non-Bedouin Arab citizens, the number of volunteers for military service—some Christian Arabs and even a few Muslim Arabs—is minute, and the government makes no special effort to increase it. Six Israeli Arabs have received orders of distinction as a result of their military service; of them the most famous is a Bedouin officer, Lieutenant Colonel Abd el-Majid Hidr (also known as Amos Yarkoni), who received the Order of Distinction. Vahid el Huzil was the first Bedouin to be a battalion commander. Recently, a Bedouin officer was promoted to the rank of Colonel.
Until the second term of Yitzhak Rabin as Prime Minister (1992–1995), social benefits given to families in which at least one member (including a grandfather, uncle or cousin) had served at some time in the armed forces were significantly higher than to "non-military" families, which was considered a means of blatant discrimination between Jews and Arabs. Rabin had led the abolition of the measure, in the teeth of strong opposition from the Right. At present, the only official advantage from military service is the attaining of security clearance and serving in some types of government positions (in most cases, security-related), as well as some indirect benefits.
Rather than perform army service, Israeli Arab youths have the option to volunteer to national service and receive benefits similar to those received by discharged soldiers. The volunteers are generally allocated to Arab populations, where they assist with social and community matters. s of 2010[ [update]] there are 1,473 Arabs volunteering for national service. According to sources in the national service administration, Arab leaders are counseling youths to refrain from performing services to the state. According to a National Service official, "For years the Arab leadership has demanded, justifiably, benefits for Arab youths similar to those received by discharged soldiers. Now, when this opportunity is available, it is precisely these leaders who reject the state's call to come and do the service, and receive these benefits".
Although Arabs are not obligated to serve in IDF, any Arab can volunteer. A Muslim Arab woman is currently serving as a medic with unit 669.
Cpl. Elinor Joseph from Haifa became the first female Arab combat soldier for IDF. Elinor said:
Other Arab-Muslim officers in the IDF are Hisham Abu Varia, who is currently a Second Lieutenant, and Major Ala Wahib, who is currently the highest ranking Muslim officer in the IDF.
In October 2012, the IDF promoted Mona Abdo to become the first female Christian Arab to the rank of combat commander. Abdo had voluntarily enlisted in the IDF, which her family had encouraged, and transferred from the Ordnance Corps to the Caracal Battalion, a mixed-gender unit with both Jewish and Arab soldiers.
Recently, there has been an increase of Israeli Christian Arabs joining the army.
Ethiopian Jews.
The IDF carried out extended missions in Ethiopia and neighboring states, whose purpose was to protect Ethiopian Jews (Beta Israel) and to help their immigration to Israel. The IDF adopted policies and special activities for absorption and integration of Ethiopian immigrant soldiers, which resulted in great positive impact on the achievements and integration of those soldiers in the army as well as Israeli society in general. Statistical research showed that the Ethiopian soldiers are esteemed as excellent soldiers and many aspire to be recruited to combat units.
Haredim.
Men in the Haredi community may choose to defer service while enrolled in "yeshivot" (see Tal committee); many avoid conscription altogether. This special arrangement is called Torato Omanuto, and has given rise to tensions between the Israeli religious and secular communities. While options exist for Haredim to serve in the IDF in an atmosphere conducive to their religious convictions, most Haredim do not choose to serve in the IDF.
Haredi males have the option of serving in the 97th "Netzah Yehuda" Infantry Battalion. This unit is a standard IDF infantry battalion focused on the Jenin region. To allow Haredi soldiers to serve, the Netzah Yehuda military bases follow the highest standards of Jewish dietary laws; the only women permitted on these bases are wives of soldiers and officers. Additionally, some Haredim serve in the IDF via the Hesder system, principally designed for the Religious Zionist sector; it is a 5-year program which includes 2 years of religious studies, 1½ years of military service and 1½ years of religious studies during which the soldiers can be recalled to active duty at any moment. Haredi soldiers are permitted to join other units of the IDF as well, but rarely do.
The IDF has identified an urgent gap of hundreds of soldiers in their technical units that might be filled by the Haredi. The IAF is currently using defense contractors to fill in the gaps and continue operations.
Although the IDF claims it will not discriminate against women, it is offering Haredim "women free and secular free" recruitment centers. Defense Minister Moshe Ya'alon expressed his willingness to relax regulations to meet the demands of ultra-Orthodox rabbis. Regulations regarding gender equality had already been relaxed so that Haredim could be assured that men would not receive physical exams from female medical staff.
LGBT people.
Israel is one of 24 nations that allow openly gay individuals to serve in the military. Since the early 1990s, sexual identity presents no formal barrier in terms of soldiers' military specialization or eligibility for promotion.
Until the 1980s, the IDF tended to discharge soldiers who were openly gay. In 1983, the IDF permitted homosexuals to serve, but banned them from intelligence and top-secret positions. A decade later, Professor Uzi Even, an IDF reserves officer and chairman of Tel Aviv University’s Chemistry Department revealed that his rank had been revoked and that he had been barred from researching sensitive topics in military intelligence, solely because of his sexual orientation.
His testimony to the Knesset in 1993 raised a political storm, forcing the IDF to remove such restrictions against gays.
The chief of staff's policy states that it is strictly forbidden to harm or hurt anyone's dignity or feeling based on their gender or sexual orientation in any way, including signs, slogans, pictures, poems, lectures, any means of guidance, propaganda, publishing, voicing, and utterance. Moreover, gays in the IDF have additional rights, such as the right to take a shower alone if they want to. According to a University of California, Santa Barbara study, a brigadier general stated that Israelis show a "great tolerance" for gay soldiers. Consul David Saranga at the Israeli Consulate in New York, who was interviewed by the "St. Petersburg Times", said, "It's a non-issue. You can be a very good officer, a creative one, a brave one, and be gay at the same time."
A study published by the Israel Gay Youth (IGY) Movement in January 2012 found that half of the homosexual soldiers who serve in the IDF suffer from violence and homophobia, although the head of the group said that "I am happy to say that the intention among the top brass is to change that."
Deaf and hard-of-hearing people.
Israel is the only country in the world that requires the deaf and hard-of-hearing people to serve in the conscription or military. Sign language interpreters are provided during training, and many of them serve in the non-combat capacities such as mappers, office work, and the like. The deaf and hard-of-hearing people who have served in the IDF have better opportunities in employment, housing, education, and other areas than those who do not serve. In addition, they gain a greater respect and recognition for their service and contribution to the country as well as stronger self-esteem and motivation.
Vegans.
According to a Care2 report, vegans in the IDF may refuse vaccination if they oppose animal testing. They are provided with special allowances to buy their own food. They are also given artificial leather boots.
Volunteers.
In cases when a citizen cannot be normally drafted by the law (old age, served as a soldier in a different country, severe health problems, handicaps, autism, etc.), the person could enroll as a volunteer in places where his knowledge can be used or in cases where there is a base that accepts volunteer service from one day per week up to full 24/7 service based upon that person's abilities and wishes.
Overseas volunteers.
Non-immigrating foreign volunteers typically serve with the IDF in one of five ways:
- Provide a supportive Yeshiva environment to religious students who choose to serve in the IDF.
- Offer a thorough learning program which combines high-level Torah study, army-designed physical training, and an advanced Hebrew ulpan, in order to provide the absolute best ideological, spiritual, and physical preparation for their contribution to Am Yisrael.
- Provide guidance for each student as he navigates through the immigration and military bureaucracies.
- Serve as a “home base” for students preparing for the IDF and for the duration of their Army Service.
Doctrine.
Mission.
The IDF mission is to "defend the existence, territorial integrity and sovereignty of the state of Israel. To protect the inhabitants of Israel and to combat all forms of terrorism which threaten the daily life."
Main doctrine.
The main doctrine consists of the following principles:
Code of conduct.
In 1992, the IDF drafted a Code of Conduct that combines international law, Israeli law, Jewish heritage and the IDF's own traditional ethical code—the IDF Spirit (Hebrew: רוח צה"ל‎, "Ru'ah Tzahal").
Stated values of the IDF.
The document defines three core values for all IDF soldiers to follow, as well as ten secondary values (the first being most important, and the others appearing sorted in Hebrew alphabetical order):
Military ethics of fighting terror.
In 2005, Asa Kasher and Amos Yadlin co-authored a noticed article published in the "Journal of Military Ethics" under the title: "Military Ethics of Fighting Terror: An Israeli Perspective". The article was meant as an "extension of the classical Just War Theory", and as a "[needed] third model" or missing paradigm besides which of "classical war (army) and law enforcement (police).", resulting in a "doctrine (...) on the background of the IDF fight against acts and activities of terror performed by Palestinian individuals and organizations."
In this article, Kasher and Yadlin came to the conclusion that targeted killings of terrorists were justifiable, even at the cost of hitting nearby civilians. In a 2009 interview to "Haaretz", Asa Kasher later confirmed, pointing to the fact that in an area in which the IDF does not have effective security control (e.g., Gaza, vs. Est-Jerusalem), soldiers' lives protection takes priority over avoiding injury to enemy civilians. Some, along with Avishai Margalit and Michael Walzer, have recused this argument, advancing that such position was "contrary to centuries of theorizing about the morality of war as well as international humanitarian law", since drawing "a sharp line between combatants and noncombatants" would be "the only morally relevant distinction that all those involved in a war can agree on."
The article was intended to (then Chief of Staff) Moshe Ya'alon, to serve as a basis for a new "code of conduct". Although Moshe Ya'alon did endorse the article's views, and is reported to have presented it numerous times before military forums, it was never actually turned into a binding IDF document or an actual "code", neither by Ya'alon nor its successors. However, the document have since reportedly been adapted to serve as educational material, designed to emphasizes the right behavior in low intensity warfare against terrorists, where soldiers must operate within a civilian population.
As of today "The Spirit of the IDF" (cf. supra) is still considered the only biding moral code that formally applies to the IDF troops. In 2009, Amos Yadlin (then head of Military Intelligence) suggested that the article he co-authored with Asa Kasher be ratified as a formal binding code, arguing that "the current code ['The Spirit of the IDF'] does not sufficiently address one of the army's most pressing challenges: asymmetric warfare against terrorist organizations that operate amid a civilian population".
The 11 key points highlighted in the article and educational material mentioned above:
Command and Control.
According to the Israeli "Basic Law: The IDF" adopted in 1976, the IDF is subject to the
authority of the Government. The Minister in charge of the IDF on behalf of the
Government is the Minister of Defense. The supreme command level in the military, the
Chief of the General Staff - who is the military’s Commander in Chief - is appointed by
and subject to the authority of the civilian Government and is subordinate to the Minister
of Defense (not the Ministry of Defense itself).
However in the years after the establishment of Israel, the Military establishment
enjoyed a degree of independence given to it by Ben-Gurion. This was evident in the
attendance of the Chief of General Staff in Cabinet and security Cabinet meetings as an
equal and not as a subordinate. Even after the Agranat Commission inquiry following the 1973 Yom Kippur War, when the roles, the powers, and the duties of the Prime Minister, Defense Minister and Chief of General Staff were clarified and the rules and standards of monitoring where established between the military and the political spheres, the military still continued to enjoy an overlarge status on the expense of the civilian authority.
Budget.
During 1950–66, Israel spent an average of 9% of its GDP on defense. Defense expenditures increased dramatically after both the 1967 and 1973 wars. They reached a high of about 24% of GDP in the 1980s, but have since come down significantly, following the signing of peace agreements with Jordan and Egypt.
On 30 September 2009 Defense Minister Ehud Barak, Finance Minister Yuval Steinitz and Prime Minister Benjamin Netanyahu endorsed an additional NIS 1.5 billion for the defense budget to help Israel address problems regarding Iran. The budget changes came two months after Israel had approved its current two-year budget. The defense budget in 2009 stood at NIS 48.6 billion and NIS 53.2 billion for 2010 – the highest amount in Israel's history. The figure constituted 6.3% of expected gross domestic product and 15.1% of the overall budget, even before the planned NIS 1.5 billion addition.
However in 2011, the prime minister Benjamin Netanyahu reversed course and moved to make significant cuts in the defense budget in order to pay for social programs. The General Staff concluded that the proposed cuts endangered the battle readiness of the armed forces. In 2012, Israel spent $15.2 billion on its armed forces, one of the highest ratios of defense spending to GDP among developed countries ($1,900 per person). However, Israel's spending per capita is below that of the USA.
Weapons and equipment.
Military technology.
The IDF possesses top-of-the-line weapons and computer systems. Some gear comes from the US (with some equipment modified for IDF use) such as the M4A1 and M16 assault rifles, the M24 SWS 7.62 mm bolt action sniper rifle, the SR-25 7.62 mm semi-automatic sniper rifle, the F-15 Eagle and F-16 Fighting Falcon fighter jets, and the AH-1 Cobra and AH-64D Apache attack helicopters. Israel has also developed its own independent weapons industry, which has developed weapons and vehicles such as the Merkava battle tank series, Nesher and Kfir fighter aircraft, and various small arms such as the Galil and Tavor assault rifles, and the Uzi submachine gun. Israel has also installed a variant of the Samson RCWS, a remote controlled weapons platform, which can include machine guns, grenade launchers, and anti-tank missiles on a remotely operated turret, in pillboxes along the Israeli Gaza Strip barrier intended to prevent Palestinian militants from entering its territory. Israel has developed observation balloons equipped with sophisticated cameras and surveillance systems used to thwart terror attacks from Gaza. The IDF also possesses advanced combat engineering equipment which include the IDF Caterpillar D9 armored bulldozer, IDF Puma CEV, Tzefa Shiryon and CARPET minefield breaching rockets, and a variety of robots and explosive devices.
The IDF also has several large internal research and development departments, and it purchases many technologies produced by the Israeli security industries including IAI, IMI, Elbit Systems, Rafael, and dozens of smaller firms. Many of these developments have been battle-tested in Israel's numerous military engagements, making the relationship mutually beneficial, the IDF getting tailor-made solutions and the industries a good reputation.
In response to the price overruns on the US Littoral Combat Ship program, Israel is considering producing their own warships, which would take a decade and depend on diverting US financing to the project.
Main developments.
Israel's military technology is most famous for its firearms, armored fighting vehicles (tanks, tank-converted armored personnel carriers (APCs), armoured bulldozers, etc.), unmanned aerial vehicles, and rocketry (missiles and rockets). Israel also has manufactured aircraft including the Kfir (reserve), IAI Lavi (canceled), and the IAI Phalcon Airborne early warning System, and naval systems (patrol and missile ships). Much of the IDF's electronic systems (intelligence, communication, command and control, navigation etc.) are Israeli-developed, including many systems installed on foreign platforms (esp. aircraft, tanks and submarines), as are many of its precision-guided munitions. Israel is the world's largest exporter of drones.
Israel Military Industries (IMI) is known for its firearms. The IMI Galil, the Uzi, the IMI Negev light machine gun and the new Tavor TAR-21 Bullpup assault rifle are used by the IDF.
Israel is the only country in the world with an operational anti-ballistic missile defense system on the national level – the Arrow system, jointly funded and produced by Israel and the United States. The Iron Dome system against short-range rockets is operational and proved to be successful. David's Sling, an anti-missile system designed to counter medium range rockets is under development. Israel has also worked with the US on development of a tactical high energy laser system against medium range rockets (called Nautilus or THEL).
Israel has the independent capability of launching reconnaissance satellites into orbit, a capability shared with Russia, the United States, the United Kingdom, France, South Korea, Italy, Germany, the People's Republic of China, India, Japan, Brazil and Ukraine. Israeli security industries developed both the satellites (Ofeq) and the launchers (Shavit).
Israel is known to have developed nuclear weapons. Israel does not officially acknowledge its nuclear weapons program. It is thought Israel possesses between one hundred and four hundred nuclear warheads. It is believed that Jericho intercontinental ballistic missiles are capable of delivering nuclear warheads with a superior degree of accuracy and a range of 11,500 km. Israeli F-15 and F-16 fighter-bomber aircraft also have been cited as possible nuclear delivery systems. The U.S. Air Force F-15 has tactical nuclear weapon capability. It has been asserted that Dolphin submarines have been adapted to carry Popeye Turbo Submarine-launched cruise missiles with nuclear warheads, so as to give Israel a second strike capacity.
From 2006 Israel deployed the Wolf Armoured Vehicle APC for use in urban warfare and to protect VIPs.
Commemoration.
Commemoration.
Yom Hazikaron, Israel's day of remembrance for fallen soldiers, is observed on the 4th day of the month of Iyar of the Hebrew calendar, the day before the celebration of Independence Day. Memorial services are held in the presence of Israel's top military personnel. A two-minute siren is heard at 11:00, which marks the opening of the official military memorial ceremonies and private remembrance gatherings at each cemetery where soldiers are buried. Many Israelis visit the graves of family members and friends who were killed in action. On the evening before the remembrance day all shops, restaurants and entertainment places must close gates to the public no later than 7 P.M. (the same routine and law applies to the day of remembrance of the Holocaust which takes place a week earlier).
The main museum for Israel's armored corps is the Yad La-Shiryon in Latrun, which houses one of the largest tank museums in the world. Other significant military museums are the Israel Defense Forces History Museum (Batei Ha-Osef) in Tel Aviv, the Palmach Museum, and the Beit HaTotchan of artillery in Zikhron Ya'akov. The Israeli Air Force Museum is located at Hatzerim Airbase in the Negev Desert, and the Israeli Clandestine Immigration and Naval Museum, is in Haifa.
Israel's National Military Cemetery is at Mount Herzl. Other Israeli military cemeteries include Kiryat Shaul Military Cemetery in Tel Aviv, and Sgula military cemetery at Petah Tikva.
Parades.
Israel Defense Forces parades took place on Independence Day, during the first 25 years of the State of Israel's existence. They were cancelled after 1973 due to financial and security concerns. The Israel Defense Forces still has weapon exhibitions country-wide on Independence Day, but they are stationary.
Foreign military relations.
France.
Starting on the Independence day on 14 May 1948 (5 Iyar 5708), a strong military, commercial and political relationship were established between France and Israel until 1969. The highest level of the military collaboration was reached between 1956 and 1966. At this time France provided almost all the aircraft, tanks and military ships. In 1969 the French president Charles de Gaulle limited the export of weapons to Israel. This was the end of the "golden age" 20 years of relations between Israel and France.
United States.
In 1983, the United States and Israel established a Joint Political Military Group, which convenes twice a year. Both the U.S. and Israel participate in joint military planning and combined exercises, and have collaborated on military research and weapons development. Additionally the U.S. military maintains two classified, pre-positioned War Reserve Stocks in Israel valued at $493 million. Israel has the official distinction of being an American Major non-NATO ally. As a result of this, the US and Israel share the vast majority of their security and military technology.
Since 1976, Israel had been the largest annual recipient of U.S. foreign assistance. In 2009, Israel received $2.55 billion in Foreign Military Financing (FMF) grants from the Department of Defense. All but 26% of this military aid is for the purchase of military hardware from American companies only.
The United States has an anti-missile system base in the Negev region of Southern Israel, which is manned by 120 US Army personnel.
In October 2012, United States and Israel began their biggest joint air and missile defense exercise, known as Austere Challenge 12, involving around 3,500 U.S. troops in the region along with 1,000 IDF personnel. Germany and Britain also participated.
India.
India and Israel enjoy strong military and strategic ties. Israeli authorities consider Indian citizens to be the most pro-Israel people in the world. Apart from being Israel's second-largest economic partner in Asia, India is also the largest customer of Israeli arms in the world. In 2006, annual military sales between India and Israel stood at US$900 million. Israeli defense firms had the largest exhibition at the 2009 Aero India show, during which Israel offered several state-of-the art weapons to India. The first major military deal between the two countries was the sale of Israeli EL/W-2090 AEW radars to the Indian Air Force in 2004. In March 2009, India and Israel signed a US$1.4 billion deal under which Israel would sell India an advanced air-defense system. India and Israel have also embarked on extensive space cooperation. In 2008, India's ISRO launched Israel's most technologically advanced spy satellite TecSAR. In 2009, India reportedly developed a high-tech spy satellite RISAT-2 with significant assistance from Israel. The satellite was successfully launched by India in April 2009.
According to a Los Angeles Times news story the 2008 Mumbai attacks were an attack on the growing India-Israel partnership. It quotes retired Indian Vice Admiral Premvir S. Das thus "Their aim was to... tell the Indians clearly that your growing linkage with Israel is not what you should be doing..." In the past, India and Israel have held numerous joint anti-terror training exercises and it was also reported that in the wake of the Mumbai attacks, Israel was helping India launch anti-terror raids inside Pakistani territory.
Germany.
Germany developed the Dolphin submarine and supplied it to Israel. Two submarines were donated by Germany. The military co-operation has been discreet but mutually profitable: Israeli intelligence, for example, sent captured Warsaw Pact armour to West Germany to be analysed. The results aided the German development of an anti-tank system. Israel also trained members of GSG 9, a German counter-terrorism and special operations unit. The Israeli Merkava MK IV tank uses a German V12 engine produced under license.
In 2008, the website DefenseNews revealed that Germany and Israel had been jointly developing a nuclear warning system, dubbed Operation Bluebird.
United Kingdom.
During a secret operation in 1966, two British made "Chieftain" MBTs were brought to Israel for a 4 years long evaluation for service with the IDF. The plan was for the IDF not only to purchase the British MBTs, but for IMI (Israeli Military Industries) to buy production rights. As part of the deal during the early 60's Israel purchased second hand "Centurion" MBTs from the British, that used that money in the "Chieftain" development. After the trials were done Israeli improvement and ideas were implemented by the British manufacturer, but British politicians cancelled the agreement with Israel and the program was shut down. The knowledge earned during the improvements on the "Chieftain", together with earlier experiments in tank improvements, gave the last push for the development and production of the "Merkava" tank.
United Kingdom has supplied equipment and spare parts for Sa'ar 4.5-class missile boats and F-4 Phantom fighter-bombers, components for small-caliber artillery ammunition and air-to-surface missiles, and engines for Elbit Hermes 450 Unmanned aerial vehicles. British arms sales to Israel mainly consist of light weaponry, and ammunition and components for helicopters, tanks, armored personnel carriers, and combat aircraft.
China.
Israel is the second-largest foreign supplier of arms to the People's Republic of China, only after the Russian Federation. China has purchased a wide array of military hardware from Israel, including Unmanned aerial vehicles and communications satellites. China has become an extensive market for Israel's military industries and arms manufacturers, and trade with Israel has allowed it to obtain "dual-use" technology which the United States and European Union were reluctant to provide. In 2010 Yair Golan, head of IDF Home Front Command visited China to strengthen military ties. In 2012, IDF Chief of Staff Benny Gantz visited China for high-level talks with the Chinese defense establishment.
Cyprus.
As closely neighboring countries, Israel and Cyprus have enjoyed greatly improving diplomatic relations since 2010. During the Mount Carmel Forest Fire, Cyprus dispatched two aviation assets to assist fire-fighting operations in Israel – the first time Cypriot Government aircraft were permitted to operate from Israeli airfields in a non-civil capacity. In addition, Israel and Cyprus have closely cooperated in maritime activities relating to Gaza, since 2010, and have reportedly begun an extensive sharing program of regional intelligence to support mutual security concerns. On 17 May 2012, it was widely reported that the Israeli Air Force had been granted unrestricted access to the Nicosia Flight Information Region of Cyprus, and that Israeli aviation assets may have operated over the island itself. Cyprus, as a former S-300 air-defense system operator, was speculated by Greek media to have assisted Israel in strategic planning to challenge such air-defense systems, alongside shorter-range SAM systems, although this remains unconfirmed.
Greece.
Israel and Greece have enjoyed a very cordial military relationship since 2008, including military drills ranging from Israel to the island of Crete. Drills include air-to-air long-distance refueling, long-range flights, and most importantly aiding Israel in outmaneuvering the S-300 which Greece has. Recent purchases include 100 million euro deal between Greece and Israel for the purchase of SPICE 1000 and SPICE 2000 pound bomb kits. They have also signed many defense agreements, including Cyprus, in order to establish stability for transporting gas from Israel-Cyprus to Greece and on to the European Union-a paramount objective to the future stability and prosperity of all three countries, threatened by Turkey.
Turkey.
Israel has provided extensive military assistance to Turkey. Israel sold Turkey IAI Heron Unmanned aerial vehicles, and modernized Turkey's F-4 Phantom and Northrop F-5 aircraft at the cost of $900 million. Turkey's main battle tank is the Israeli-made Sabra tank, of which Turkey has 170. Israel later upgraded them for $500 million. Israel has also supplied Turkey with Israeli-made missiles, and the two nations have engaged in naval cooperation. Turkey allowed Israeli pilots to practice long-range flying over mountainous terrain in Turkey's Konya firing range, while Israel trains Turkish pilots at Israel's computerized firing range at Nevatim Airbase. Until 2009, the Turkish military was one of Israel's largest defense customers. Israel defense companies have sold unmanned aerial vehicles and long-range targeting pods.
However, relations have been strained in recent times. In the last two years, the Turkish military has declined to participate in the annual joint naval exercise with Israel and the United States. The exercise, known as "Reliant Mermaid" was started in 1998 and included the Israeli, Turkish and American navies. The objective of the exercise is to practice search-and-rescue operations and to familiarize each navy with international partners who also operate in the Mediterranean Sea.
Azerbaijan.
Azerbaijan and Israel have engaged in intense cooperation since 1992. Israeli military have been a major provider of battlefield aviation, artillery, antitank, and anti-infantry weaponry to Azerbaijan. In 2009, Israeli President Shimon Peres made a visit to Azerbaijan where military relations were expanded further, with the Israeli company Aeronautics Defense Systems Ltd announcing it was going to build a factory in Baku. In 2012, Israel and Azerbaijan signed an agreement according to which state-run Israel Aerospace Industries would sell $1.6 billion in drones and anti-aircraft and missile defense systems to Azerbaijan. In March 2012, the magazine "Foreign Policy" reported that the Israeli Air Force may be preparing to use the Sitalchay Military Airbase, located 500 km from the Iranian border, for air strikes against the nuclear program of Iran, later backed up by other media.
Other countries.
Israel has also sold or received supplies of military equipment from the Czech Republic, Spain, Slovakia, Italy, South Africa, Canada, Australia, Poland, Slovenia, Romania, Hungary, Belgium, Austria, Serbia, Montenegro, Bosnia and Herzegovina, Georgia, Vietnam and Colombia, among others.
Future.
The IDF is planning a number of technological upgrades for the future. As part of its plans, the M-16 rifle is currently being phased out of all ground units in favor of the IMI Tavor. In addition, the IDF is now planning for a future tank to replace the Merkava. The new tank will be able to fire lasers and electromagnetic pulses, run on a hybrid engine, run with a crew as small as two, will be faster, and will be better-protected, with emphasis on protection systems such as the Trophy over armor.
The Israeli Air Force will purchase as many as 100 F-35 Lightning II fighter jets from the United States. The aircraft will be modified and designated F-35I. They will use Israeli-built electronic warfare systems, outer-wings, guided bombs, and air-to-air missiles.
As part of a 2013 arms deal, the IAF will purchase KC-135 Stratotanker aerial refueling aircraft and V-22 Osprey multi-mission aircraft from the United States, as well as advanced radars for warplanes and missiles designed to take out radars.
In April 2013, an Israeli official stated that within 40–50 years, piloted aircraft would be phased out of service by unmanned aerial vehicles capable of executing nearly any operation that can be performed by piloted combat aircraft. Israel's military industries are reportedly on the path to developing such technology in a few decades. Israel will also manufacture tactical satellites for military use.
The Israeli Navy is expecting the delivery of a fifth Dolphin-class submarine in 2013, and a sixth in 2017. Israel is planning to upgrade its surface fleet, and is jointly developing four frigates based on the Incheon class frigate with South Korea. In addition, Israel may procure destroyers and cruisers equipped with cruise missiles with a range of some 2,000 kilometers. Israel is also developing marine artillery, including a gun capable of firing satellite-guided 155mm rounds between 75 and 120 kilometers.

</doc>
<doc id="39307" url="http://en.wikipedia.org/wiki?curid=39307" title="Adrian and Natalia of Nicomedia">
Adrian and Natalia of Nicomedia

Saint Adrian (also known as Hadrian) or Adrian of Nicomedia (died 4 March 306) was a Herculian Guard of the Roman Emperor Galerius Maximian. After becoming a convert to Christianity with his wife Natalia, Adrian was martyred at Nicomedia.
Martyrdom.
Ss. Adrian and Natalia lived in Nicomedia during the time of Emperor Maximian in the early fourth century. The twenty-eight-year-old Adrian was head of the praetorium.
It is said that while presiding over the torture of a band of Christians, he asked them what reward they expected to receive from God. They replied, "Eye hath not seen, nor ear heard, neither have entered into the heart of man, the things which God hath prepared for them that love him" (). He was so amazed at their courage that he publicly confessed his faith, though he had not himself yet been baptised. He was then immediately imprisoned himself. He was forbidden visitors, but accounts state that his wife Natalia came to visit him dressed as a boy to ask for his prayers when he entered Heaven.
The executioners wanted to burn the bodies of the dead, but a storm arose and quenched the fire. Natalia recovered one of Adrian's hands.
Historicity.
The accuracy of the recorded story has been questioned. Some sources state that there were actually two Adrians martyred at Nicomedia, one under Diocletian, and one under Licinius.
Feast day and patronage.
In the Eastern Orthodox Church, Saint Hadrian shares a feast day with his wife on 8 September; he also has feast days alone on 4 March and 26 August. In the Roman Catholic Church he is venerated alone, without his wife, on 8 September.
Saint Hadrian is protector against the plague, and patron of old soldiers, arms dealers, butchers and communications phenomena. He was the chief military saint of Northern Europe for many ages, second only to St. George, and is much revered in Flanders, Germany and the north of France. He is usually represented armed, with an anvil in his hands or at his feet.

</doc>
<doc id="39315" url="http://en.wikipedia.org/wiki?curid=39315" title="Fascist Manifesto">
Fascist Manifesto

The Manifesto of the Italian Fasci of Combat (Italian: "Il manifesto dei fasci italiani di combattimento"), commonly known as the Fascist Manifesto, was the initial declaration of the political stance of the "Fasci Italiani di Combattimento" ("Italian League of Combat") the movement founded in Milan by Benito Mussolini in 1919 and an early exponent of Fascism. The Manifesto was written by national syndicalist Alceste De Ambris and Futurist movement leader Filippo Tommaso Marinetti.
Contents of the Fascist Manifesto.
The Manifesto (published in "Il Popolo d'Italia" on June 6, 1919) is divided into four sections, describing the movement's objectives in political, social, military and financial fields.
Politically, the Manifesto calls for:
In labor and social policy, the Manifesto calls for:
In military affairs, the Manifesto advocates:
In finance, the Manifesto advocates:
The Manifesto thus reflected the early positions on what would later be characterized by Mussolini in the Doctrine of Fascism as "a series of pointers, forecasts, hints which, when freed from the inevitable matrix of contingencies, were to develop in a few years time into a series of doctrinal positions entitling Fascism to rank as a political doctrine differing from all others, past or present." as progressive movement that, in his view, should surpass the economic and political liberalization of the 19th century. It emphasized major elements of contemporary progressive thought (franchise reform, labor reform, nationalization, taxes on wealth and war profits, economic controls for the sake of national interests, etc.) and laid out some of the ideas of state-control that the Fascist movement embodied, along with some ideas that are widely accepted today. As an ideology founded on the principle of the subordination of individualism to the state, with the fasces as its symbolism, Fascism's early manifesto was the progressive foundation for what would become a totalitarian regime.
The Manifesto in practice.
Of the Manifesto's proposals, the commitment to corporative organisation of economic interests was to be the longest lasting. Far from becoming a medium of extended democracy, parliament became by law an exclusively Fascist-picked body in 1929; being replaced by the "chamber of corporations" a decade later.
Fascism's pacifist foreign policy ceased during its first year of Italian government. In September 1923, the Corfu crisis demonstrated the regime’s willingness to use force internationally. Perhaps the greatest success of Fascist diplomacy was the Lateran Treaty of February 1929: which accepted the principle of non-interference in the affairs of the Church. This ended the 59-year-old dispute between Italy and the Papacy.

</doc>
<doc id="39359" url="http://en.wikipedia.org/wiki?curid=39359" title="Disjoint sets">
Disjoint sets

In mathematics, two sets are said to be disjoint if they have no element in common. Equivalently, disjoint sets are sets whose intersection is the empty set.
For example, {1, 2, 3} and {4, 5, 6} are disjoint sets, while {1, 2, 3} and {3, 4, 5} are not.
Generalizations.
This definition of disjoint sets can be extended to any family of sets. A family of sets is pairwise disjoint or mutually disjoint if every two different sets in the family are disjoint.
For example, the collection of sets { {1}, {2}, {3}, ... } is pairwise disjoint.
Two sets are said to be almost disjoint sets if their intersection is small in some sense. For instance, two infinite sets whose intersection is a finite set may be said to be almost disjoint.
In topology, there are various notions of separated sets with more strict conditions than disjointness. For instance, two sets may be considered to be separated when they have disjoint closures or disjoint neighborhoods. Similarly, in a metric space, positively separated sets are sets separated by a nonzero distance.
Intersections.
Disjointness of two sets, or of a family of sets, may be expressed in terms of their intersections.
Two sets "A" and "B" are disjoint if and only if their intersection formula_1 is the empty set.
It follows from this definition that every set is disjoint from the empty set,
and that the empty set is the only set that is disjoint from itself.
A family "F" of sets is pairwise disjoint if, for every two sets in the family, their intersection is empty.
If the family contains more than one set, this implies that the intersection of the whole family is also empty. However, a family of only one set is pairwise disjoint, regardless of whether that set is empty, and may have a non-empty intersection. Additionally, a family of sets may have an empty intersection without being pairwise disjoint. For instance, the three sets { {1, 2}, {2, 3}, {1, 3} } have an empty intersection but are not pairwise disjoint. In fact, there are no two disjoint sets in this collection. Also the empty family of sets is pairwise disjoint.
A Helly family is a system of sets within which the only subfamilies with empty intersections are the ones that are pairwise disjoint. For instance, the closed intervals of the real numbers form a Helly family: if a family of closed intervals has an empty intersection and is minimal (i.e. no subfamily of the family has an empty intersection), it must be pairwise disjoint.
Disjoint unions and partitions.
A partition of a set "X" is any collection of mutually disjoint non-empty sets whose union is "X". Every partition can equivalently be described by an equivalence relation, a binary relation that describes whether two elements belong to the same set in the partition.
Disjoint-set data structures and partition refinement are two techniques in computer science for efficiently maintaining partitions of a set subject to, respectively, union operations that merge two sets or refinement operations that split one set into two.
A disjoint union may mean one of two things. Most simply, it may mean the union of sets that are disjoint. But if two or more sets are not already disjoint, their disjoint union may be formed by modifying the sets to make them disjoint before forming the union of the modified sets. For instance two sets may be made disjoint by replacing each element by an ordered pair of the element and a binary value indicating whether it belongs to the first or second set.
For families of more than two sets, one may similarly replace each element by an ordered pair of the element and the index of the set that contains it.

</doc>
<doc id="39374" url="http://en.wikipedia.org/wiki?curid=39374" title="Large Magellanic Cloud">
Large Magellanic Cloud

The Large Magellanic Cloud (LMC) is a nearby galaxy, and a satellite of the Milky Way. At a distance of slightly less than 50 kiloparsecs (≈163,000 light-years), the LMC is the third closest galaxy to the Milky Way, with the Sagittarius Dwarf Spheroidal (~ 16 kiloparsecs) and the putative Canis Major Dwarf Galaxy (~ 12.9 kiloparsecs, though its status as a galaxy is under dispute) lying closer to the center of the Milky Way. The LMC has a diameter of about 14,000 light-years (~ 4.3 kpc) and a mass approximately 10 billion times the mass of the Sun (1010 solar masses), making it roughly 1/100 as massive as the Milky Way. The LMC is the fourth largest galaxy in the Local Group, after the Andromeda Galaxy (M31), the Milky Way, and the Triangulum Galaxy (M33).
In the past, the LMC was often considered an irregular type galaxy. However, it is now recognized as a disrupted barred spiral galaxy. The NASA Extragalactic Database, however, still lists the Hubble sequence type as Irr/SB(s)m). In reality, the LMC contains a very prominent bar in its center, suggesting that it may have previously been a standard barred spiral galaxy before being interrupted, likely by the Milky Way gravitational tug, resulting in the interruption of its spiral arms . The present irregular appearance LMC's is likely the result of tidal interactions with both the Milky Way and the Small Magellanic Cloud (SMC).
It is visible as a faint "cloud" in the night sky of the southern hemisphere straddling the border between the constellations of Dorado and Mensa, and it appears from Earth more than 20 times the width of the full moon.
History.
The very first recorded mention of the Large Magellanic Cloud was by the Persian astronomer `Abd al-Rahman al-Sufi Shirazi, (later known in Europe as "Azophi"), in his "Book of Fixed Stars" around 964 AD.
The next recorded observation was in 1503–4 by Amerigo Vespucci in a letter about his third voyage. In this letter he mentions "three Canopes, two bright and one obscure"; "bright" refers to the two Magellanic Clouds, and "obscure" refers to the Coalsack.
Ferdinand Magellan sighted the LMC on his voyage in 1519, and his writings brought the LMC into common Western knowledge. The galaxy now bears his name.
Announced in 2006, measurements with the Hubble Space Telescope suggest the Large and Small Magellanic Clouds may be moving too fast to be orbiting the Milky Way.
Geometry.
The Large Magellanic Cloud is usually considered an irregular galaxy. However, it shows signs of a bar structure, and is often reclassified as a Magellanic-type dwarf spiral galaxy.
The Large Magellanic Cloud has a prominent central bar and a spiral arm. The central bar seems to be warped so that the east and west ends are nearer the Milky Way than the middle. In 2014, measurements from the Hubble Space Telescope made it possible to determine that the LMC has a rotation period of 250 million years.
The LMC was long considered to be a planar galaxy that could be assumed to lie at a single distance from us. However, in 1986, Caldwell and Coulson found that field Cepheid variables in the northeast portion of the LMC lie closer to the Milky Way than Cepheids in the southwest portion. More recently, this inclined geometry for field stars in the LMC has been confirmed via observations of Cepheids, core helium-burning red clump stars and the tip of the red giant branch. All three of these papers find an inclination of ~35°, where a face-on galaxy has an inclination of 0°. Further work on the structure of the LMC using the kinematics of carbon stars showed that the LMC's disk is both thick and flared. Regarding the distribution of star clusters in the LMC, Schommer et al. measured velocities for ~80 clusters and found that the LMC's cluster system has kinematics consistent with the clusters moving in a disk-like distribution. These results were confirmed by Grocholski et al., who calculated distances to a number of clusters and showed that the LMC's cluster system is in fact distributed in the same plane as the field stars.
Distance.
Determining a precise distance to the LMC, as with any other galaxy, was challenging due to the use of standard candles for calculating distances, with the primary problem being that many of the standard candles are not as 'standard' as one would like; in many cases, the age and/or metallicity of the standard candle plays a role in determining the intrinsic luminosity of the object. The distance to the LMC has been calculated using a variety of standard candles, with Cepheid variables being one of the most popular. Cepheids have been shown to have a relationship between their absolute luminosity and the period over which their brightness varies. However, Cepheids appear to suffer from a metallicity effect, where Cepheids of different metallicities have different period–luminosity relations. Unfortunately, the Cepheids in the Milky Way typically used to calibrate the period–luminosity relation are more metal rich than those found in the LMC.
In the era of 8-meter-class telescopes, eclipsing binaries have been found throughout the Local Group. Parameters of these systems can be measured without mass or compositional assumptions. The light echoes of supernova 1987A are also geometric measurements, without any stellar models or assumptions.
Recently, the Cepheid absolute luminosity has been re-calibrated using Cepheid variables in the galaxy Messier 106 that cover a range of metallicities. Using this improved calibration, they find an absolute distance modulus of formula_118.41, or 48 kpc (~157,000 light years). This distance, which is slightly shorter than the typically assumed distance of 50 kpc, has been confirmed by other authors.
By cross-correlating different measurement methods, one can bound the distance; the residual errors are now less than the estimated size parameters of the LMC. Further work involves measuring the position of a target star or star system within the galaxy (i.e. toward or away from the observer).
The results of a study using late-type eclipsing binaries to determine the distance more accurately was published in Nature in March 2013. A distance of 49.97 kpc (162,983 light-years) with an accuracy of 2.2% was obtained.
Features.
Like many irregular galaxies, the LMC is rich in gas and dust, and it is currently undergoing vigorous star formation activity. It is home to the Tarantula Nebula, the most active star-forming region in the Local Group.
The LMC is full of a wide range of galactic objects and phenomena that make it aptly known as an "astronomical treasure-house, a great celestial laboratory for the study of the growth and evolution of the stars," as described by Robert Burnham, Jr. Surveys of the galaxy have found roughly 60 globular clusters, 400 planetary nebulae, and 700 open clusters, along with hundreds of thousands of giant and supergiant stars. Supernova 1987a—the nearest supernova in recent years—was also located in the Large Magellanic Cloud. The Lionel-Murphy SNR is nitrogen-abundant supernova remnant (SNR) N86 in the Large Magellanic Cloud named by astronomers at the Australian National University's Mount Stromlo Observatory in acknowledgement of Australian High Court Justice Lionel Murphy's interest in science and because of SNR N86's perceived resemblance to his large nose.
There is a bridge of gas connecting the Small Magellanic Cloud (SMC) with the LMC, which is evidence of tidal interaction between the galaxies. The Magellanic Clouds have a common envelope of neutral hydrogen indicating they have been gravitationally bound for a long time. This bridge of gas is a star-forming site.
X-ray sources.
No X-rays above background were observed from the Magellanic Clouds during the September 20, 1966, Nike-Tomahawk flight. A second Nike-Tomahawk rocket was launched from Johnston Atoll on September 22, 1966, at 17:13 UTC and reached an apogee of 160 km, with spin-stabilization at 5.6 rps. The LMC was not detected in the X-ray range 8–80 keV.
Another Nike-Tomahawk was launched from Johnston Atoll at 11:32 UTC on October 29, 1968, to scan the LMC for X-rays. The first discrete X-ray source in Dorado was at RA 05h 20m Dec °, and it was the Large Magellanic Cloud. This X-ray source extended over about 12° and is consistent with the Cloud. Its emission rate between 1.5–10.5 keV for a distance of 50 kpc is 4 x 1038 ergs/s. An X-ray astronomy instrument was carried aboard a Thor missile launched from Johnston Atoll on September 24, 1970, at 12:54 UTC and altitudes above 300 km, to search for the Small Magellanic Cloud and to extend previous observations of the LMC. The source in the LMC appeared extended and contained the star ε Dor. The X-ray luminosity (Lx) over the range 1.5–12 keV was 6 × 1031 W (6 × 1038 erg/s).
The Large Magellanic Cloud (LMC) is in the constellations Mensa and Dorado. LMC X-1 (the first X-ray source in the LMC) is at RA 05h 40m 05s Dec ° 45′ 51″, and is a high mass X-ray binary source (HMXB). Of the first five luminous LMC X-ray binaries: LMC X-1, X-2, X-3, X-4, and A 0538–66 (detected by Ariel 5 at A 0538–66); LMC X-2 is the only one that is a bright low-mass X-ray binary system (LMXB) in the LMC.
DEM L316 in the Large Magellanic Cloud consists of two supernove remnants. Chandra X-ray spectra show that the hot gas shell on the upper left contains a high abundance of iron. This implies that the upper left SNR is the product of a Type Ia supernova. The much lower iron abundance in the lower SNR indicates a Type II supernova.
A 16 ms X-ray pulsar is associated with SNR 0538-69.1. SNR 0540-697 was resolved using ROSAT.
View from the LMC.
From a viewpoint in the LMC, the Milky Way's total apparent magnitude would be −2.0—over 14 times brighter than the LMC appears to us on Earth—and it would span about 36° across the sky, the width of over 70 full moons. Furthermore, because of the LMC's high galactic latitude, an observer there would get an oblique view of the entire galaxy, free from the interference of interstellar dust that makes studying in the Milky Way's plane difficult from Earth. The Small Magellanic Cloud would be about magnitude 0.6, substantially brighter than the LMC appears to us.
External links.
Coordinates: 

</doc>
<doc id="39380" url="http://en.wikipedia.org/wiki?curid=39380" title="Maximilian II, Holy Roman Emperor">
Maximilian II, Holy Roman Emperor

Maximilian II (31 July 1527 – 12 October 1576), a member of the Austrian House of Habsburg, was Holy Roman Emperor from 1564 until his death. He was crowned King of Bohemia in Prague on 14 May 1562 and elected King of Germany (King of the Romans) on 24 November 1562. On 8 September 1563 he was crowned King of Hungary and Croatia in the Hungarian capital Pressburg (Pozsony in Hungarian; now Bratislava, Slovakia). On 25 July 1564 he succeeded his father Ferdinand I as ruler of the Holy Roman Empire.
Maximilian's rule was shaped by the confessionalization process after the 1555 Peace of Augsburg. Though a Habsburg and a Catholic, he approached the Lutheran Imperial estates with a view to overcome the denominational schism, which ultimately failed. He also was faced with the ongoing Ottoman–Habsburg wars and rising conflicts with his Habsburg Spain cousins.
According to Fitchner, he failed to achieve his three major aims: rationalizing the government structure, unifying Christianity, and evicting the Turks from Hungary.
Biography.
Born in Vienna, Austria, he was the eldest son of the Habsburg archduke Ferdinand I, younger brother of Emperor Charles V, Holy Roman Emperor, and the Jagiellonian princess Anne of Bohemia and Hungary (1503–1547). He was named after his great-grandfather, Emperor Maximilian I. At the time of his birth, his father Ferdinand succeeded his brother-in-law King Louis II in the Kingdom of Bohemia and the Kingdom of Hungary, laying the grounds for the global Habsburg Monarchy.
Having spent his childhood years at his fathers's court in Innsbruck, Tyrol, he was educated principally in Italy. Among his teachers were humanist scholars like Kaspar Ursinus Velius and Georg Tannstetter. Maximilian also came in contact with the Lutheran teaching and early on corresponded with the Protestant prince Augustus of Saxony, suspiciously eyed by his Habsburg relatives. From the age of 17, he gained some experience of warfare during the Italian War campaign of his uncle Charles V against King Francis I of France in 1544, and also during the Schmalkaldic War. Upon Charles' victory in the 1547 Battle of Mühlberg, Maximilian put in a good word for the Schmalkaldic leaders Elector John Frederick I of Saxony and Landgrave Philip I, Landgrave of Hesse, and soon began to take part in Imperial business.
Heir apparent.
On 13 September 1548 Emperor Charles V married Maximilian to Charles's daughter (Maximilian's cousin) Mary of Spain in the Castile residence of Valladolid. By the marriage his uncle intended to strengthen the ties with the Spanish branch of the Habsburgs, but also to consolidate his nephew's Catholic faith. Maximilian temporarily acted as the emperor's representative in Spain, however not as stadtholder of the Habsburg Netherlands as he had hoped for. To his indignation, King Ferdinand appointed his younger brother Ferdinand II administrator in the Kingdom of Bohemia, nevertheless Maximilian's right of succession as the future king was recognised in 1549. He returned to Germany in December 1550 in order to take part in the discussion over the Imperial succession.
Maximilian's relations with his uncle worsened, as Charles V, again embattled by rebellious Protestant princes led by Elector Maurice of Saxony, wished his son Philip II of Spain to succeed him as emperor. However, Charles' brother Ferdinand, who had already been designated as the next occupant of the imperial throne, and his son Maximilian objected to this proposal. Maximilian sought the support of the German princes such as Duke Albert V of Bavaria and even contacted Protestant leaders like Maurice of Saxony and Duke Christoph of Württemberg. At length a compromise was reached: Philip was to succeed Ferdinand, but during the former's reign Maximilian, as King of the Romans, was to govern Germany. This arrangement was not carried out, and is only important because the insistence of the emperor seriously disturbed the harmonious relations which had hitherto existed between the two branches of the Habsburg family; an illness which befell Maximilian in 1552 was attributed to poison given to him in the interests of his cousin and brother-in-law, Philip II of Spain.
The relationship between the two cousins was uneasy. While Philip had been raised a Spaniard and barely travelled out of the kingdom during is life, Maximilian identified himself as the quintessential German prince and often displayed a strong dislike of Spaniards, whom he considered as intolerant and arrogant. While his cousin was reserved and shy, Maximilian was outgoing and charismatic. His adherence to humanism and religious tolerance put him at odds with Philip who was more committed to the defence of the Catholic faith. Also, he was considered a promising commander, while Philip disliked war and only once did he personally command an army. Nonetheless, the two remained committed to the unity of their dynasty. 
In 1551 Maximilian attended the Council of Trent and the next year took up his residence at Hofburg Palace in Vienna, celebrated by a triumphal return into the city with a large entourage including the elephant Suleiman. While his father Ferdinand concluded the 1552 Treaty of Passau with the Protestant estates and finally reached the Peace of Augsburg in 1555, Maximilian was engaged mainly in the government of the Austrian hereditary lands and in defending them against Ottoman incursions. In Vienna, he had his Hofburg residence extended with the Renaissance "Stallburg" wing, the site of the later Spanish Riding School, and also ordered the construction of Neugebäude Palace in Simmering. The court held close ties to the University of Vienna and employed scholars like the botanist Carolus Clusius and the diplomat Ogier Ghiselin de Busbecq. Maximilian's library curated by Hugo Blotius later became the nucleus of the Austrian National Library. He implemented the Roman School of composition with his court orchestra, however, his plans to win Giovanni Pierluigi da Palestrina as "Kapellmeister" foundered on financial reasons. In the 1550s, Vienna had more than 50,000 inhabitants, making it the largest city in Central Europe with Prague and before Nuremberg (40,000 inhabitants).
The religious views of the future King of Bohemia had always been somewhat uncertain, and he had probably learned something of Lutheranism in his youth; but his amicable relations with several Protestant princes, which began about the time of the discussion over the succession, were probably due more to political than to religious considerations. However, in Vienna he became very intimate with Sebastian Pfauser, a court preacher influenced by Heinrich Bullinger with strong leanings towards Lutheranism, and his religious attitude caused some uneasiness to his father. Fears were freely expressed that he would definitely leave the Catholic Church, and when his father Ferdinand became emperor in 1558 he was prepared to assure Pope Paul IV that his son should not succeed him if he took this step. Eventually Maximilian remained nominally an adherent of the older faith, although his views were tinged with Lutheranism until the end of his life. After several refusals he consented in 1560 to the banishment of Pfauser, and began again to attend the Masses of the Catholic Church.
Succession.
In November 1562 Maximilian was chosen King of the Romans, or German king, by the electoral college at Frankfurt, where he was crowned a few days later, after assuring the Catholic electors of his fidelity to their faith, and promising the Protestant electors that he would publicly accept the confession of Augsburg when he became emperor. He also took the usual oath to protect the Church, and his election was afterwards confirmed by the papacy. He was the first King of the Romans not to be crowned in Aachen. In September 1563 he was crowned King of Hungary by the Archbishop of Esztergom, Nicolaus Olahus, and on his father's death, in July 1564, he succeeded to the empire and to the kingdoms of Hungary, Croatia and Bohemia.
The new emperor had already shown that he believed in the necessity for a thorough reform of the Church. He was unable, however, to obtain the consent of Pope Pius IV to the marriage of the clergy, and in 1568 the concession of communion in both kinds to the laity was withdrawn. On his part Maximilian granted religious liberty to the Lutheran nobles and knights in Austria, and refused to allow the publication of the decrees of the council of Trent. Amidst general expectations on the part of the Protestants he met his first Diet of Augsburg in March 1566. He refused to accede to the demands of the Lutheran princes; on the other hand, although the increase of sectarianism was discussed, no decisive steps were taken to suppress it, and the only result of the meeting was a grant of assistance for the Turkish War, which had just been renewed. Collecting a large army Maximilian marched to defend his territories; but no decisive engagement had taken place when a truce was made in 1568, and the emperor continued to pay tribute to the sultan as the price of peace in the western and northern areas of the Hungarian kingdom still under Habsburg control.
Meanwhile the relations between Maximilian and Philip of Spain had improved; and the emperor's increasingly cautious and moderate attitude in religious matters was doubtless because the death of Philip's son, Don Carlos, had opened the way for the succession of Maximilian, or of one of his sons, to the Spanish throne. Evidence of this friendly feeling was given in 1570, when the emperor's daughter, Anna, became the fourth wife of Philip; but Maximilian was unable to moderate the harsh proceedings of the Spanish king against the revolting inhabitants of the Netherlands. In 1570 the emperor met the diet of Speyer and asked for aid to place his eastern borders in a state of defence, and also for power to repress the disorder caused by troops in the service of foreign powers passing through Germany. He proposed that his consent should be necessary before any soldiers for foreign service were recruited in the empire; but the estates were unwilling to strengthen the imperial authority, the Protestant princes regarded the suggestion as an attempt to prevent them from assisting their co-religionists in France and the Netherlands, and nothing was done in this direction, although some assistance was voted for the defense of Austria. The religious demands of the Protestants were still unsatisfied, while the policy of toleration had failed to give peace to Austria. Maximilian's power was very limited; it was inability rather than unwillingness that prevented him from yielding to the entreaties of Pope Pius V to join in an attack on the Turks both before and after the victory of Lepanto in 1571; and he remained inert while the authority of the empire in north-eastern Europe was threatened.
In 1575, Maximilian was elected by the part of Polish and Lithuanian magnates to be the King of Poland in opposition to Stephan IV Bathory, but he did not manage to become widely accepted there and was forced to leave Poland.
Maximilian died on 12 October 1576 in Regensburg while preparing to invade Poland. On his deathbed he refused to receive the last sacraments of the Church. He is buried in St. Vitus Cathedral in Prague.
By his wife Maria he had a family of nine sons and six daughters. He was succeeded by his eldest surviving son, Rudolf, who had been chosen king of the Romans in October 1575. Another of his sons, Matthias, also became emperor; three others, Ernest, Albert and Maximilian, took some part in the government of the Habsburg territories or of the Netherlands, and a daughter, Elizabeth, married Charles IX of France.
Religious policies.
Maximilian's policies of religious neutrality and peace in the Empire afforded its Roman Catholics and Protestants a breathing-space after the first struggles of the Reformation. His reign also saw the high point of Protestantism in Austria and Bohemia and unlike his successors, Maximilian did not try to suppress it.
He disappointed the German Protestant princes by his refusal to invest Lutheran administrators of prince-bishoprics with their imperial fiefs. Yet on a personal basis he granted freedom of worship to the Protestant nobility and worked for reform in the Roman Catholic Church, including the right of priests to marry. This failed because of Spanish opposition.
Maximilian II was a member of the Order of the Golden Fleece.
Marriage and children.
On 13 September 1548, Maximilian married his first cousin Mary of Spain, daughter of Emperor Charles V and Isabella of Portugal. Despite Mary's commitment to Habsburg Spain and her strong Catholic manners, the marriage was a happy one. The couple had sixteen children:
Official style.
"Maximilian II, by the grace of God elected Holy Roman Emperor, forever August, King in Germany, of Hungary, Bohemia, Dalmatia, Croatia, Slavonia, etc. Archduke of Austria, Duke of Burgundy, Brabant, Styria, Carinthia, Carniola, Luxemburg, Württemberg, the Upper and Lower Silesia, Prince of Swabia, Margrave of the Holy Roman Empire, Burgau, Moravia, the Upper and Lower Lusatia, Princely Count of Habsburg, Tyrol, Ferrette, Kyburg, Gorizia, Landgrave of Alsace, Lord of the Wendish March, Pordenone and Salins, etc. etc."

</doc>
<doc id="39431" url="http://en.wikipedia.org/wiki?curid=39431" title="New Scientist">
New Scientist

New Scientist is a UK-based weekly non-peer-reviewed English-language international science magazine, founded in 1956. Since 1996 it has also run a website.
Sold in retail outlets and on subscription, the magazine covers current developments, news, reviews and commentary on science and technology. It also prints speculative articles, ranging from the technical to the philosophical. There is a readers' letters section which discusses recent articles, and discussions also take place on the website.
Readers contribute observations on examples of pseudoscience to Feedback, and questions and answers on scientific and technical topics to Last Word; extracts from the latter have been compiled into several books.
"New Scientist" is based in London and publishes editions in the UK, the United States, and Australia.
History.
The magazine was founded in 1956 by Tom Margerison, Max Raison and Nicholas Harrison as "The New Scientist", with Issue 1 on 22 November, priced one shilling (£<br>{Inflation} - Amount must not have "" prefix: 0.05.   today).
The British science magazine "Science Journal", published 1965–71, was merged with "New Scientist" to form "New Scientist and Science Journal".
Originally, the cover had a text list of articles rather than a picture. Pages were numbered sequentially for an entire volume of many issues, as is the norm for academic journals (i.e., so that the first page of a March issue could be 651 instead of 1); later each issue's pages were numbered separately. Colour was not used except for blocks of colour on the cover. From the beginning of 1961 "The" was dropped from the title and from 1965 the front cover was illustrated. In 1964 there was a regular "Science in British Industry" section with several items. An article published on their tenth anniversary provides some anecdotes on the founding of the magazine.
In 1970, the company Albert E. Reed acquired "New Scientist" when it merged with IPC Magazines, retaining the magazine when it sold most of its consumer magazines in a management buyout to what is now IPC Media.
The Grimbledon Down comic strip appeared from 1970 to 1994. Ariadne, which later moved to "Nature", commented every week on the lighter side of science and technology and the plausible but impractical humorous inventions of (fictitious) inventor Daedalus, often developed by the (fictitious) DREADCO corporation.
Issues of "(The) New Scientist" from Issue 1 to the end of 1989 have been made free to read online. Subsequent issues require a subscription.
As of the first half of 2013, the international circulation averaged 125,172, a 4.3% reduction on the previous year's figure, but a considerably smaller reduction than many other mainstream magazines of similar or greater circulation. For 2014 UK circulation decreased 3.2% but boosted by stronger international sales, circulation increased to 129,585. See also #Website below.
Modern format.
"New Scientist" currently contains the following sections: Leader, News, Technology, Opinion (interviews, point-of-view articles and letters), Features (including cover article), CultureLab (book and event reviews), Feedback (humour), The Last Word (questions and answers) and Jobs & Careers.
There are 51 issues a year; the Christmas and New Year double issue covers two weeks. The double issue in 2014 was the 3,000th edition of the magazine.
Staff.
Editor-in-chief is Jeremy Webb and the editor is Sumit Paul-Choudhury. Consultants include Fred Pearce (environment) and Marcus Chown (cosmology). Simon Ings is an editor.
Advertising.
"New Scientist" runs advertisements for jobs and academic opportunities in the fields of science and technology. Originally in a "Classified Advertisements" section with subsections "Official Appointments", "Appointments and Situations Vacant", and "Travel" (coach holidays and prices), the section became "NewScientist Jobs".
Most advertising is full-page between sections.
Website.
The "New Scientist" website carries blogs, reports and news articles; users with free-of-charge registration have limited access to new content and can receive emailed "New Scientist" newsletters. Subscribers to the print edition have full access to all articles and the archive of past content that has so far been digitised.
The magazine had a weekly podcast, SciPod, which was discontinued in October 2007. In 2004 "NewScientist.com" added a subdomain, "nomoresocks" (No More Socks), where visitors could search for, rate, and discuss innovative gifts. Falling interest in the site resulted in its being discontinued in 2005.
From mid-2006 some "New Scientist" content was made available to users of Newsvine, a community-driven social news website. From mid-December 2009 to March 2010 non-subscribers could read up to seven articles per month.
In November 2009 "New Scientist" started The S Word, a blog providing a forum for the discussion of "The science of politics – and vice versa". It was so named because "Despite the central role that science plays in our world, politicians often seem reluctant to engage with it", with the aim of the blog being to help "persuade politicians that 'the s word' belongs at the heart of political debate".
The technology, environment and space sites were discontinued in 2008, with the content being integrated into the main site.
Online readership takes various forms. Overall global views of an online database of over 100,000 articles are 8.0m by 3.6m unique users according to Adobe Reports & Analytics, as of September 2014[ [update]]. On social media there are 1.47m+ Twitter followers, 2.3m+ Facebook likes and 365,000+ Google+ followers as of January 2015[ [update]].
Spin-offs.
"New Scientist" has published books derived from its content, many of which are selected questions and answers from the "Last Word" section of the magazine and website -
Other books published by New Scientist include -
In 2012 "Arc", "a new digital quarterly from the makers of "New Scientist", exploring the future through the world of science fiction" and fact was launched. In the same year the magazine launched a dating service, NewScientistConnect, operated by The Dating Lab.
Criticism.
Greg Egan's criticism of the EmDrive article.
In September 2006, "New Scientist" was criticised by science fiction writer Greg Egan, who wrote that "a sensationalist bent and a lack of basic knowledge by its writers" was making the magazine's coverage sufficiently unreliable "to constitute a real threat to the public understanding of science". In particular, Egan found himself "gobsmacked by the level of scientific illiteracy" in the magazine's coverage of Roger Shawyer's "electromagnetic drive", where "New Scientist" allowed the publication of "meaningless double-talk" designed to bypass a fatal objection to Shawyer's proposed space drive, namely that it violates the law of conservation of momentum. Egan urged others to write to "New Scientist" and pressure the magazine to raise its standards, instead of "squandering the opportunity that the magazine's circulation and prestige provides".
The editor of "New Scientist", then Jeremy Webb, replied defending the article, saying that it is "an ideas magazine—that means writing about hypotheses as well as theories".
"Darwin was wrong" cover.
In January 2009, "New Scientist" ran a cover with the title "Darwin was wrong". The actual story stated that specific details of Darwin's evolution theory had been shown incorrectly, mainly the shape of phylogenetic trees of interrelated species, which should be represented as a web instead of a tree. Some evolutionary biologists who actively oppose the intelligent design movement thought the cover was both sensationalist and damaging to the scientific community. Jerry Coyne, author of the book "Why Evolution Is True", called for a boycott of the magazine, which was supported by evolutionary biologists Richard Dawkins and P.Z. Myers.

</doc>
<doc id="39467" url="http://en.wikipedia.org/wiki?curid=39467" title="Adam Oehlenschläger">
Adam Oehlenschläger

Adam Gottlob Oehlenschläger (14 November 1779 – 20 January 1850) was a Danish poet and playwright. He introduced romanticism into Danish literature.
Biography.
He was born in Vesterbro, then a suburb of Copenhagen, on 14 November 1779. His father, a Schleswiger by birth, was at that time organist, and later became keeper, of the royal palace of Frederiksberg; he was a very brisk and cheerful man. The poet's mother, on the other hand, who was partly German by extraction, suffered from depression, which afterwards deepened into melancholy madness.
Oehlenschläger and his sister Sofia were allowed their own way throughout their childhood, and were taught nothing, except to read and write, until their twelfth year. At the age of nine, Oehlenschläger began to make fluent verses. Three years later, while walking in Frederiksberg Gardens, he attracted the notice of the poet Edvard Storm, and the result of the conversation was that he received a nomination to the college called Posterity's High School, an important institution of which Storm was the principal. Storm himself taught the class of Scandinavian mythology, and thus Oehlenschläger received his earliest bias towards the poetical religion of his ancestors.
Oehlenschläger was confirmed in 1795, and was to have been apprenticed to a tradesman in Copenhagen. To his great delight there was a hitch in the preliminaries, and he returned to his father's house. He now, in his eighteenth year, suddenly took up study with great zeal, but soon again abandoned his books for the stage, where he was offered a small position. In 1797 he made his appearance on the boards in several successive parts, but soon discovered that he possessed no real histrionic talent. The brothers Ørsted, with whom he had formed an intimacy that proved quite profitable to him, persuaded him to quit the stage, and in 1800 he entered the University of Copenhagen as a student. He was doomed, however, to disturbance in his studies, first from the death of his mother, next from his inveterate tendency towards poetry, and finally from the First Battle of Copenhagen in April 1801, which, however, inspired a dramatic sketch ("April the Second 1801") which is the first thing of the kind by Oehlenschläger that we possess.
In the summer of 1802, when Oehlenschläger had an old Scandinavian romance, as well as a volume of lyrics, in the press, the young Norse philosopher, Henrik Steffens, came back to Copenhagen after a long visit to Schelling in Germany, full of new romantic ideas. His lectures at the university, in which Goethe and Schiller were revealed to the Danish public for the first time, created a great sensation. Steffens and Oehlenschläger met one day at Dreier's Club, and after a conversation of sixteen hours the latter went home, suppressed his two coming volumes, and wrote at a sitting his splendid poem "Guldhornene", in a manner totally new to Danish literature. The result of his new enthusiasm speedily showed itself in a somewhat hasty volume of poems, published in 1803, now chiefly remembered as containing the lovely piece called "Sanct Hansaften-Spil".
The next two years saw the production of several exquisite works, in particular the epic of "Thors Reise til Jotunheim", the charming poem in hexameters called "Langelandsreisen", and the bewitching piece of fantasy "Aladdin" (1805). At the age of twenty-six, Oehlenschläger was universally recognised, even by the opponents of the romantic revival, as the leading poet of Denmark. He now collected his "Poetical Writings" in two volumes. He found no difficulty in obtaining a grant for foreign travel from the government, and he left his native country for the first time, joining Steffens at Halle in August 1805. Here he wrote the first of his great historical tragedies, "Hakon Jarl", which he sent off to Copenhagen, and then proceeded for the winter months to Berlin, where he associated with Humboldt, Fichte, and the leading men of the day, and met Goethe for the first time.
In the spring of 1806 he went on to Weimar, where he spent several months in daily intercourse with Goethe. The autumn of the same year he spent with Tieck in Dresden, and proceeded in December to Paris. Here he resided eighteen months and wrote his three famous masterpieces, "Baldur hin Gode" (1808), "Palnatoke" (1809), and "Axel og Valborg" (1810). Oehlenschläger had also made his own translation of "Aladdin" into German, adding some extra new material which does not appear in the 1805 edition; this revised version was published in Amsterdam in 1808. Ferruccio Busoni later used the text of this translation for the last (choral) movement of his Piano Concerto Op. 39. Later editions of Oehlenschläger's play do not contain this text.
In July 1808 he left Paris and spent the autumn and winter in Switzerland as the guest of Madame de Staël at Coppet, in the midst of her circle of wits. In the spring of 1809 Oehlenschläger went to Rome to visit Bertel Thorvaldsen, and in his house wrote his tragedy of "Correggio." He hurriedly returned to Denmark in the spring of 1810, partly to take the chair of aesthetics at the University of Copenhagen, partly to marry the sister-in-law of Rahbek, to whom he had been long betrothed. His first course of lectures dealt with his Danish predecessor Johannes Ewald, the second with Schiller. From this time forward his literary activity became very great; in 1811 he published the Oriental tale of "Ali og Gulhyndi", and in 1812 the last of his great tragedies, "Stærkodder".
From 1814 to 1819 he, or rather his admirers, were engaged in a long and angry controversy with Baggesen, who represented the old didactic school. This contest seems to have disturbed the peace of Oehlenschläger's mind and to have undermined his genius. His talent may be said to have culminated in the glorious cycle of verse-romances called "Helge", published in 1814. The tragedy of "Hagbarth og Signe", (1815), showed a distinct falling-off in style. In 1817 he went back to Paris, and published "Hroars Saga" and the tragedy of "Fostbrødrene". In 1818 he was again in Copenhagen, and wrote the idyll of "Den lille Hyrdedreng" and the Eddaic cycle called "Nordens Guder". His next productions were the tragedies of "Erik og Abel" (1820) and "Væringerne i Miklagaard" (1826), and the epic of "Hrolf Krake" (1829). His last volumes were "Tordenskjold" (1833), "Dronning Margrethe" (1833), "Sokrates" (1835), "Olaf den Hellige" (1836), "Knud den Store" (1838), "Dina" (1842), "Erik Glipping" (1843), and "Kiartan og Gudrun" (1847). On his seventieth birthday, 14 November 1849, a public festival was arranged in his honour, and he was decorated by the king of Denmark under circumstances of great pomp. He died on 20 January 1850 and was buried in the cemetery of Frederiksberg. Immediately after his death his "Recollections" were published in two volumes.
Legacy.
With the exception of Ludvig Holberg, no Danish writer before 1870 has exercised so wide an influence as Oehlenschläger. His great work was to awaken in the breasts of his countrymen an enthusiasm for the poetry and religion of their ancestors, and this he performed to so complete an extent that his name remains to this day synonymous with Scandinavian romance. He supplied his countrymen with romantic tragedies at the very moment when all eyes were turned to the stage, and when the old-fashioned pieces were felt to be inadequate. His plays, partly no doubt in consequence of his own early familiarity with acting, fulfilled the stage requirements of the day, and were popular beyond all expectation. The earliest are the best: Oehlenschlager's dramatic masterpiece being, without doubt, his first tragedy, "Hakon Jarl". In his poems and plays alike his style is limpid, elevated, profuse; his flight is sustained at a high pitch without visible excitement. His fluent tenderness and romantic zest have been the secrets of his extreme popularity. Although his inspiration came from Germany, he is not much like a German poet, except when he is consciously following Goethe; his analogy is rather to be found among English poets than his contemporaries. His mission towards antiquity reminds us of Scott; he sometimes has touches of exquisite diction and of overwrought sensibility which recall Coleridge. In his wide ambition and profuseness he possessed some characteristics of Robert Southey, although his style has far more vitality. With all his faults he was a very great writer, and one of the principal pioneers of the romantic movement in Europe.
In 1829 he was publicly crowned with laurel as the "king of Nordic poetry" and the "Scandinavian King of Song" (by Bishop Esaias Tegnér, who would be his Swedish parallel) in the cathedral of Lund, Sweden, based on a vast production of poetry, theatre plays and prose, inspired by Johann Wolfgang von Goethe, Gottlieb Fichte, and Friedrich von Schelling. (See also Jens Immanuel Baggesen)
He wrote the song "Der er et yndigt land", which is now the national anthem of Denmark.

</doc>
<doc id="39482" url="http://en.wikipedia.org/wiki?curid=39482" title="Mai Zetterling">
Mai Zetterling

Mai Elisabeth Zetterling (]; 24 May 1925 – 17 March 1994) was a Swedish actress and film director.
Early life.
Zetterling was born in Västerås, Västmanland, Sweden to a working-class family. She started her career as an actress at the age of 17 at Dramaten, the Swedish national theater, appearing in war-era films.
Career.
Zetterling appeared in film and television productions spanning six decades from the 1940s to the 1990s. Her breakthrough came in the 1944 film "Torment" written by Ingmar Bergman, in which she played a controversial role as a tormented shopgirl. Shortly afterwards she moved to England and gained instant success there with her title role in Basil Dearden's "Frieda" (1947) playing opposite David Farrar. After a brief return to Sweden in which she worked with Bergman again in his film "Music in Darkness" (1948), she returned to England and starred in a number of English films, playing against such leading men as Tyrone Power, Dirk Bogarde, Richard Widmark, Laurence Harvey, Peter Sellers, Herbert Lom, Richard Attenborough, Keenan Wynn, Stanley Baker, and Dennis Price.
Some of her notable films as an actress include "Quartet" (1948), a film based on some of W. Somerset Maugham's short stories, "The Romantic Age" (1949) directed by Edmond T. Gréville, "Only Two Can Play" (1962) co-starring Peter Sellers and directed by Sidney Gilliat, and "The Witches" (1990), an adaptation of Roald Dahl's book directed by Nicolas Roeg. Having gained a reputation as a sex symbol in dramas and thrillers, she was equally effective in comedies, and also was very active in British television in the '50s and '60s.
She began directing in the early 1960s, starting with political documentaries and a short film called "The War Game" (1962), which was nominated for a BAFTA award, and won a Silver Lion at Venice. Her first feature film "Älskande par" (1964, "Loving Couples"), based on the novels of Agnes von Krusenstjerna, was banned at the Cannes Film Festival for its sexual explicitness and nudity. Kenneth Tynan of "The Observer" later called it "one of the most ambitious debuts since "Citizen Kane"." It was not the only film she made that would stir up controversy for its frank sexuality.
When critics reviewing her debut feature said that "Mai Zetterling directs like a man," she began to explore feminist themes more explicitly in her work. "The Girls", which had an all-star Swedish cast including Bibi Andersson and Harriet Andersson, discussed women's liberation (or lack thereof) in a society controlled by men, as the protagonists compare their lives to characters in the play "Lysistrata", and find that things have not progressed very much for women since ancient times.
Personal life.
In her autobiography, "All Those Tomorrows," published in 1985, Zetterling details love affairs with actor Herbert Lom and later Tyrone Power, with whom she lived from 1956 until early 1958.
She was married to Norwegian actor Tutte Lemkow from 1944 to 1953. Lemkow and Zetterling have a daughter, Etienne and a son, Louis, who is professor of environmental sociology at the Autonomous University of Barcelona. From 1958 to 1976 she was married to British author David Hughes, who collaborated with her on her first films as director.
She died in London, from cancer on 17 March 1994, at the age of 68, a year after her final role on television. Recently released documents at the National Archives in London show that she, a member of the Hollywood Left, was watched by British security agents as a suspected Communist. However, the UK never had a system along the lines of the American Hollywood Blacklist.
Filmography.
A partial filmography as director:
Actress

</doc>
<doc id="39487" url="http://en.wikipedia.org/wiki?curid=39487" title="1494">
1494

Year 1494 (MCDXCIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar).
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="39493" url="http://en.wikipedia.org/wiki?curid=39493" title="1485">
1485

Year 1485 (MCDLXXXV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="39499" url="http://en.wikipedia.org/wiki?curid=39499" title="1472">
1472

Year 1472 (MCDLXXII) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="39508" url="http://en.wikipedia.org/wiki?curid=39508" title="1455">
1455

Year 1455 (MCDLV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
January–December.
</onlyinclude>

</doc>
<doc id="39519" url="http://en.wikipedia.org/wiki?curid=39519" title="1352">
1352

Year 1352 (MCCCLII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
<doc id="39529" url="http://en.wikipedia.org/wiki?curid=39529" title="United States presidential election, 1992">
United States presidential election, 1992

 George H. W. Bush
Bill Clinton
The United States presidential election of 1992 was the 52nd quadrennial presidential election. It was held on Tuesday, November 3, 1992. There were three major candidates: Incumbent Republican President George H. W. Bush; Democratic Arkansas Governor Bill Clinton, and independent Texas businessman Ross Perot.
Bush had alienated much of his conservative base by breaking his 1988 campaign . The economy was also in a recession and Bush's perceived greatest strength, foreign policy, was regarded as much less important following the dissolution of the Soviet Union and the relatively peaceful climate in the Middle East after the defeat of Iraq in the Gulf War.
Clinton won a plurality in the popular vote, and a wide Electoral College margin. The election was a significant realigning election after three consecutive Republican landslides. Northeastern, Upper Midwest, and West Coast states which had previously been competitive began voting reliably Democratic. This is the most recent election in which an incumbent president was defeated.
Nominations.
Republican Party nomination.
Republican candidates
Candidates gallery.
Conservative journalist Pat Buchanan was the primary opponent of President Bush; Ron Paul, who had been the Libertarian Presidential nominee in 1988 had himself been planning to run against the President, but dropped out shortly after Buchanan's entry in December. Buchanan's best showing was in the New Hampshire primary on February 18, 1992—where Bush won by a . President Bush won 73% of all primary votes, with 9,199,463 votes. Buchanan won 2,899,488 votes; unpledged delegates won 287,383 votes, and David Duke, a former Democratic Louisiana state representative and Grand Wizard of the Ku Klux Klan won 119,115 votes. Just over 100,000 votes were cast for all other candidates, half of which were write-in votes for H. Ross Perot
President George H. W. Bush and Vice President Dan Quayle easily won renomination by the Republican Party. However, the success of the conservative opposition forced the moderate Bush to move further to the right than in 1988, and to incorporate many socially conservative planks in the party platform. Bush allowed Buchanan to give the keynote address at the Republican National Convention in Houston, Texas, and his culture war speech alienated many moderates.
With intense pressure on the Buchanan delegates to relent, the tally for president went as follows:
Vice President Dan Quayle was renominated by voice vote.
Democratic Party nomination.
Democratic candidates
Overview.
After the successful performance by U.S. and coalition forces in the Persian Gulf War, President George H.W. Bush's approval ratings were 89%. His re-election was considered very likely. As a result, several high profile candidates, such as Mario Cuomo, refused to seek the Democratic nomination. In addition, Senator Al Gore refused to seek the nomination due to the fact his son was struck by a car and was undergoing extensive surgery as well as physical therapy. However, Tom Harkin, Paul Tsongas, Jerry Brown, Bob Kerrey, Douglas Wilder and Bill Clinton chose to run as candidates.
U.S. Senator Tom Harkin (Iowa) ran as a populist liberal with labor union support. Former U.S. Senator Paul Tsongas (Massachusetts) highlighted his political independence and fiscal conservatism. Former California Governor Jerry Brown, who had run for the Democratic nomination in 1976 and 1980 while he was still Governor, declared a significant reform agenda, including Congressional term limits, campaign finance reform, and the adoption of a flat income tax. Nebraska Senator Bob Kerrey was an attractive candidate based on his business and military background, but made several gaffes on the campaign trail. Arkansas Governor Bill Clinton positioned himself as a centrist, or New Democrat. He was still relatively unknown nationally before the primary season. That quickly changed however, when a woman named Gennifer Flowers appeared in the press to reveal allegations of an affair. Clinton rebutted the story by appearing on "60 Minutes" with his wife, Hillary Rodham Clinton.
The primary season began with U.S. Senator Tom Harkin winning his native Iowa as expected. Paul Tsongas of Massachusetts won the New Hampshire primary on February 18 but Clinton's second-place finish, helped by his speech labeling himself "The Comeback Kid," energized his campaign. Jerry Brown won the Maine caucus and Bob Kerrey won South Dakota. Clinton won his first primary in Georgia. Tsongas won the Utah and Maryland primaries and a caucus in Washington. Harkin won caucuses in Idaho and Minnesota while Jerry Brown won Colorado. Bob Kerrey dropped out two days later. Clinton won the South Carolina and Wyoming primaries and Tsongas won Arizona. Harkin dropped out. Jerry Brown won the Nevada caucus. Clinton swept nearly all of the Super Tuesday primaries on March 10 making him the solid front runner. Clinton won the Michigan and Illinois primaries. Tsongas dropped out after finishing 3rd in Michigan. Jerry Brown, however, began to pick up steam, aided by using a 1–800 number to receive funding from small donors. Brown scored surprising wins in Connecticut, Vermont and Alaska. As the race moved to the primaries in New York and Wisconsin, Brown had taken the lead in polls in both states. Then he made a serious gaffe by announcing to an audience of New York City's Jewish community that, if nominated, he would consider Reverend Jesse Jackson as a Vice Presidential candidate. Clinton won dramatically in New York (41%–26%) and closely in Wisconsin (37%–34%). Clinton then proceeded to win a long streak of primaries leading up to Jerry Brown's home state of California. Clinton won this primary 48% to 41% and secured the delegates needed to clinch the nomination.
The convention met in New York, New York, and the official tally was:
Clinton chose U.S. Senator Al Gore (D-Tennessee) to be his running mate on July 9, 1992. Choosing fellow Southerner Gore went against the popular strategy of balancing a Southern candidate with a Northern partner. Gore did serve to balance the ticket in other ways, as he was perceived as strong on family values and environmental issues, while Clinton was not. Also, Gore's similarities to Clinton allowed him to push some of his key campaign themes, such as centrism and generational change.
Third Parties and Independents.
Ross Perot's candidacy.
The public's concern about the federal budget deficit and fears of professional politicians allowed the independent candidacy of billionaire Texan Ross Perot to explode on the scene in dramatic fashion—at one point Perot was leading the major party candidates in the polls. Perot crusaded against the North American Free Trade Agreement (NAFTA), internal and external national debt, tapping into voters' potential fear of the deficit. His volunteers succeeded in collecting enough signatures to get his name on the ballot in all 50 states. In June, Perot led the national public opinion polls with support from 39% of the voters (versus 31% for Bush and 25% for Clinton). Perot severely damaged his credibility by dropping out of the presidential contest in July and remaining out of the race for several weeks before re-entering. He compounded this damage by eventually claiming, without evidence, that his withdrawal was due to Republican operatives attempting to disrupt his daughter's wedding.
Perot and Stockdale drew 19,743,821 votes (18.91% of the popular vote)
Libertarian Party Nomination.
Libertarian candidates:
The 6th Libertarian Party National Convention was held in Chicago, Illinois. There, the Libertarian Party nominated Andre Marrou, former Alaska State Representative and the Party's 1988 vice-presidential candidate, for President. Nancy Lord was his running mate.
Marrou and Lord drew 291,627 votes (0.28% of the popular vote).
New Alliance Party Nomination.
New Alliance candidate:
Lenora Fulani, who was the 1988 presidential nominee of the New Alliance Party, received a second consecutive nomination from the Party in 1992. Unlike in 1988, Fulani failed to gain ballot access in every state, deciding to concentrate some of that campaign funding towards exposure of her candidacy and the Party to the national public.
Fulani also sought the endorsement of the Peace and Freedom Party of California, but despite winning a majority in that party's primary, she would lose the nomination to Ronald Daniels, the former Director the National Rainbow Coalition. Rather than pursuing a ballot space of her own, Fulani would endorse Daniel's candidacy in California.
Fulani and her running mate Maria Elizabeth Munoz received 73,622 votes (0.07% of the popular vote).
Natural Law Party Nomination.
Natural Law candidate:
The newly formed Natural Law Party nominated scientist and researcher John Hagelin for President and Mike Tompkins for Vice President. The Natural Law Party had been founded in 1992 by Hagelin and 12 others who felt that governmental problems could be solved more effectively by following "Natural Laws." The party platform included preventive health care, sustainable agriculture and renewable energy technologies. During this and future campaigns, Hagelin favored abortion rights without public financing, campaign finance law reform, improved gun control, a flat tax, the eradication of PACs, a ban on soft money contributions, and school vouchers.
The party's first presidential ticket appeared on the ballot in 28 states and drew 37,137 votes (0.04% of the popular vote).
U.S. Taxpayers' Party Nomination.
U.S. Taxpayers' candidates:
The U.S. Taxpayers Party ran its first presidential ticket in 1992, having only been formed the prior year. Initially Howard Phillips had hoped to successfully entice a prominent conservative politician, such as the former Senator Gordon J. Humphrey of New Hampshire, or even Patrick Buchanan who at the time had only been mulling over running against President Bush "(he would officially declare in December of '91)".
No one however announced their intention to seek the Taxpayers Party nomination, Buchanan himself in the end endorsing President Bush at the Republican National Convention in Houston. Phillips had been unofficially nominated earlier in the year so as to allow the Party to be able to properly seek ballot access, a temporary post that was made permanent in September, with Phillips and Albion Knight being named the official presidential ticket of the party.
Phillips and Knight drew 43,369 votes (0.04% of the popular vote).
Populist Party Nomination.
Populist candidate:
Former United States Army Special Forces officer and Vietnam veteran Bo Gritz was the nominee of the Populist Party, facing virtually no opposition. Under the campaign slogan "God, Guns and Gritz" and publishing his political manifesto "The Bill of Gritz" (playing on his last name rhyming with "rights"), he called for staunch opposition to what he called "global government" and "The New World Order", ending all foreign aid, abolishing federal income tax, and abolishing the Federal Reserve System. During the campaign, Gritz openly proclaimed the United States to be a "Christian Nation", stating that the country's legal statutes "should reflect unashamed acceptance of Almighty God and His Laws." His run on the America First/Populist Party ticket was prompted by his association with another far-right political Christian talk radio host, Tom Valentine. During his campaign, part of Gritz's standard stump speech was an idea to pay off the National debt by minting a coin at the Treasury and sending it to the Federal Reserve. This predates the 2012 Trillion dollar coin concept.
During August 1992, Gritz attracted national attention as mediator during the government standoff with Randy Weaver at Ruby Ridge, Idaho.
He received 106,152 votes nationwide (0.14% of the popular vote). In two states he had a respectable showing for a minor third party candidate: Utah, where he received 3.84% of the vote and Idaho, where he received 2.13% of the vote. In some counties, his support topped 10%, and in Franklin County, Idaho, was only a few votes away from pushing Bill Clinton into fourth place in the county.
Lyndon LaRouche's Candidacy.
While officially running for the Democratic Presidential nomination, Lyndon LaRouche also decided to run as an Independent in the general election, standing as the National Economic Recovery candidate. LaRouche was in jail at the time, having been convicted of conspiracy to commit mail fraud in December 1988; it was only the second time in history that the presidency was sought from a prison cell. His running-mate was James Bevel, a civil rights activist who had represented the LaRouche movement in its pursuit of the Franklin child prostitution ring allegations.
In addition to the displayed states, LaRouche had nearly made the ballot in the states of New York and Mississippi. In the case of New York, while his petition was valid and had enough signatures, none of his electors filed declarations of candidacy; in the cases of Mississippi a sore-loser law was in place, and because he ran in that state's Democratic presidential primary he was ineligible to run as an Independent in the general. Ohio also had a sore-loser law, but it was ruled in Brown vs. Taft that it did not apply to presidential candidates.
LaRouche and Beval drew 22,863 votes (0.02% of the popular vote).
Socialist Workers Party Nomination.
Socialist Workers candidate:
James Warren, who was the 1988 presidential nominee of the Socialist Workers Party, received a second consecutive nomination from the Party on the first of November 1991. Warren had two running mates that varied from state to state; Estelle DeBates and Willie Mae Reid, the latter also a resident of Illinois.
Warren received 22,882 votes (0.02% of the popular vote).
Ron Daniels Candidacy.
Ronald Daniels was the former executive director for the Center for Constitutional Rights, the former director of the National Rainbow Coalition, and the worked on both of Jesse Jackson's campaigns for the Democratic presidential nomination. Asiba Tupahache, a Native American activist from New York was his running-mate.
Though running an Independent campaign under the label "Campaign for a Better Tomorrow", Daniels was endorsed by a number of third parties across the states, most notably the Peace and Freedom Party of California; though he had lost that party's presidential primary to Lenora Fulani, the nominee of the New Alliance Party, the delegates at its convention voted in favor of his candidacy 110-91, the only time it has ever nominated someone other than the winner of the primary.
Daniels and Tupachache drew 27,396 votes (0.03% of the popular vote).
Other nominations.
The 1992 campaign also marked the entry of Ralph Nader into presidential politics as a candidate. Despite the advice of several liberal and environmental groups, Nader did not formally run. Rather, he tried to make an impact in the New Hampshire primaries, urging members of both parties to write-in his name. As a result, several thousand Democrats and Republicans wrote-in Nader's name. Despite supporting mostly liberal legislation during his career as a consumer advocate, Nader received more votes from Republicans than Democrats.
The Worker's League nominated Helen Halyard for President; she was the party's nominee for Vice President in 1984 and 1988. Fred Mazelis was nominated for Vice President. Halyard and Mazelis drew 3,050 votes.
Ballot Access: Michigan, New Jersey "(33 Electoral)"
John Yiamouyiannis, a major opponent of water fluoridation, ran as an Independent under the label "Take Back America". Allen C. McCone was his running-mate. Yiamouyiannis and McCone drew 2,199 votes.
Ballot Access: Arkansas, Iowa, Louisiana, Tennessee "(33 Electoral)"
The Socialist Party nominated J. Quinn Brisben for President and Barbara Garson for Vice President. Brisben and Garson drew 2,909 votes.
Ballot Access: DC, Tennessee, Utah, Wisconsin "(30 Electoral)"
The Grassroots Party nominated Jack Herer, a noted cannabis activist for President and Derrick Grimmer for Vice President. Herer and Grimmer drew 3,875 votes.
Ballot Access: Iowa, Minnesota, Wisconsin "(28 Electoral)"
The Prohibition Party nominated Earl Dodge, the party's chairman for President and George Ormsby for Vice President. Dodge and Ormsby drew 935 votes.
Ballot Access: Arkansas, New Mexico, Tennessee "(22 Electoral)"
Drew Bradford was an Independent candidate for the Presidency; he did not have a running-mate. Bradford drew 4,749 votes.
Ballot Access: New Jersey "(15 Electoral)"
Eugene R. Hem was an Independent candidate for the Presidency, running under the label "The Third Party". His running-mate was Joanne Roland. Hem and Roland drew 405 votes.
Ballot Access: Wisconsin "(11 Electoral)"
Delbert Ehlers was an Independent candidate for the Presidency. His running-mate was Rick Wendt. Ehlers and Wendt drew 1,149 votes.
Ballot Access: Iowa "(7 Electoral)"
Jim Boren was an Independent candidate for the Presidency, running under the label "Apathy". His running-mate was Bill Weidman. Boren and Weidman drew 956 votes.
Ballot Access: Arkansas "(6 Electoral)"
Professor Isabell Masters was an Independent candidate for the Presidency, running under the label "Looking Back". Her running-mate was her son, Walter Ray Masters. Masters drew 327 votes.
Ballot Access: Arkansas "(6 Electoral)"
The American Party nominated Robert J. Smith for President and Doris Feimer for Vice President. However for a time neither the Utah or South Carolina state parties would endorse the ticket. The American Party of South Carolina would ultimately endorse the candidacy of Howard Phillips, the nominee of the U.S. Taxpayers Party, while the American Party of Utah would decide to endorse Smith. Smith and Feimer drew 291 votes.
Ballot Access: Utah "(5 Electoral)"
The Workers World Party nominated Gloria La Riva for President and Larry Holmes (activist) for Vice President. Initially the party had voted not to field a presidential candidate in 1992, but it was later found that the party would need to get at least half a percent of the vote in New Mexico in order to maintain its ballot access in that state. La Riva and Holmes drew 181 votes.
Ballot Access: New Mexico "(5 Electoral)"
General election.
Campaign.
After Bill Clinton secured the Democratic Party's nomination in the spring of 1992, polls showed Ross Perot leading the race, followed by President Bush and Clinton in third place after a grueling nomination process. Two way trial heats between Bush and Clinton in early 1992 showed Bush in the lead, however. But as the economy continued to grow sour and the President's approval rating continued to slide, the Democrats began to rally around their nominee. On July 9, 1992, Clinton chose Tennessee Senator and former 1988 Presidential candidate Al Gore to be his running mate. As Governor Clinton's nomination acceptance speech approached, Ross Perot dropped out of the race, convinced that staying in the race with a "revitalized Democratic Party" would cause the race to be decided by the United States House of Representatives. Clinton gave his acceptance speech on July 17, 1992, promising to bring a "new covenant" to America, and to work to heal the gap that had developed between the rich and the poor during the Reagan/Bush years. The Clinton campaign received the biggest convention "bounce" in history which brought him from 25 percent in the spring, behind Bush and Perot, to 55 percent versus Bush's 31 percent.
After the convention, Clinton and Gore began a bus tour around the United States, while the Bush/Quayle campaign began to criticize Clinton's character, highlighting accusations of infidelity and draft dodging. The Bush campaign emphasized its foreign policy successes such as Desert Storm, and the end of the Cold War. Bush also contrasted his military service to Clinton's lack thereof, and criticized Clinton's lack of foreign policy expertise. However, as the economy was the main issue, Bush's campaign floundered across the nation, even in strongly Republican areas, and Clinton maintained leads with over 50 percent of the vote nationwide consistently, while Bush typically saw numbers in the upper 30s. As Bush's economic edge had evaporated, his campaign looked to energize its socially conservative base at the 1992 Republican National Convention in Houston, Texas. At the Convention, Bush's primary campaign opponent Pat Buchanan gave his famous "culture war" speech, criticizing Clinton and Gore's social progressiveness, and voicing skepticism on his "New Democrat" brand. After President Bush accepted his renomination, his campaign saw a small bounce in the polls, but this was short lived, as Clinton maintained his lead. The campaign continued with a lopsided lead for Clinton through September, until Ross Perot decided to re-enter the race. Ross Perot's re-entry in the race was welcome by the Bush campaign, as Fred Steeper, a poll taker for Bush, said, "He'll be important if we accomplish our goal, which is to draw even with Clinton." Initially, Perot's return saw the Texas billionaire's numbers stay low, until he was given the opportunity to participate in a trio of unprecedented three-man debates. The race narrowed, as Perot's numbers significantly improved as Clinton's numbers declined, while Bush's numbers remained more or less the same from earlier in the race as Perot and Bush began to hammer at Clinton on character issues once again.
Character issues.
Many character issues were raised during the campaign, including allegations that Clinton had dodged the draft during the Vietnam War, and had used marijuana, which Clinton claimed he had pretended to smoke, but "didn't inhale." Bush also accused Clinton of meeting with communists on a trip to Russia he took as a student. Clinton was often accused of being a philanderer by political opponents.
Allegations were also made that Bill Clinton had engaged in a long-term extramarital affair with Gennifer Flowers. Clinton denied ever having an affair with Flowers.
Results.
On November 3, Bill Clinton won the election to be the 42nd President of the United States by a wide margin in the Electoral College, receiving 43 percent of the popular vote against Bush's 37 percent and Perot's 19%. It was the first time since 1968 that a candidate won the White House with under 50 percent of the popular vote. Only Washington, D.C. and Clinton's home state of Arkansas gave the majority of their votes to a single candidate in the entire country; the rest were won by pluralities of the vote.
President Bush's 37.4% was the lowest percentage total for a sitting president seeking re-election since William Howard Taft in 1912 (23.2%). The 1912 election was also a three way race (between Taft, Woodrow Wilson, and Theodore Roosevelt).
It was also the lowest percentage for a major-party candidate since Alf Landon received 36.5% of the vote in 1936. Bush had a lower percentage of the popular vote than even Herbert Hoover who was defeated in 1932 (Hoover received 39.7%). However, none of these races included a major third candidate. As of the 2012 election, Bush is the last president voted out of office after one term as Clinton, George W. Bush, and Barack Obama were all re-elected to second terms in office.
Independent candidate Ross Perot received 19,741,065 with 18.9 percent of the popular vote for President. The billionaire used his own money to advertise extensively, and is the only third-party candidate ever allowed into the nationally televised presidential debates with both major party candidates (Independent John Anderson debated Republican Ronald Reagan in 1980, but without Democrat Jimmy Carter who had refused to appear in a three-man debate). Speaking about the North American Free Trade Agreement, Perot described its effect on American jobs as causing a "giant sucking sound". Perot was ahead in the polls for a period of almost two months – a feat not accomplished by an independent candidate in almost 100 years. Perot lost much of his support when he temporarily withdrew from the election, only to declare himself a candidate again soon after.
Perot's almost 19% of the popular vote made him the most successful third-party presidential candidate in terms of popular vote since Theodore Roosevelt in the 1912 election. Also, Ross Perot's 19% of the popular vote was the highest ever percent of the popular vote for a candidate who did not win any electoral votes.
Although he did not win any states, Perot managed to finish ahead of one of the two major party candidates in two states: In Maine, Perot received 30.44% of the vote to Bush's 30.39% (Clinton won Maine with 38.77%); in Utah, Perot received 27.34% of the vote to Clinton's 24.65% (Bush won Utah with 43.36%).
The election was the most recent in which Georgia and Montana voted for the Democratic presidential candidate. 1992 was also the first time since Texas' admission to the Union in 1845 that a Democrat won the White House without winning the state and the second time since Florida's admission (also in 1845) that a Democrat won without winning the state (John F. Kennedy in 1960 was the first). He was also the only Democrat at that point to win every electoral vote in the Northeast except for Lyndon Johnson in 1964. Every Democrat since Clinton has repeated this result, except for Al Gore, who narrowly lost New Hampshire in 2000. Also, this was the first time since 1964 that many states voted Democratic, such as California, Colorado, Illinois, Montana, Nevada, New Hampshire, New Jersey, New Mexico, and Vermont.
Analysis.
Several factors made the results possible. First, the campaign came on the heels of an economic slowdown. Exit polling shows that 75% thought the economy was in Fairly Bad or Very Bad shape while 63% thought their personal finances were better or the same as four years ago. The decision by Bush to accept a tax increase adversely affected his re-election bid. Pressured by rising budget deficits, Bush agreed to a budget compromise with Congress which raised taxes. Clinton was able to condemn the tax increase effectively on both its own merits and as a reflection of Bush's honesty. Effective Democratic TV ads were aired showing a clip of Bush's infamous 1988 campaign speech in which he promised "" Most importantly, Bush's coalition was in disarray, for both the aforementioned reasons and for unrelated reasons. The end of the Cold War allowed old rivalries among conservatives to re-emerge and meant that other voters focused more on domestic policy, to the detriment of Bush, a social and fiscal moderate. The consequence of such a perception depressed conservative turnout.
Unlike Bush, Clinton was able to unite his fractious and ideologically diverse party behind his candidacy, even when its different wings were in conflict. To garner the support of moderates and conservative Democrats, he attacked Sister Souljah, a little-known rap musician whose lyrics Clinton condemned. Furthermore, Clinton made clear his support of the death penalty and would later champion making school uniforms in public schools a requirement. Clinton could also point to his centrist record as Governor of Arkansas. More liberal Democrats were impressed by Clinton's record on abortion and affirmative action. His strong connections to African Americans also played a key role. In addition, he organized significant numbers of young voters and became a symbol of the rise of the Baby Boomer generation to political power. Supporters remained energized and confident, even in times of scandal or missteps.
The effect of Ross Perot's candidacy has been a contentious point of debate for many years. In the ensuing months after the election, various Republicans asserted that Perot had acted as a spoiler, enough to the detriment of Bush to lose him the election. While many disaffected conservatives may have voted for Ross Perot to protest Bush's tax increase, further examination of the Perot vote in the Election Night exit polls not only showed that Perot siphoned votes nearly equally among Bush and Clinton, but of the voters who cited Bush's broken "No New Taxes" pledge as "very important," two thirds voted for Bill Clinton. A mathematical look at the voting numbers reveals that Bush would have had to win 12.2% of Perot's 18.8% of the vote, 65% of Perot's support base, to earn a majority of the vote, and would have needed to win nearly every state Clinton won by less than five percentage points. Furthermore, Perot was most popular in states that strongly favored either Clinton or Bush, limiting his real electoral impact for either candidate. He gained relatively little support in the Southern states and happened to have the best showing in states with few electoral votes. Perot appealed to disaffected voters all across the political spectrum who had grown weary of the two-party system. NAFTA played a role in Perot's support, and Perot voters were relatively moderate on hot button social issues.
Clinton, Bush, and Perot did not focus on abortion during the campaign. Exit polls, however, showed that attitudes toward abortion "significantly influenced" the vote, as pro-choice Republicans defected from Bush.
Implications.
Clinton's election ended an era in which the Republican Party had controlled the White House for 12 consecutive years, and for 20 of the previous 24 years. That election also brought the Democrats full control of the legislative and executive branches of the federal government, including both houses of U.S. Congress and the presidency, for the first time since the administration of the last Democratic president, Jimmy Carter. This would not last for very long, however, as the Republicans won control of both the House and Senate in 1994. Reelected in 1996, Clinton would become the first Democratic President since Franklin D. Roosevelt to serve two full terms in the White House.
1992 was arguably a "realigning" election. It made the Democratic Party dominant in presidential elections in the Northeast, the Great Lakes region, and the West Coast, where many states had previously either been swing states or Republican-leaning. Clinton picked up several states that went Republican in 1988, and which have remained in the Democratic column ever since: California, Connecticut, Delaware, Illinois, Maine, Maryland, Michigan, New Jersey, Pennsylvania, and Vermont. Vermont, carried by Clinton, had been heavily Republican for generations prior to the election, voting for a Democrat only once (in 1964). The state has been won by the Democratic nominee in every presidential election since. Bill Clinton narrowly defeated Bush in New Jersey (by two points), which had voted for the Republican nominee all but twice since 1948. Clinton would later win the state in 1996 by eighteen points; like Vermont, Republicans have not won the state since. California, which had previously been a Republican stronghold from 1952 to 1988, was now solidly Democratic. Clinton, a lifelong Southerner, was able to carry several states in the South that the GOP had been winning for decades, but ultimately won only four of eleven former Confederate states. This reflected the final shift of the South to the Republican Party.
Detailed results.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Close states.
States with margin of victory less than 5% (202 electoral votes):
States with margin of victory between 5% and 10% (131 electoral votes): 
Voter demographics.
Source: Voter News Service exit poll, reported in "The New York Times", November 10, 1996, 28.

</doc>
<doc id="39560" url="http://en.wikipedia.org/wiki?curid=39560" title="1408">
1408

Year 1408 (MCDVIII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
Date unknown.
</onlyinclude>

</doc>
